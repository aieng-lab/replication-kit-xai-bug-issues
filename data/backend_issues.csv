,id,title,description,dataset,issue_id,project,prediction
0,228259,Correctly set ivy.resolved.configurations property when the conf string includes a negation operator.,"Really this is a continuation of the work begun in IVY-894.  The previous work allowed the resolve task to handle a configuration list with negation operators, by transforming the list during the wildcard fixup.  Unfortunately however, in this type of scenario the ivy.resolved.configurations property is still set to the original string with negation operators intact.  This situation breaks postresolve tasks (e.g. retrieve) that simply reuse the value of the ivy.resolved.configurations property when it's not explicitly passed to the subsequent postresolve task.  The new patch attached to this bug report fixes the issue by appropriately setting the ivy.resolved.configurations with the post-fixup list of configurations.  The patch includes an update to the testcase as well.",smartshark_2_2,537,ant-ivy,"""[0.07509122788906097, 0.9249087572097778]"""
1,228260,"ivy:resolve ignores branch in ""dynamic"" resolve mode","When I call ivy:resolve as following
<ivy:resolve file=""${deps.file}"" showProgress=""false"" conf=""compile"" resolveMode=""dynamic""/>
and having 
<ivy pattern=""${ivy.local.default.root}/[organisation]/[branch]/[module]-ivys/ivy-[revision].xml""/>
<artifact pattern=""${ivy.local.default.root}/[organisation]/[branch]/[artifact]-[revision].[ext]""/> 
set in my defaultResolver
ivy:resolver ignores [branch] and looks for ivy.xml in ""${ivy.local.default.root}/[organisation]//[module]-ivys/ivy-[revision].xml""
and artifact in  ""${ivy.local.default.root}/[organisation]//[artifact]-[revision].[ext]"".

However if I call <ivy:resolve file=""${deps.file}"" showProgress=""false"" conf=""compile""/> without resolveMode set, everything is fine and resolve follows [organisation]/[branch] path.

",smartshark_2_2,126,ant-ivy,"""[0.09419238567352295, 0.905807614326477]"""
2,228261,NullPointerException when resolving module without revision in the pattern,"When I switched to RC1 or RC2 I experianced the following error (which worked fine in previuos versions).

When there is a dependancy which uses the rev=""+"" and there is no revision on the artifact in the repository the resolver throws a null pointer.

[ivy:resolve] :::: WARNINGS
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           :: acme#SomeJar;+: java.lang.NullPointerException at org.apache.ivy.plugins.resolver.Bas
icResolver.resolveAndCheckRevision(BasicResolver.java:457)
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::


I have a filesystem resolver with the following pattern:
<filesystem name=""remote3rdParty"" checkmodified=""false"" cache=""systemCahce"">
        <artifact pattern=""\\someServer\IvyRepo\external\[organisation]\[artifact]-[revision].[ext]""/>
        <artifact pattern=""\\someServer\IvyRepo\external\[organisation]\[artifact].[ext]""/>
</filesystem>

(Some artifacts in the 3rd party library don't have versions).

The file path is:
\\someServer\IvyRepo\external\acme\SomeJar.jar

The dependancy is:
<dependency name=""SomeJar"" org=""acme"" rev=""+"" conf=""runtime-jar->default;""/>


From a client point of view, the module doesn't care what version... just get the latest.... and if there is no version just get the current one.

There are a number of work arounds...

a) Ensure all JARs in the repository have a version ... Invent a version on ones which don't have any version. (Probably a good idea anyway).
       - If this is the case then thats fine. It should just be a documented feature :)

b) Use the beta version of ivy.

c) Change the client module to specify no revision rev=""""



",smartshark_2_2,664,ant-ivy,"""[0.0640421062707901, 0.9359579086303711]"""
3,228262,ivy.cache.dir.${settingsRef} is set to default instead of the defaultCacheDir from the ivysettings.xml after ivy:resolve,"ivy.cache.dir.${settingsRef} is not set properly after a call to resolve.

e.g.

ivysettings.xml
...
<caches defaultCacheDir=""someDirectory""/>
...

build.xml
<ivy:settings id=""test"" file=""ivysettings.xml""/>
<ivy:resolve settingsRef=""test""/>
<echo message=""Cache Dir : ${ivy.cache.dir.test}""

ivy.cache.dir.test is set to the default ${user.home}/.ivy2/cache instead of ""someDirectory"" that is the value of the defaultCacheDir attribute set in the ivysettings.xml.",smartshark_2_2,529,ant-ivy,"""[0.11140400171279907, 0.8885959386825562]"""
4,228263,Impossible to get artifacts when data has not been loaded for multiple dynamic revisions,"When some transitive dependencies depend on the same artifact with the same dynamic revision (say, ""1.0.0.+""), and some depend on the actual latest version (say, ""1.0.0.m4""), sometimes it results in an IllegalStateException:
{code}
java.lang.IllegalStateException: impossible to get artifacts when data has not been loaded. IvyNode = rev#artifact;1.0.0.m4
	at org.apache.ivy.core.resolve.IvyNode.getArtifacts(IvyNode.java:762)
	at org.apache.ivy.core.resolve.IvyNode.getSelectedArtifacts(IvyNode.java:739)
	at org.apache.ivy.core.report.ResolveReport.setDependencies(ResolveReport.java:235)
	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:238)
	at org.apache.ivy.Ivy.resolve(Ivy.java:512)
{code}
This exception is reproducable for a given combination of artifacts. Changing the dependencies slightly so the module depends directly on a specific version of the artifact seems to fix this problem, as does changing some of the transitive dependencies.

The issue seems to be caused by the {{LatestRevisionStrategy}} comparing two IvyNodes as equal because they have the same resolved id, but their actual IDs differ. The following debug log shows the conflict manager trying to resolve the 1.0.0.+ and 1.0.0.m4 dependencies:
{quote}
found conflicting revisions for rev#artifact;1.0.0.m4 in rev#another-artifact;1.0.0.beta: [rev#artifact;1.0.0.m4, rev#artifact;1.0.0.m4]
{quote}
When the {{LatestRevisionStrategy}} is asked to pick the latest revision out of those two artifacts, it just chooses the last one in the list (since they compare as equal the order is maintained). At different points in the resolve, the list is passed to the {{LatestRevisionStrategy}} in a different order and a different artifact is considered the latest one, so both versions end up being evicted.

{quote}
loadData of rev#artifact;1.0.0.m4 of rootConf=dist
dist is evicted.  Skip loading
{quote}

The fix is to make the order deterministic when the resolved ids are equal but unresolved ids are not. A simple hacky workaround is to try the regular comparison, and if that doesn't work, try comparing the real revisions. This seems hacky because the {{ArtifactInfo}} class doesn't provide any information about the real revisions, only the resolved revisions. I have a local hacked subclass of {{LatestRevisionStrategy}} which fixes this problem, if it helps.",smartshark_2_2,1201,ant-ivy,"""[0.07027755677700043, 0.9297224879264832]"""
5,228264,ApacheURLLister does not allow directories not containing a dot,"I need to use a different revision scheme that looks like R1A01, i.e. not the normal scheme x.y.z, for my company internal dependencies. When doing this, I discovered that ApacheURLLister does not handle directories without a dot (.) properly as it filters those out. If it does not find a dot in the directory name, it compares the href with the text but forgets to check for a trailing slash in href.",smartshark_2_2,1257,ant-ivy,"""[0.10916371643543243, 0.8908362984657288]"""
6,228265,OBR RequirementFilterParser does not handle ! (NOT) correctly.,"I have attached a patch for 

org.apache.ivy.osgi.obr.RequirementFilterTest.java
org.apache.ivy.osgi.obr.filter.RequirementFilterParser.java

The bug can be shown when trying to parse the following filter:
---
(&     (version>=3.5.0)     (!(version>=4.0.0))     (bundle=org.eclipse.core.runtime)    )
---

Without the patch the bundle fragment is dropped from the filter.",smartshark_2_2,1445,ant-ivy,"""[0.06984764337539673, 0.9301523566246033]"""
7,228266,Circular dependencies causes StackOverflowError in report,see http://www.jayasoft.org/node/396#comment-1094 for example of the problem,smartshark_2_2,419,ant-ivy,"""[0.12439112365245819, 0.8756088614463806]"""
8,228267,Can't use latest.release for pom dependencies,"When using the ibibio resolver, the generated ivy.xml doesn't include source and javadoc artifacts declaration.
AFAIU the Maven doc, the pom.xml doesn't declare them. They declare themself by being available or not.

IvyDE does this check itself, so it works fine in Eclipse. So some code should be ported from IvyDE to Ivy. Then we will be able to get the source also with an ant task.

",smartshark_2_2,1226,ant-ivy,"""[0.6078041791915894, 0.3921958804130554]"""
9,228268,Ivy Files are not found in the cache,"For some reason, even though dependency files themselves can be resovled to the cache, it
does not seem as if the Ivy configuration files can be loaded from the cache.

In my ivyconf.xml:

<resolvers>
      <chain name=""remote"">
          <filesystem name=""internal"">
              <ivy pattern=""/"" />
              <artifact pattern=""/"" />
          </filesystem>          
          <ivyrep name=""ibiblio"" />
      </chain>
      
      <filesystem name=""local"">
          <ivy pattern=""/home/jira/.ivy-cache/"" />
          <artifact pattern=""/home/jira/.ivy-cache/"" />          
      </filesystem>

      <filesystem name=""internal.publish"">
          <ivy pattern=""/"" />
          <artifact pattern=""/"" />
      </filesystem>

  </resolvers>


And when a developer builds, we do an
    <ivy:publish resolver=""local"" />

Only when we do a full automated build do we want to publish to the internal.publish
repository (which the same as internal, but i seperate them because the ""internal""
will be exposed by http, and internal.publish will be filesystem.

However, when I have two modules, A and B, with ivy files:

A
-------
<ivy-module version=""1.0"">
    <info organisation=""test"" module=""A"" revision=""1.0"" />
    <publications>
        <artifact name=""A"" type=""jar"" />
    </publications>
</ivy-module>

B
-------
<ivy-module version=""1.0"">
    <info organisation=""test"" module=""B"" revision=""1.0"" />
    <publications>
        <artifact name=""B"" type=""jar"" />
    </publications>
    <dependencies>
        <dependency org=""test"" module=""A"" rev=""1.0"">
            <artifact name=""A"" type=""jar"" />
        </dependency>
    </dependencies>
</ivy-module>


I keep getting a failure to resolve A when i try to build B (after publishing A to cache).
Basically the reason is that the Ivy.xml is not in the actual repository yet, and for some
reason it won't try to find it in the cache.  In my opinion, it should try to find it in the cache
if it's there, and I view this is a bug (similar to what maven would do with a pom file)

",smartshark_2_2,859,ant-ivy,"""[0.5807363986968994, 0.419263631105423]"""
10,228269,inline resolve ignores the transitive attribute,"If you do an inline resolve (post-resolve-task), the transitive attribute is ignored.",smartshark_2_2,1238,ant-ivy,"""[0.09199654310941696, 0.9080034494400024]"""
11,228270,XmlModuleDescriptorWriter does not produce matcher attribute on include and exclude rules,"When using XmlModuleDescriptorWriter to create an ivy file, it does not produce matcher attribute on include and exclude rules, thus losing some information compared to the in memory module descriptor.",smartshark_2_2,449,ant-ivy,"""[0.11283864080905914, 0.8871613144874573]"""
12,228271,ivy variable substitution in ivy files,"Documentation says that ivy variable can be used in ivy files, but it's only the case in some attributes. Others are not replaced.",smartshark_2_2,372,ant-ivy,"""[0.909889280796051, 0.09011072665452957]"""
13,228272,properties tag in ivy conf do not support relative path,"If in the ivyconf I put a tag 
<properties file=""./src/build.properties"" />

it do not work , I get: 

java.text.ParseException: failed to configure with file:/C:/javadev/src/Hermes/ivyconf.xml: problem in config file
	at fr.jayasoft.ivy.xml.XmlIvyConfigurationParser.parse(XmlIvyConfigurationParser.java:65)
	at fr.jayasoft.ivy.Ivy.configure(Ivy.java:259)
	at au.id.oneill.necessitas.core.providers.IvyLibraryProvider.createIvy(IvyLibraryProvider.java:218)
	at au.id.oneill.necessitas.core.providers.IvyLibraryProvider.processDependencies(IvyLibraryProvider.java:148)
	at au.id.oneill.necessitas.core.providers.IvyLibraryProvider.updateAvailableVersions(IvyLibraryProvider.java:86)
	at au.id.oneill.necessitas.ui.NecessitasContainerPage$12.run(NecessitasContainerPage.java:438)
	at au.id.oneill.necessitas.ui.NecessitasContainerPage$13.run(NecessitasContainerPage.java:464)
	at org.eclipse.jface.operation.ModalContext$ModalContextThread.run(ModalContext.java:113)
Caused by: java.net.MalformedURLException: no protocol: ./src/build.properties
	at fr.jayasoft.ivy.xml.XmlIvyConfigurationParser.startElement(XmlIvyConfigurationParser.java:140)
	at org.apache.crimson.parser.Parser2.maybeElement(Parser2.java:1674)
	at org.apache.crimson.parser.Parser2.content(Parser2.java:1963)
	at org.apache.crimson.parser.Parser2.maybeElement(Parser2.java:1691)
	at org.apache.crimson.parser.Parser2.parseInternal(Parser2.java:667)
	at org.apache.crimson.parser.Parser2.parse(Parser2.java:337)
	at org.apache.crimson.parser.XMLReaderImpl.parse(XMLReaderImpl.java:448)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:345)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:143)
	at fr.jayasoft.ivy.xml.XmlIvyConfigurationParser.parse(XmlIvyConfigurationParser.java:59)
	... 7 more
",smartshark_2_2,517,ant-ivy,"""[0.2039901316165924, 0.7960098385810852]"""
14,228273,Downloads from maven repository fail when using transparent HTTP proxies,"The package without dependencies cannot fetch dependencies and samples in the package with dependencies do not work.

Fetching dependencies fails with the following log:
{quote}
Buildfile: build.xml

init-ivy:

retrieve-all:
[ivy:retrieve] :: Ivy 2.1.0 - 20090925235825 :: http://ant.apache.org/ivy/ ::
[ivy:retrieve] :: loading settings :: url = jar:file:/C:/utils/apache/apache-ivy-2.1.0-wo/ivy.jar!/org/apache/ivy/core/settings/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache.ivy#ivy;2.1.0
[ivy:retrieve] 	confs: [core, httpclient, oro, vfs, sftp, standalone, ant, default, test, source]
[ivy:retrieve] 	found commons-httpclient#commons-httpclient;3.0 in public
[ivy:retrieve] 	found commons-codec#commons-codec;1.2 in public
[ivy:retrieve] 	found oro#oro;2.0.8 in public
[ivy:retrieve] 	found commons-vfs#commons-vfs;1.0 in public
[ivy:retrieve] 	found com.jcraft#jsch;0.1.31 in public
[ivy:retrieve] 	found ant#ant;1.6.2 in public
[ivy:retrieve] 	found ant#ant-nodeps;1.6.2 in public
[ivy:retrieve] 	found ant#ant-trax;1.6.2 in public
[ivy:retrieve] 	found junit#junit;3.8.2 in public
[ivy:retrieve] 	found commons-lang#commons-lang;2.4 in public
[ivy:retrieve] 	found org.apache.ant#ant-testutil;1.7.0 in public
[ivy:retrieve] 	found ant#ant-launcher;1.6.2 in public
[ivy:retrieve] 	found xerces#xercesImpl;2.6.2 in public
[ivy:retrieve] 	found xerces#xmlParserAPIs;2.6.2 in public
[ivy:retrieve] downloading http://repo1.maven.org/maven2/junit/junit/3.8.2/junit-3.8.2.jar ...
[ivy:retrieve] ............ (117kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve] :: resolution report :: resolve 1250ms :: artifacts dl 250ms
[ivy:retrieve] 	:: evicted modules:
[ivy:retrieve] 	commons-logging#commons-logging;1.0.3 by [commons-logging#commons-logging;1.0.4] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|       core       |   0   |   0   |   0   |   0   ||   0   |   0   |
	|    httpclient    |   3   |   0   |   0   |   0   ||   2   |   0   |
	|        oro       |   1   |   0   |   0   |   0   ||   1   |   0   |
	|        vfs       |   2   |   0   |   0   |   0   ||   1   |   0   |
	|       sftp       |   1   |   0   |   0   |   0   ||   1   |   0   |
	|    standalone    |   0   |   0   |   0   |   0   ||   0   |   0   |
	|        ant       |   1   |   0   |   0   |   0   ||   1   |   0   |
	|      default     |   10  |   0   |   0   |   1   ||   8   |   0   |
	|       test       |   6   |   0   |   0   |   0   ||   6   |   0   |
	|      source      |   0   |   0   |   0   |   0   ||   0   |   0   |
	---------------------------------------------------------------------

[ivy:retrieve] :: problems summary ::
[ivy:retrieve] :::: WARNINGS
[ivy:retrieve] 	problem while downloading module descriptor: http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0.3/commons-logging-1.0.3.pom: invalid sha1: expected=<
[ivy:retrieve] Ð³ computed=88c58ea4a562116ab15fb76c9097ee1f25cc750b (62ms)
[ivy:retrieve] 		module not found: commons-logging#commons-logging;1.0.3
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/local/commons-logging/commons-logging/1.0.3/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact commons-logging#commons-logging;1.0.3!commons-logging.jar:
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/local/commons-logging/commons-logging/1.0.3/jars/commons-logging.jar
[ivy:retrieve] 	==== shared: tried
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/shared/commons-logging/commons-logging/1.0.3/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact commons-logging#commons-logging;1.0.3!commons-logging.jar:
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/shared/commons-logging/commons-logging/1.0.3/jars/commons-logging.jar
[ivy:retrieve] 	==== public: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0.3/commons-logging-1.0.3.pom
[ivy:retrieve] 	problem while downloading module descriptor: http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0.4/commons-logging-1.0.4.pom: invalid sha1: expected=<
[ivy:retrieve] 		module not found: commons-logging#commons-logging;1.0.4
[ivy:retrieve] 	==== local: tried
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/local/commons-logging/commons-logging/1.0.4/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact commons-logging#commons-logging;1.0.4!commons-logging.jar:
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/local/commons-logging/commons-logging/1.0.4/jars/commons-logging.jar
[ivy:retrieve] 	==== shared: tried
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/shared/commons-logging/commons-logging/1.0.4/ivys/ivy.xml
[ivy:retrieve] 	  -- artifact commons-logging#commons-logging;1.0.4!commons-logging.jar:
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/shared/commons-logging/commons-logging/1.0.4/jars/commons-logging.jar
[ivy:retrieve] 	==== public: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0.4/commons-logging-1.0.4.pom
[ivy:retrieve] 		[FAILED     ] junit#junit;3.8.2!junit.jar: invalid sha1: expected=<
[ivy:retrieve] 		[FAILED     ] junit#junit;3.8.2!junit.jar:  (0ms)
[ivy:retrieve] 	==== shared: tried
[ivy:retrieve] 	  C:\Documents and Settings\Constantin.Plotnikov\.ivy2/shared/junit/junit/3.8.2/jars/junit.jar
[ivy:retrieve] 	==== public: tried
[ivy:retrieve] 	  http://repo1.maven.org/maven2/junit/junit/3.8.2/junit-3.8.2.jar
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		:: commons-logging#commons-logging;1.0.3: not found
[ivy:retrieve] 		:: commons-logging#commons-logging;1.0.4: not found
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		::              FAILED DOWNLOADS            ::
[ivy:retrieve] 		:: ^ see resolution messages for details  ^ ::
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 		:: junit#junit;3.8.2!junit.jar
[ivy:retrieve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:retrieve] 
[ivy:retrieve] 
[ivy:retrieve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
{quote}

The sample in package with dependencies  fails with the following log:
{quote}
Buildfile: build.xml

download-ivy:
    [mkdir] Created dir: C:\utils\apache\apache-ivy-2.1.0\src\example\go-ivy\ivy
     [echo] installing ivy...
      [get] Getting: http://repo1.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar
      [get] To: C:\utils\apache\apache-ivy-2.1.0\src\example\go-ivy\ivy\ivy.jar

install-ivy:
  [taskdef] java.util.zip.ZipException: error in opening zip file
  [taskdef] 	at java.util.zip.ZipFile.open(Native Method)
  [taskdef] 	at java.util.zip.ZipFile.<init>(ZipFile.java:203)
  [taskdef] 	at java.util.zip.ZipFile.<init>(ZipFile.java:234)
  [taskdef] 	at org.apache.tools.ant.AntClassLoader.getResourceURL(AntClassLoader.java:919)
  [taskdef] 	at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.findNextResource(AntClassLoader.java:126)
  [taskdef] 	at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.<init>(AntClassLoader.java:88)
  [taskdef] 	at org.apache.tools.ant.AntClassLoader.findResources(AntClassLoader.java:869)
  [taskdef] 	at java.lang.ClassLoader.getResources(ClassLoader.java:1015)
  [taskdef] 	at org.apache.tools.ant.taskdefs.Definer.resourceToURLs(Definer.java:267)
  [taskdef] 	at org.apache.tools.ant.taskdefs.Definer.execute(Definer.java:211)
  [taskdef] 	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:275)
  [taskdef] 	at org.apache.tools.ant.Task.perform(Task.java:364)
  [taskdef] 	at org.apache.tools.ant.Target.execute(Target.java:341)
  [taskdef] 	at org.apache.tools.ant.Target.performTasks(Target.java:369)
  [taskdef] 	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1216)
  [taskdef] 	at org.apache.tools.ant.Project.executeTarget(Project.java:1185)
  [taskdef] 	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:40)
  [taskdef] 	at org.apache.tools.ant.Project.executeTargets(Project.java:1068)
  [taskdef] 	at org.apache.tools.ant.Main.runBuild(Main.java:668)
  [taskdef] 	at org.apache.tools.ant.Main.startAnt(Main.java:187)
  [taskdef] 	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:246)
  [taskdef] 	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:67)
  [taskdef] Could not load definitions from resource org/apache/ivy/ant/antlib.xml. It could not be found.

generate-src:

go:
     [echo] using ivy to resolve commons-lang 2.1...

BUILD FAILED
C:\utils\apache\apache-ivy-2.1.0\src\example\go-ivy\build.xml:85: Could not create task or type of type: antlib:org.apache.ivy.ant:cachepath.

Ant could not find the task or a class this task relies upon.

This is common and has a number of causes; the usual 
solutions are to read the manual pages then download and
install needed JAR files, or fix the build file: 
 - You have misspelt 'antlib:org.apache.ivy.ant:cachepath'.
   Fix: check your spelling.
 - The task needs an external JAR file to execute
     and this is not found at the right place in the classpath.
   Fix: check the documentation for dependencies.
   Fix: declare the task.
 - The task is an Ant optional task and the JAR file and/or libraries
     implementing the functionality were not found at the time you
     yourself built your installation of Ant from the Ant sources.
   Fix: Look in the ANT_HOME/lib for the 'ant-' JAR corresponding to the
     task and make sure it contains more than merely a META-INF/MANIFEST.MF.
     If all it contains is the manifest, then rebuild Ant with the needed
     libraries present in ${ant.home}/lib/optional/ , or alternatively,
     download a pre-built release version from apache.org
 - The build file was written for a later version of Ant
   Fix: upgrade to at least the latest release version of Ant
 - The task is not an Ant core or optional task 
     and needs to be declared using <taskdef>.
 - You are attempting to use a task defined using 
    <presetdef> or <macrodef> but have spelt wrong or not 
   defined it at the point of use

Remember that for JAR files to be visible to Ant tasks implemented
in ANT_HOME/lib, the files must be in the same directory or on the
classpath

Please neither file bug reports on this problem, nor email the
Ant mailing lists, until all of these causes have been explored,
as this is not an Ant bug.

Total time: 1 second
{quote}

The reason for the bug is the squid. When request comes and it has a cached content with gzip content encoding, it return a content with encoded content, event if no encoding was specified in the request or ""Accept-Encoding: identity"" was specified. The below is a sample log that demonstrates the problem:  

{quote}
DEBUG output created by Wget 1.11.4 on cygwin.

--2010-06-02 12:41:55--  http://repo1.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar
Resolving repo1.maven.org... 38.97.124.18
Caching repo1.maven.org => 38.97.124.18
Connecting to repo1.maven.org|38.97.124.18|:80... connected.
Created socket 4.
Releasing 0x006d9100 (new refcount 1).

---request begin---
GET /maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar HTTP/1.0
User-Agent: Wget/1.11.4
Accept: */*
Host: repo1.maven.org
Connection: Keep-Alive

---request end---
HTTP request sent, awaiting response... 
---response begin---
HTTP/1.0 200 OK
Server: nginx/0.7.62
Date: Tue, 01 Jun 2010 13:09:36 GMT
Content-Type: application/java-archive
Last-Modified: Mon, 12 Oct 2009 21:19:02 GMT
Content-Encoding: gzip
X-Cache: MISS from squid_havp_node1
X-Cache-Lookup: MISS from squid_havp_node1:3128
Age: 70339
Content-Length: 834513
X-Cache: HIT from squid_havp_node1
X-Cache-Lookup: HIT from squid_havp_node1:3128
Connection: keep-alive

---response end---

  HTTP/1.0 200 OK
  Server: nginx/0.7.62
  Date: Tue, 01 Jun 2010 13:09:36 GMT
  Content-Type: application/java-archive
  Last-Modified: Mon, 12 Oct 2009 21:19:02 GMT
  Content-Encoding: gzip
  X-Cache: MISS from squid_havp_node1
  X-Cache-Lookup: MISS from squid_havp_node1:3128
  Age: 70339
  Content-Length: 834513
  X-Cache: HIT from squid_havp_node1
  X-Cache-Lookup: HIT from squid_havp_node1:3128
  Connection: keep-alive
Registered socket 4 for persistent reuse.
Length: 834513 (815K) [application/java-archive]
Saving to: `ivy-2.1.0.jar'

     0K .......... .......... .......... .......... ..........  6% 8.49M 0s
    50K .......... .......... .......... .......... .......... 12% 12.0M 0s
   100K .......... .......... .......... .......... .......... 18% 7.26M 0s
   150K .......... .......... .......... .......... .......... 24% 9.28M 0s
   200K .......... .......... .......... .......... .......... 30% 5.33M 0s
   250K .......... .......... .......... .......... .......... 36% 9.31M 0s
   300K .......... .......... .......... .......... .......... 42% 5.26M 0s
   350K .......... .......... .......... .......... .......... 49% 9.15M 0s
   400K .......... .......... .......... .......... .......... 55% 7.27M 0s
   450K .......... .......... .......... .......... .......... 61% 7.42M 0s
   500K .......... .......... .......... .......... .......... 67% 9.21M 0s
   550K .......... .......... .......... .......... .......... 73% 6.11M 0s
   600K .......... .......... .......... .......... .......... 79% 9.50M 0s
   650K .......... .......... .......... .......... .......... 85% 8.84M 0s
   700K .......... .......... .......... .......... .......... 92% 9.54M 0s
   750K .......... .......... .......... .......... .......... 98% 6.04M 0s
   800K .......... ....                                       100% 16.2K=1.0s

2010-06-02 12:41:56 (795 KB/s) - `ivy-2.1.0.jar' saved [834513/834513]
{quote}

In our environment, a bunch of files from repo1.maven.org are cached in the squid. So there were a lot of conflicts with content encoding.

The possible workarounds:
1. Check returned content encoding from http client, if it is not identity, reissue the request with disabling the caches.
2. Ungzip files fetched with gzip content encoding.
3. Support gzip content encoding in common HTTP.
3. Use local maven proxies 
",smartshark_2_2,1506,ant-ivy,"""[0.9007785320281982, 0.09922147542238235]"""
15,228274,pb with force when it comes after a conflict has already been solved,"project foo depends on A, B and T 1.0 and forces it (in this order)
A depends on T 1.0
B depends on T 1.1

T 1.1 is selected despite the force attribute.

Workaround:
put dependency on T at the first place in project foo
",smartshark_2_2,943,ant-ivy,"""[0.11154386401176453, 0.8884561061859131]"""
16,228275,Can't deal with [VERSION] version pattern from Maven,"If you want a specific version of a resource in Maven, [VERSION] is a valid request - e.g. [1.5.8]

I am using grape (from Groovy) and 

grape -d install org.slf4j slf4j-api [1.5.8] 

will not work. These dependencies are all over the place in our maven repo (and presumably others). I fixed it by adding to ModuleRevisionId.java 

    protected static String normalizeRevision(String asked) {
        if ( asked.startsWith( ""["" ) && asked.endsWith( ""]"" ) && asked.indexOf(',') == -1 ) {
            return asked.substring(1, asked.length() - 1 );
        } else
            return asked;
    }

and using it in the constructor:

this.revision = revision == null ? Ivy.getWorkingRevision() : normalizeRevision(revision);",smartshark_2_2,584,ant-ivy,"""[0.20372751355171204, 0.7962724566459656]"""
17,228276,PomModuleDescriptorParser fails with nested profile dependency.,"I recently started working with struts 2.0 series.  Do the complexity of all the dependent libraries, I thought it would be a good time to test drive ivy as well.  Unfortunately, it was unable to handle one of the struts 2.0 poms found at http://people.apache.org/maven-snapshot-repository/org/apache/struts/struts2-core/2.0.2-SNAPSHOT/struts2-core-2.0.2-SNAPSHOT.pom.  A ""no groupId found in pom"" message was being displayed when trying to resolve this pom.

After a little debugging and a small refresher course in SAX(its been a while since using straight SAX :)), I noticed the PomModuleDescriptorParser was resetting the instance variable prematurely due to nested dependency elements under a maven build profile.  This must not be a common feature in maven, but I am by no means a maven expert.  Anyway, I will attach the patch I applied to resolve the issue.",smartshark_2_2,261,ant-ivy,"""[0.10789038240909576, 0.8921095728874207]"""
18,228277,Out of memory/Stack overflow for new highly coupled project.,"We're trying to convert an existing large project to use Ivy. It's an OSGi project, so it's already sorted into modules (bundles) and Ivy makes a lot of sense. The problem is that the project has a lot of modules (83) and since we're moving from a monolithic project it's massively interdependent, there are circular dependencies all through it. When we try to run an ivy:buildlist on the whole project, it just barfs with a java.lang.OutOfMemoryError: Java heap space. If we try to run the buildlist giving it a single module as a root it gives a java.lang.StackOverflowError.

Attached are the project ivy.xml files required to reproduce the problem, as well as a simple build.xml with two targets to reproduce the problem.",smartshark_2_2,318,ant-ivy,"""[0.9687697291374207, 0.03123033419251442]"""
19,228278,Local conflict managers in ivy.xml not working,"I'm trying to retrieve the single dependency selenium-server from maven-central, using the default conflict manager ""latest-compatible"".

This leads to an error because ""xml-apis"" is referenced in two different versions. Setting the default resolver to latest-revision indeed fixes this.

I do not want to change the entire build to latest-revision though, so I'd like use the ""conflict"" tag in ivy.xml:
{code}
<conflict manager=""latest-revision""/>
{code}
(ideally even with org and module set).

Alas, this doesn't work. It will use the default conflict manager anyway, ignoring the latest-revision manager altogether.

{code}
org.apache.ivy.plugins.conflict.StrictConflictException: xml-apis#xml-apis;1.4.01 (needed by [xerces#xercesImpl;2.10.0]) conflicts with xml-apis#xml-apis;1.3.04 (needed by [xalan#serializer;2.7.1])
	at org.apache.ivy.plugins.conflict.LatestCompatibleConflictManager.handleUnsolvableConflict(LatestCompatibleConflictManager.java:292)
	at org.apache.ivy.plugins.conflict.LatestCompatibleConflictManager.handleIncompatibleConflict(LatestCompatibleConflictManager.java:173)
	at org.apache.ivy.plugins.conflict.LatestCompatibleConflictManager.resolveConflicts(LatestCompatibleConflictManager.java:114)
	at org.apache.ivy.core.resolve.ResolveEngine.resolveConflicts(ResolveEngine.java:1018)
	at org.apache.ivy.core.resolve.ResolveEngine.resolveConflict(ResolveEngine.java:895)
	at org.apache.ivy.core.resolve.ResolveEngine.resolveConflict(ResolveEngine.java:945)
	at org.apache.ivy.core.resolve.ResolveEngine.resolveConflict(ResolveEngine.java:833)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:692)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:768)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:768)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:780)
	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:703)
	at org.apache.ivy.core.resolve.ResolveEngine.getDependencies(ResolveEngine.java:575)
	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:233)
	at org.apache.ivyde.eclipse.resolve.IvyResolver.doResolve(IvyResolver.java:230)
	at org.apache.ivyde.eclipse.resolve.IvyResolver.resolve(IvyResolver.java:137)
	at org.apache.ivyde.eclipse.resolve.IvyResolveJob$1.run(IvyResolveJob.java:243)
	at java.lang.Thread.run(Thread.java:662)
{code}
",smartshark_2_2,1415,ant-ivy,"""[0.4442533552646637, 0.5557466745376587]"""
20,228279,File system resolver fails to rename artifact directory during atomic publish,"I am attempting to use the new atomic publishing functionality in the filesystem resolver (fix for IVY-492.)

The publishing process appears to complete, and does not show any errors, but the artifacts are left in a directory named '[revision].part'.

See attached file for the ivy settings I am using.

I am running Ivy from an MSBuild script using a command line like the following:

java.exe -cp ivy.jar;commons-cli.jar -Drepository.dir=C:\Temp org.apache.ivy.Main -settings ivysettings.Nightly.xml -ivy ivy.xml -revision 1.0.0.0 -publish Nightly -deliverto build\ivy.xml -status integration -publishpattern build\[artifact](.[ext])",smartshark_2_2,1075,ant-ivy,"""[0.09040606021881104, 0.909593939781189]"""
21,228280,"ParseException ""Unsupported repository, resources names are not uris"" for ivy.xml with parent on Windows","(Remark: I already posted this on dev mailing list, but I think it's better to add an issue)

When using Ivy's OSGi support with an ivy.xml (snippet) like

<ivy-module version=""2.2"" xmlns:o=""http://ant.apache.org/ivy/osgi"">
    <info organisation=""meta-level"" module=""osgi-parent"">
        <extends organisation=""bundle""   module=""mymodule""
          revision=""1.0.0.qualifier""
          location=""META-INF/MANIFEST.MF""/>
    </info>
....

then after calling e.g. ant task ivy:resolve on Windows the following error can be seen on console output:

Buildfile: C:\java\workspace\de.metalevel.eclipse.mlbuild\build.xml

ivy:configure:
[ivy:configure] :: Apache Ivy 2.3.0 - 20130110142753 ::
http://ant.apache.org/ivy/ ::
[ivy:configure] :: loading settings :: file =
C:\java\workspace\ivysettings.xml

ivy:resolve:
[ivy:resolve] Problem occurred while parsing ivy file: Unsupported
repository, resources names are not uris in
file:/C:/java/workspace/de.metalevel.eclipse.mlbuild/ivy.xml

BUILD FAILED
C:\java\workspace\de.metalevel.eclipse.mlbuild\build.xml:55: syntax
errors in ivy file: java.text.ParseException: Problem occurred while
parsing ivy file: Unsupported repository, resources names are not uris
in file:/C:/java/workspace/de.metalevel.eclipse.mlbuild/ivy.xml
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.parse(XmlModuleDescriptorParser.java:278)
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser.parseDescriptor(XmlModuleDescriptorParser.java:117)
        at
org.apache.ivy.plugins.parser.AbstractModuleDescriptorParser.parseDescriptor(AbstractModuleDescriptorParser.java:48)
        at
org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:185)
        at org.apache.ivy.Ivy.resolve(Ivy.java:507)
        at org.apache.ivy.ant.IvyResolve.doExecute(IvyResolve.java:326)
        at org.apache.ivy.ant.IvyTask.execute(IvyTask.java:277)
        at
org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at
org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
        at org.apache.tools.ant.Task.perform(Task.java:348)
        at org.apache.tools.ant.Target.execute(Target.java:392)
        at org.apache.tools.ant.Target.performTasks(Target.java:413)
        at
org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)
        at org.apache.tools.ant.Project.executeTarget(Project.java:1368)
        at
org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
        at org.apache.tools.ant.Project.executeTargets(Project.java:1251)
        at org.apache.tools.ant.Main.runBuild(Main.java:811)
        at org.apache.tools.ant.Main.startAnt(Main.java:217)
        at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
        at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
Caused by: java.lang.RuntimeException: Unsupported repository, resources
names are not uris
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.startElement(XmlModuleDescriptorParser.java:363)
        at
com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:501)
        at
com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:179)
        at
com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:739)
        at
com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:377)
        at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2755)
        at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)
        at
com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
        at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:511)
        at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)
        at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at
com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
        at
com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at
com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
        at org.apache.ivy.util.XMLHelper.parse(XMLHelper.java:146)
        at org.apache.ivy.util.XMLHelper.parse(XMLHelper.java:109)
        at org.apache.ivy.util.XMLHelper.parse(XMLHelper.java:99)
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.parse(XmlModuleDescriptorParser.java:253)
        ... 23 more
Caused by: java.lang.RuntimeException: Unsupported repository, resources
names are not uris
        at
org.apache.ivy.osgi.core.OSGiManifestParser.parseDescriptor(OSGiManifestParser.java:69)
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.parseParentModuleOnFilesystem(XmlModuleDescriptorParser.java:631)
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.extendsStarted(XmlModuleDescriptorParser.java:408)
        at
org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.startElement(XmlModuleDescriptorParser.java:304)
        ... 41 more
Caused by: java.net.URISyntaxException: Illegal character in opaque part
at index 2:
C:\java\workspace\de.metalevel.eclipse.mlbuild\META-INF\MANIFEST.MF
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parse(URI.java:3019)
        at java.net.URI.<init>(URI.java:578)
        at
org.apache.ivy.osgi.core.OSGiManifestParser.parseDescriptor(OSGiManifestParser.java:67)
        ... 44 more
",smartshark_2_2,970,ant-ivy,"""[0.15301008522510529, 0.8469899296760559]"""
22,228281,ModuleRevisionId encode/decode doesn't work if revision is empty,"If the revision is an empty string, the ModuleRevisionId encode/decode methods doesn't work:

for instance,
ModuleRevisionId revId = new ModuleRevisionId(new ModuleId(""J2EE"", ""ejb""), """");
String encoded = revId.encodeToString();
ModuleRevisionId.decode(encoded);

this will throw the following exception:
java.lang.IllegalArgumentException: badly encoded module revision id: 'J2EE:#@#:
ejb:#@#:'
        at fr.jayasoft.ivy.ModuleRevisionId.decode(ModuleRevisionId.java:105)",smartshark_2_2,230,ant-ivy,"""[0.07264595478773117, 0.9273540377616882]"""
23,228282,modules splitted across a chain are not handled correctly,"when a module is split across a resolver chain (ivy file in one resolver, artifacts in another), the chain doesn't find the whole module correctly if the resolver having the artifacts is before the resolver with the module file.",smartshark_2_2,229,ant-ivy,"""[0.0755629688501358, 0.9244370460510254]"""
24,228283,ResolveEngine.getDependencies does not work using extra attributes.,"I wrote these two tests for 'test/java/org/apache/ivy/core/resolve/ResolveEngineTest.java'.  The one using extra attributes to build a dependency list does not return the complete set of dependencies.


    public void testBuildDependencyList_ExtraAtt() throws Exception {
        System.setProperty(""ivy.local.default.root"", new File(""test/repositories/extra-attributes"").getAbsolutePath());
        System.setProperty(""ivy.settings.file"", ""test/repositories/extra-attributes/ivysettings.xml"");
        String org = ""apache"";
        String mod = ""mymodule"";
        String rev = ""1749"";
        Map extraAttributes = new HashMap();
        extraAttributes.put(""eatt"", ""task2"");
        extraAttributes.put(""eatt2"", ""test"");
        
        ResolveEngine engine = new ResolveEngine(ivy.getSettings(), 
            ivy.getEventManager(), ivy.getSortEngine());

        ModuleRevisionId mRevId = ModuleRevisionId.newInstance(org, mod, rev, extraAttributes);
        DefaultModuleDescriptor md = DefaultModuleDescriptor.newCallerInstance(mRevId, new String[] {""*""}, true, false);
        String resolveId = ResolveOptions.getDefaultResolveId(md);
        ResolveOptions options = new ResolveOptions();
        options.setConfs(new String[] {""*""});
        options.setResolveId(resolveId);
        ResolveReport report = new ResolveReport(md, options.getResolveId());

        IvyNode[] deps = engine.getDependencies(md, options, report);

        assertEquals(2, deps.length);
        assertTrue(Arrays.toString(deps).contains(""apache#mymodule;1749""));
        assertTrue(Arrays.toString(deps).contains(""apache#module2;1976""));
    }
    
    public void testBuildDependencyList_MultiDeps() throws Exception {
        System.setProperty(""ivy.local.default.root"", new File(""test/repositories/1"").getAbsolutePath());
        System.setProperty(""ivy.settings.file"", ""test/repositories/ivysettings-1.xml"");
        String org = ""org2"";
        String mod = ""mod2.3"";
        String rev = ""0.4"";

        ResolveEngine engine = new ResolveEngine(ivy.getSettings(), 
            ivy.getEventManager(), ivy.getSortEngine());

        ModuleRevisionId mRevId = ModuleRevisionId.newInstance(org, mod, rev);
        DefaultModuleDescriptor md = DefaultModuleDescriptor.newCallerInstance(mRevId, new String[] {""*""}, true, false);
        String resolveId = ResolveOptions.getDefaultResolveId(md);
        ResolveOptions options = new ResolveOptions();
        options.setConfs(new String[] {""*""});
        options.setResolveId(resolveId);
        ResolveReport report = new ResolveReport(md, options.getResolveId());

        IvyNode[] deps = engine.getDependencies(md, options, report);

        assertEquals(4, deps.length);
    }


Update:
There seems to be parsing issue using extra attributes on the artifact, or perhaps there is a test setup issue.  Lookup with only module level extra attributes seems to work.

java.text.ParseException: [xml parsing: ivy.xml.original:28:60: cvc-complex-type.3.2.2: Attribute 'platform' is not allowed to appear in element 'artifact'. in file:/Project/ivy/test/repositories/extra-attributes/cache/apache/mymodule/task2/1749/ivy.xml.original
, xml parsing: ivy.xml.original:29:58: cvc-complex-type.3.2.2: Attribute 'platform' is not allowed to appear in element 'artifact'. in file:/Project/ivy/test/repositories/extra-attributes/cache/apache/mymodule/task2/1749/ivy.xml.original
]
	at org.apache.ivy.plugins.parser.AbstractModuleDescriptorParser$AbstractParser.checkErrors(AbstractModuleDescriptorParser.java:89)
	at org.apache.ivy.plugins.parser.AbstractModuleDescriptorParser$AbstractParser.getModuleDescriptor(AbstractModuleDescriptorParser.java:342)
	at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser.parseDescriptor(XmlModuleDescriptorParser.java:103)
	at org.apache.ivy.plugins.parser.AbstractModuleDescriptorParser.parseDescriptor(AbstractModuleDescriptorParser.java:48)
	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager$MyModuleDescriptorProvider.provideModule(DefaultRepositoryCacheManager.java:638)
	at org.apache.ivy.core.cache.ModuleDescriptorMemoryCache.getStale(ModuleDescriptorMemoryCache.java:68)
	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.getStaledMd(DefaultRepositoryCacheManager.java:655)
	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.cacheModuleDescriptor(DefaultRepositoryCacheManager.java:942)
	at org.apache.ivy.plugins.resolver.BasicResolver.parse(BasicResolver.java:542)
	at org.apache.ivy.plugins.resolver.BasicResolver.getDependency(BasicResolver.java:263)
	at org.apache.ivy.core.resolve.IvyNode.loadData(IvyNode.java:170)
	at org.apache.ivy.core.resolve.VisitNode.loadData(VisitNode.java:274)
	at org.apache.ivy.core.resolve.VisitNodeTest.testLoadData_ExtraAtt_artifact(VisitNodeTest.java:84)



I don't have a test for it, yet, but I think 

...
        ModuleRevisionId[] mridArr = ...
        DefaultModuleDescriptor md = DefaultModuleDescriptor.newCallerInstance(mridArr, true, true);
        IvyNode[] deps = engine.getDependencies(md, options, report);
...

Is not transitive in that it only returns the immediate dependencies for the mridArr objects but not the rest of the dependencies.",smartshark_2_2,144,ant-ivy,"""[0.08122283965349197, 0.9187771677970886]"""
25,228284,ivy:publish ant target with ivy file with defaultconf generates non-existing configuration,"When having an ivy.xml with multiple configurations, e.g. x, y, z and a defaultconf containing z. Then when calling the ant target ivy:publish with conf=""x"", the defaultconf in the generated ant file will still contain a defaultconf containing configuration z. This results in the following exception:
{code}
impossible to publish artifacts for com.example#example;working@machine: java.lang.IllegalStateException: bad ivy file for com.example#example;working@machine: /path/to/file/ivy_generated.xml: java.text.ParseException: Cannot add dependency 'com.example#example;1.0.0.23' to configuration 'z' of module com.example#example;1.2.0.119 because this configuration doesn't exist! in file:/path/to/file/ivy_generated.xml
{code}",smartshark_2_2,1192,ant-ivy,"""[0.15412941575050354, 0.8458705544471741]"""
26,228285,Failure to transitively install Maven2 artifacts with namespace,"I have Maven2 style repository with namespace elements in ivyconf.xml as in the examples coming with ivy. The namespaces mainly deal with apache namespace organisation/module pairs, e.g. converts commons-logging/commons-logging to apache/commons-logging.

In normal operation the namespace conversion works fine.

The problem is in ivy:install when I request ""transitive='true'"".

The debug log I get is:
[ivy:install] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:install] WARN:     ::          UNRESOLVED DEPENDENCIES         ::
[ivy:install] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:install] WARN:     :: [ axis | axis | 1.1 ]: inconsistent module descriptor file found in '/home/ivy-repository/thirdparty-maven-style/axis/axis/1.1/axis- 1.1.pom': bad organisation: expected='axis' found='apache';
[ivy:install] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::

The POM header is:
<project>
  <modelVersion> 4.0.0</modelVersion>
  <groupId>axis</groupId>
  <artifactId>axis</artifactId>
  <version>1.1</version>
   ...

The ivyconf.xml section is:
    <resolvers>
        <filesystem name=""thirdparty"">
            <ivy pattern=""${ivy.thirdparty.default.root}/${ivy.thirdparty.default.ivy.pattern}"" />
            <artifact pattern=""${ ivy.thirdparty.default.root}/${ivy.thirdparty.default.artifact.pattern}"" />
        </filesystem>
       
        <filesystem name=""thirdparty-maven"" m2compatible=""yes"" namespace=""ibiblio-maven2"">
            <ivy pattern=""${ivy.thirdparty-maven.default.root}/${ivy.thirdparty-maven.default.ivy.pattern}"" />
            <artifact pattern=""${ivy.thirdparty-maven.default.root}/${ivy.thirdparty-maven.default.artifact.pattern }"" />
        </filesystem>
    </resolvers>

    <namespaces>
        <namespace name=""ibiblio-maven2"">
           
            <rule>    <!-- imported apache maven1 projects -->
                <fromsystem>
                    <src org=""apache"" module="".+""/>
   
                    <dest org=""$m0"" module=""$m0""/>
                </fromsystem>
                <tosystem>
                    <src org=""commons-.+"" module=""commons-.+"" />
                    <src org=""axis"" module=""axis"" />
                    ...
                    <src org=""xmlrpc"" module=""xmlrpc"" />
                   
                    <dest org=""apache"" module=""$m0""/>
                </tosystem>
            </rule>

            <rule> <!-- new apache projects -->
                <fromsystem>
                    <src org=""apache"" />
                    <dest org="" org.apache""/>
                </fromsystem>
                <tosystem>
                    <src org=""org.apache"" />
                    <dest org=""apache"" />
                </tosystem>
            </rule>

        </namespace>
    </namespaces>


As I understand, Ivy already translated axis/axis to apache/axis and it now cannot understand the POM and therefore cannot define its transitive dependencies. ",smartshark_2_2,1285,ant-ivy,"""[0.5195627808570862, 0.4804372489452362]"""
27,228286,NPE when no organisation or no name is provided in module element of ivyconf,"When you don't provide an organisation in your module element, Ivy fails with an NPE instead of a clean error:
{code}
	<modules>
		<module name=""ivy"" resolver=""ivy""/>
	</modules>
{code}",smartshark_2_2,742,ant-ivy,"""[0.07355110347270966, 0.9264488220214844]"""
28,228287,ivy.settings.dir space escaping problem,"When an ivysettings file is included from a path with spaces in it, the ivy.settings.dir property becomes doubly escaped. I run into this problem when trying to configure the ivy classpath. This is causing a lot of pain for our team members using windows since they want to keep everything under their home directory which includes spaces in the path due to the ""C:\Documents and Settings"" directory. I'll be attaching a testcase which throws an exception when ""java -jar ivy-2.1.0.jar -settings ivysettings.xml"" is called. The exception root cause is:

Caused by: java.lang.RuntimeException: impossible to define new type: class not found: Foo in [file:/tmp/x/dir%2520with%2520spaces/foo.jar] nor Ivy classloader
	at org.apache.ivy.core.settings.IvySettings.classForName(IvySettings.java:648)
	at org.apache.ivy.core.settings.IvySettings.typeDef(IvySettings.java:632)
	at org.apache.ivy.core.settings.IvySettings.typeDef(IvySettings.java:628)
	at org.apache.ivy.core.settings.XmlSettingsParser.typedefStarted(XmlSettingsParser.java:503)
	at org.apache.ivy.core.settings.XmlSettingsParser.startElement(XmlSettingsParser.java:205)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:504)
	at com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:182)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1315)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2723)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:624)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:486)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:810)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:740)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:110)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1208)
	at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:525)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:392)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:274)
	at org.apache.ivy.core.settings.XmlSettingsParser.doParse(XmlSettingsParser.java:160)
	... 23 more
",smartshark_2_2,179,ant-ivy,"""[0.11002911627292633, 0.8899708390235901]"""
29,228288,'null' status attribute in module descriptor,"The module descriptor writer blindly outputs module descriptor attributes without checking for null.

So the descriptor in the cache ends up with ""null"":

<info organisation=""jayasoft"" module=""depending"" revision=""working@hostname"" status=""null""/>
",smartshark_2_2,404,ant-ivy,"""[0.12497206777334213, 0.8750279545783997]"""
30,228289,NPE in case of eviction,"Since the fix of IVY-603, we have NPE in some cases when we have evicted modules.

(I have slightly updated the code to have a more precise trace)

java.lang.NullPointerException: getDescriptor() is null for [ junit | junit | 3.8.1 ]
        at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:256)
        at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:193)
        at org.apache.ivy.Ivy.resolve(Ivy.java:256)
        at org.apache.ivy.ant.IvyResolve.doExecute(IvyResolve.java:212)
        at org.apache.ivy.ant.IvyTask.execute(IvyTask.java:281)
        at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:288)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:615)
        at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:105)
        at org.apache.tools.ant.Task.perform(Task.java:348)
        at org.apache.tools.ant.Target.execute(Target.java:357)
        at org.apache.tools.ant.Target.performTasks(Target.java:385)
        at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1329)
        at org.apache.tools.ant.Project.executeTarget(Project.java:1298)
        at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
        at org.apache.tools.ant.Project.executeTargets(Project.java:1181)
        at org.apache.tools.ant.Main.runBuild(Main.java:698)
        at org.apache.tools.ant.Main.startAnt(Main.java:199)
        at org.apache.tools.ant.launch.Launcher.run(Launcher.java:257)
        at org.apache.tools.ant.launch.Launcher.main(Launcher.java:104)

Reverting the change done to ResolveEngine.java in IVY-603 seems to fix the regression.  But the right fix might be to fill the descriptor even when the module is evicted...
",smartshark_2_2,287,ant-ivy,"""[0.06153012812137604, 0.9384698867797852]"""
31,228290,Nullpointer at PomModuleDescriptorBuilder.addDependency,"While iwas trying to resolve a new dependency on 
		<dependency name=""jwebunit-htmlunit-plugin"" org=""net.sourceforge.jwebunit"" rev=""1.5"" conf=""test->default""/>
i got the following nullpointer exception:

[ivy:retrieve] 	public3: found md file for net.sourceforge.jwebunit#jwebunit-htmlunit-plugin;1.5
[ivy:retrieve] 		=> http://repo1.maven.org/maven2/net/sourceforge/jwebunit/jwebunit-htmlunit-plugin/1.5/jwebunit-htmlunit-plugin-1.5.pom (1.5)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/net/sourceforge/jwebunit/jwebunit-htmlunit-plugin/1.5/jwebunit-htmlunit-plugin-1.5.pom ...
[ivy:retrieve] 	public3: downloading http://repo1.maven.org/maven2/net/sourceforge/jwebunit/jwebunit-htmlunit-plugin/1.5/jwebunit-htmlunit-plugin-1.5.pom
[ivy:retrieve] 	public3: downloading http://repo1.maven.org/maven2/net/sourceforge/jwebunit/jwebunit-htmlunit-plugin/1.5/jwebunit-htmlunit-plugin-1.5.pom.sha1
[ivy:retrieve] sha1 OK for http://repo1.maven.org/maven2/net/sourceforge/jwebunit/jwebunit-htmlunit-plugin/1.5/jwebunit-htmlunit-plugin-1.5.pom
[ivy:retrieve] 	[SUCCESSFUL ] net.sourceforge.jwebunit#jwebunit-htmlunit-plugin;1.5!jwebunit-htmlunit-plugin.pom(pom.original) (588ms)
[ivy:retrieve] default: Checking cache for: dependency: net.sourceforge.jwebunit#jwebunit;1.5 {}
[ivy:retrieve] default: module revision found in cache: net.sourceforge.jwebunit#jwebunit;1.5
[ivy:retrieve] problem occured while resolving dependency: net.sourceforge.jwebunit#jwebunit-htmlunit-plugin;1.5 {test=[default]} with public3: java.lang.NullPointerException
[ivy:retrieve] 	at org.apache.ivy.plugins.parser.m2.PomModuleDescriptorBuilder.addDependency(PomModuleDescriptorBuilder.java:291)
[ivy:retrieve] 	at org.apache.ivy.plugins.parser.m2.PomModuleDescriptorParser.parseDescriptor(PomModuleDescriptorParser.java:235)
[ivy:retrieve] 	at org.apache.ivy.plugins.parser.m2.PomModuleDescriptorParser.parseDescriptor(PomModuleDescriptorParser.java:105)
[ivy:retrieve] 	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager$MyModuleDescriptorProvider.provideModule(DefaultRepositoryCacheManager.java:633)
[ivy:retrieve] 	at org.apache.ivy.core.cache.ModuleDescriptorMemoryCache.getStale(ModuleDescriptorMemoryCache.java:68)
[ivy:retrieve] 	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.getStaledMd(DefaultRepositoryCacheManager.java:650)
[ivy:retrieve] 	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.cacheModuleDescriptor(DefaultRepositoryCacheManager.java:939)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.BasicResolver.parse(BasicResolver.java:538)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.BasicResolver.getDependency(BasicResolver.java:261)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.IBiblioResolver.getDependency(IBiblioResolver.java:500)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.ChainResolver.getDependency(ChainResolver.java:126)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.ChainResolver.getDependency(ChainResolver.java:126)
[ivy:retrieve] 	at org.apache.ivy.plugins.resolver.ChainResolver.getDependency(ChainResolver.java:126)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.IvyNode.loadData(IvyNode.java:169)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.VisitNode.loadData(VisitNode.java:271)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:671)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.doFetchDependencies(ResolveEngine.java:757)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.fetchDependencies(ResolveEngine.java:679)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.getDependencies(ResolveEngine.java:551)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:235)
[ivy:retrieve] 	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:193)
[ivy:retrieve] 	at org.apache.ivy.Ivy.resolve(Ivy.java:502)
[ivy:retrieve] 	at org.apache.ivy.ant.IvyResolve.doExecute(IvyResolve.java:234)
[ivy:retrieve] 	at org.apache.ivy.ant.IvyTask.execute(IvyTask.java:277)
[ivy:retrieve] 	at org.apache.ivy.ant.IvyPostResolveTask.ensureResolved(IvyPostResolveTask.java:207)
[ivy:retrieve] 	at org.apache.ivy.ant.IvyPostResolveTask.prepareAndCheck(IvyPostResolveTask.java:154)
[ivy:retrieve] 	at org.apache.ivy.ant.IvyRetrieve.doExecute(IvyRetrieve.java:49)
",smartshark_2_2,665,ant-ivy,"""[0.07101568579673767, 0.9289843440055847]"""
32,228291,Referenced resolver not found in macro,"When in a macrodef one want to reference an existing resolver, Ivy complains about not finding the referenced resolver.

Example:
{code}
<resolvers>
  <ibiblio name=""test"" />
</resolvers>
<macrodef name=""mymacro"">
  <chain name=""c"">
    <resolver ref=""test"" />
  </chain>
</macrodef>
{code}",smartshark_2_2,1079,ant-ivy,"""[0.10430512577295303, 0.8956948518753052]"""
33,228292,Ivy standalone is passing null args to main method when invoking with no args,"When using the -main option with no -args argument with Ivy on the command line, a null String[] is passed as parameter to the main method, instead of an empty array, which may cause the main method invoked to fail since it doesn't respect the usual main contract.",smartshark_2_2,998,ant-ivy,"""[0.16252751648426056, 0.8374724984169006]"""
34,228293,"When in ssh plugin we does not set username in scheme, Ivy always try to connect with guest username, even if we change one in panel.","When in ssh plugin we does not set username in scheme, Ivy always try to connect with username 'guest', even if we change one in panel.
(It is becouse CredentialPanel is shown from SshCache.getSession(), where user is already set.)
",smartshark_2_2,1106,ant-ivy,"""[0.11012285202741623, 0.889877200126648]"""
35,228294,specifying a defaultconfmapping adds dependencies of each unlisted configuration,"If you specify the defaultconfmapping, the dependencies of unlisted configurations are added to the listed configurations
See http://www.jayasoft.org/node/589, point 2 for more information

I'll upload a JUnit test and a patch in a minute.

regards,
Maarten",smartshark_2_2,709,ant-ivy,"""[0.24726581573486328, 0.7527341842651367]"""
36,228295,ClassCastException in Eclipse 4.4.1,"h3. problem

When trying to use IvyDE within Eclipse 4.4.1 the following Exception happens which renders IvyDE unusable:

{code}
An internal error occurred during: ""IvyDE resolve"".

java.lang.ClassCastException: org.eclipse.osgi.internal.framework.EquinoxConfiguration$1 cannot be cast to java.lang.String
	at org.apache.ivy.core.settings.IvySettings.addAllVariables(IvySettings.java:610)
	at org.apache.ivy.core.settings.IvySettings.addAllVariables(IvySettings.java:604)
	at org.apache.ivy.core.settings.IvySettings.addSystemProperties(IvySettings.java:308)
	at org.apache.ivy.core.settings.IvySettings.<init>(IvySettings.java:303)
	at org.apache.ivy.core.settings.IvySettings.<init>(IvySettings.java:222)
	at org.apache.ivyde.internal.eclipse.CachedIvy.createIvySettings(CachedIvy.java:235)
	at org.apache.ivyde.internal.eclipse.CachedIvy.getIvyFromFile(CachedIvy.java:200)
	at org.apache.ivyde.internal.eclipse.CachedIvy.doGetIvy(CachedIvy.java:163)
	at org.apache.ivyde.internal.eclipse.CachedIvy.getIvy(CachedIvy.java:124)
	at org.apache.ivyde.internal.eclipse.resolve.IvyResolveJob.doRun(IvyResolveJob.java:145)
	at org.apache.ivyde.internal.eclipse.resolve.IvyResolveJob.run(IvyResolveJob.java:85)
	at org.eclipse.core.internal.jobs.Worker.run(Worker.java:54)

{code}",smartshark_2_2,1341,ant-ivy,"""[0.1987735480070114, 0.8012264966964722]"""
37,228296,XmlModuleDescriptorWriterTest is broken,"XmlModuleDescriptorWriterTest is using pure String comparison of serialized module descriptor.

From a quick look, this does not work as it relies on ordering of values within a HashMap so it can only work if the ordering of the hashmap is the same as the one that was done when serializing the file used for reference.

So if you try running the test with Sun JDK 6 it will fail while it will work with Sun JDK 5 since the hash method used as a default has been changed. 

To fix it as I can see right now:
1) you guarantee ordering and sort each element (lexicographically ?) within XmlModuleDescriptorWriter when serializing things
2) you don't guarantee ordering but you implement equals method for DefaultModuleDescriptor (which I assume will have a cascading effect for other classes)

I think 1) makes more sense for now and we should go toward 2) so that more comparisons of objects can be done using equals() (esp. as there are a lot of objects that end up in maps)

",smartshark_2_2,735,ant-ivy,"""[0.7856239080429077, 0.21437601745128632]"""
38,228297,SearchEngine.listModules returns MRID without extra attributes.,"The ModuleRevisionId objects that are returned by the SearchEngine.listModules(ModuleRevisionId, PatternMatcher) method does not contain any extra attributes.  If extra attributes are passed in then they should also be added to the returned module.  Without the extra attributes then then the modules returned are practically useless for any further use.  This method is published in the Ivy class, too.

See attached for test and fix patch.",smartshark_2_2,69,ant-ivy,"""[0.23133528232574463, 0.7686647176742554]"""
39,228298,extra attributes lost from info when ivy file is merged with parent,"This is in relation to the new ""extends"" functionality in 2.2.0-rc1.

When publishing an ivy file that extends a parent ivy file (using the default setting of merging the files), any extra attributes get dropped.  This happens regardless of whether the extra attributes are in the child ivy file, parent ivy file, or both.

Ideally, all extra attributes would undergo the same merge process as all the standard attributes (allowing them to be inherited from parent to child.  But at the very least, they should not be removed from the child ivy file.

Example:
info section in original ivy file
{noformat}
<info module=""core-util"" e:sourceurl=""${svn.info.url}"" e:sourcerev=""${svn.info.wcversion}"" e:user=""${user.name}"">
    <extends organisation=""our-organisation"" module=""our-parent"" revision=""latest.integration""/>
</info>
{noformat}

info section in published (merged) ivy file
{noformat}
<info organisation=""our-organisation"" module=""core-util"" branch=""trunk"" revision=""2.2.51-SNAPSHOT.r30249"" status=""integration"" publication=""20100714140802"">
    <!-- <extends organisation=""our-organisation"" module=""our-parent"" revision=""working@mycomputer""/> -->
    
    <!-- description inherited from parent -->
    <description></description>
</info>
{noformat}

As you can see, all the ""e:*"" attributes have been dropped.",smartshark_2_2,1160,ant-ivy,"""[0.13200974464416504, 0.8679901957511902]"""
40,228299,Module inheritance sometimes fails to locate parent descriptor in deliver process,"I recently discovered a bug in Module Inheritance feature (i.e. extends).

{code:xml} 
<extends organisation=""net.foo"" module=""bar"" revision=""latest.integration"" location=""../myParent"" extendTypes=""configurations,dependencies""/>
{code}

The feature works fine if the parent descriptor is already published in your repository, it will use the MRID to retrieve the parent module. 
We also can use a ""location"" attribute (for dev mode) which defines where to find the parent descriptor. This can make particular sense if you need to share metadatas (dependencies, configurations, whatever)  across many subprojects.

If we get into technical details when ""extend"" element is found ivy will :
* check on filesystem if parent module exists (based on location attribute). If no location attribute is found ivy will look in the default one ""../ivy.xml"".
* if not found, it will check in the cache using MRID. However it just ignore the version attribute. To be exact it ask for WorkingRevision.
* if not found it will query repositories using the real MRID (here the version attribute is taken in consideration)
* if found it will try to flush the parent descriptor in the cache

Everything works :)

But when you invoke ivy:deliver (directly or through ivy:publish) ivy parse the resolved module descriptor in the cache. And the complication comes here.
Imagine you never publish the ""parent module"" in the repository. When you will invoke ivy:deliver / ivy:publish ivy will :
* parse your module descriptor (the one <extend>ing your parent) from the cache
* while parsing ivy will find the <extend> element and will try to locate the parent descriptor
* check the location attribute <-- fail because cache pattern is fully configurable in ivy and almost never match with the default value ""../ivy.xml""
* check the cache with the given MRID (organisation=""net.foo"" module=""bar"" but with working revision ) <-- almost never find it
* check the repositories with the real mrid (organisation=""net.foo"" module=""bar"" revision=""latest.revision"") <-- fail as we never published our parent descriptor.
* deliver process will fail !

We never really encoutered the problem as this feature was developped in easyant context and we were doing a lot of stuff around this feature. In almost all cases parent descriptor was published by an other mechanism of easyant  in a ""build scoped repository"" (kind of filesystem based repository local to the project).

While refactoring the code in easyant i just realized that somethings was wrong.",smartshark_2_2,597,ant-ivy,"""[0.08143706619739532, 0.9185629487037659]"""
41,228300,Maven test scope not mapped to test conf,"The pom to ivy file translator incorrectly assigns the test scope of a dependency to the runtime conf in the ivy file.

Consider the ivy beta-1 pom from http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/ivy/ivy/2.0.0-beta1/ivy-2.0.0-beta1.pom

the current parsed Ivy file is:
{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<ivy-module version=""1.0"" xmlns:m=""http://ant.apache.org/ivy/maven"">
	<info organisation=""org.apache.ivy""
		module=""ivy""
		revision=""2.0.0-beta1""
		status=""integration""
		publication=""20071213094244""
	/>
	<configurations>
		<conf name=""default"" visibility=""public"" description=""runtime dependencies and master artifact can be used with this conf"" extends=""runtime,master""/>
		<conf name=""master"" visibility=""public"" description=""contains only the artifact published by this module itself, with no transitive dependencies""/>
		<conf name=""compile"" visibility=""public"" description=""this is the default scope, used if none is specified. Compile dependencies are available in all classpaths.""/>
		<conf name=""provided"" visibility=""public"" description=""this is much like compile, but indicates you expect the JDK or a container to provide it. It is only available on the compilation classpath, and is not transitive.""/>
		<conf name=""runtime"" visibility=""public"" description=""this scope indicates that the dependency is not required for compilation, but is for execution. It is in the runtime and test classpaths, but not the compile classpath."" extends=""compile""/>
		<conf name=""test"" visibility=""private"" description=""this scope indicates that the dependency is not required for normal use of the application, and is only available for the test compilation and execution phases."" extends=""runtime""/>
		<conf name=""system"" visibility=""public"" description=""this scope is similar to provided except that you have to provide the JAR which contains it explicitly. The artifact is always available and is not looked up in a repository.""/>
		<conf name=""optional"" visibility=""public"" description=""contains all optional dependencies""/>
	</configurations>
	<publications>
		<artifact name=""ivy"" type=""jar"" ext=""jar"" conf=""master""/>
	</publications>
	<dependencies>
		<dependency org=""ant"" name=""ant"" rev=""1.6"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""commons-httpclient"" name=""commons-httpclient"" rev=""3.0"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""commons-cli"" name=""commons-cli"" rev=""1.0"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""oro"" name=""oro"" rev=""2.0.8"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""commons-vfs"" name=""commons-vfs"" rev=""1.0"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""jsch"" name=""jsch"" rev=""0.1.25"" force=""true"" conf=""optional->compile(*),master(*)""/>
		<dependency org=""junit"" name=""junit"" rev=""3.8.2"" force=""true"" conf=""runtime->compile(*),runtime(*),master(*)""/>
		<dependency org=""commons-lang"" name=""commons-lang"" rev=""2.3"" force=""true"" conf=""runtime->compile(*),runtime(*),master(*)""/>
	</dependencies>
</ivy-module>
{code}

note that junit and commons-lang have runtime-> when they should have test->

patch forthcoming...",smartshark_2_2,1110,ant-ivy,"""[0.8500508069992065, 0.14994923770427704]"""
42,228301,Dependent jars missing in ivy binaries,"The dependent jars have been removed from the 2.0-alpha-1-incubating binary.
This is done on purpose, to minimize download size. However, a script should have been included to download these dependencies from the internet. This should be described in the installation instructions.

Also, for people behind restrictive internet access it's easier to use a single bundle. 

Therefore two binary distributions should be provided for the next release:
* One without dependencies, but with installation script to download them
* The other with dependencies
",smartshark_2_2,1084,ant-ivy,"""[0.9980747699737549, 0.0019252680940553546]"""
43,228302,New ivy:resolve inline dependencies should support exta attributes,"When using the new inline dependency declaration for the Ant ivy:resolve task, I need to be able to set extra attributes like below. Otherwise, I am still forced to create temporary ivy.xml files for this use case.

<ivy:resolve>
    <dependency org=""org.slf4j"" module=""slf4j"" e:color=""green"" rev=""1.6"" conf=""api,log4j"" />
</ivy:resolve>",smartshark_2_2,1178,ant-ivy,"""[0.998150646686554, 0.0018493183888494968]"""
44,228303,"Patch: Fixed more than a hundred (100+) obsolete ""configuration"" references; replaced with ""settings""","Please apply the attached patch to:
* effectively rename old ""configuration"" directory to ""settings"" (keeping the old files in place, adding the new directory)
* replace all documentation pages in the old directory with HTML redirects to their new location under settings/
* replace several (probably all?) obsolete documentation references to ""configuration"" with the correct reference to ""settings""
* update xooki table of contents respectively
* add common Eclipse and Windows files (.project, Thumbs.db) to svn ignore list

There are well over 100 individual corrections made. Enjoy!",smartshark_2_2,1277,ant-ivy,"""[0.9979532957077026, 0.002046736655756831]"""
45,228304,repository listing,"it would be nice to add a feature on dependency resolvers to enable to list the
modules available. This feature would enable tools (such a GUI on ivy file) to
propose dependencies based what exists in repository.",smartshark_2_2,772,ant-ivy,"""[0.9983235001564026, 0.001676440122537315]"""
46,228305,ivyconf include syntax,"It would be a handy thing if the <include/> tag within an ivyconf.xml could resolve files relative to the file requested, For example,
I might have a series of ivyconf files:

http://www.myserver.com/ivy/ivyconf-default.xml
http://www.myserver.com/ivy/ivyconf-extras.xml

The only difference between ivyconf-default and ivyconf-extras is that extras adds a different resolver. 
If possible I'd like to utilize all the stuff in ivyconf-default and not repeat that config in ivyconf-extras.
Ideally, ivyconf-extras would like sort of like this.

<ivyconf>

  <include file=""ivyconf-default.xml""/>

  <resolvers>
    <!-- New resolvers -->
  <resolvers/>

  <!-- Additional property -->

</ivyconf>

Now when I ivy:configure url=""http://www.myserver.com/ivy/ivyconf-extras.xml"", what would 
be nice would be for ivy to look at the include and say, I'm going to resolve that against the
path of the config file I'm processing, rather than resolve against the local filesystem; so it
would use http://www.myserver.com/ivy/ivyconf-default.xml from the remote server.


",smartshark_2_2,819,ant-ivy,"""[0.9984244108200073, 0.0015756275970488787]"""
47,228306,Ant retrieve task returns inconsistent jar order,"Is it expected behavior that path variables set by the retrieve task will have an inconsistent ordering, or is this a bug? 

I am using the retrieve task within an Ant script and setting a path variable.  When I run the script on Win7 the path contains the jars in the order in which they are listed in the ivy.xml file; however, when i run the script on Linux, the order of the two jars is reversed.

I have two dependencies in a ""test"" configuration....JMockit and JUnit.  My ""test"" conf is the first conf listed.  JMockit is listed as the first dependency and JUnit is listed as the second dependency.

Due to the nature of the jars in use, the order of the entries in the path variable important. 

I've also cleared the .ivy2 cache several times just to be sure that I am getting fresh resolutions.

",smartshark_2_2,1347,ant-ivy,"""[0.7212610840797424, 0.27873891592025757]"""
48,228307,Code / XML Polished,"Here patches can be added which only fix indentation, spacing, etc things in the code / xml files.",smartshark_2_2,1448,ant-ivy,"""[0.9982296824455261, 0.0017703528283163905]"""
49,228308,publish sha512 sums with releases,"according to http://www.apache.org/dev/release-distribution.html#sigs-and-sums it would be nice to have sha512 sums published for release artifacts.

looking at http://www.apache.org/dist/ant/ivy/2.4.0/ and http://www.apache.org/dist/ant/ivy/2.4.0/maven2/2.4.0/ there are only sha1 sums posted",smartshark_2_2,1580,ant-ivy,"""[0.9984703660011292, 0.001529612927697599]"""
50,228309,URLHandlerDispatcher Class description is misleading,"[https://github.com/apache/ant-ivy/blob/9003b5b5aa4465e84c934819f45c1ab4365dc51c/src/java/org/apache/ivy/util/url/URLHandlerDispatcher.java#L31]

{code}
 * This class is used to dispatch downloading requests
{code}

https://github.com/apache/ant-ivy/blob/9003b5b5aa4465e84c934819f45c1ab4365dc51c/src/java/org/apache/ivy/util/url/URLHandlerDispatcher.java#L127
{code}
    public void upload(final File src, final URL dest, final CopyProgressListener listener, final TimeoutConstraint timeoutConstraint) throws IOException {
{code}

This isn't a download request...

Note: this code is definitely used.
I'm jumping through trying to get SBT & Apache Ivy & OkHttp to consistently and authentication credentials...",smartshark_2_2,1583,ant-ivy,"""[0.9949010610580444, 0.005098862573504448]"""
51,228310,Add a new trigger event to ivysettings that will be called immediately.,"h3. Use Case
In our legacy build system, we have an ant file which has a series of property set in it which specify the versions of the dependencies in it. Basically, something like this:

{code:xml|title=depends.xml}
<project>
    <property name=""projectA"" value=""1.2.3""/>
    <property name=""projectB"" value=""4.5.6""/>
</project>
{code}

Unfortunately, these properties cannot be used in the ivy settings, as the ivy settings only supports reading of properties files via the <properties/> element.

What is needed is a new type of ivy trigger which is called immediately (as soon as it is defined) which can be used to call an ANT target which in my case can be used to read in our depends.xml and  convert it to a properties file.

It is suggested that this new type of trigger event be called ""*now*"".

h3. Example

{code:xml|title=ivysettings.xml}
<ivysettings>
    <triggers>
        <ant-build antfile=""${ivy.settings.dir}/build.xml"" target=""make-ivy-properties"" event=""now""/>
        <properties file=""ivy.properties"" override=""true""/>
    </triggers>
</ivysettings>
{code}

{code:xml|title=build.xml}
<project name=""projectX"">
    <target name=""make-ivy-properties"">
        <copy file=""depends.xml"" tofile=""ivy.properties"">
            <filterchain>
                <replaceregex pattern='&ltproperty=""(.*)"" value=""(.*)""' replace=""\1=\2"" flags=""gi""/>
            </filterchain>
        </copy>
    <target>
</project>
{code}",smartshark_2_2,460,ant-ivy,"""[0.9982196688652039, 0.0017802630318328738]"""
52,228311,CLONE - SSH agent support for SSH and SFTP transports,"At present, Ivy is unable to leverage agent support to access SSH keys unlocked and stored in memory. For sites using encrypted RSA keys, requiring additional user intervention when those keys are already decrypted and stored in non-pageable memory is unfortunate.

This support is available using companion libraries from the author of Jsch; I have a patch adding it.",smartshark_2_2,1498,ant-ivy,"""[0.9980615973472595, 0.0019384126644581556]"""
53,228312,Ant retrieve should have flag to avoid copying file multiply times if many [conf]s,"If we specified ""[conf]"" string in the ""pattern"" parameter of retrieve task, some files will be copied multiply times.

Hard to explain. I'll show example.

Say, we have two confs: ""default"" and ""test"" extends=""default""

ivy.xml contains:

===
<dependency name=""xxx""/>
<dependency name=""junit"" conf=""test""/>
===

build.xml:

===
<ivy:retrieve pattern=""lib/[conf]/[artifact].[ext]""/>
===

So after executing ""ant retrieve"" we have files:

===
lib/default:
xx.jar

lib/test:
xx.jar
junit.jar
===

File xx.jar exists in both directories.

It would be great if after ""ant retrieve"" directory lib/test contained only files junit.jar:

===
lib/default:
xx.jar

lib/test:
junit.jar
===

""ivy:retrieve"" task should have additional parameter for that.",smartshark_2_2,1047,ant-ivy,"""[0.9961341619491577, 0.0038658217526972294]"""
54,228313,Hide private or specific conf when publishing,"When publishing a module it could be nice to choose which conf to publish or not. Or, at least, be able to hide ""private"" conf. 
Private conf, which should not be accessible by others modules, should not be visible in the ivy.xml files. The other  issue is that the list of dependencies displayed contains all dependencies even those  specific to provate conf which should never be retrieve.

Module A  ivy.conf
 <configurations>
  <conf name=""default"" />
  <conf name=""dev"" visibility=""private"" extends=""default"" />
</configurations>

<dependencies>
 <dependency org="""" name=""junit"" rev=""3.8.1"" conf=""dev->*""/>
 <dependency org="""" name=""log4j"" rev=""1.2.9"" />
</dependencies>

Published Module A ivy conf :
  <configurations>
  <conf name=""default"" />  
</configurations>

<dependencies>
 <dependency org="""" name=""log4j"" rev=""1.2.9"" />
</dependencies>

",smartshark_2_2,1007,ant-ivy,"""[0.9985281229019165, 0.0014718115562573075]"""
55,228314,More function for url resolver,Make the url resolver to support the publish task in ant.,smartshark_2_2,701,ant-ivy,"""[0.9984310269355774, 0.0015689011197537184]"""
56,228315,Retain original dependency constraint rules in resolved ivy file,"ivy file metadata is lost when an ivy file is delivered since it resolves all constraint rules into specific revision numbers for publishing.  This metadata should be retained by allowing a new attribute on the dependency tag, perhaps called ""original"".  Then deliver can resolve all constraint rules to place in the rev attribute but retain the original rule in the original attribute.

I am not exactly sure if this is how it should be implemented but it shows the idea.  Here is another explanation:

> keep two versions 
> information per dependency when publishing to the repository : the 
> original version constraint, and the version that was used when the 
> module was published.
""",smartshark_2_2,828,ant-ivy,"""[0.9849249720573425, 0.015075021423399448]"""
57,228316,comments are lost when delivering ivy files,"If you deliver an Ivy file, the comments in the original Ivy file are lost.
I'll upload a patch",smartshark_2_2,872,ant-ivy,"""[0.7287093997001648, 0.2712906002998352]"""
58,228317,"ivy:retrieve with sync=""true"" removes the .svn directory",The sync option of ivy:retrieve removes everything in the target directory including the hidden .svn directory. I think it's sensible to preserve this directory since it's in no way related to an artifact.,smartshark_2_2,492,ant-ivy,"""[0.6022189855575562, 0.3977810740470886]"""
59,228318,Highlight current page in menu,"When you are on a page, it would be nice to somehow highlight the title of the current page in the left menu.  It can be a different style, font, color, background, whatever (my sens of the esthetic is rather limited).

Note: When doing that, you should keep in mind that (with the current menu) the title of the current page might be hidden by collapsing a parent.",smartshark_2_2,428,ant-ivy,"""[0.9985185265541077, 0.0014814457390457392]"""
60,228319,PomReader ignores finalName and directory,"In my project setup I am working with maven poms and need to utilize the values of {{finalName}} and {{directory}} in an artifact pattern, but the Ivy PomReader does not take those elements into account.
In my local installation I patched the code to make use of those elements. From how I understand it, I can only access extra attributes in the {{info}} element in artifact patterns, so that is where the customized code stores those values. Please let me know when there is a better way to handle that use-case in a more ivy-like fashion.
I will attach my patch as an example.

Information about finalName and directory:
http://maven.apache.org/pom.html#BaseBuild_Element",smartshark_2_2,165,ant-ivy,"""[0.6406069993972778, 0.3593929409980774]"""
61,228320,Module wide exclude,"For the moment Ivy provides a way to exclude transitive dependencies provided by a dependency in the dependency declaration itself:
{code:xml}
<dependency name=""A"" rev=""1.0"">
  <exclude module=""B""/>
</dependency>
{code}
But sometimes the same transitive dependency is obtained by several direct dependencies, so you have to exclude it in several dependency declarations.

So what would be nice is a way to exclude a module more globally:
- at a module level (declared somewhere in the Ivy file)
- at a global level (declared somewhere in ivyconf?)

The problem I see with defining such a global exclude in ivyconf is that it makes the ivy file not self contained. It's already the case with conflict-manager defined globally in ivyconf, so I don't know if it's a big problem or not. May be we should at least warn about that, or inject ivyconf wide settings in the ivy file at deliver time.

Proposed syntax:
In Ivy files, add a top level exclude section (like the conflicts section) containing rules of modules/artifacts to exclude:
{code:xml}
<exclude>
  <rule module=""*servlet*"" matcher=""glob"" conf=""weblib"" /> 
  <rule org=""acme"" module=""*test"" artifact=""test*"" type=""source"" ext=""jar"" matcher=""glob"" /> 
</exclude>
{code}
Note that the conf in the first rule corresponds to conf of the module in which I define the exclusion rule, and restricts the exclusion to this conf only (similar to the conf attribute on the exclude element of dependency).

In ivyconf, either use the same syntax, or use the modules section to define exclusion the same way conflict manager are defined. The advantage of the latter is that it follows the existing syntax, but I think it's less clear, and doesn't allow to exclude artifacts only. If we follow the same syntax as in ivy files, the conf wouldn't be allowed since it has no meaning.",smartshark_2_2,279,ant-ivy,"""[0.9984000325202942, 0.0015999373281374574]"""
62,228321,Add retrieve task 'overwriteMode' option to commandline,"Right now, the default overwriteMode during retrieve is 'newer'.
This is causing problems for us now (incorrect artifacts being loaded).

Since we don't use ant, but commandline access to Ivy, there's no way for us to fix this issue.
It would be great to either:
- add a commandline option to manage overwriteMode
- add overwriteMode to ivysettings.xml, so it can be overridden there",smartshark_2_2,1200,ant-ivy,"""[0.9984657764434814, 0.001534259063191712]"""
63,228322,Add new cache information to resolver doc,Patch attached to add description of cache attribute to resolver doc.,smartshark_2_2,1121,ant-ivy,"""[0.9982784986495972, 0.001721482607536018]"""
64,228323,allow setting default for useOrigin,"it would be very nice to be able to set a property that defines the default value for useOrigin, for example, ivy.useOrigin. it would default to false for backwards compatibility. in a use case where only a local (filesystem) repo is used, it is tedious to set useOrigin on every ivy:resolve task. in our case, we have a large project with 50 modules and over 50 calls to resolve.",smartshark_2_2,276,ant-ivy,"""[0.9983981251716614, 0.0016018105670809746]"""
65,228324,"A conflict manager that is able to allow implementation changes but not api changes (1.2.x - OK), (1.x.x - Not OK)","We have a situation where our api changes are characterized by the first two version numbers and the third number characterizes implementation changes.

1.2.3 --> 1.2 5 --> 1.2.10 are implementation changes
whereas
1.2.3 --> 1.3.0 --> 2.0.0 are API changes.

We don't want the implementation changes to cause conflicts but the api changes must be signalled as conflicts.

I have implemented a simple RegexpConflictManager for this and I'll try to attach it to this issue.

Anders",smartshark_2_2,1383,ant-ivy,"""[0.9930803179740906, 0.006919738370925188]"""
66,228325,remove deprecated defaultCache setting from examples,"src/example/dependence uses deprecated settings defaultCache.

The DEPRECATED warning appears in the console screen:

http://ant.apache.org/ivy/history/latest-milestone/tutorial/dependence.html

This example should be updated to use caches defaultCache.",smartshark_2_2,1165,ant-ivy,"""[0.9984413981437683, 0.001558639225549996]"""
67,228326,Use file-system repository directly without cache,"Currently, all dependencies from all repositories are downloaded into cache first. Only then they are copied to lib directory or referenced by the ""cachepath"" or ""cachefilset"" Ant tasks.

I thought it would be better to refer file-system's artifacts directly without first copying them into cache. It saves disk space and copy time. It also doing maintainance easier, since all artifacts are managed in a network shared directory. No per-user additional unnecessary cache need to be used and maintained.

The alternative is to use a shared network cache to be shared among all users, but I am not sure this is a good design. Anyway, keeping it all simple is a well-known and proven design :-) so how about making it simple by not using a cache for file system repositories?",smartshark_2_2,965,ant-ivy,"""[0.9983148574829102, 0.0016851108521223068]"""
68,228327,"Remove standalone jars, ivy.xml and xsd from distribution","Currently when making a release we end up with ivy jars (core and classic), ivy.xml and ivy xsd in the distrib directory.. Now that we distribute Ivy in the maven repository, it doesn't make sense to provide these files in the distrib dir. 

ivy.xsd is a particular case, we should document in the ""make a release"" page that we should update it on our site. Maybe this could be automated in our release script.",smartshark_2_2,307,ant-ivy,"""[0.9982059001922607, 0.0017941428814083338]"""
69,228328,Make common attributes of Ant tasks documentation more obvious,"For example the ""inline"" attribute of the <ivy:retrieve> is not undocumented. <ivy:retrieve> extends an abstract class which defines that attribute. I think there could be more undocumented attributes on that and other tasks.",smartshark_2_2,476,ant-ivy,"""[0.9983745813369751, 0.001625336823053658]"""
70,228329,Import Bushel into Ivy core,"As discussed on the [ant-dev mailing list|http://mail-archives.apache.org/mod_mbox/ant-dev/201010.mbox/%3cB53D948C-5DA9-48D8-B81D-2F8C44DBA969@hibnet.org%3e], here is the code of [Bushel|http://code.google.com/p/bushel/] to import",smartshark_2_2,598,ant-ivy,"""[0.9984173774719238, 0.0015825899317860603]"""
71,228330,"Add a ""delete"" parameter to the retrieve task","When I do a retrieve I would like to tell the task to delete any artifact in the retrieve directory not in the artifact retrieved.
As a result, if upgrade comm-1.0.jar to comm-1.1.jar, Ivy retrieve task would have deleted the comm-1.0.jar because it is not in the conf retrieved",smartshark_2_2,2,ant-ivy,"""[0.9978442192077637, 0.002155828522518277]"""
72,228331,Let user specify Ivy file when using a post-resolve task,"When executing a post-resolve task, the task performs a resolve in some situations.
Add an extra attribute to the post-resolve task so the user can specify the Ivy file which must be resolved in these cases.",smartshark_2_2,740,ant-ivy,"""[0.9984626770019531, 0.0015373314963653684]"""
73,228332,support maven:classifier on dependency element,"Right now, I can say:

{code:xml}
<dependency org=""org.jboss.teiid.connectors"" name=""connector-accumulo"" conf=""ais.dist"" rev=""${teiid.version}"" transitive=""false"">
  <artifact name=""connector-accumulo"" maven:classifier=""lib""/>
</dependency>
{code}

But this does not work:

{code:xml}
<dependency org=""org.jboss.teiid.connectors"" name=""connector-accumulo"" conf=""ais.dist"" rev=""${teiid.version}"" transitive=""false"" maven:classifier=""lib"" />
{code}

which would be simpler.

",smartshark_2_2,1338,ant-ivy,"""[0.9618808627128601, 0.03811917081475258]"""
74,228333,install task does not allow specification of conf,"When running <ivy:install...> task there is no option to specify which config should be considered.  As it is currently, it seems the default conf is the one that is used, therefore if a dependency exists which does not specify a mapping from master default conf, the dependency will not be included during the install.  In that case the artifacts installed in the *to* resolver will *not* be able to resolve all needed artifacts under some configurations.  

For example, say I had a module defined as in the following example:
<info organisation=""my-org"" module=""my-module"" revision=""1.0""/>
  <configurations>
    <conf name=""default"" visibility=""public"" description=""runtime dependencies and master artifact can be used with this conf"" extends=""runtime,master""/>
    [...]
    <conf name=""endorsed"" visibility=""public"" description=""this scope is used for components/lib/endorsed dependencies.""/>
  </configurations>
  <publications>
    <artifact name=""my-org"" ext=""jar""/>
  </publications>
  <dependencies>
    <dependency org=""org.apache.xerces"" name=""xml-apis"" rev=""2.9.1"" conf=""endorsed-&gt;default""/>
  </dependencies>
</ivy-module>

In that case the ant task to install this module from my-resolver to another-resolver would look something like this:
    <ivy:install organisation=""my-org"" module=""my-module"" revision=""1.0"" from=""my-resolver"" to=""another-resolver"" transitive=""true"" overwrite=""true""/>
This task would *not* install org.apache.xerces#xml-apis;2.9.1 onto another-resolver.  As a result, using <ivy:retrieve...conf=""endorsed""/> against another-resolver would fail since it would attempt to find dependency org.apache.xerces#xml-apis;2.9.1 which does not exist in the repository (since it was never installed).

<ivy:install...> should have the option of specifying which configuration(s) should be considered.  If not specified then it should default to * meaning all configurations defined in module being configured should be considered when locating which modules should be installed.",smartshark_2_2,1388,ant-ivy,"""[0.4931371510028839, 0.5068628787994385]"""
75,228334,Maven Dependency Management is not used to determine artifact version,"We recently hit an issue with resolving some POMs with the ibiblio resolver and usepoms=true. The problem is that the version for a dependency can be stored in the dependency management section of a parent POM.

The apache directory server is an example.

http://repo1.maven.org/maven2/org/apache/directory/server/apacheds-server-main/1.5.0/apacheds-server-main-1.5.0.pom


Note that the POM for the parent build artifact has the version information for the last 5 dependencies.

The PomModuleDescriptorParser currently only builds a dependency descriptor if the groupId, artifactId and version are all set. So for the case of apache directory server, many of the transitive dependencies are ignored.

I have been working on an extension to the PomModuleDescriptorParser that build the dependency management information at the end of the project/parent element by parsing the parent chain. Then if a dependency is found without a version, then the dependency management information is used.  I attach the patch after some more testing.",smartshark_2_2,293,ant-ivy,"""[0.3181604743003845, 0.6818395853042603]"""
76,228335,Add file inclusion in configuration xml file,"It would be nice to be able to include another ivyconf file in an ivyconf file. It would especially be useful with macrodef, defining macro in one file and using it in another.",smartshark_2_2,775,ant-ivy,"""[0.9977549910545349, 0.0022449668031185865]"""
77,228336,"returnFirst should be attribute of chain resolver, rather than standard resolver","Currently, the returnFirst setting is an attribute of standard resolvers.

This creates a significant problem with the use case where there is a standard resolver which should be returned first in some chains, but not in others.   The only workaround is to define duplicates of the same resolver, one where returnFirst is true, and one where it is false.   This kludge can cause the number of resolvers to increase geometrically if there are many of them.

I think this could be easily solved by making returnFirst an attribute of chain resolvers.   Any chain resolver where returnFirst is set would return the first resolver in the chain.   ",smartshark_2_2,1313,ant-ivy,"""[0.9877908229827881, 0.012209190987050533]"""
78,228337,Unable to specify xsi:schemaLocation or xsi:noSchemaLocation with ivy.xsd,"I'm trying to get auto-completion of ivy.xml in eclipse but I just can make it work :(

h2. ivy.xml with defaultNamespace
{code:xml} 
<?xml version=""1.0"" encoding=""UTF-8""?>
<ivy-module version=""2.0""
	xmlns=""http://ant.apache.org/ivy""
	xmlns:xsi=""http://http://www.w3.org/2001/XMLSchema-instance""
	xsi:noSchemaLocation=""http://ant.apache.org/ivy/schemas/ivy.xsd"">
	
	<info organisation=""com.blogspot.francisoud.ivy"" module=""ivyxsd"" />
	<configurations defaultconfmapping=""default"">
		<conf name=""default"" />
		<conf name=""source"" />
		<conf name=""javadoc"" />
	</configurations>
	<publications>
		<artifact name=""ivyxsd"" type=""jar"" conf=""default"" ext=""jar"" />
		<artifact name=""ivyxsd"" type=""source"" conf=""source"" ext=""zip"" />
		<artifact name=""ivyxsd"" type=""javadoc"" conf=""javadoc"" ext=""zip"" />
	</publications>
	<dependencies defaultconf=""default"">
		<dependency org=""commons-logging"" name=""commons-logging"" rev=""1.1"" transitive=""false"" />
		<dependency org=""log4j"" name=""log4j"" rev=""1.2.15"" transitive=""false"" />
		<dependency org=""com.sun.jersey"" name=""jersey-server"" rev=""1.0-ea-SNAPSHOT"" />
	</dependencies>
</ivy-module>
{code} 

* Error message
{noformat}
[ivy:retrieve] :: Ivy 2.0.0 - 20090108225011 :: http://ant.apache.org/ivy/ ::
:: loading settings :: file = E:\development\eclipse\workspaces\spot\Ivy-Xsd\ivy\ivysettings.xml
[ivy:retrieve] [xml parsing: ivy.xml:5:67: cvc-elt.1: Cannot find the declaration of element 'ivy-module'. in file:/E:/development/eclipse/workspaces/
spot/Ivy-Xsd/ivy.xml
[ivy:retrieve] ]
{noformat}

h2. ivy.xml with targetNamespace
{code:xml} 
<?xml version=""1.0"" encoding=""UTF-8""?>
<ivy:ivy-module version=""2.0""
		xmlns:ivy=""http://ant.apache.org/ivy""
		xmlns:xsi=""http://http://www.w3.org/2001/XMLSchema-instance""
		xsi:schemaLocation=""http://ant.apache.org/ivy http://ant.apache.org/ivy/schemas/ivy.xsd"">
	<ivy:info organisation=""com.blogspot.francisoud.ivy"" module=""ivyxsd"" />
	<ivy:configurations defaultconfmapping=""default"">
		<ivy:conf name=""default"" />
		<ivy:conf name=""source"" />
		<ivy:conf name=""javadoc"" />
	</ivy:configurations>
	<ivy:publications>
		<ivy:artifact name=""ivyxsd"" type=""jar"" conf=""default"" ext=""jar"" />
		<ivy:artifact name=""ivyxsd"" type=""source"" conf=""source"" ext=""zip"" />
		<ivy:artifact name=""ivyxsd"" type=""javadoc"" conf=""javadoc"" ext=""zip"" />
	</ivy:publications>
	<ivy:dependencies defaultconf=""default"">
		<ivy:dependency org=""commons-logging"" name=""commons-logging"" rev=""1.1"" transitive=""false"" />
		<ivy:dependency org=""log4j"" name=""log4j"" rev=""1.2.15"" transitive=""false"" />
		<ivy:dependency org=""com.sun.jersey"" name=""jersey-server"" rev=""1.0-ea-SNAPSHOT"" />
	</ivy:dependencies>
</ivy:ivy-module>
{code} 

* Error message:
{noformat}
[ivy:retrieve] [xml parsing: ivy.xml:5:92: cvc-elt.1: Cannot find the declaration of element 'ivy:ivy-module'. in file:/E:/development/eclipse/workspa
ces/spot/Ivy-Xsd/ivy.xml
[ivy:retrieve] , unknown tag ivy:ivy-module in file:/E:/development/eclipse/workspaces/spot/Ivy-Xsd/ivy.xml
[ivy:retrieve] ]
{noformat}


Same error message in eclipse, what am I missing ?

Fix? Just add {noformat}targetNamespace=""http://ant.apache.org/ivy""{noformat} attribute on root node in ivy.xsd ?",smartshark_2_2,15,ant-ivy,"""[0.6025609970092773, 0.39743903279304504]"""
79,228338,Specify multiple maven 2 repositories easily without settings tweaking,"It would be nice if we can specify multiple maven 2 repositories easily without settings tweaking, say by specify Ant properties?

To keep things simple, we try not to modify the Ivy's default settings and keep the Ant and Ivy configuration files as small as possible.

See also mailing-list  subject:Insert JAR into Ivy's cache?
",smartshark_2_2,450,ant-ivy,"""[0.9983806610107422, 0.0016192840412259102]"""
80,228339,Add command line support for installing projects from one repository to another. ,"When using Ivy from Ant, there is a task that allows to install a project from one repository to another (this is most often used to ""download"" projects from an online repository to an enterprise, shared or local one). This can't be done when using Ivy in standalone mode: there is no command line option for it. Request that support for this is added. ",smartshark_2_2,56,ant-ivy,"""[0.9985137581825256, 0.0014862642856314778]"""
81,228340,Broken link : http://incubator.apache.org/ivy/doc/concept#circular.html,"The page http://incubator.apache.org/ivy/doc/configuration/conf.html has a link to http://incubator.apache.org/ivy/doc/concept#circular.html which is broken.

In the page http://incubator.apache.org/ivy/doc/configuration/conf.html,
the link : <a href=""../../doc/concept#circular.html""> should be replaced by <a href=""../../doc/concept.html#circular""> 
(or use [ ] notation.  I don't if it works)


",smartshark_2_2,929,ant-ivy,"""[0.9961097836494446, 0.0038902556989341974]"""
82,228341,"FAQ question ""why two XML files"" still refers to 'configuration files' instead of 'settings files'","The FAQ question on ""why two XML files"" refers to there being configuration files and ivy files. However, the terminology page states that as of Ivy 2.0, configuration files are now known as settings files. ",smartshark_2_2,35,ant-ivy,"""[0.9959360361099243, 0.004063980653882027]"""
83,228342,resolver attribute for buildnumber task,"The BuildNumber Task should have an resolver Attribute that allows to specify a resovler used for searching existing versions. Currently the task looks at all configured resolvers. This is a performance issue in our environment where we have different ""views"" of the same repository (url, filesystem, ssh)  and the correct view is picked depending on where the build happens.",smartshark_2_2,22,ant-ivy,"""[0.9984563589096069, 0.0015436093090102077]"""
84,228343,ivy:makepom - Name,The following patch uses the module name for the <name> element in the generated POM file,smartshark_2_2,594,ant-ivy,"""[0.9869365096092224, 0.013063447549939156]"""
85,228344,doc/settings/resolvers.html should contain example of how to add resolved into existing/default chain,"Maybe something like this:
{code:xml}
<ivysettings>
    <settings defaultResolver=""default""/>
    <include url=""${ivy.default.conf.dir}/ivysettings-public.xml""/>
    <include url=""${ivy.default.conf.dir}/ivysettings-shared.xml""/>
    <resolvers>
        <ibiblio name=""robocode-public"" m2compatible=""true"" root=""http://robocode.sourceforge.net/maven2""/>
        <chain name=""web"" dual=""true"">
            <resolver ref=""robocode-public""/>
            <resolver ref=""public""/>
        </chain>
        <chain name=""main"" dual=""true"">
            <resolver ref=""shared""/>
            <resolver ref=""web""/>
        </chain>
    </resolvers>
    <include url=""${ivy.default.conf.dir}/ivysettings-local.xml""/>
    <include url=""${ivy.default.conf.dir}/ivysettings-default-chain.xml""/>
</ivysettings>
{code}
BTW: quality of documentation is excellent :-)
Cheers!",smartshark_2_2,95,ant-ivy,"""[0.9980143308639526, 0.00198565237224102]"""
86,228345,Scope the settings credentials,"The credentialStore is not scoped yet.

See IvyAntSettings.java",smartshark_2_2,1011,ant-ivy,"""[0.9751209020614624, 0.02487906627357006]"""
87,228346,makepom child element dependency should support the type and classifier attributes.,"It would be nice to be able to create a POM file with a dependency which has specific <type> and <classifier>.

Example of wish:

        <ivy:makepom ivyfile=""./ivy.xml"" pomfile=""${ant.project.name}.pom"">
            <mapping conf=""default"" scope=""compile""/>
            <mapping conf=""runtime"" scope=""runtime""/>
            <dependency 
                group=""com.company.component""
                artifact=""${ant.project.name}""
                version=""1.0.0""
                type=""txt""
                classifier=""license""/>
        </ivy:makepom>

Currently the type="""" and classifier="""" attributes are not available.",smartshark_2_2,1431,ant-ivy,"""[0.9984558820724487, 0.001544119673781097]"""
88,228347,include jsch classes in ivy jar,It would be good include copy of jsch in ivy.jar,smartshark_2_2,12,ant-ivy,"""[0.9950799942016602, 0.004919955972582102]"""
89,228348,Allow to resolve dependencies directly without using an ivy file.,"In some cases it would be interesting to use resolve withtout using an ivy file, but by simply defining one dependency. In ant, the resolve task could be used for that. It should then look like that:

<ivy:resolve organisation=""apache"" module=""commons-lang"" revision=""2+""/>",smartshark_2_2,959,ant-ivy,"""[0.9982656836509705, 0.001734295510686934]"""
90,228349,Display full graph when a circular dependency is detected,"When a circular dependency is detected, the nodes in the loop are printed. The last node is printed as ""..."". This way, we loose information, like which node is causing the problem. Therefor, it would be better if the last node was also printed (instead of ""..."").

cfr. IVY-672",smartshark_2_2,297,ant-ivy,"""[0.998149037361145, 0.0018509491346776485]"""
91,228350,List in a single page the pattern matcher and always link to it from others.,"http://www.jayasoft.org/ivy/doc/configuration/module

Is missing the the list of possible matchers to use. It should send back to another page listing them.

For instance if we go to http://www.jayasoft.org/ivy/doc/concept
Then we have a list of 3 default matchers.


",smartshark_2_2,243,ant-ivy,"""[0.998498797416687, 0.001501242397353053]"""
92,228351,conf not supported as pattern filtering token,"it seems that [conf] is not currently supported for use as an artifact pattern in an ivy:publish task or as a pattern token for use in resolvers (as raised in ivy-user list here: http://www.nabble.com/publishing-configurations-tf3555931.html). With configurations being such a key component of ivy, it would be nice if the [conf] pattern were better supported across different filtering processes. ",smartshark_2_2,546,ant-ivy,"""[0.980933427810669, 0.01906663551926613]"""
93,228352,Ivy doesn't work with Ant 1.6.2,"Ivy doesn't work with Ant 1.6.2 as stated on the website.
cfr http://incubator.apache.org/ivy/download.html",smartshark_2_2,457,ant-ivy,"""[0.9564163088798523, 0.04358363524079323]"""
94,228353,Add support for using ivysettings.xml properties in HttpClientHandler,"Currently, the only way to set proxy settings is via the http.proxy* system properties.  However, Ivy has the ability to specify properties in the ivysettings.xml file as well.  So this request is to add support for looking for properties from ivysettings.xml if they aren't found in the system properties.  For example, by adding this to the ivysettings.xml:

    <property name=""http.proxyHost"" value=""myproxy"" />
    <property name=""http.proxyPort"" value=""8080"" />

This way ivysettings.xml files that point to corporate proxy servers could be checked into version control to avoid each team member having to specify this.  However, if for some reason they need to override those settings, they could do so by setting system properties.

It looks like supporting this would require a change to the HttpClientHandler.configureProxy method and also to pass the settings object into the URLHandlerRegistry singleton initialization (or optionally just to stuff the properties into a Properties object and pass that into the URLHandlerRegistry singleton initialization).

",smartshark_2_2,102,ant-ivy,"""[0.9985062479972839, 0.0014936961233615875]"""
95,228354,Exclude modules from install task,"I would like the ability to exclude/ignore dependencies when making use of the ivy:install task. Log4J, for example, depends on various Sun libraries that aren't included in any repository, so to get it to install without error, I have to set transitive=""false"" and add the other dependencies as yet more install tasks. There are also other cases where I might not want to try and install a dependency. I'm not sure if I would want them to be excluded from the Ivy file, or simply not downloaded, but still included in the Ivy file.",smartshark_2_2,618,ant-ivy,"""[0.9984754920005798, 0.0015244872774928808]"""
96,228355,Explicitly specify artifact download URL,"It would be great if I could specify URL, where artifact can be downloaded from, like

<artifact url=""http://some-location/lib.jar""/>

It could be useful, for example, if some library is released but not yet uploaded to ibiblio (Spring 2.0-rc3 is released today, but not on ibilblio yet), so I could use latest version right now, and drop url attribute when uploaded.",smartshark_2_2,389,ant-ivy,"""[0.9983763694763184, 0.0016235541552305222]"""
97,228356,ivy standalone does not return error status,Please see: http://www.jayasoft.org/node/432,smartshark_2_2,874,ant-ivy,"""[0.5054107308387756, 0.49458926916122437]"""
98,228357,Resolve Slowness in 1.4.1,"Copied from e-mail log:

On 12/14/06, David R Robison <drrobison@openroadsconsulting.com> wrote:
>
> Is there any plans to fix this in the next release? It is really causing
> problems with time required to compile our system. Thanks, David Robison


JIRA is now migrated on ASF infrastructure, so you can use it to file an
improvement issue. Then we'll see if somebody in the community provide a fix
for it. I can't say if it'll be part of the next release or not, the plan
for the next release is more focused on apache migration, code cleaning and
additional testing for the moment.

Xavier

Xavier Hanin wrote:
> > On 11/21/06, David R Robison <drrobison@openroadsconsulting.com> wrote:
> >>
> >> I'm attaching the log from the ""ant -d"". The following, from the log,
> >> looks interesting:
> >>
> >> [ivy:retrieve]     using default-chain to resolve [ xmlBlaster |
> >> xmlBlaster | 1.4 ]
> >> [ivy:retrieve] default-chain: no latest strategy defined: using default
> >> [ivy:retrieve] orci: no namespace defined: using system
> >> [ivy:retrieve] pre 1.3 ivy file: using exactOrRegexp as default matcher
> >> [ivy:retrieve]     found ivy file in cache for [ xmlBlaster |
> xmlBlaster
> >> | 1.4 ] (resolved by orci):
> >>
> >>
> C:\Workspaces\DataGateway\DataGateway-VDOT-OnCall-Task36\ivy\ivy-cache\xmlBlaster\xmlBlaster\ivy-
> >>
> >> 1.4.xml
> >> [ivy:retrieve]     orci: found revision in cache: [ xmlBlaster |
> >> xmlBlaster | 1.4 ] (resolved by orci): but it's a default one, maybe we
> >> can find a better one
> >> [ivy:retrieve] orci: no latest strategy defined: using default
> >> [ivy:retrieve]      trying
> >>
> >>
> http://www.openroadsconsulting.com/maven/xmlBlaster/jars/xmlBlaster-1.4.jar
> >>
> >> [ivy:retrieve]     orci: no ivy file found for [ xmlBlaster |
> xmlBlaster
> >> | 1.4 ]: using default data
> >> [ivy:retrieve]         tried no ivy pattern => no attempt to find
> module
> >> descriptor file for [ xmlBlaster | xmlBlaster | 1.4 ]
> >> [ivy:retrieve]     checking [ xmlBlaster | xmlBlaster | 1.4 ][default]
> >> from orci against null
> >> [ivy:retrieve]     module revision kept as first found: [ xmlBlaster |
> >> xmlBlaster | 1.4 ][default] from orci
> >> [ivy:retrieve] ibiblio: no namespace defined: using system
> >> [ivy:retrieve] pre 1.3 ivy file: using exactOrRegexp as default matcher
> >> [ivy:retrieve]     found ivy file in cache for [ xmlBlaster |
> xmlBlaster
> >> | 1.4 ] (resolved by orci):
> >>
> >>
> C:\Workspaces\DataGateway\DataGateway-VDOT-OnCall-Task36\ivy\ivy-cache\xmlBlaster\xmlBlaster\ivy-
> >>
> >> 1.4.xml
> >> [ivy:retrieve] found module in cache but with a different resolver:
> >> discarding: [ xmlBlaster | xmlBlaster | 1.4 ]
> >>
> >>
> >> It finds it in the cache but thinks there may be a better one so it
> >> keeps looking. It eventually scans through all the configured
> >> repositories and ends up using the one in the cache. Does anyone know
> >> why it would not just use the one in the cache? Does this help any?
> >
> >
> > OK, the problem is due to a modification in Ivy 1.4 which has a bad side
> > effect in your case. When Ivy find a default module descriptor (i.e. a
> > module descriptor generated because there is no ivy file for a
> > module), then
> > in a chain it checks if there is no other element in the chain with a
> > real
> > ivy file. This is the cause of your performance problem I think, so
> > the only
> > workaround I see is to add an ivy file even for simple modules with
> > only one
> > artifact. I'd also suggest submitting an improvement request in jira,
> but
> > jira is currently in read only mode until it is migrated to apache.
> >
> > Xavier
> >
> > Thanks, David
> >>
> >>
> >
>
> -- 
>
> David R Robison
> Open Roads Consulting, Inc.
> 708 S. Battlefield Blvd., Chesapeake, VA 23322
> phone: (757) 546-3401
> e-mail: drrobison@openroadsconsulting.com
> web: http://openroadsconsulting.com
> blog: http://therobe.blogspot.com
> book: http://www.xulonpress.com/bookstore/titles/1597816523.htm
>
>
>
>
>

",smartshark_2_2,250,ant-ivy,"""[0.9547370672225952, 0.04526297748088837]"""
99,228358,still use defaultconf when defaultconfmapping is defined,"Per this forum post:

http://www.nabble.com/defaultconf-and-defaultconfmapping-td25582330.html#a25582330

I think it would be possible (and more intuitive) if defaultconf and defaultconfmapping could be used at the same time.",smartshark_2_2,171,ant-ivy,"""[0.8621644973754883, 0.13783545792102814]"""
100,228359,Provide anchors in the FAQ,If the FAQ would contain anchors (<a name>) you could point directly to a FAQ-entry.,smartshark_2_2,446,ant-ivy,"""[0.9982619881629944, 0.001737968879751861]"""
101,228360,Need global exclude,"If I have two depedencies each of them depends on xerces, for example, and I don't want to download xerces, I have to add <exclude/> tag near description of each dependency. That's not handy, I think ivy.xml shoud accept <exclude/> tag as child of <dependencies/> element, to exclude specified dependency from all dependencies.",smartshark_2_2,994,ant-ivy,"""[0.9743075966835022, 0.025692375376820564]"""
102,228361,support resolve refresh in command line,There's currently no option to run a resolve in refresh mode when used from command line,smartshark_2_2,20,ant-ivy,"""[0.9980403780937195, 0.0019596414640545845]"""
103,228362,provide versioned doc online,"The current online version of the documentation reflects the latest change, and can cause confusion for new users who download Ivy 1.4.1.

The suggestion is to provide a snapshot of the 1.4.1 doc as well as latest doc version online.",smartshark_2_2,912,ant-ivy,"""[0.9974150657653809, 0.0025848594959825277]"""
104,228363,Links section contains Codehaus Repository,"The Home > Links Section (http://ant.apache.org/ivy/links.html) , points to ""Codehaus Maven Repository"", which has been shut down.

See: http://www.codehaus.org/
{quote}All Codehaus services have now been terminated.{quote}

That should be removed.",smartshark_2_2,967,ant-ivy,"""[0.9956912398338318, 0.004308769479393959]"""
105,228364,"Maven accepts illegal XML for its pom's, Ivy not."," Maven seems to generate illegal XML for its pom's. For example if the developers have a special character in there name.

Two examples:

http://repo1.maven.org/maven2/qdox/qdox/1.6/qdox-1.6.pom
http://repo1.maven.org/maven2/org/codehaus/plexus/plexus/1.0.4/plexus-1.0.4.pom

I have run lately a couple of times into problems with this. Maven itself seems to be happy with that. Ivy definitely not:

[ivy:retrieve] :::: ERRORS
[ivy:retrieve]  The entity ""oslash"" was referenced, but not declared.",smartshark_2_2,1302,ant-ivy,"""[0.4026755392551422, 0.5973244905471802]"""
106,228365,typos on quickstart page,as in summary.  Will attach patch.,smartshark_2_2,265,ant-ivy,"""[0.9965560436248779, 0.0034440169110894203]"""
107,228366,Exclude Ant jar from bin-with-deps distribution format,"Currently we package in bin-with-deps distribution all Ivy dependencies, including ant jar. But this doesn't make sense, people who want to use Ivy with Ant will first install Ant, then Ivy. So we should exclude ant jar from our bin-with-deps distributions format.",smartshark_2_2,477,ant-ivy,"""[0.9978662133216858, 0.002133865375071764]"""
108,228367,Private configs are ignored when using ivy retrieve target with a * for conf,"In Ivy 1.4.1, using a conf=""*"" for the retrieve task does not retrieve jars in private configurations.  This worked in Ivy 1.4.",smartshark_2_2,334,ant-ivy,"""[0.12205039709806442, 0.877949595451355]"""
109,228368,branch and extra attributes missing from info Ant task,The ivy:info ant task does not populate ant properties with the branch attribute or any extra attributes given in the info tag metadata,smartshark_2_2,1108,ant-ivy,"""[0.9387721419334412, 0.061227843165397644]"""
110,228369,Make ivy.xml <conf description> available,"You can (an IMHO should) write an explanation of the configuration in the ivy file
<ivy-module version=""2.0"">
    <configurations>
        <conf name=""runtime"" description=""Runtime Libraries""/>
    </configurations>
</ivy-module>

The Ant task <ivy:info/> makes the names names available, but not the descriptions. 
So the first RFE is making them available (accoding to 'infotest.configurations') as 'infotest.configuration.descriptions'.

Another RFE is enhanding <ivy:retrieve/> to create a describing file in the destination directory which contains this information.
E.e. with Ivy pattern lib/[conf]/[artifact].[ext] you would create with <ivy:retrieve descriptionfile=""readme.txt""/> a file lib/runtime/readme.txt with content ""Runtime Libraries"".
(!! Recreate old files, but beware that multiple confs could result in the same dir if the ivy pattern doesnt contain a [conf]. )
",smartshark_2_2,175,ant-ivy,"""[0.9985702037811279, 0.0014297999441623688]"""
111,228370,Describe what does m2compatible,"http://www.jayasoft.org/ivy/doc/resolver/filesystem

Would be nice to describe what exactly is doing 'm2compatible'.
I assume from using it that it does transform the directory into 'com.company' into 'com/company' but I cannot be sure it is the only thing it does.",smartshark_2_2,888,ant-ivy,"""[0.9984666109085083, 0.0015334096970036626]"""
112,228371,Support for URL's with unknown content length in URLResolver,"Original email thread sent to ivy-users:

I've been trying to use the URLResolver with a custom protocol that
returns -1 for URLConnection.getContentLength(). The problem I'm seeing
is that for my custom protocol, resources can never be found.

After a little debugging I found the following code in
org.apache.ivy.util.url.BasicURLHandler.getURLInfo() which explains why
I'm running into issues:


    public URLInfo getURLInfo(URL url, int timeout) {

...
            if (con instanceof HttpURLConnection) {
               ...
            } else {

                int contentLength = con.getContentLength();

                if (contentLength <= 0) {

                    return UNAVAILABLE;

                } else {

                   ...
                }

            }

        } 

Is this a bug in the URLResolver? For certain resources which are
streamed, the content length will not be known so I'm not sure that
UNAVAILABLE should be dependent on the content length.

Additional comment:

It's unclear how an unknown content length should be supported because there appears to be no standard way to determine if a resource is unavailable. I wrote a quick patch for Ivy which involved checking for availability by trying to open an InputStream from the URLConnection and seeing if an IOException is thrown. This is not ideal but worked as a workaround when the content length was -1. 

This is not a critical issue that I personally need fixing because I've updated my custom protocol to return the content length, however it is something to be aware of since a content length of -1 means unknown rather than unavailable.
",smartshark_2_2,661,ant-ivy,"""[0.5436416864395142, 0.45635828375816345]"""
113,228372,Enable access to extra attibutes at any level,"It would be great if we can access extra attributes supplied at the info level of a resolved dependency.
See below mail thread for more information.


On Wed, Apr 16, 2008 at 4:58 PM, Foreman, Alex (IT) < Alexander.Foreman@morganstanley.com> wrote:

> If I had for instance:
> <info organisation=""apache"" ......  Color=""blue"">
>
> When I use a trigger I cannot access the color.  Or at least I cannot 
> see how to. For instance: ${dep.download-size} prints but I cannot 
> seem to get access to the extras.
>
> Is this documented anywhere?  Or shall I add a JIRA?

Checking the code, it seems that some extra attributes should be available, but maybe not those that you'd like...

The extra attributes which are available in pre and post resolve-dependency are the extra attributes you have on your dependency declaration, not those declared in the module descriptor of the dependency.

For the pre-reslve-dependency event, we can't do better, since before resolving the dependency we don't have the module descriptor yet. In the post resolve dependency, we could improve Ivy to put the extra attributes declared in the resolved module.

Since it's so easy to do I've just checked in a change in Ivy which should make these extra attributes available in post-resolve-dependency. Could you check it works for you? Whatever the result, could you please open an issue in jira at least to ask for documenting this?

Xavier


>
>
> Alex Foreman
> Morgan Stanley | Technology
> 20 Cabot Square | Canary Wharf | Floor 06 London, E14 4QW
> Phone: +44 20 7677-5732
> Alexander.Foreman@morganstanley.com
> --------------------------------------------------------
>
> NOTICE: If received in error, please destroy and notify sender. Sender 
> does not intend to waive confidentiality or privilege. Use of this 
> email is prohibited when received in error.
>",smartshark_2_2,512,ant-ivy,"""[0.9985007047653198, 0.0014993150252848864]"""
114,228373,configuration groups,"In modules having a lot of configurations, sometimes it would be nice to be able to define a group of configurations based on their attributes (including extra attributes). The proposed syntax is {{*[att=value]}}

For instance, one could define an extra attribute named 'axis' on the configuration, and then refer to all configurations where 'axis' is 'platform' with {{*[axis=platform]}}. 

Here is an example of Ivy file using this notation:
{noformat}
<ivy-module version=""1.0"" xmlns:e=""http://ant.apache.org/ivy/extra"">
	<info organisation=""org5""
	       module=""mod5.1""
	       revision=""4.5""
	       status=""integration""
	       publication=""20090501110000""
	/>
	<configurations>
		<conf name=""A"" />
		<conf name=""B"" />
		
		<conf name=""windows"" e:axis=""platform"" />
		<conf name=""linux"" e:axis=""platform""/>
	</configurations>
	<publications>
		<artifact name=""art51A"" type=""jar"" conf=""A,*[axis=platform]""/>
		<artifact name=""art51B"" type=""jar"" conf=""B,*[axis=platform]""/>
		<artifact name=""art51B"" type=""dll"" conf=""B,windows""/>
		<artifact name=""art51B"" type=""so"" conf=""B,linux""/>
	</publications>
	<dependencies>
		<dependency org=""org1"" name=""mod1.2"" rev=""2.0"" conf=""B,*[axis=platform]->default""/>
	</dependencies>
</ivy-module>
{noformat}

In this ivy file, {{*[axis=platform]}} is equivalent to {{windows,linux}}",smartshark_2_2,120,ant-ivy,"""[0.9983978867530823, 0.0016020667972043157]"""
115,228374,HOWTO - Resolve dependencies on demand,"Scenario: To run task A, B, C, etc, only a common set of jars/zips (the set D) is needed; to run task X, a large number of jars/zips (the set Y) is needed. In most cases, task A, B, C, etc will be run. And in very rare cases, task X needs to be performed.

Issue: We don't want ivy to resolve (and may retrieve) the jars/zips in set D and Y every time we run the ant task, due to the performance concern and our internal network latency.
",smartshark_2_2,1019,ant-ivy,"""[0.9981080293655396, 0.0018919917056337]"""
116,228375,Artifact requests with a .xml.zip extension fail with a 404 Error,"This issue might also apply to any multi dots extension request but I didn't check.

Regards,

CÃ©dric
",smartshark_2_2,1236,archiva,"""[0.19224104285240173, 0.8077589273452759]"""
117,228376,"Register link appears after logging in and back out even ""Disable registration Link"" is checked","I have the ""Disable registration Link"" checkbox checked and initially hitting the Archiva homepage the register button is not shown.

But after I have logged in and then log out, the link appears and is clickable",smartshark_2_2,1302,archiva,"""[0.2602783441543579, 0.7397216558456421]"""
118,228377,SearchService.quickSearch returns artifacts with null as repositoryId,"Executing following code results getting null's


List<Artifact> list = searchService.quickSearch(""test"");
for(Artifact a:list){
    System.out.println(a.getRepositoryId());
}",smartshark_2_2,1824,archiva,"""[0.0785115659236908, 0.9214884638786316]"""
119,228378,Unable to get artifacts from repositories which requires Repository Manager role using repository group,"To reproduce:
# Create managed repository 'releases'
# Create a new user account with the following credentials
#* username: newuser
#* password: pass1
# Assign Repository Manager role for 'releases' repository to newuser
# Deploy a test artifact to 'releases' repository. Make sure that the credentials are configured in settings.xml
# Create a new repository group named 'internal.group'
# Add the 'releases' repository and the pre-configured 'internal' repository to 'internal.group'
# Add the following configuration to your settings.xml:
{code}
<mirrors>
  <mirror>
    <id>internal.group</id>
   <name>Local Mirror</name>
   <url>http://localhost:8080/archiva/repository/internal.group</url>
   <mirrorOf>*</mirrorOf>
  </mirror>
<mirrors>
<servers>
  <server>
    <id>internal.group</id>
    <username>newuser</username>
    <password>pass1</password>
  </server>  
</server>
{code}
# Build a project with a dependency on the test artifact you deploy in step 4. The build would fail with unable to resolve artifact ... error.
# Edit guest account and assign Repository Observer role for repository 'releases'
# Build the project again. You would get a successful build.",smartshark_2_2,249,archiva,"""[0.18197761476039886, 0.8180223107337952]"""
120,228379,Search and Browse do not work for snapshots,"When I to browse or search an artifact with snapshots, I usually get an error when I try to go to the artifact info page. Looking at the logs, Archiva is complaining that the version (x.y.z-SNAPSHOT) does not match the pom file's version (x.y.z-timestamp-buildno).

Ideally, sequence for browse would be group->artifact->base version->snapshot version. In other words, I would first browse to the base version, under which would be listed all the snapshot versions. The pom snippet on the base version page would have <version>x.y.z-SNAPSHOT</version> and the pom snippet on the snapshot version page would have <version>x.y.z-timestamp-buildno</version>. I think this would be a lot more clear than how it currently works - mixing base versions and snapshot versions on the same page.",smartshark_2_2,1137,archiva,"""[0.14868289232254028, 0.8513171076774597]"""
121,228380,Duplicate repositories show up while editing.,"Environment: Using a pre 1.0-beta-2 archiva install that has been upgraded to 1.0-beta-2.
Situation: When editing the repositories, the repositories get duplicated.

This is likely due to the fact that the configuration ""upgrade"" technique from <repositories> to <managedRepositories> and <remoteRepositories> is not removing the <repositories> section properly..

Several efforts in the code have been made to correct this, but with no success. See revision 583903 in archiva/trunk for details.",smartshark_2_2,1262,archiva,"""[0.1398288905620575, 0.8601710796356201]"""
122,228381,"fail to resolve artifactId for libs that contain versionKeyword in artifactId, like ""maven-test-plugin""","Trying to get maven-test-plugin (using maven1) fails in 404. Archiva is searching for
""\maven\maven\test-plugin-1.8.2\maven-test-plugin-1.8.2.jar""

RepositoryLayoutUtils tries to detect the artifact Id by searching for some version keyword in the request string. As ""test[09]*"" is a version keyword, it resolves to ""maven"".

The attached path add the feature of checking that all tokens in the version part - all after artifactId expect the latest (classifier) - are valid version keywords.",smartshark_2_2,1718,archiva,"""[0.08437695354223251, 0.9156230688095093]"""
123,228382,"Creation of MetadataReposiotryIndex fails if provided index directory exists, but works if the directory does not exists","Here is  the stacktrace:

org.apache.maven.repository.indexing.RepositoryIndexException: C:\dev\index is not a valid index directory.
	at org.apache.maven.repository.indexing.AbstractRepositoryIndex.indexExists(AbstractRepositoryIndex.java:257)
	at org.apache.maven.repository.indexing.MetadataRepositoryIndex.deleteIfIndexed(MetadataRepositoryIndex.java:195)
	at org.apache.maven.repository.indexing.MetadataRepositoryIndex.indexMetadata(MetadataRepositoryIndex.java:173)
	at org.apache.maven.repository.indexing.MetadataRepositoryIndex.index(MetadataRepositoryIndex.java:67)
 ...

the directory exists, if you will pass the path that does not exists, all works fine.",smartshark_2_2,833,archiva,"""[0.08497226983308792, 0.9150276780128479]"""
124,228383,Users - Manage section sort by name doesn't work as expected,"Recently, I was able to get Archiva working with Microsoft Active Directory.

The mapped BaseDn for groups returns a very large number of users (although that BaseDn is way further down in the tree than the BaseDn that I actually need).  As a result, the ""Manage"" page has 2001 boxes rendering at the bottom of the screen for page navigation.

Archiva is telling me that my AD user account is locked, but it isn't.  I can still use my AD account credentials for other apps, such as Continuum, Bloodhound, and Jenkins.  So, now I'm trying to find my user by clicking on each of the 2001 pages.

I tried to use Sort by Name, however it doesn't seem to sort Names alphabetically.  Actually, ""Name"" could imply ""User Name"" or ""Full Name"".

I've attached a screen shot post-sort.",smartshark_2_2,1416,archiva,"""[0.1135895773768425, 0.8864104151725769]"""
125,228384,unable to schedule cron expressions that contain a comma,"while the save/load of the these expressions was fixed, it is now being scheduled incorrectly, resulting in the following error:

Cron expression [0 0\,30 * * * ?] for repository [snapshots] is invalid.  Defaulting to hourly.


",smartshark_2_2,1533,archiva,"""[0.08692777901887894, 0.9130722284317017]"""
126,228385,Archiva-plexus-runtime doesn't run,"Without changing any of the configuration files, I get the log included belog. Fiddling with the database configuration doesn't really help at all. Running jetty:run in archiva-webapp gives a java.lang.NoSuchMethodError: org.codehaus.plexus.DefaultPlexusContainer.<init>(Ljava/lang/String;Ljava/lang/ClassLoader;Ljava/io/Reader;Ljava/util/Map;)V

Using PLEXUS_HOME:   /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva
Using PLEXUS_CONF:   /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/conf
Using PLEXUS_TMPDIR: /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/temp
Using JAVA_HOME:     /usr/java/jdk1.5.0_06
[INFO] Services will be deployed in: '/home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/services'.
[INFO] Applications will be deployed in: '/home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps'.
[INFO] Service Supervisor is deploying plexus-appserver-service-jetty-2.0-alpha-3-SNAPSHOT.
[INFO] Loading on start [role,roleHint]: [org.codehaus.plexus.appserver.service.PlexusService,jetty]
Sep 21, 2006 9:44:37 PM org.mortbay.http.HttpServer doStart
INFO: Version Jetty/5.1.10
Sep 21, 2006 9:44:37 PM org.mortbay.util.Container start
INFO: Started org.mortbay.jetty.Server@80d1ff
[INFO] Application Supervisor is deploying archiva-plexus-application-1.0-SNAPSHOT.
[INFO] Extracting /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva-plexus-application-1.0-SNAPSHOT.jar to '/home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva'.
[INFO] Deploying application 'archiva' at '/home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva'.
[INFO] Using application configurator file /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva/conf/application.xml.
[INFO] Using appDir = /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva
[INFO] Deploying /home/wilfred/workspace/maven-repository-manager/archiva-plexus-runtime/target/archiva/apps/archiva/webapp with context path of /
[INFO] Using standard webapp classloader for webapp.
[INFO] Deploying appserver 'archiva'.
[INFO] Adding HTTP listener on *:8080
Sep 21, 2006 9:44:40 PM org.mortbay.http.SocketListener start
INFO: Started SocketListener on 0.0.0.0:8080
[INFO] Starting Jetty Context /
Sep 21, 2006 9:44:41 PM org.mortbay.util.FileResource <clinit>
INFO: Checking Resource aliases
Sep 21, 2006 9:44:41 PM org.mortbay.util.Container start
INFO: Started org.mortbay.jetty.servlet.WebApplicationHandler@15f7107
Sep 21, 2006 9:44:41 PM org.mortbay.jetty.servlet.ServletHandler$Context log
INFO: Loading plexus context properties from: '/WEB-INF/plexus.properties'
Sep 21, 2006 9:44:42 PM org.mortbay.jetty.servlet.ServletHandler$Context log
INFO: Could not load plexus context properties from: '/WEB-INF/plexus.properties'
2006-09-21 21:44:42,447 [main] INFO  PlexusContainer                - Loading on start [role]: [org.apache.maven.archiva.scheduler.RepositoryTaskScheduler]
2006-09-21 21:44:42,633 [main] INFO  RAMJobStore                    - RAMJobStore initialized.
2006-09-21 21:44:42,634 [main] INFO  StdSchedulerFactory            - Quartz scheduler 'defaultScheduler' initialized from an externally provided properties instance.
2006-09-21 21:44:42,634 [main] INFO  StdSchedulerFactory            - Quartz scheduler version: 1.4.5
2006-09-21 21:44:42,634 [main] INFO  QuartzScheduler                - Scheduler defaultScheduler_$_NON_CLUSTERED started.
2006-09-21 21:44:43,085 [main] WARN  ConfigurationStore             - Configuration file: /home/wilfred/.m2/archiva.xml not found. Using defaults.
2006-09-21 21:44:43,086 [main] INFO  RepositoryTaskScheduler        - Not scheduling indexer - index path is not configured
2006-09-21 21:44:43,086 [main] INFO  PlexusContainer                - Loading on start [role]: [org.apache.maven.archiva.web.ArchivaSecurityDefaults]
Sep 21, 2006 9:44:43 PM org.mortbay.jetty.servlet.ServletHandler$Context log
INFO: ********** FATAL ERROR STARTING UP PLEXUS-WEBWORK INTEGRATION **********
Looks like the Plexus listener was not configured for your web app!
You need to add the following to web.xml:

    <listener>
        <listener-class>org.codehaus.plexus.xwork.PlexusLifecycleListener</listener-class>
    </listener>
[ERROR] Error while deploying appserver archiva-plexus-application-1.0-SNAPSHOT.jar.
org.codehaus.plexus.appserver.ApplicationServerException: Error in the app server lifecycle post-app-container-init-service-call phase.
        at org.codehaus.plexus.appserver.application.deploy.DefaultApplicationDeployer.deployJar(DefaultApplicationDeployer.java:110)
        at org.codehaus.plexus.appserver.application.deploy.DefaultApplicationDeployer.deploy(DefaultApplicationDeployer.java:83)
        at org.codehaus.plexus.appserver.lifecycle.phase.ApplicationDeploymentPhase$1.onJarDiscovered(ApplicationDeploymentPhase.java:39)
        at org.codehaus.plexus.appserver.supervisor.DefaultSupervisor.scanDirectory(DefaultSupervisor.java:100)
        at org.codehaus.plexus.appserver.supervisor.DefaultSupervisor.scan(DefaultSupervisor.java:73)
        at org.codehaus.plexus.appserver.lifecycle.phase.ApplicationDeploymentPhase.execute(ApplicationDeploymentPhase.java:48)
        at org.codehaus.plexus.appserver.DefaultApplicationServer.start(DefaultApplicationServer.java:164)
        at org.codehaus.plexus.personality.plexus.lifecycle.phase.StartPhase.execute(StartPhase.java:16)
        at org.codehaus.plexus.lifecycle.AbstractLifecycleHandler.start(AbstractLifecycleHandler.java:102)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.startComponentLifecycle(AbstractComponentManager.java:110)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.createComponentInstance(AbstractComponentManager.java:100)
        at org.codehaus.plexus.component.manager.ClassicSingletonComponentManager.getComponent(ClassicSingletonComponentManager.java:92)
        at org.codehaus.plexus.DefaultComponentLookupManager.lookup(DefaultComponentLookupManager.java:77)
        at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:247)
        at org.codehaus.plexus.appserver.PlexusApplicationHost.start(PlexusApplicationHost.java:127)
        at org.codehaus.plexus.appserver.PlexusApplicationHost.main(PlexusApplicationHost.java:262)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
        at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
        at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
        at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: org.codehaus.plexus.appserver.application.deploy.lifecycle.AppDeploymentException: Error calling service in pre-app init phase.
        at org.codehaus.plexus.appserver.application.deploy.lifecycle.phase.AfterAppStartServiceSetupPhase.execute(AfterAppStartServiceSetupPhase.java:32)
        at org.codehaus.plexus.appserver.application.deploy.DefaultApplicationDeployer.deployJar(DefaultApplicationDeployer.java:101)
        ... 23 more
Caused by: org.codehaus.plexus.jetty.ServletContainerException: Error while starting the context /
        at org.codehaus.plexus.jetty.AbstractJettyServletContainer.startApplication(AbstractJettyServletContainer.java:162)
        at org.codehaus.plexus.service.jetty.JettyPlexusService.afterApplicationStart(JettyPlexusService.java:185)
        at org.codehaus.plexus.appserver.application.deploy.lifecycle.phase.AfterAppStartServiceSetupPhase.execute(AfterAppStartServiceSetupPhase.java:28)
        ... 24 more
Caused by: org.jpox.exceptions.ConnectionFactoryNotFoundException: Connection Factory ""java:comp/env/jdbc/users"" not found
NestedThrowables:
javax.naming.NameNotFoundException
        at org.jpox.AbstractPersistenceManagerFactory.lookupDataSource(AbstractPersistenceManagerFactory.java:175)
        at org.jpox.AbstractPersistenceManagerFactory.freezeConfiguration(AbstractPersistenceManagerFactory.java:212)
        at org.jpox.PersistenceManagerFactoryImpl.getPersistenceManagerFactory(PersistenceManagerFactoryImpl.java:99)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:534)
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:478)
        at org.codehaus.plexus.jdo.AbstractConfigurableJdoFactory.doConfigure(AbstractConfigurableJdoFactory.java:108)
        at org.codehaus.plexus.jdo.AbstractConfigurableJdoFactory.configure(AbstractConfigurableJdoFactory.java:75)
        at org.codehaus.plexus.jdo.AbstractConfigurableJdoFactory.getPersistenceManagerFactory(AbstractConfigurableJdoFactory.java:43)
        at org.codehaus.plexus.security.authorization.rbac.store.jdo.JdoTool.initialize(JdoTool.java:72)
        at org.codehaus.plexus.personality.plexus.lifecycle.phase.InitializePhase.execute(InitializePhase.java:16)
        at org.codehaus.plexus.lifecycle.AbstractLifecycleHandler.start(AbstractLifecycleHandler.java:102)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.startComponentLifecycle(AbstractComponentManager.java:110)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.createComponentInstance(AbstractComponentManager.java:100)
        at org.codehaus.plexus.component.manager.ClassicSingletonComponentManager.getComponent(ClassicSingletonComponentManager.java:92)
        at org.codehaus.plexus.DefaultComponentLookupManager.lookup(DefaultComponentLookupManager.java:77)
        at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:247)
        at org.codehaus.plexus.component.composition.CompositionUtils.findRequirement(CompositionUtils.java:87)
        at org.codehaus.plexus.component.composition.FieldComponentComposer.assignRequirementToField(FieldComponentComposer.java:70)
        at org.codehaus.plexus.component.composition.FieldComponentComposer.assignRequirement(FieldComponentComposer.java:61)
        at org.codehaus.plexus.component.composition.AbstractComponentComposer.assembleComponent(AbstractComponentComposer.java:103)
        at org.codehaus.plexus.component.composition.DefaultComponentComposerManager.assembleComponent(DefaultComponentComposerManager.java:70)
        at org.codehaus.plexus.personality.plexus.lifecycle.phase.CompositionPhase.execute(CompositionPhase.java:31)
        at org.codehaus.plexus.lifecycle.AbstractLifecycleHandler.start(AbstractLifecycleHandler.java:102)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.startComponentLifecycle(AbstractComponentManager.java:110)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.createComponentInstance(AbstractComponentManager.java:100)
        at org.codehaus.plexus.component.manager.ClassicSingletonComponentManager.getComponent(ClassicSingletonComponentManager.java:92)
        at org.codehaus.plexus.DefaultComponentLookupManager.lookup(DefaultComponentLookupManager.java:77)
        at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:247)
        at org.codehaus.plexus.component.composition.CompositionUtils.findRequirement(CompositionUtils.java:87)
        at org.codehaus.plexus.component.composition.FieldComponentComposer.assignRequirementToField(FieldComponentComposer.java:70)
        at org.codehaus.plexus.component.composition.FieldComponentComposer.assignRequirement(FieldComponentComposer.java:61)
        at org.codehaus.plexus.component.composition.AbstractComponentComposer.assembleComponent(AbstractComponentComposer.java:103)
        at org.codehaus.plexus.component.composition.DefaultComponentComposerManager.assembleComponent(DefaultComponentComposerManager.java:70)
        at org.codehaus.plexus.personality.plexus.lifecycle.phase.CompositionPhase.execute(CompositionPhase.java:31)
        at org.codehaus.plexus.lifecycle.AbstractLifecycleHandler.start(AbstractLifecycleHandler.java:102)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.startComponentLifecycle(AbstractComponentManager.java:110)
        at org.codehaus.plexus.component.manager.AbstractComponentManager.createComponentInstance(AbstractComponentManager.java:100)
        at org.codehaus.plexus.component.manager.ClassicSingletonComponentManager.getComponent(ClassicSingletonComponentManager.java:92)
        at org.codehaus.plexus.DefaultComponentLookupManager.lookup(DefaultComponentLookupManager.java:77)
        at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:247)
        at org.codehaus.plexus.container.initialization.StartLoadOnStartComponentsPhase.execute(StartLoadOnStartComponentsPhase.java:38)
        at org.codehaus.plexus.DefaultPlexusContainer.initializePhases(DefaultPlexusContainer.java:610)
        at org.codehaus.plexus.DefaultPlexusContainer.initialize(DefaultPlexusContainer.java:564)
        at org.codehaus.plexus.DefaultPlexusContainer.<init>(DefaultPlexusContainer.java:235)
        at org.codehaus.plexus.DefaultPlexusContainer.<init>(DefaultPlexusContainer.java:197)
        at org.codehaus.plexus.xwork.PlexusLifecycleListener.contextInitialized(PlexusLifecycleListener.java:59)
        at org.mortbay.jetty.servlet.WebApplicationContext.doStart(WebApplicationContext.java:495)
        at org.mortbay.util.Container.start(Container.java:72)
        at org.codehaus.plexus.jetty.AbstractJettyServletContainer.startApplication(AbstractJettyServletContainer.java:158)
        ... 26 more
Caused by: javax.naming.NameNotFoundException
        at org.mortbay.jndi.NamingContext.lookup(NamingContext.java:614)
        at org.mortbay.jndi.NamingContext.lookup(NamingContext.java:657)
        at org.mortbay.jndi.local.localContext.lookup(localContext.java:159)
        at javax.naming.InitialContext.lookup(InitialContext.java:351)
        at org.jpox.AbstractPersistenceManagerFactory.lookupDataSource(AbstractPersistenceManagerFactory.java:171)
        ... 78 more
[INFO] The appserver server has been initialized.
[INFO] The appserver server has started.
",smartshark_2_2,920,archiva,"""[0.5125787854194641, 0.4874212145805359]"""
127,228386,Ping service in not working,"I can't use ping service with xmlrpc client

The reponse is ""No handler for the method""

I have tried several methods :
- TestService.ping
- PingServiceImpl.ping
- PingService.ping

I suppose that it's a bug because SearchService and AdministrationService are working in xmlrpc",smartshark_2_2,1820,archiva,"""[0.3638455867767334, 0.6361544132232666]"""
128,228387,SQLSyntaxErrorException is thrown when browsing for artifacts after upgrading from 1.3.1 to 1.2,"I upgraded from Archiva 1.3.1 to 1.2 by replacing the existing 1.3.1 war file with the new 1.2 war file. When I browse for a file, I can see the group ID's and package ID's with no problem but when I click on a version link the following error is thrown:

h3. Exception
javax.servlet.ServletException: org.apache.maven.archiva.database.ArchivaDatabaseException: Error in JDO during get of Database object id [com.miginfocom:migcalendarbean:6.6.3] of type org.apache.maven.archiva.model.ArchivaProjectModel using no fetch-group

h3. Root Cause
org.apache.maven.archiva.database.ArchivaDatabaseException: Error in JDO during get of Database object id [com.miginfocom:migcalendarbean:6.6.3] of type org.apache.maven.archiva.model.ArchivaProjectModel using no fetch-group

h3. Root Cause
javax.jdo.JDODataStoreException: Error(s) were found while auto-creating/validating the datastore for classes. The errors are printed in the log, and are attached to this exception.
NestedThrowables:
java.sql.SQLSyntaxErrorException: In an ALTER TABLE statement, the column 'ARCHIVA_INDIVIDUAL_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.
java.sql.SQLSyntaxErrorException: In an ALTER TABLE statement, the column 'ARCHIVA_ISSUE_MANAGEMENT_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.
java.sql.SQLSyntaxErrorException: In an ALTER TABLE statement, the column 'ARCHIVA_ORGANIZATION_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.
java.sql.SQLSyntaxErrorException: In an ALTER TABLE statement, the column 'ARCHIVA_CIMANAGEMENT_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.
java.sql.SQLException: Constraint 'ARCHIVA_PROJTY_FK7' is invalid: there is no unique or primary key constraint on table 'APP.ARCHIVA_CIMANAGEMENT' that matches the number and types of the columns in the foreign key.
java.sql.SQLException: Constraint 'ARCHIVA_PROJTY_FK8' is invalid: there is no unique or primary key constraint on table 'APP.ARCHIVA_ORGANIZATION' that matches the number and types of the columns in the foreign key.
java.sql.SQLException: Constraint 'ARCHIVA_PROJTY_FK9' is invalid: there is no unique or primary key constraint on table 'APP.ARCHIVA_ISSUE_MANAGEMENT' that matches the number and types of the columns in the foreign key.

h3. Root Cause
java.sql.SQLSyntaxErrorException: In an ALTER TABLE statement, the column 'ARCHIVA_INDIVIDUAL_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.

h3. Root Cause
org.apache.derby.client.am.SqlException: In an ALTER TABLE statement, the column 'ARCHIVA_INDIVIDUAL_ID' has been specified as NOT NULL and either the DEFAULT clause was not specified or was specified as DEFAULT NULL.",smartshark_2_2,212,archiva,"""[0.11751337349414825, 0.8824866414070129]"""
129,228388,properties in pom are not resolved (at least while browsing),"I extensively use properties in pom.xml (see attached one).

But Archiva does not resolve pom configuration. When I browse my project I see not resolved values.

Attached mentioned project's pom.xml and screenshot of browse page.
",smartshark_2_2,1378,archiva,"""[0.15861228108406067, 0.8413877487182617]"""
130,228389,Selenium tests fail in full multi-module build,"If you build with ""mvn clean install -PintegrationTest"" from the root of the project, the following occurs:

2010-01-18 18:32:18,321 [main] ERROR org.springframework.web.context.ContextLoader  - Context initialization failed org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'mavenSettingsBuilder' defined in null: Could not resolve placeholder 'maven.home'
   at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.processProperties(PropertyPlaceholderConfigurer.java:268)
   at org.springframework.beans.factory.config.PropertyResourceConfigurer.postProcessBeanFactory(PropertyResourceConfigurer.java:75)
   at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:553)
   at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:527)
   at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:362)
   at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:255)
   at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:199)
   at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:45)
   at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3764)
   at org.apache.catalina.core.StandardContext.start(StandardContext.java:4216)
   at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:760)
   at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:740)
   at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:544)
   at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:626)
   at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:553)
   at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:488)
   at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1150)
   at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:311)
   at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:120)
   at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1022)
   at org.apache.catalina.core.StandardHost.start(StandardHost.java:736)
   at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014)
   at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
   at org.apache.catalina.core.StandardService.start(StandardService.java:448)
   at org.apache.catalina.core.StandardServer.start(StandardServer.java:700)
   at org.apache.catalina.startup.Catalina.start(Catalina.java:552)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
   at java.lang.reflect.Method.invoke(Method.java:592)
   at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:295)
   at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:433)

However, if you run directly from archiva-webapp-tests, it is fine.",smartshark_2_2,637,archiva,"""[0.9455714821815491, 0.05442854389548302]"""
131,228390,Changing the internal repository directory has no effect,"I changed the directory for the internal archiva repository from the default setting

${ARCHIVA_HOME}/data/repositories/internal

to a different location. But new dependencies were still downloaded to the default/old directory. 

But after I restarted archiva everything is fine. ",smartshark_2_2,1244,archiva,"""[0.24815469980239868, 0.7518453001976013]"""
132,228391,Fail to start on ibm jdk (was8),"Caused by: java.lang.IllegalStateException: The current state is not
START_DOCUMENT.
at com.ibm.xml.xlxp2.api.stax.msg.StAXMessageProvider.throwIllegalStateException(StAXMessageProvider.java:49)
at com.ibm.xml.xlxp2.api.stax.XMLStreamReaderImpl.getCharacterEncodingScheme(XMLStreamReaderImpl.java:1638)
at com.ibm.xml.xlxp2.api.stax.XMLInputFactoryImpl$XMLStreamReaderProxyImpl.getCharacterEncodingScheme(XMLInputFactoryImpl.java:457)
at com.ibm.xml.xlxp2.api.wssec.WSSXMLInputFactory$WSSStreamReaderProxy.getCharacterEncodingScheme(WSSXMLInputFactory.java:55)
at org.apache.archiva.redback.role.model.io.stax.RedbackRoleModelStaxReader.read(RedbackRoleModelStaxReader.java:1071)",smartshark_2_2,1129,archiva,"""[0.31881940364837646, 0.6811806559562683]"""
133,228392,NPE when updating consumers of known content without setting any of the checkboxes (enabled),"java.lang.NullPointerException
	at org.apache.maven.archiva.web.action.admin.scanning.RepositoryScanningAction.filterAddedConsumers(RepositoryScanningAction.java:319)
	at org.apache.maven.archiva.web.action.admin.scanning.RepositoryScanningAction.updateKnownConsumers(RepositoryScanningAction.java:269)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at com.opensymphony.xwork2.DefaultActionInvocation.invokeAction(DefaultActionInvocation.java:404)
	at com.opensymphony.xwork2.DefaultActionInvocation.invokeActionOnly(DefaultActionInvocation.java:267)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:229)
	at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)
	at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.maven.archiva.web.interceptor.ConfigurationInterceptor.intercept(ConfigurationInterceptor.java:50)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.codehaus.plexus.redback.struts2.interceptor.PolicyEnforcementInterceptor.intercept(PolicyEnforcementInterceptor.java:159)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.codehaus.plexus.redback.struts2.interceptor.SecureActionInterceptor.intercept(SecureActionInterceptor.java:154)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ParameterFilterInterceptor.intercept(ParameterFilterInterceptor.java:143)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)
	at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ConversionErrorInterceptor.intercept(ConversionErrorInterceptor.java:123)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:184)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.StaticParametersInterceptor.intercept(StaticParametersInterceptor.java:105)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.interceptor.CheckboxInterceptor.intercept(CheckboxInterceptor.java:83)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:207)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ModelDrivenInterceptor.intercept(ModelDrivenInterceptor.java:74)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ScopedModelDrivenInterceptor.intercept(ScopedModelDrivenInterceptor.java:127)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.interceptor.ProfilingActivationInterceptor.intercept(ProfilingActivationInterceptor.java:107)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.interceptor.debugging.DebuggingInterceptor.intercept(DebuggingInterceptor.java:206)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ChainingInterceptor.intercept(ChainingInterceptor.java:115)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:143)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.PrepareInterceptor.doIntercept(PrepareInterceptor.java:121)
	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.interceptor.ServletConfigInterceptor.intercept(ServletConfigInterceptor.java:170)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.AliasInterceptor.intercept(AliasInterceptor.java:123)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:176)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.codehaus.plexus.redback.struts2.interceptor.AutoLoginInterceptor.intercept(AutoLoginInterceptor.java:167)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.codehaus.plexus.redback.struts2.interceptor.ForceAdminUserInterceptor.intercept(ForceAdminUserInterceptor.java:77)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
	at org.apache.struts2.impl.StrutsActionProxy.execute(StrutsActionProxy.java:50)
	at org.apache.struts2.dispatcher.Dispatcher.serviceAction(Dispatcher.java:504)
	at org.apache.struts2.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:419)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)
	at com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)
	at com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)
	at org.apache.struts2.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:99)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)",smartshark_2_2,279,archiva,"""[0.0643061026930809, 0.9356938600540161]"""
134,228393,Random 400 Error when uploading maven-metadata.xml to Archiva 1.2/1.2.1,"When maven tries to deploy a new/updated maven-metadata.xml or its md5/sha, occasionally a HTTP 400 - Bad Request is returned from Archiva.  The following message is returned from Archiva:  Content Header length was 32 but was 0

Detailed HTTP Request/Response:

PUT /archiva/repository/releases/components/gui-project/maven-metadata.xml.md5 HTTP/1.1
User-Agent: Java/1.5.0_16
Host: maven.local
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive
Authorization: Basic YmFtYm9vOmJAbWIwbw==
Content-Length: 32

24877de78f50a1cdb8bd71e2dfa933cfHTTP/1.1 400 Bad Request
Date: Tue, 02 Jun 2009 20:20:36 GMT
Set-Cookie: JSESSIONID=77ECDD66382B0B02356E1027D344D5FF; Path=/archiva
last-modified: Tue, 02 Jun 2009 15:04:37 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 1085
Connection: close

<html><head><title>Apache Tomcat/6.0.18 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} H3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} BODY {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} B {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} P {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;}A {color : black;}A.name {color : black;}HR {color : #525D76;}--></style> </head><body><h1>HTTP Status 400 - Content Header length was 32 but was 0</h1><HR size=""1"" noshade=""noshade""><p><b>type</b> Status report</p><p><b>message</b> <u>Content Header length was 32 but was 0</u></p><p><b>description</b> <u>The request sent by the client was syntactically incorrect (Content Header length was 32 but was 0).</u></p><HR size=""1"" noshade=""noshade""><h3>Apache Tomcat/6.0.18</h3></body></html>

When the actual packets are examined they look like this:

  1   0.000000 192.168.102.68 -> 192.168.102.68 TCP 42242 > http [SYN] Seq=0 Win=32792 Len=0 MSS=16396 TSV=21157988 TSER=0 WS=7
  2   0.000024 192.168.102.68 -> 192.168.102.68 TCP http > 42242 [SYN, ACK] Seq=0 Ack=1 Win=32768 Len=0 MSS=16396 TSV=21157988 TSER=21157988 WS=7
  3   0.000040 192.168.102.68 -> 192.168.102.68 TCP 42242 > http [ACK] Seq=1 Ack=1 Win=32896 Len=0 TSV=21157988 TSER=21157988
  4   0.000539 192.168.102.68 -> 192.168.102.68 HTTP PUT /archiva/repository/releases/components/gui-project/maven-metadata.xml.md5 HTTP/1.1 (packet with headers)
  5   0.000561 192.168.102.68 -> 192.168.102.68 TCP http > 42242 [ACK] Seq=1 Ack=315 Win=33920 Len=0 TSV=21157988 TSER=21157988
  6   0.000591 192.168.102.68 -> 192.168.102.68 HTTP Continuation or non-HTTP traffic (packet with the 32 bytes of content)
  7   0.000603 192.168.102.68 -> 192.168.102.68 TCP http > 42242 [ACK] Seq=1 Ack=347 Win=33920 Len=0 TSV=21157988 TSER=21157988
  8   0.055038 192.168.102.68 -> 192.168.102.68 HTTP HTTP/1.1 400 Bad Request  (text/html) (packet with 400 error)
  9   0.055066 192.168.102.68 -> 192.168.102.68 TCP 42242 > http [ACK] Seq=347 Ack=1349 Win=35584 Len=0 TSV=21158002 TSER=21158002
 10   0.055083 192.168.102.68 -> 192.168.102.68 TCP http > 42242 [FIN, ACK] Seq=1349 Ack=347 Win=33920 Len=0 TSV=21158002 TSER=21158002
 11   0.055892 192.168.102.68 -> 192.168.102.68 TCP 42242 > http [FIN, ACK] Seq=347 Ack=1350 Win=35584 Len=0 TSV=21158002 TSER=21158002
 12   0.055910 192.168.102.68 -> 192.168.102.68 TCP http > 42242 [ACK] Seq=1350 Ack=348 Win=33920 Len=0 TSV=21158002 TSER=21158002",smartshark_2_2,236,archiva,"""[0.31411170959472656, 0.6858882904052734]"""
135,228394,NullPointerException when deleting an artifact,"INFO   | jvm 1    | 2009/12/08 21:29:54 | java.lang.NullPointerException
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.maven.archiva.web.action.DeleteArtifactAction.updateMetadata(DeleteArtifactAction.java:347)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.maven.archiva.web.action.DeleteArtifactAction.doDelete(DeleteArtifactAction.java:229)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at java.lang.reflect.Method.invoke(Unknown Source)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invokeAction(DefaultActionInvocation.java:404)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invokeActionOnly(DefaultActionInvocation.java:267)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:229)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.maven.archiva.web.interceptor.ConfigurationInterceptor.intercept(ConfigurationInterceptor.java:50)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.codehaus.plexus.redback.struts2.interceptor.PolicyEnforcementInterceptor.intercept(PolicyEnforcementInterceptor.java:159)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.codehaus.plexus.redback.struts2.interceptor.SecureActionInterceptor.intercept(SecureActionInterceptor.java:173)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ParameterFilterInterceptor.intercept(ParameterFilterInterceptor.java:143)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ConversionErrorInterceptor.intercept(ConversionErrorInterceptor.java:123)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:186)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.StaticParametersInterceptor.intercept(StaticParametersInterceptor.java:105)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.CheckboxInterceptor.intercept(CheckboxInterceptor.java:83)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:207)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ModelDrivenInterceptor.intercept(ModelDrivenInterceptor.java:74)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ScopedModelDrivenInterceptor.intercept(ScopedModelDrivenInterceptor.java:127)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.ProfilingActivationInterceptor.intercept(ProfilingActivationInterceptor.java:107)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.debugging.DebuggingInterceptor.intercept(DebuggingInterceptor.java:206)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ChainingInterceptor.intercept(ChainingInterceptor.java:115)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:143)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.PrepareInterceptor.doIntercept(PrepareInterceptor.java:121)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.interceptor.ServletConfigInterceptor.intercept(ServletConfigInterceptor.java:170)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.AliasInterceptor.intercept(AliasInterceptor.java:123)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:176)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.codehaus.plexus.redback.struts2.interceptor.AutoLoginInterceptor.intercept(AutoLoginInterceptor.java:167)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.codehaus.plexus.redback.struts2.interceptor.ForceAdminUserInterceptor.intercept(ForceAdminUserInterceptor.java:77)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.impl.StrutsActionProxy.execute(StrutsActionProxy.java:50)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.dispatcher.Dispatcher.serviceAction(Dispatcher.java:504)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:419)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1148)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.module.sitemesh.filter.PageFilter.parsePage(PageFilter.java:118)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:52)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1148)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.apache.struts2.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:99)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1148)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:387)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.Server.handle(Server.java:326)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:879)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
INFO   | jvm 1    | 2009/12/08 21:29:54 | 	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)
",smartshark_2_2,448,archiva,"""[0.06665460765361786, 0.9333454370498657]"""
136,228395,java.lang.ArrayIndexOutOfBoundsException from parsing maven-metadata.xml,"ArrayIndexOutOfBoundsException is thrown when parsing some maven-metadata.xml files.  Please see the attached sample maven-metadata.xml and the proposed patch to fix the problem.


2012-04-17 00:04:43,098 WARN 
[org.apache.archiva.repository.metadata.MetadataTools] Unable to read metadata:
/home/y/var/yarchiva/data/repositories/public/yahoo/yinst/listings_reconciliation_framework
/listings-reconciliation-framework/maven-metadata-dist.xml
org.apache.archiva.repository.metadata.RepositoryMetadataException: Unable to
parse metadata xml
file:/home/y/var/yarchiva/data/repositories/public/yahoo/yinst/listings_reconciliation_framework/listing
s-reconciliation-framework/maven-metadata-dist.xml: null Nested exception: null
        at
org.apache.archiva.repository.metadata.RepositoryMetadataReader.read(RepositoryMetadataReader.java:95)
        at
org.apache.archiva.repository.metadata.MetadataTools.readProxyMetadata(MetadataTools.java:394)
        at
org.apache.archiva.repository.metadata.MetadataTools.getMetadatasForManagedRepository(MetadataTools.java:527)
        at
org.apache.archiva.repository.metadata.MetadataTools.updateMetadata(MetadataTools.java:437)
        at


Caused by: org.apache.archiva.xml.XMLException: Unable to parse metadata xml
file:/home/y/var/yarchiva/data/repositories/public/yahoo/yinst/listings_reconciliation_framework/listings-reconciliation-framework/maven-metadata-dist.xml:
null Nested exception: null
        at org.apache.archiva.xml.XMLReader.init(XMLReader.java:113)
        at org.apache.archiva.xml.XMLReader.<init>(XMLReader.java:81)
        at
org.apache.archiva.repository.metadata.RepositoryMetadataReader.read(RepositoryMetadataReader.java:52)
        ... 55 more
Caused by: org.dom4j.DocumentException: null Nested exception: null
        at org.dom4j.io.SAXReader.read(SAXReader.java:484)
        at org.dom4j.io.SAXReader.read(SAXReader.java:365)
        at org.apache.archiva.xml.XMLReader.init(XMLReader.java:109)
        ... 57 more

",smartshark_2_2,1715,archiva,"""[0.07173056155443192, 0.9282695055007935]"""
137,228396,unable to include *.xml in artifacts list as it picks up maven-metadata.xml,metadata should be explicitly removed from the artifact consumers list,smartshark_2_2,1509,archiva,"""[0.9325143098831177, 0.06748569011688232]"""
138,228397,Cannot download a strut-module artifact in a Legacy repository,"
I still have legacy Maven 1.1 projects which uses the struts-module plugin.
I was using maven-proxy until now but decided to switch to archiva.

However when I try to download a struts-module in my Maven 1.X repository, I get the following exception:

org.apache.maven.archiva.repository.layout.LayoutException: Invalid path to Artifact: mismatch on extension [dule] and layout specified type [struts-modules] (which maps to extension: [struts.module]) on path [WebPortal/struts-modules/eventsDB-1.2.3.struts-module]
	at org.apache.maven.archiva.repository.content.LegacyPathParser.toArtifactReference(LegacyPathParser.java:154)
	at org.apache.maven.archiva.repository.content.RepositoryRequest.toArtifactReference(RepositoryRequest.java:125)
	at org.apache.maven.archiva.repository.content.RepositoryRequest.toNativePath(RepositoryRequest.java:271)
	at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:193)
	at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
	at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:428)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:830)
	at com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:189)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:39)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:471)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:568)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1530)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:633)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1482)
	at org.mortbay.http.HttpServer.service(HttpServer.java:909)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:816)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:982)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:833)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

The file WebPortal/struts-modules/eventsDB-1.2.3.struts-module does exist (and would be returned correctly by maven-proxy), but it seems as it archiva is looking for WebPortal/struts-modules/eventsDB-1.2.3.struts.module which does not exist. (replacing the dash by a dot in the extension)

",smartshark_2_2,1519,archiva,"""[0.2870388329029083, 0.7129611372947693]"""
139,228398,Unable to download uploaded artifact from repository,"Reported from the dev list by Piotr Szwed:

After uploading an artifact to the newly created repository, I am unable
to download it. The problem seems to be in the URL generated by Jetty.

When I click on the particular artifact's URL, I get the following link:
*//*
repository/playground/com/gerrit-architecture/1.0/gerrit-architecture-1.0.zip

which results with the following message:

*Problem accessing
//repository/playground/com/gerrit-architecture/1.0/gerrit-architecture-1.0.zip.
Reason:    Not Found*
After removing the first slash ""/"" from the URL - downloading
starts automatically. So I suspect that one of the ""/"" is redundant and
shouldn't be generated.

my default config entry seems to be fine:

./conf/archiva.xml:<location>/mnt/localdata/gerrit/archiva/apache-archiva-js-1.4-M3/repositories/playground</location>",smartshark_2_2,1135,archiva,"""[0.3739662170410156, 0.6260337233543396]"""
140,228399,LayoutException when downloading SNAPSHOT test-sources,"If you deploy a SNAPSHOT to Archiva that deploys test sources, and then try to download the test sources, the download will fail with an error.

To reproduce: Using the attached project, deploy to Archiva, then go to http://localhost:8080/archiva/repository/snapshots/com/mycompany/app/my-app/1.0-SNAPSHOT/ and try to download the test-sources.jar.  (An example settings.xml file is also provided in the archive; change its password and then do ""mvn deploy -s settings.xml"".)

You'll get a 404 error, with this exception:

{noformat}
org.apache.maven.archiva.repository.layout.LayoutException: Invalid path to Artifact: filename format is invalid,expected timestamp format in filename.
	at org.apache.maven.archiva.repository.content.DefaultPathParser.toArtifactReference(DefaultPathParser.java:134)
	at org.apache.maven.archiva.repository.content.RepositoryRequest.toArtifactReference(RepositoryRequest.java:121)
	at org.apache.maven.archiva.repository.content.RepositoryRequest.toNativePath(RepositoryRequest.java:271)
	at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:193)
	at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
	at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:428)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:830)
	at com.opensymphony.webwork.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:189)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at com.opensymphony.module.sitemesh.filter.PageFilter.doFilter(PageFilter.java:39)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at com.opensymphony.webwork.dispatcher.ActionContextCleanUp.doFilter(ActionContextCleanUp.java:88)
	at org.mortbay.jetty.servlet.WebApplicationHandler$CachedChain.doFilter(WebApplicationHandler.java:821)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:471)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:568)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1530)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:633)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1482)
	at org.mortbay.http.HttpServer.service(HttpServer.java:909)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:816)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:982)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:833)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
{noformat}",smartshark_2_2,1480,archiva,"""[0.9159801006317139, 0.08401992172002792]"""
141,228400,Remote Repositories with empty <username> and <password> fields shouldn't be created in configuration.,"It is possible for a <remoteRepository> section in the archiva configuration file to be created with an empty <username> and/or <password> fields.  

Need to prevent empty/blank username and password fields from being created in the archiva configuration.

Also need to prevent loading blank username and password fields from the archiva configuration.",smartshark_2_2,1362,archiva,"""[0.115452341735363, 0.8845477104187012]"""
142,228401,incorrect default cron expression for snapshots repository,it is '0 0' which is not a valid value - it should probably be the same as for releases,smartshark_2_2,1169,archiva,"""[0.7205087542533875, 0.2794913053512573]"""
143,228402,DigestUtils fails to verify checksum from ibiblio,"Downloading servletapi-24.pom fails.

DigestUtils.cleanChecksum check for filename in remote checksum file to be same as local, but tets is inverted :

                String filename = m.group( 2 );
                if ( !path.endsWith( filename ) )
                {
                    throw new DigesterException( ""Supplied checksum does not match checksum pattern"" );
                }

filename = ""/home/projects/maven/repository-staging/to-ibiblio/maven2/servletapi/servletapi/2.4/servletapi-2.4.pom""
path = ""servletapi-2.4.pom"".


Patch provided to test if ( !path.endsWith( filename ) )",smartshark_2_2,710,archiva,"""[0.5632960796356201, 0.4367039203643799]"""
144,228403,507 Insufficient Storage when deploying artifact with webdav,"Sometimes when deploying with dav I get a ""507 Insufficient Storage"" error.

The artifact (.../group/artifact/version/artifact-version.jar) gets deployed (with checksums).
The metadata (.../group/artifact/version/maven-metatdata.xml) gets deployed (with checksums).

It seems to happen when maven tries to upload the updated parent metadata (.../group/artifact/maven-metadata.xml). The checksums for this metadata deploys OK.",smartshark_2_2,1541,archiva,"""[0.13543716073036194, 0.8645628690719604]"""
145,228404,'Specified key was too long' error during startup when using MySQL databases with UTF8 charset,Related to http://bugs.mysql.com/bug.php?id=25480 and http://jira.codehaus.org/browse/CONTINUUM-1113,smartshark_2_2,1690,archiva,"""[0.14475972950458527, 0.8552402853965759]"""
146,228405,metadata-updater is changing lastUpdating timestamp when it shouldn't,"This is a bit of a trivial 'bug' and the result is not harmfull, but I think the metadata-updater should not change data that is valid.

As far as I could find the lastUpdated field is the timestamp on which the metadata was last updated, so should not be touched in this case.

During scanning, some (~200) maven-metadata.xml files are updated in my repository .
The metadata being updated is:
<?xml version=""1.0"" encoding=""UTF-8""?><metadata>
  <groupId>com.informatica.profiling.services</groupId>
  <artifactId>profiling-model-persist</artifactId>
  <version>1.0-SNAPSHOT</version>
  <versioning>
    <snapshot>
      <timestamp>20070213.050129</timestamp>
      <buildNumber>79</buildNumber>
    </snapshot>
    <lastUpdated>20070213050130</lastUpdated>
  </versioning>
</metadata>

After update the lastUpdated field is changed to 20070213050129:
    <lastUpdated>20070213050129</lastUpdated>

As far as I could find the lastUpdated field is the timestamp on which the metadata was last updated, so should not be touched in this case.

Scanlog:
2007-10-05 14:38:41,043 [pool-2-thread-1] DEBUG org.apache.maven.archiva.repository.scanner.RepositoryScanner:default  - Sending to consumer: metadata-updater
2007-10-05 14:38:41,090 [pool-2-thread-1] INFO  org.apache.maven.archiva.consumers.KnownRepositoryContentConsumer:metadata-updater  - Updated metadata: com/xxx/profiling/services/profiling-model-persist/1.0-SNAPSHOT/maven-metadata.xml
2007-10-05 14:38:46,356 [pool-2-thread-1] INFO  org.apache.maven.archiva.consumers.KnownRepositoryContentConsumer:metadata-updater  - Updated metadata: com/xxx/profiling/services/profiling-model-persist/maven-metadata.xml
",smartshark_2_2,1254,archiva,"""[0.38931605219841003, 0.6106839179992676]"""
147,228406,[Repository Group] Page will be broken if entered identifier is too long,See screen shot attached.,smartshark_2_2,1574,archiva,"""[0.1162397637963295, 0.8837602138519287]"""
148,228407,upgrade to wagon 1.0-beta-5,this will allow removing a workaround for MRM-631 in archiva-proxy,smartshark_2_2,132,archiva,"""[0.9982863068580627, 0.0017136615933850408]"""
149,228408,revise /repository and /proxy handling,"as highlighted on dev@ list, we need to review the duality of /repsitory and /proxy and come up with solid usage patterns",smartshark_2_2,595,archiva,"""[0.9984540939331055, 0.0015458761481568217]"""
150,228409,Archiva does not start up on Solaris 64-bit.,"Attempting to launch causes causes error:
$ ./apache-archiva-1.3.4/bin/archiva console

Bad string
Bad string
Bad string
Bad string
Unable to locate any of the following binaries:
  /export/home/astine/apache-archiva-1.3.4/bin/./wrapper---32
  /export/home/astine/apache-archiva-1.3.4/bin/./wrapper---64
  /export/home/astine/apache-archiva-1.3.4/bin/./wrapper


necessary files appear to be missing or misnamed.",smartshark_2_2,482,archiva,"""[0.8168291449546814, 0.183170884847641]"""
151,228410,Ability to delete an artifact from the web interface,"Sometimes when viewing an artifact through the Archiva Web UI, I'd like to delete it from the repository.

Currently the only way to do this by deleting it from the managed repository filesystem.",smartshark_2_2,107,archiva,"""[0.9979641437530518, 0.0020359233021736145]"""
152,228411,Allow the user to take all proxy connectors offline and online, Currently you can only take an individual connector offline/online. The UI should be extended to allow all proxy connectors to be enabled or disabled.,smartshark_2_2,46,archiva,"""[0.9984825253486633, 0.0015174687141552567]"""
153,228412,indexing needs assistance for those upgrading from 1.1,"since the format has changed, a new index needs to be created. However, since the database has already processed the artifacts, they won't be indexed. 

the previous directories will be ignored (.index), and should be cleaned up

if using a custom index location, the indexer will fail since one already exists

",smartshark_2_2,529,archiva,"""[0.9744057059288025, 0.025594325736165047]"""
154,228413,Automatic generation of REST Api documentation,"We must find to auto generate documentation on REST api
Enunciate looks to need some code changes on method names 
Other solutions (swagger) are not open source.
So maybe write our own stuff !
Maybe NIH syndrom :-)",smartshark_2_2,992,archiva,"""[0.9982781410217285, 0.0017219007713720202]"""
155,228414,remove archiva-repository-layer and archiva-model module,"there are still several uses of this API, but when dependant issues are completed it should be in a position that it is no longer needed and can be removed",smartshark_2_2,387,archiva,"""[0.9982251524925232, 0.0017748652026057243]"""
156,228415,Ability to delete a specific version of an an artifact from the artifact screen,"The ability to delete artifacts in context while browsing is AWESOME, but there is one more place it would be handy.  When drilling down to the artifact root where it shows all available versions, it would be nice to be able to remove the entire version (see Capture1.png).  

Currently I must drill into the version, click the ""Artifacts"" tab and removed each artifact individually (see Capture2.png).  This isn't a big deal with Release versions but for SNAPSHOT versions, there may be MANY artifacts.  ",smartshark_2_2,999,archiva,"""[0.9984519481658936, 0.0015479810535907745]"""
157,228416,A standard error message should display anytime an incorrect User ID or password is entered,"The error message should be:
""You have entered an incorrect username and/or password""",smartshark_2_2,844,archiva,"""[0.9978870749473572, 0.002112975111231208]"""
158,228417,Document how to set up a redundant mirror configuration of Archiva,"I have tried setting up on archiva and pointing to another, but this doesn't work.
Could someone document how to set a redundant pair of servers, or to point to where this is covered as I could not find it?",smartshark_2_2,524,archiva,"""[0.9984884262084961, 0.0015115314163267612]"""
159,228418,investigate using compass for the indexing api,http://www.opensymphony.com/compass/,smartshark_2_2,1368,archiva,"""[0.9984176158905029, 0.0015823208959773183]"""
160,228419,add notes on reindexing to the upgrade docs,"this is needed to take advantage of new index features, such as the java class indexing",smartshark_2_2,19,archiva,"""[0.9984226226806641, 0.0015774124767631292]"""
161,228420,"Improve the ""Administration - Proxy Connectors"" view","When clicking the ""expand"" button I would assume it to read ""collapse"" after I press it. Also, when pressing it it take around 5 seconds for it to actually collapse. Browser: opera 9.25 on os x.",smartshark_2_2,663,archiva,"""[0.9982330799102783, 0.0017669170629233122]"""
162,228421,last-modified header is missing,last-modified header is missing. Wagon (and other clients) use this to determine a files freshness.,smartshark_2_2,1657,archiva,"""[0.7697939872741699, 0.23020599782466888]"""
163,228422,XMLRPC Basic Compliance ( Call archiva XMLRPC Service from other language like Php ),The XMLRPC API used by archiva is not compliant with other language and client xml-rpc,smartshark_2_2,1823,archiva,"""[0.9983547329902649, 0.0016452450072392821]"""
164,228423,Virtual repositories or repository grouping,"A number of managed repositories can  be grouped together with that group having only one url. So you only need to specify that url in the settings.xml file and when Archiva receives a request via that url, it would look for that artifact from the repositories belonging to that group. 

More details are dicussed here:
http://www.nabble.com/Archiva-1.1-Roadmap-td15262645.html#a15263879",smartshark_2_2,1568,archiva,"""[0.9800369739532471, 0.019963007420301437]"""
165,228424,Improve delete artifact from webapp,"Move delete artifact in artifact information page instead of as a separate form/menu. 

See the following thread in the dev list for more details:

http://www.mail-archive.com/dev@archiva.apache.org/msg00539.html ",smartshark_2_2,987,archiva,"""[0.9983958601951599, 0.001604101387783885]"""
166,228425,introduce a metadata content repository API,see MRM-1025 for justification,smartshark_2_2,463,archiva,"""[0.9975220561027527, 0.002477871486917138]"""
167,228426,repogroup merged index ttl configurable per repository group,"currently this ttl is system property value.
It must be configurable per repository group.",smartshark_2_2,1265,archiva,"""[0.99802565574646, 0.00197433028370142]"""
168,228427,Move archiva parent project to gitbox,"To unify the infrastructure, we will move the archiva parent project to gitbox too.",smartshark_2_2,1994,archiva,"""[0.9979175925254822, 0.002082411665469408]"""
169,228428,Add repository allows empty url name,"If the field in url name is not filled the submition works
it should have also a star as mandatory field",smartshark_2_2,747,archiva,"""[0.49701496958732605, 0.5029850602149963]"""
170,228429,Determine HTTP Cache-Control rules for all repository content.,"With the recently added Cache-Control on maven-metadata.xml files from MRM-503, it becomes apparent that we need to investigate the Cache-Control rules for other repository content such as ...

* Release Artifacts
* Snapshot Artifacts
* Hashcodes (sha1, md5)

",smartshark_2_2,1841,archiva,"""[0.9977976083755493, 0.002202382544055581]"""
171,228430,Minor usability enhancement for the web app footer,"Each page of the web app has a footer with links to the Archiva web site and to the main ASF web site. It's a little annoying that these targets come up in the same browser window / tab as the Archiva app itself, rendering the app inaccessible (unless and until the user hits the back button). The attached patch causes these links to open their targets in a new browser window / tab instead.

Also, for the link to the main ASF web site, only the text ""The Apache Software Foundation"" is linked rather than the entire copyright notice.

Patch attached.",smartshark_2_2,1668,archiva,"""[0.9983223080635071, 0.0016776432748883963]"""
172,228431,provide mechanism to obtain the latest version of an artifact,"It will be very useful and convenient for a user to be able to download the ""latest"" version of an artifact. Sonatype Nexus provides such a mechanism, please see http://stackoverflow.com/questions/7911620/using-the-nexus-rest-api-to-get-latest-artifact-version-for-given-groupid-artfic and https://maven.java.net/nexus-core-documentation-plugin/core/docs/rest.artifact.maven.redirect.html

Thanks!",smartshark_2_2,1419,archiva,"""[0.9984442591667175, 0.0015557612059637904]"""
173,228432,Create another pom and jar in repository,"
we use Archiva to internal maven repository. Sometimes is created others pom.xml and jars in some artefects content informations like:

<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html><head>
<title>301 Moved Permanently</title>
</head><body>
<h1>Moved Permanently</h1>
<p>The document has moved <a href=""https://repository.primefaces.org/sentinela/sentinela-authentication/2.1.3/sentinela-authentication-2.1.3.pom"">here</a>.</p>
<hr>
<address>Apache/2.4.18 (Ubuntu) Server at repository.primefaces.org Port 80</address>
</body></html>


So the artefact have two pom and two jars generating errors for applications.
",smartshark_2_2,1986,archiva,"""[0.9615104794502258, 0.03848947584629059]"""
174,228433,"admin account locked on Archiva UI, unable to create a new user","Admin account for Archiva has been locked out due to incorrect password attempts and I am unable to create another user account or reset the current password through UI as when I reset the password it says ""password reset"" is there a way to find out which email the new password is being sent to or reset the password from Archiva application on the server.",smartshark_2_2,2000,archiva,"""[0.33633509278297424, 0.6636649370193481]"""
175,228434,Nexus indexer drags in unnecessary dependencies,"the use of nexus-indexer drags in some libraries we don't need (for example, commons-cli, and some XML APIs). Look at excluding those where possible.",smartshark_2_2,549,archiva,"""[0.9981122016906738, 0.0018878414994105697]"""
176,228435,Make Statistics API independent of metadata store implementations,"The repository-statistics module has direct references to the store implementations. I would like to decouple the statistics api from the implementation.
* Create new metadata-statistics-api module where the interfaces and statistic POJO reside.
* Add a new RepositoryStatisticsProvider interface that can be implemented by MedadataRepository and is for populating the statistic data. Can be implemented by the MetadataRepository and is statistic generation is delegated to this implementation if it exists.

With this, we can remove the dependencies to the store specific APIs like JCR.",smartshark_2_2,1971,archiva,"""[0.9984136819839478, 0.001586303347721696]"""
177,228436,Allow hard-fail configuration for a remote repository,see maven-proxy configuration http://cvs.maven-proxy.codehaus.org/maven-proxy/core/src/test/resources/org/apache/maven/proxy/config/PropertyLoaderTest1.properties?rev=1.1&view=markup,smartshark_2_2,722,archiva,"""[0.9985149502754211, 0.0014849959407001734]"""
178,228437,Configuration page should display http proxy settings ,"When adding a proxied repository, there is a checkbox for 'Use HTTP Proxy'.  It's not obvious where to go to set up the proxy.

The Configuration -> Settings page only displays information about the index, it should also display the http proxy settings.

",smartshark_2_2,1020,archiva,"""[0.9936953186988831, 0.006304659880697727]"""
179,228438,Publish the reference docs for 1.0,The javadocs and other reports for Archiva 1.0 need to be published at http://maven.apache.org/archiva/ref/1.0 .,smartshark_2_2,1462,archiva,"""[0.9975758194923401, 0.002424231730401516]"""
180,228439,Remove VersionedReference/ProjectReference/ArtifactReference from RepositoryProxyConnectors,"Remove VersionedReference/ProjectReference/ArtifactReference from RepositoryProxyConnectors.

Converting from request paths to VersionedReference/ProjectReference/ArtifactReference is not always 100% correct. ",smartshark_2_2,84,archiva,"""[0.9963502883911133, 0.0036497446708381176]"""
181,228440,Updated Archiva User Guide,"Updated/Added the following sections:

* Browsing
* Deploying an Artifact
* Identifying an Artifact
* Deleting an Artifact",smartshark_2_2,1682,archiva,"""[0.9983766078948975, 0.0016233456553891301]"""
182,228441,Unable to specify the location of the index files through the web ui,"There is currently no way to configure the location of the Lucene index files through the web ui.

Workaround:  edit archiva.xml and provide <repository> ... <indexDir>/path/to/index-dir</indexDir> 

",smartshark_2_2,1469,archiva,"""[0.7465302348136902, 0.2534697949886322]"""
183,228442,Add remote repository health check,"A cron based check of remote repositories (cron value per remote).
Option:
* disable the repository until the next check ?
* send an email to users with admin role ?",smartshark_2_2,1305,archiva,"""[0.9984298348426819, 0.001570095308125019]"""
184,228443,current configuration code is too likely to complain about multiple sources,"those with alpha-1 are likely to have config in ~/.m2/archiva.xml.

But the default is to distribute and work with ${appserver.base}/conf/archiva.xml

This will complain whenever saving by default. I need to review the best distribution situation.",smartshark_2_2,1163,archiva,"""[0.8458828926086426, 0.1541171371936798]"""
185,228444,Ability to merge repositories,"Need to be able to merge one repository into another, updating metadata and not overwriting existing artifacts.

The source repository should remain unchanged.

It should produce output listing what was merged and what wasn't.

Additions to the target repo should show in the audit log.

Related thread:  http://www.nabble.com/Merging-repositories-ts19850359.html",smartshark_2_2,422,archiva,"""[0.9983980059623718, 0.0016020347829908133]"""
186,228445,Abstraction of Reporting test cases,Please apply junit patch for isolating common features needed in setup and teardown methods and a utility method used for testing.,smartshark_2_2,741,archiva,"""[0.9974488615989685, 0.0025511160492897034]"""
187,228446,Typo of Legacy Support option on the left nav menu,"Please see attached screen shot. I guess it must be ""Legacy Support"". Thanks! :)",smartshark_2_2,169,archiva,"""[0.9942837357521057, 0.005716282874345779]"""
188,228447,allow nested search parameters,"Search parameters should be nested to allow searches like  

       ( ( groupId=1 AND artifactId=2 ) AND ( version=1 OR version=2 ) )

",smartshark_2_2,866,archiva,"""[0.9984474778175354, 0.001552576432004571]"""
189,228448,Proxy Connector access in Managed Repository Page,"It would be great if Archiva had a link to Proxy Connector Settings of a specific Managed Repository from within the Manage Repository Page. A simple link within the bounding box of managed repo like ""configure proxy connector for this managed repo,"" or something more concise perhaps would do. 

Also, I think it would be very informative if the repos being proxied by a managed repo be also listed in this same box. something like:

Proxied Repos: central, java.net",smartshark_2_2,1280,archiva,"""[0.9983587861061096, 0.0016412429977208376]"""
190,228449,Need support for private key authorization.,Authentication with the password is fine for some but the private key authorization supported by maven shoul be supported by archiva. This would allow organizations to have true security when installing artifacts or fetching them from servers.,smartshark_2_2,577,archiva,"""[0.998516857624054, 0.0014831327134743333]"""
191,228450,Update the screenshots in the docs,"Update the following to reflect the changes in the UI (new sections like Legacy Support and Upload Artifact):
- Feature Tour
- Reports
- Search
- Identifying An Artifact",smartshark_2_2,1572,archiva,"""[0.9981507658958435, 0.0018492102390155196]"""
192,228451,"Patch for several issues while processing poms (effective model, expressions resolving, storing to database)","I'm submitting a bigger patch for serveral issues that drived my crazy after installation of archiva in our company.
The patch is for 1.1.1 and 1.1.2 releases. 

The patch:
- fixes problems with expression resolving (${pom.version}) in dependencies,
- adds support for parent.{groupId, artifactId, version) properties,
- fixes issues with jdo detachCopy called from ProjectModelToDatabaseListener while creating effective pom,
- fixes inconsistency in key format used in effective model cache,
- add merging parentProject and properties while creating effective pom,
- ArchivaProjectModel.setOrgin(""filesystem"") was moved to readers (where IMO should be),
- adds few new test cases.

I will be very happy if you apply the patch on the code base. It will save me a bit of work 
with merging my changes with new releases in the future.


",smartshark_2_2,67,archiva,"""[0.3375902473926544, 0.662409782409668]"""
193,228452,Add feedback when remote check failed,"If remote checks fail there's a cloud and a warning sign only currently. It'd be useful to get some feedback why the remote check failed in the web frontend.

Referencing the log file should be find (as long as it's copyable, not too space consuming in the web frontend and really contains helpful information (i.e. please don't add `An error occured. Please check the logs`, but `The error ""[useful error title] occured because [useful error cause]. Please check the log file [full path to log file] for details`)",smartshark_2_2,1927,archiva,"""[0.9984883069992065, 0.0015116853173822165]"""
194,228453,project builder cache needs to be configurable to prevent memory leaking,"need to investigate if there is a memory leak, since the server received an OOME some time after successfully completing a full index with 1Gb heap with just some browsing/searching. It is most likely in the indexing itself, but there may be some in the browse/search interface too",smartshark_2_2,616,archiva,"""[0.7722227573394775, 0.22777725756168365]"""
195,228454,"add some caching headers to css, javascript and images to improve performance, especially if using a HTTPD reverse proxy",also document the best way to configure if httpd is in front of it,smartshark_2_2,1840,archiva,"""[0.9975245594978333, 0.002475421642884612]"""
196,228455,Please provide some UI element which shows degraded and/or badly connected remote repository,It would be helpful to users to be able to see from the admin/repositories.action if a particular remote repository is in a bad state.,smartshark_2_2,1418,archiva,"""[0.9984326958656311, 0.001567238592542708]"""
197,228456,Adding new mimetypes to archiva should be configurable,"Sometimes it is necessary to add a new mime type for a local installation.
Doing this is currently done by:

1. edit the file WEB-INF/applicationContext.xml in the webapp folder by uncommenting:
{code}
  <!--
  <bean name=""mimeTpes"" class=""org.apache.archiva.webdav.util.MimeTypes"">
    <property name=""resource"" value=""archiva-mime-types.txt""/>
  </bean>
  -->
{code}
So you can use your own file.

2. Then Copy the original
(http://svn.apache.org/repos/asf/archiva/trunk/archiva-modules/archiva-web/archiva-webdav/src/main/resources/org/apache/archiva/webdav/util/mime.types) to WEB-INF/classes/archiva-mime-types.txt and add or change a mime type  line.



This would be much easier if this could be configurable by beeing able to set the mime-types file in the achiva config file. And preferably being able to do it via the GUI.

As a minimum we should be able to set the filename. 
As an advanced and possible future solution we could set and unset each line from that file.",smartshark_2_2,1930,archiva,"""[0.9986030459403992, 0.001397021347656846]"""
198,228457,improve search results page,"currently, the search results are very plain. We should:
- provide more information about the artifact
- provide more information about the relevance of the result
- provide information about how the search hit occurred (what field)
- paginate the results",smartshark_2_2,981,archiva,"""[0.9983372688293457, 0.0016627246513962746]"""
199,228458,various clean up tasks related to database removal,"- review FIXME and TODO markers in new code (to fix, or to migrate to issues instead and mark in code)
- review test coverage of new code (should be ok, but worth checking with EMMA)
- remove the duplication of mocks and test repositories across various modules to do the same thing
- review suitable places for logging
- go back over new code and remove PlexusInSpringTestCase where possible
** in favour of just creating the objects, or look into an alternative with Mockito
** remove getTestFile and look up from resources or user.dir
",smartshark_2_2,1907,archiva,"""[0.9983715415000916, 0.0016285086749121547]"""
200,228459,discover standalone POMs,where the pom is the artifact,smartshark_2_2,826,archiva,"""[0.9982882142066956, 0.0017117472598329186]"""
201,228460,"create a second index with a subset of information, and have it compressed",required for the eclipse plugin. Will need to follow up with Jason on exactly what data is required.,smartshark_2_2,712,archiva,"""[0.9980553388595581, 0.0019446902442723513]"""
202,228461,Display account name of the user who uploaded the artifact in the artifact information (if the artifact was deployed/uploaded in the Archiva repo),"When I search for an artifact (or browse for one), I'd like archiva to inform me which repository it is stored in.  In addition, if someone uploaded it, I'd like to know the account for who uploaded it if I have a problem with the file.",smartshark_2_2,109,archiva,"""[0.997247040271759, 0.0027528873179107904]"""
203,228462,Use commons-io instead of Plexus Utils,"Per conversation in Archiva-Dev.

Migrate away from Plexus Utils to commons-io (where appropriate)

In our effort to migrate away from plexus towards Spring, we need to eliminate our usage of plexus-utils.
",smartshark_2_2,795,archiva,"""[0.9982818365097046, 0.0017181425355374813]"""
204,228463,Users - Manage section: pagination needs to change,"Hi.  I've configured Archiva to pull users from Active Direcotry.

The number of users being returned results in 2001 boxes being populated under the users table.

I'm trying to find my AD user, but I have to click on each page navigation box.  This gets to be a real pain having to scroll down to the bottom, click the link, and then scroll back to the top.  Since the table re-renders via AJAX, there was no indication that the current page had changed.

It seems like a better approach would be to have the pagination UI controlled via a:
<First Page Previous Page Next Page Last Page>
...setup or something similar.",smartshark_2_2,1594,archiva,"""[0.9132785797119141, 0.08672136813402176]"""
205,228464,Test failure in RepositoryContentConsumerUtilTest,"Building from archiva/trunk, using 'mvn clean build'  from the root, JDK 1.5.0_11
1 test is failing in RepositoryContentConsumerUtilTest:

-------------------------------------------------------------------------------
Test set: org.apache.maven.archiva.repository.scanner.RepositoryContentConsumerUtilTest
-------------------------------------------------------------------------------
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.313 sec <<< FAILURE!
testExecution(org.apache.maven.archiva.repository.scanner.RepositoryContentConsumerUtilTest)  Time elapsed: 0.031 sec  <<< FAILURE!
junit.framework.AssertionFailedError: 
  Expectation failure on verify:
    processFile(""path/to/test-file.txt""): expected: 1, actual: 0
	at org.easymock.MockControl.verify(MockControl.java:205)
	at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumerUtilTest.testExecution(RepositoryContentConsumerUtilTest.java:140)

",smartshark_2_2,1239,archiva,"""[0.9585715532302856, 0.041428420692682266]"""
206,228465,"Add a time based hold on ""delete released snapshots""","It would be nice, if the ""delete released snapshots"" flag could preserve recently build snapshots for a while. 

In general the idea is, that when we release an artifact we want to give the consumers a few days to update their POMs - If we delete the snapshot right away a lot of builds will be failing. It would be nice if one could define a number of days where snapshots should still be available (e.g. only delete snapshots which are older than e.g. 30 days or so) ",smartshark_2_2,1938,archiva,"""[0.9984276294708252, 0.0015724442200735211]"""
207,228466,"repository purge consumer should check that it is running first, and if it consumes an artifact it should prevent other consumers from running",this is to prevent the index/database getting inconsistent if it scans an artifact for the first time and decides to delete it,smartshark_2_2,1238,archiva,"""[0.8911845684051514, 0.10881546139717102]"""
208,228467,move repository purge consumers to the new repository API,"the purge has been updated to properly clean up metadata in the new repository (by using the deletion event listener), however it's behaviour still calls on the old archiva-model and archiva-repository-layer. This should be updated to the repository storage interfaces.",smartshark_2_2,391,archiva,"""[0.9985692501068115, 0.0014307538513094187]"""
209,228468,Ability to disable the checksum applet used in Find Artifact,"Applets that access the local filesystem are forbidden in some corporate environments.  We need a way to disable the Find Artifact applet without breaking the web UI.  

The ability to find an artifact by providing the md5 checksum should be preserved.",smartshark_2_2,1070,archiva,"""[0.9984591007232666, 0.0015408707549795508]"""
210,228469,"Accomodate ""query everything"" in the index",Index must accomodate google-like search wherein the keyword or term can be searched on all the fields in the index.,smartshark_2_2,895,archiva,"""[0.9981661438941956, 0.0018339107045903802]"""
211,228470,Web application to add new artifacts to the repository,"A web application to add new artifacts to the repository might be a good feature. If an organization have existing non-Maven 2 builds and would like to migrate, it would be easier for them to use a form to do so instead of a command line deploy.",smartshark_2_2,1027,archiva,"""[0.9974684715270996, 0.0025315145030617714]"""
212,228471,Release artifacts are missing license and notice files,"All released artifacts must contain license and notice files.  The files are missing from the 0.9-alpha-1 build.

Probably needs to be fixed on trunk also.",smartshark_2_2,582,archiva,"""[0.997077465057373, 0.0029226006008684635]"""
213,228472,integration with discovery and scheduling,"I presume the regular indexer will process an individual file and add it to the index, or remove a file from the index. This should integrated with the discovery mechanism so that it can trigger the indexing of files (or removal - though this is not currently in the discovery mechanism).

As for scheduling, it may be that the index updates are processed on a scheduled interval so that integration might be needed.",smartshark_2_2,882,archiva,"""[0.9984822869300842, 0.0015176968881860375]"""
214,228473,There is no image in the carousel for a couple of seconds,The carousel of screenshots doesn't display any image for a couple of second because there is no active image at first.,smartshark_2_2,1611,archiva,"""[0.7702288031578064, 0.22977125644683838]"""
215,228474,archiva homepage should highlight the zip version,"Due to MRM-671, and the more general fact that tar isn't very well standardized across OSes, the zip version should be more visible.  Right now you have to click through to ""All Downloads"" to find the zip file; I very nearly posted a request to users asking someone to make me a zip version, thinking that there wasn't one.",smartshark_2_2,1474,archiva,"""[0.9983729124069214, 0.001627085730433464]"""
216,228475,"Should be able to set the ""sender"" address on registration emails","Registration emails are sent as from archiva@localhost on this server. The local Exim mail server rejects this as unroutable and refuses to deliver. I'd prefer not to work around this by altering the Exim configs.

Is there a way to add this to the javamail configs in plexus.xml?

I'll check the source code to find out more but please let me know if this is already covered off.",smartshark_2_2,1478,archiva,"""[0.9855549335479736, 0.014445056207478046]"""
217,228476,search should tokenize on a hypthen,"I was searching for ""j2ee"" in my Archiva instance and it turned up nothing, even though there was a ""jboss-j2ee"".",smartshark_2_2,134,archiva,"""[0.9952623844146729, 0.0047375853173434734]"""
218,228477,Encoding issues with Archiva UI: add Spring CharacterEncodingFilter,"Please do like Continuum and add the CharacterEncodingFilter to force UTF-8 encoding.

See attached patch.",smartshark_2_2,1620,archiva,"""[0.9969630837440491, 0.003036867594346404]"""
219,228478,Implement WS for creating stage reporitory,Implement WS for creating stage reporitory,smartshark_2_2,1811,archiva,"""[0.9974775910377502, 0.0025223633274435997]"""
220,228479,Test failures in BytecodeIndexTest due to NPE in AbstractIndexCreationTestCase,"Commented out the following tests to be able to release 1.0-beta-1:
- testAddRecordNoIndex()
- testAddRecordExistingEmptyIndex()
- testAddRecordInIndex()
- testDeleteRecordInIndex()",smartshark_2_2,1210,archiva,"""[0.8907122015953064, 0.1092878207564354]"""
221,228480,change default skin to match Continuum,"look into the WW templates being used, as well as updating the sitemesh template.

Try to drive as much as possible by the sitemesh template and CSS - however if the WW theme needs to be changed, try to have it in a reusable JAR instead of copying the files from Continuum",smartshark_2_2,849,archiva,"""[0.9984847903251648, 0.0015152134001255035]"""
222,228481,Encrypt network proxy password on archiva.xml,"It is common to most of companies to provide Internet Services through network proxies. But it is unlikely to have anonymous access on such nodes. 
Archiva stores passwords in a plain text format, generating a security risk or security flaw.

It is really critical to have a encrypted password on Archiva's configuration file.

",smartshark_2_2,403,archiva,"""[0.99111407995224, 0.008885965682566166]"""
223,228482,move to a struts2 theme based on bootstrap css and jquery,so the name of this new struts2 theme will be jqboot (jquery/bootstrap :-) ),smartshark_2_2,773,archiva,"""[0.996338963508606, 0.003661029040813446]"""
224,228483,Archivas relocation feature should be configurable,"Archiva automatically delivers the new pom and jar for a relocated artifact to a maven client.

The downside of this feature is that clients do not get a warning that the artifact is relocated anymore.

In a ""All-Maven2"" environment this warning is quite good, and gives the developer a hint and a motivation (get rid of the warning ;-) ) to use the new groupId.

So I think it would be a good idea to make this feature configurable, so the archiva admin can turn it off.

",smartshark_2_2,1390,archiva,"""[0.9984717965126038, 0.001528184744529426]"""
225,228484,Consumer Archetype not GAV not customisable,"Using consumer archetype this way:

mvn archetype:generate \
   -DarchetypeRepository=repo1.maven.org \
   -DarchetypeGroupId=org.apache.archiva \
   -DarchetypeArtifactId=archiva-consumer-archetype \
   -DarchetypeVersion=2.0.0 

leads to a archetype created with GAV from release of archiva not the one you specify in the CLI or in IDE.

",smartshark_2_2,1326,archiva,"""[0.8671503663063049, 0.1328495889902115]"""
226,228485,use new UI web framework ,replace struts2 with a new web framework (plain js/html application),smartshark_2_2,777,archiva,"""[0.9976860284805298, 0.0023139878176152706]"""
227,228486,Upgrade to jackrabbit-webdav 1.5.0,Upgrade to jackrabbit-webdav 1.5.0,smartshark_2_2,146,archiva,"""[0.99811851978302, 0.0018815267831087112]"""
228,228487,RelJsonWriter outputs invalid JSON,"Serializing a RelNode to JSON using org.apache.calcite.rel.externalize.RelJsonWriter outputs invalid JSON. JSON requires that key names be quoted, but the output uses unquoted key names. This makes it impossible to interoperate with tools that require valid JSON.",smartshark_2_2,389,calcite,"""[0.09882412850856781, 0.901175856590271]"""
229,228488,Rules fail to match because of missing link to parent equivalence set,"There is no ""parent link"" in RelSubset, it is only for RelSet. However, RelSubset provides a ""getParentRels()"" link.
{code}
  public Collection<RelNode> getParentRels() {
    final Set<RelNode> list = new LinkedHashSet<RelNode>();
  parentLoop:
    for (RelNode parent : set.getParentRels()) {
      for (RelSubset rel : inputSubsets(parent)) {
        if (rel.set == set && rel.getTraitSet().equals(traitSet)) {
          list.add(parent);
          continue parentLoop;
        }
      }
    }
    return list;
  }
{code}

Now that we have a RelNode {{P1}} and its input subset is {{S1}}, and we have a rule that matches {{P1}} and does the following:
{code}
onMatch(RelOptRuleCall call) {
  RelNode P1 = call.rel(0);
  RelNode S1 = P1.getInput(0);
  RelTraitSet t1 = S1.getTraitSet();
  RelTraitSet t2 = t1.replace(tx);
  RelNode S2 = convert(S1, t2);
  RelNode P2 = P1.copy(P1.getCluster, P1.getTraitSet(), S2);
  call.transformTo(P2);
}
{code}
Note that {{t1}} is *NOT equal* to {{t2}}. So right now we have two subsets, {{S1}} and {{S2}}, in the same set; and their parents {{P1}} and {{P2}} in another set.
{code}
P1            P2
|             |
S1(t1)        S2(t2)
{code}
{{getParentRels(S1)}} would return {{P1}}, and {{getParentRels(S2)}} would return {{P2}}. This works fine except when {{t1}} satisfies {{t2}} (for instance, if {{tx}} is the empty collation trait) and as a result {{S2}} subsumes {{S1}}. Thus the expression {{S2 = convert(S1, t2)}} would eventually turn into {{S1}} after conversion, so {{P2}} is also a parent of {{S1}}, and {{getParentRels(S1)}} should return {{P1, P2}} instead.
Suppose we have a second rule that should match {{P2 - R}} subsequently, {{R}} as some Rel in {{S1}}, it would NOT be fired since {{P2}} is not returned in the parent list of {{S1}}. It would not match {{R}} in {{S2}} either since it is in the form of {{P2 - AbstractConverter - R}}.
",smartshark_2_2,719,calcite,"""[0.07276042550802231, 0.9272395968437195]"""
230,228489,Parameter 'signed' metadata is always returned as false,"Prepare a statement with parameters
ie. {noformat}request: {
    ""request"":""prepare"",
    ""connectionId"":""1646a1b9-334e-4a21-ade8-47c3d0c8e5a3"",
    ""sql"":""SELECT * FROM employees where emp_id =?"",
    ""maxRowCount"":-1
    }{noformat}
response
{noformat}{
""response"":""prepare"",
""statement"":{
    ""connectionId"":""1646a1b9-334e-4a21-ade8-47c3d0c8e5a3"",
    ""id"":972365839,
    ""signature"":{
        ""columns"":[
            {""ordinal"":0,""autoIncrement"":false,""caseSensitive"":false,""searchable"":true,""currency"":false,""nullable"":0,""signed"":true,""displaySize"":40,""label"":""EMP_ID"",""columnName"":""EMP_ID"",""schemaName"":"""",""precision"":0,""scale"":0,""tableName"":""EMPLOYEES"",""catalogName"":"""",""type"":{""type"":""scalar"",""id"":4,""name"":""INTEGER"",""rep"":""PRIMITIVE_INT""},""readOnly"":true,""writable"":false,""definitelyWritable"":false,""columnClassName"":""java.lang.Integer""},
            {""ordinal"":1,""autoIncrement"":false,""caseSensitive"":false,""searchable"":true,""currency"":false,""nullable"":1,""signed"":false,""displaySize"":40,""label"":""LAST_NAME"",""columnName"":""LAST_NAME"",""schemaName"":"""",""precision"":0,""scale"":0,""tableName"":""EMPLOYEES"",""catalogName"":"""",""type"":{""type"":""scalar"",""id"":12,""name"":""VARCHAR"",""rep"":""STRING""},""readOnly"":true,""writable"":false,""definitelyWritable"":false,""columnClassName"":""java.lang.String""},
            {""ordinal"":2,""autoIncrement"":false,""caseSensitive"":false,""searchable"":true,""currency"":false,""nullable"":1,""signed"":false,""displaySize"":40,""label"":""FIRST_NAME"",""columnName"":""FIRST_NAME"",""schemaName"":"""",""precision"":0,""scale"":0,""tableName"":""EMPLOYEES"",""catalogName"":"""",""type"":{""type"":""scalar"",""id"":12,""name"":""VARCHAR"",""rep"":""STRING""},""readOnly"":true,""writable"":false,""definitelyWritable"":false,""columnClassName"":""java.lang.String""}],
        ""sql"":""SELECT * FROM employees where emp_id=?"",
        ""parameters"":[
            {""signed"":false,""precision"":0,""scale"":0,""parameterType"":4,""typeName"":""INTEGER"",""className"":""java.lang.Integer"",""name"":""?1""}
        ],
        ""cursorFactory"":{""style"":""LIST"",""clazz"":null,""fieldNames"":null}}
    }
}{noformat}",smartshark_2_2,1115,calcite,"""[0.13304924964904785, 0.8669508099555969]"""
231,228490,"Check year, month, day, hour, minute and second ranges for date and time literals","Currently, if the year that is passed into {{DateString}} constructor has five digits, the first digit is trimmed. This trimming happens in the {{DateTimeStringUtils.int4()}} method:
{code:sql}
  private static void int4(StringBuilder buf, int i) {
    buf.append((char) ('0' + (i / 1000) % 10));
    buf.append((char) ('0' + (i / 100) % 10));
    buf.append((char) ('0' + (i / 10) % 10));
    buf.append((char) ('0' + i % 10));
  }
{code}
The same problem with month and day values.

Instead of trimming the value, the correct behaviour is to throw an exception if any of the values are outside the expected range.",smartshark_2_2,2183,calcite,"""[0.41160228848457336, 0.5883976817131042]"""
232,228491,JDBC adapter incorrectly pushes windowed aggregates down to HSQLDB,"JDBC adapter incorrectly pushes windowed aggregates down to HSQLDB. Queries containing window functions fail when using HSQLDB (or any other DB that does not support window functions) because the optimizer converts them to native SQL with window functions which are not supported by HSQLDB. For example:
{code:sql}
select ""store_id"", ""product_id"", sum(""unit_sales"") unit_sales, row_number() over (
partition by ""store_id"" order by sum(""unit_sales"") desc
) row_num
from ""sales_fact_1998""
group by ""store_id"", ""product_id""
{code}",smartshark_2_2,2396,calcite,"""[0.08482932299375534, 0.9151706695556641]"""
233,228492,"In DateRangeRules, make either TIMESTAMP or DATE literal, according to target type","The following test at org.apache.calcite.test.DruidAdapterIT
{code}
@Test
  public void testCombinationOfValidAndNotValidAndInterval() {
    final String sql = ""SELECT COUNT(*) FROM \""foodmart\"" ""
        + ""WHERE  \""timestamp\"" < CAST('1997-01-02' as TIMESTAMP) AND ""
        + ""EXTRACT(MONTH FROM \""timestamp\"") = 01 AND EXTRACT(YEAR FROM \""timestamp\"") = 01 "";
    sql(sql, FOODMART)
        .runs();
  }
{code}
Leads to 
{code}
java.lang.RuntimeException: exception while executing [SELECT COUNT(*) FROM ""foodmart"" WHERE  ""timestamp"" < CAST('1997-01-02' as TIMESTAMP) AND EXTRACT(MONTH FROM ""timestamp"") = 01 AND EXTRACT(YEAR FROM ""timestamp"") = 01 ]

	at org.apache.calcite.test.CalciteAssert$AssertQuery.runs(CalciteAssert.java:1411)
	at org.apache.calcite.test.DruidAdapterIT.testCombinationOfValidAndNotValidAndInterval(DruidAdapterIT.java:3497)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: java.lang.RuntimeException: With materializationsEnabled=false, limit=0
	at org.apache.calcite.test.CalciteAssert.assertQuery(CalciteAssert.java:600)
	at org.apache.calcite.test.CalciteAssert$AssertQuery.runs(CalciteAssert.java:1407)
	... 23 more
Caused by: java.sql.SQLException: Error while executing SQL ""SELECT COUNT(*) FROM ""foodmart"" WHERE  ""timestamp"" < CAST('1997-01-02' as TIMESTAMP) AND EXTRACT(MONTH FROM ""timestamp"") = 01 AND EXTRACT(YEAR FROM ""timestamp"") = 01 "": Error while applying rule FilterDateRangeRule, args [rel#19:LogicalFilter.NONE.[](input=rel#18:Subset#0.BINDABLE.[],condition=AND(<($0, CAST('1997-01-02'):TIMESTAMP(0) NOT NULL), =(EXTRACT(FLAG(MONTH), $0), 1), =(EXTRACT(FLAG(YEAR), $0), 1)))]
	at org.apache.calcite.avatica.Helper.createException(Helper.java:56)
	at org.apache.calcite.avatica.Helper.createException(Helper.java:41)
	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)
	at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:218)
	at org.apache.calcite.test.CalciteAssert.assertQuery(CalciteAssert.java:568)
	... 24 more
Caused by: java.lang.RuntimeException: Error while applying rule FilterDateRangeRule, args [rel#19:LogicalFilter.NONE.[](input=rel#18:Subset#0.BINDABLE.[],condition=AND(<($0, CAST('1997-01-02'):TIMESTAMP(0) NOT NULL), =(EXTRACT(FLAG(MONTH), $0), 1), =(EXTRACT(FLAG(YEAR), $0), 1)))]
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:236)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:650)
	at org.apache.calcite.tools.Programs$5.run(Programs.java:326)
	at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:387)
	at org.apache.calcite.prepare.Prepare.optimize(Prepare.java:188)
	at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:319)
	at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:230)
	at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:781)
	at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:640)
	at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:610)
	at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:221)
	at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:603)
	at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:638)
	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:149)
	... 26 more
Caused by: java.lang.ClassCastException: org.apache.calcite.util.TimestampString cannot be cast to org.apache.calcite.util.DateString
	at org.apache.calcite.util.DateString.compareTo(DateString.java:32)
	at org.apache.calcite.rex.RexSimplify.processRange(RexSimplify.java:1109)
	at org.apache.calcite.rex.RexSimplify.simplifyAnd2ForUnknownAsFalse(RexSimplify.java:711)
	at org.apache.calcite.rex.RexSimplify.simplifyAnd2ForUnknownAsFalse(RexSimplify.java:579)
	at org.apache.calcite.rex.RexSimplify.simplifyAnds(RexSimplify.java:253)
	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:936)
	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:926)
	at org.apache.calcite.rel.rules.DateRangeRules$FilterDateRangeRule.onMatch(DateRangeRules.java:179)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:212)
	... 39 more
{code}",smartshark_2_2,2325,calcite,"""[0.8165429830551147, 0.18345706164836884]"""
234,228493,"SQL standard REAL is 4 bytes, FLOAT is 8 bytes","Per the SQL standard,
* the SQL REAL type is a 4 byte floating point number (thus maps to Java float),
* FLOAT is up to 8 bytes (thus maps to Java double), and
* DOUBLE is 8 bytes (thus maps to Java double).

A number of places in Calcite map FLOAT to float values, and I think a few others map REAL to double.",smartshark_2_2,275,calcite,"""[0.5385066270828247, 0.4614933729171753]"""
235,228494,CAST('<string>' as TIMESTAMP) adds part of sub-second fraction to the value,"{noformat}
select   
  TIMESTAMP '2016-02-26 19:06:00',
  CAST('2016-02-26 19:06:00' as TIMESTAMP);
 +---------------------+---------------------+
 | EXPR$0              | EXPR$1              |
 +---------------------+---------------------+
 | 2016-02-26 19:06:00 | 2016-02-26 19:06:00 |
 +---------------------+---------------------+
!ok

select
  TIMESTAMP '2016-02-26 19:06:00.1',
  CAST('2016-02-26 19:06:00.1' as TIMESTAMP),
  TIMESTAMPDIFF(SECOND,
    TIMESTAMP '2016-02-26 19:06:00.1',
    CAST('2016-02-26 19:06:00.1' as TIMESTAMP));
 +---------------------+---------------------+--------+
 | EXPR$0              | EXPR$1              | EXPR$2 |
 +---------------------+---------------------+--------+
 | 2016-02-26 19:06:00 | 2016-02-26 19:06:00 |      0 |
 +---------------------+---------------------+--------+
!ok

select
  TIMESTAMP '2016-02-26 19:06:00.123456',
  CAST('2016-02-26 19:06:00.123456' as TIMESTAMP),
  TIMESTAMPDIFF(SECOND,
    TIMESTAMP '2016-02-26 19:06:00.123456',
    CAST('2016-02-26 19:06:00.123456' as TIMESTAMP));
 +---------------------+---------------------+--------+
 | EXPR$0              | EXPR$1              | EXPR$2 |
 +---------------------+---------------------+--------+
 | 2016-02-26 19:06:00 | 2016-02-26 19:08:03 |    123 |
 +---------------------+---------------------+--------+
!ok

select
  TIMESTAMP '2016-02-26 19:06:00.12345678',
  CAST('2016-02-26 19:06:00.12345678' as TIMESTAMP),
  TIMESTAMPDIFF(SECOND,
    TIMESTAMP '2016-02-26 19:06:00.123456789',
    CAST('2016-02-26 19:06:00.123456789' as TIMESTAMP));
 +---------------------+---------------------+--------+
 | EXPR$0              | EXPR$1              | EXPR$2 |
 +---------------------+---------------------+--------+
 | 2016-02-26 19:06:00 | 2016-02-26 22:31:46 | 123456 |
 +---------------------+---------------------+--------+
!ok
{noformat}

Note how {{TIMESTAMP <string>}} parses the value correctly (not sure if the sub-second fraction is parsed though) but {{CAST}} adds part of the sub-second fraction as a seconds to the value.",smartshark_2_2,1856,calcite,"""[0.10070238262414932, 0.8992976546287537]"""
236,228495,Exception when pushing postaggregates into Druid,"After Calcite is upgraded to 1.14 and the rule to push post-aggregations to Druid is enabled, the following query will fail:
{code}
EXPLAIN
SELECT language, robot, sum(added) - sum(delta) AS a
FROM druid_table_1
WHERE extract (week from `__time`) IN (10,11)
  AND robot='Bird Call'
GROUP BY language, robot;
{code}

The error we get is the following:
{code}
Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" language, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" robot, DOUBLE a) NOT NULL
expression type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" language, DOUBLE postagg#0) NOT NULL
set is rel#1507:HiveProject.HIVE.[](input=HepRelVertex#1514,language=$0,robot=CAST(_UTF-16LE'Bird Call'):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"",a=-($1, $2))
expression is DruidQuery#1516
{code}",smartshark_2_2,2084,calcite,"""[0.11158506572246552, 0.8884149789810181]"""
237,228496,AbstractRelNode.getId() may produce duplicate IDs in multi-threaded environment,"RelOptNode.getId() is supposed to be unique per instance; however, the generation ofÂ AbstractRelNode.id is not thread safe; in a multithreaded environment, the behavior is undeterministic, and there is a non-trivial possibility that duplicate IDs are generated for different nodes.

This ID is used for comparingÂ nodes in several places, therefore it may affect programÂ stability and correctness. For example, if we set id=0 for all nodes, Calcite's own test suite will have multiple failures.Â 

In our own test environment,Â the optimizers are expected to be deterministic, yet occasionallyÂ they mysteriously produce different plans. While we cannot be 100% sure that it is caused byÂ AbstractRelNode.id, we can verify that fudging with the ID will reliably change the output of our optimizers.",smartshark_2_2,2410,calcite,"""[0.07792375981807709, 0.9220762252807617]"""
238,228497,ConventionTraitDef.getConversionData() is not thread-safe,"Test suite hangs with several threads in {{WeakHashMap.get}}, all called from {{ConventionTraitDef.getConversionData}} and using the same table.

It is not a deadlock; all but one threads are waiting for a lock, but the thread that has the lock and is runnable.

Abbreviated stack trace:

{noformat}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode):
...
""pool-1-thread-7"" #20 daemon prio=5 os_prio=0 tid=0x00007f3cf4003000 nid=0x1f12 runnable [0x00007f3d3f8f4000]
   java.lang.Thread.State: RUNNABLE
	at java.util.WeakHashMap.expungeStaleEntries(WeakHashMap.java:341)
	- locked <0x00000000a08d14a8> (a java.lang.ref.ReferenceQueue)
	at java.util.WeakHashMap.getTable(WeakHashMap.java:350)
	at java.util.WeakHashMap.get(WeakHashMap.java:397)
	at org.apache.calcite.plan.ConventionTraitDef.getConversionData(ConventionTraitDef.java:203)
	at org.apache.calcite.plan.ConventionTraitDef.convert(ConventionTraitDef.java:129)
	at org.apache.calcite.plan.ConventionTraitDef.convert(ConventionTraitDef.java:46)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1197)
...
""pool-1-thread-8"" #19 daemon prio=5 os_prio=0 tid=0x00007f3cec001000 nid=0x1f11 waiting for monitor entry [0x00007f3d3f9f5000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at java.util.WeakHashMap.expungeStaleEntries(WeakHashMap.java:319)
	- waiting to lock <0x00000000a08d14a8> (a java.lang.ref.ReferenceQueue)
	at java.util.WeakHashMap.getTable(WeakHashMap.java:350)
	at java.util.WeakHashMap.get(WeakHashMap.java:397)
	at org.apache.calcite.plan.ConventionTraitDef.getConversionData(ConventionTraitDef.java:203)
	at org.apache.calcite.plan.ConventionTraitDef.convert(ConventionTraitDef.java:129)
	at org.apache.calcite.plan.ConventionTraitDef.convert(ConventionTraitDef.java:46)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraitsUsingConverters(VolcanoPlanner.java:1197)
...
""pool-1-thread-6"" #17 daemon prio=5 os_prio=0 tid=0x00007f3ce8001000 nid=0x1f0f waiting for monitor entry [0x00007f3d3fbf8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at java.util.WeakHashMap.expungeStaleEntries(WeakHashMap.java:319)
	- waiting to lock <0x00000000a08d14a8> (a java.lang.ref.ReferenceQueue)
	at java.util.WeakHashMap.getTable(WeakHashMap.java:350)
	at java.util.WeakHashMap.get(WeakHashMap.java:397)
	at org.apache.calcite.plan.ConventionTraitDef.getConversionData(ConventionTraitDef.java:203)
...
{noformat}

Hypothesis: we should be synchronizing access to {{ConventionTraitDef.plannerConversionMap}}.",smartshark_2_2,1662,calcite,"""[0.3547007739543915, 0.6452992558479309]"""
239,228498,AssertionError in GROUPING SETS query,"If AggregateJoinTransposeRule and AggregateProjectMergeRule are both enabled, and you run a query with grouping sets, you can get an AssertionError. For example, {code}Error while executing command CheckResultCommand [sql: select emp.deptno as e, dept.deptno as d
> from ""scott"".emp join ""scott"".dept using (deptno)
> group by cube(emp.deptno, dept.deptno)
> ]
> java.lang.AssertionError: Internal error: Error while applying rule AggregateProjectMergeRule, args [rel#1144:LogicalAggregate.NONE.[](input=rel#1143:Subset#4.NONE.[],group={0, 1},groups=[{0, 1}, {0}, {1}, {}],indicator=true), rel#1163:LogicalProject.NONE.[](input=rel#1162:Subset#7.NONE.[],DEPTNO=$1,DEPTNO0=$0)]
> 	at org.apache.calcite.util.Util.newInternal(Util.java:743)
> 	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:251)
> 	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:798)
> 	at org.apache.calcite.tools.Programs$5.run(Programs.java:272)
> 	at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:320)
> 	at org.apache.calcite.prepare.Prepare.optimize(Prepare.java:142)
> 	at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:280)
> 	at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:188)
> 	at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:521)
> 	at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:417)
> 	at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:386)
> 	at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:174)
> 	at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:489)
> 	at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:474)
> 	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109)
> 	at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:129)
> 	at net.hydromatic.quidem.Quidem$CheckResultCommand.execute(Quidem.java:690)
> 	at net.hydromatic.quidem.Quidem$CompositeCommand.execute(Quidem.java:926)
> 	at net.hydromatic.quidem.Quidem.execute(Quidem.java:193)
> 	at org.apache.calcite.test.JdbcTest.checkRun(JdbcTest.java:4401)
> 	at org.apache.calcite.test.JdbcTest.testRunJoin(JdbcTest.java:4349)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
> 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
> 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
> 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
> 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
> 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
> 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
> 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
> 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
> 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
> 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
> 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
> 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
> 	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
> 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
> 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
> 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
> Caused by: java.lang.AssertionError: [{0, 1}, {1}, {0}, {}]
> 	at org.apache.calcite.rel.core.Aggregate.<init>(Aggregate.java:132)
> 	at org.apache.calcite.rel.logical.LogicalAggregate.<init>(LogicalAggregate.java:65)
> 	at org.apache.calcite.rel.logical.LogicalAggregate.copy(LogicalAggregate.java:105)
> 	at org.apache.calcite.rel.logical.LogicalAggregate.copy(LogicalAggregate.java:43)
> 	at org.apache.calcite.rel.rules.AggregateProjectMergeRule.apply(AggregateProjectMergeRule.java:114)
> 	at org.apache.calcite.rel.rules.AggregateProjectMergeRule.onMatch(AggregateProjectMergeRule.java:61)
> 	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228){code}

The problem is that if you have a sorted list of grouping sets [[0, 2], [0], [2], []] and you permute the fields 0->2 and 2->1, then the permuted result [[2, 1], [2], [1], []] is no longer sorted. ([2] should occur after [1] but occurs before.)",smartshark_2_2,295,calcite,"""[0.0700041800737381, 0.9299958348274231]"""
240,228499,Resource leak on creation of CassandraSchema,"Found via the following Coverity scan

{noformat}
*** CID 138163:  Resource leaks  (RESOURCE_LEAK)
/cassandra/src/main/java/org/apache/calcite/adapter/cassandra/CassandraSchema.java: 115 in org.apache.calcite.adapter.cassandra.CassandraSchema.<init>(java.lang.String, java.lang.String, java.lang.String, java.lang.String, org.apache.calcite.schema.SchemaPlus, java.lang.String)()
109         } catch (Exception e) {
110           throw new RuntimeException(e);
111         }
112         this.parentSchema = parentSchema;
113         this.name  = name;
114
>>    CID 138163:  Resource leaks  (RESOURCE_LEAK)
>>    Ignoring resource created by ""org.apache.calcite.runtime.Hook.TRIMMED.add(this.new org.apache.calcite.adapter.cassandra.CassandraSchema.1())"" leaks it.
115         Hook.TRIMMED.add(new Function<RelNode, Void>() {
116           public Void apply(RelNode node) {
117             CassandraSchema.this.addMaterializedViews();
118             return null;
119           }
120         });
{noformat}",smartshark_2_2,1497,calcite,"""[0.17328518629074097, 0.826714813709259]"""
241,228500,Resolve Java user-defined functions that have Double and BigDecimal arguments,"Overloaded user-defined functions that have Double and BigDecimal arguments will goes wrong. We add the following udf method in {{Smalls.AllTypesFunction}}:
{code:java}
   public double toDouble(BigDecimal var) {
      return var.doubleValue();
   } 
   public double toDouble(Double var) {
      return var;
    }
   
{code}
when test it :
{code:java}
@Test
public void testBigDecimalAndLong() {
 final CalciteAssert.AssertThat with = withUdf();
 with.query(""values \""adhoc\"".\""toDouble\""(cast(1.0 as double))"")
 .returns(""EXPR$0=1.0\n"");
}{code}
where price is a double value in table tb, exception occurs:
{code:java}
java.lang.AssertionError: DECIMAL(19, 0) at org.apache.calcite.sql.type.SqlTypeExplicitPrecedenceList.compareTypePrecedence(SqlTypeExplicitPrecedenceList.java:154) at org.apache.calcite.sql.SqlUtil.bestMatch(SqlUtil.java:626) at org.apache.calcite.sql.SqlUtil.filterRoutinesByTypePrecedence(SqlUtil.java:592) at org.apache.calcite.sql.SqlUtil.lookupSubjectRoutines(SqlUtil.java:446) at org.apache.calcite.sql.SqlUtil.lookupRoutine(SqlUtil.java:371) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:245) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:223) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5432) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5419) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:138) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1606) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1591) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:236) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:407) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5136) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:115) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:235) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:407) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5136) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:115) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:903) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:613) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:553) at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:264) at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:230) at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:781) at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:640) at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:610) at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:221) at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:603) at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:638) at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:149) at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:218) at org.apache.calcite.test.CalciteAssert.assertQuery(CalciteAssert.java:568) at org.apache.calcite.test.CalciteAssert$AssertQuery.returns(CalciteAssert.java:1346) at org.apache.calcite.test.CalciteAssert$AssertQuery.returns(CalciteAssert.java:1329) at org.apache.calcite.test.CalciteAssert$AssertQuery.returns(CalciteAssert.java:1293) at org.apache.calcite.test.UdfTest.testBigDecimalAndLong(UdfTest.java:891) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:42) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:262) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:84) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147){code}",smartshark_2_2,2040,calcite,"""[0.12113865464925766, 0.8788613080978394]"""
242,228501,Project.getPermutation() should return null if a column is repeated,"Project.getPermutation() returns true when the project is referencing [$1, $1].  If a column repeats, it should not be considered a permutation.",smartshark_2_2,1969,calcite,"""[0.15946052968502045, 0.8405394554138184]"""
243,228502,AggregateJoinTransposeRule fails when process aggregateCall above SqlSumEmptyIsZeroAggFunction without groupKeys,"{code}
   final HepProgram preProgram = new HepProgramBuilder()
            .addRuleInstance(AggregateProjectMergeRule.INSTANCE)
            .build();
    final HepProgram program = new HepProgramBuilder()
            .addRuleInstance(AggregateReduceFunctionsRule.INSTANCE)
            .addRuleInstance(AggregateJoinTransposeRule.EXTENDED)
            .build();
    final String sql = ""select sum(sal)\n""
            + ""from (select * from sales.emp where empno = 10) as e\n""
            + ""join sales.dept as d on e.job = d.name"";
{code}
AggregateJoinTransposeRule fails when run the above sql, the exception is as following.  
{code}
java.lang.AssertionError: type mismatch:
aggCall type:
INTEGER NOT NULL
inferred type:
INTEGER

	at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
	at org.apache.calcite.plan.RelOptUtil.eq(RelOptUtil.java:1838)
	at org.apache.calcite.rel.core.Aggregate.typeMatchesInferred(Aggregate.java:428)
	at org.apache.calcite.rel.core.Aggregate.<init>(Aggregate.java:161)
	at org.apache.calcite.rel.logical.LogicalAggregate.<init>(LogicalAggregate.java:65)
	at org.apache.calcite.rel.logical.LogicalAggregate.create_(LogicalAggregate.java:110)
	at org.apache.calcite.rel.logical.LogicalAggregate.create(LogicalAggregate.java:100)
	at org.apache.calcite.rel.core.RelFactories$AggregateFactoryImpl.createAggregate(RelFactories.java:213)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1267)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1825)
	at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:337)
{code}

*Notes*: This is different with the issue referred in https://issues.apache.org/jira/browse/CALCITE-2105. This question caused by  the SqlSplittableAggFunction of SqlSumEmptyIsZeroAggFunction is SumSplitter. If groupByKeys is empty, the aggregateCalls returnTypes of Sum is nullable, while the returnTypes of Sum0 is not nullable.
",smartshark_2_2,2313,calcite,"""[0.07458239793777466, 0.9254176020622253]"""
244,228503,Simplification of point ranges that are open above or below yields wrong results,"Discovered while testing 1.15.0 RC0 with Hive. It seems this regression was introduced by CALCITE-1995.

Consider the following query:
{code}
select * from (
select a.*,b.d d1,c.d d2 from
  t1 a join t2 b on (a.id1 = b.id)
       join t2 c on (a.id2 = b.id) where b.d <= 1 and c.d <= 1
) z where d1 > 1 or d2 > 1
{code}

We end up generating the following plan:
{code}
HiveProject(id1=[$0], id2=[$1], d1=[$3], d2=[$4])
  HiveJoin(condition=[OR(=($3, 1), =($4, 1))], joinType=[inner], algorithm=[none], cost=[not available])
    HiveJoin(condition=[AND(=($0, $2), =($1, $2))], joinType=[inner], algorithm=[none], cost=[not available])
      HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($1))])
        HiveProject(id1=[$0], id2=[$1])
          HiveTableScan(table=[[default.t1]], table:alias=[a])
      HiveFilter(condition=[AND(<=($1, 1), IS NOT NULL($0))])
        HiveProject(id=[$0], d=[$1])
          HiveTableScan(table=[[default.t2]], table:alias=[b])
    HiveFilter(condition=[<=($0, 1)])
      HiveProject(d=[$1])
        HiveTableScan(table=[[default.t2]], table:alias=[c])
{code}
Observe that the condition in the top join is not correct.

I can reproduce this in {{RexProgramTest.simplifyFilter}} with the following example:
{code}
    // condition ""a > 5 or b > 5""
    // with pre-condition ""a <= 5 and b <= 5""
    // should yield ""false"" but yields ""a = 5 or b = 5""
    checkSimplifyFilter(or(gt(aRef, literal5),gt(bRef, literal5)),
        RelOptPredicateList.of(rexBuilder,
            ImmutableList.of(le(aRef, literal5), le(bRef, literal5))),
        ""false"");
{code}

",smartshark_2_2,2101,calcite,"""[0.05946914479136467, 0.9405308961868286]"""
245,228504,MetaImpl#fieldMetaData wrongly uses 1-based column ordinals,"Method {{org.apache.calcite.avatica.MetaImpl#fieldMetaData(Class)}} creates Column metadata info from a metadata class. At the same time, the method provides the column ordinal, which is starts at 1, whereas {{ColumnMetadata}} ordinal starts at 0 (according to field comment and usage inside avatica).",smartshark_2_2,1749,calcite,"""[0.06426146626472473, 0.9357385635375977]"""
246,228505,Not (C='a' or C='b') as well as Not (C='a' and C='b') causes NPE,"A where clause like Not (C='a' or C='b') causes NPE if C has NULL value.

The generated code snippet looks like:

{code}
/*  65 */           public boolean moveNext() {
/*  66 */             while (inputEnumerator.moveNext()) {
/*  67 */               final Object[] current = (Object[]) inputEnumerator.current();
/*  68 */               final String inp21_ = current[21] == null ? (String) null : current[21].toString();
/*  69 */               final Boolean v = inp21_ == null ? (Boolean) null : Boolean.valueOf(org.apache.calcite.runtime.SqlFunctions.eq(inp21_, ""A""));
/*  70 */               final Boolean v0 = inp21_ == null ? (Boolean) null : Boolean.valueOf(org.apache.calcite.runtime.SqlFunctions.eq(inp21_, ""B""));
/*  71 */               if (!(v == null ? (v0 == null || !v0 ? (Boolean) null : Boolean.TRUE) : v ? Boolean.TRUE : v0)) {
/*  72 */                 return true;
/*  73 */               }
/*  74 */             }
/*  75 */             return false;
/*  76 */           }
{code}

And NPE is thrown at line #71 if inp21_ is null.

Stacktrace:

{code}
Caused by: java.lang.NullPointerException
	at Baz$1$1.moveNext(ANONYMOUS.java:71)
	at org.apache.calcite.linq4j.EnumerableDefaults.groupBy_(EnumerableDefaults.java:737)
	at org.apache.calcite.linq4j.EnumerableDefaults.groupBy(EnumerableDefaults.java:677)
	at org.apache.calcite.linq4j.DefaultEnumerable.groupBy(DefaultEnumerable.java:301)
	at Baz.bind(Baz.java:95)
	at org.apache.calcite.jdbc.CalcitePrepare$CalciteSignature.enumerable(CalcitePrepare.java:281)
	at org.apache.calcite.jdbc.CalciteConnectionImpl.enumerable(CalciteConnectionImpl.java:235)
	at org.apache.calcite.jdbc.CalciteMetaImpl.createIterable(CalciteMetaImpl.java:533)
	at org.apache.calcite.avatica.AvaticaResultSet.execute(AvaticaResultSet.java:184)
	at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:63)
	at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:42)
	at org.apache.calcite.avatica.AvaticaConnection$1.execute(AvaticaConnection.java:473)
	at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:566)
	at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:477)
	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109)
	... 29 more
{code}
",smartshark_2_2,959,calcite,"""[0.06590458750724792, 0.9340953826904297]"""
247,228506,Druid adapter: Avoid duplication of fields names during Druid query planing,"This issue occurs if two projects layers use the same fields name, it will lead to a Druid query with a duplicated field names.
I can not reproduce this in Calcite but it is reproducible in [Hive|https://issues.apache.org/jira/browse/HIVE-19044] (it has to deal on how different layers of project are getting names)
Here is an example of faulty query where ""$f4"" is used twice.
{code}
{""queryType"":""groupBy"",""dataSource"":""druid_tableau.calcs"",""granularity"":""all"",""dimensions"":[{""type"":""default"",""dimension"":""key"",""outputName"":""key"",""outputType"":""STRING""}],""limitSpec"":{""type"":""default""},""aggregations"":[{""type"":""doubleSum"",""name"":""$f1"",""fieldName"":""num0""},{""type"":""filtered"",""filter"":{""type"":""not"",""field"":{""type"":""selector"",""dimension"":""num0"",""value"":null}},""aggregator"":{""type"":""count"",""name"":""$f2"",""fieldName"":""num0""}},{""type"":""doubleSum"",""name"":""$f3"",""expression"":""(\""num0\"" * \""num0\"")""},{""type"":""doubleSum"",""name"":""$f4"",""expression"":""(\""num0\"" * \""num0\"")""}],""postAggregations"":[{""type"":""expression"",""name"":""$f4"",""expression"":""pow(((\""$f4\"" - ((\""$f1\"" * \""$f1\"") / \""$f2\"")) / \""$f2\""),0.5)""}],""intervals"":[""1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z""]}
{code}",smartshark_2_2,2368,calcite,"""[0.2960028648376465, 0.7039971351623535]"""
248,228507,Substring operator broken for MSSQL,"The format mssql uses looks like:
SUBSTRING(input, start, length)

but the default is currently to unparse to SUBSTRING(input FROM start FOR length)",smartshark_2_2,1984,calcite,"""[0.09441440552473068, 0.9055855870246887]"""
249,228508,java.lang.IndexOutOfBoundsException when building aggregate on top of project,"The following test in RelBuilderTest:

{code:java}
  @Test public void testAggregateProjectWithAliases() {
    final RelBuilder builder = RelBuilder.create(config().build());
    RelNode root =
        builder.scan(""EMP"")
            .project(builder.field(""DEPTNO""))
            .aggregate(builder.groupKey(builder.alias(builder.field(""DEPTNO""), ""departmentNo"")))
            .build();
    final String expected = """"
        + ""LogicalAggregate(group=[{1}])\n"" +
        ""  LogicalProject(DEPTNO=[$0], departmentNo=[$0])\n"" +
        ""    LogicalProject(DEPTNO=[$7])\n"" +
        ""      LogicalTableScan(table=[[scott, EMP]])\n"";
    assertThat(str(root), is(expected));
  }
{code}

throws the following exception: 

{noformat}
java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)

	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:310)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:293)
	at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:41)
	at org.apache.calcite.rel.metadata.RelMdColumnUniqueness.areColumnsUnique(RelMdColumnUniqueness.java:206)
	at GeneratedMetadataHandler_ColumnUniqueness.areColumnsUnique_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnUniqueness.areColumnsUnique(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.areColumnsUnique(RelMetadataQuery.java:542)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.areColumnsUnique(RelMetadataQuery.java:522)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1067)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1046)
	at org.apache.calcite.test.RelBuilderTest.testAggregateProjectWithAliases(RelBuilderTest.java:668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
{noformat}

It is caused by the fact RelBuilder can check uniqueness on non-existing columns if groupKey in aggregate contains something more elaborate than a reference to the column in the underlying node. In this case it's an column alias but can also be some expression.",smartshark_2_2,2070,calcite,"""[0.06979252398014069, 0.9302074313163757]"""
250,228509,PreparedStatement does not process Date type correctly ,"I have problem when passing date parameter in PreparedStatement. Following is the sample code:
TimeZone tzUtc = TimeZone.getTimeZone(""UTC"");
Calendar cUtc = Calendar.getInstance(tzUtc);
cUtc.set(2013, 10, 5, 0, 0, 0);
java.sql.Date passSqlDate = new java.sql.Date(cUtc.getTimeInMillis());
statement.setDate(1, passSqlDate, cUtc);
I try to pass Date'2013/11/5' to the database, but from the debug info I found the 
database received '2013/10/4' (GMT -8 Timezone) always. It ignored the Calendar I passed in.

There are some Calcite code a little strange, I have marked by color:
Why setTimestamp and setTime have effective parameter 'calendar', but setDate ignore the
parameter. Is that the root cause?
AvaticaSite.java
Line 189-199
  public void setTimestamp(Timestamp x, Calendar calendar) {
    slots[index] = wrap(ColumnMetaData.Rep.JAVA_SQL_TIMESTAMP, x, calendar);
  }

  public void setTime(Time x, Calendar calendar) {
    slots[index] = wrap(ColumnMetaData.Rep.JAVA_SQL_TIME, x, calendar);
  }

  public void setDate(Date x, Calendar cal) {
    slots[index] = wrap(ColumnMetaData.Rep.JAVA_SQL_DATE, x, calendar);
  }
",smartshark_2_2,1369,calcite,"""[0.08679008483886719, 0.9132099747657776]"""
251,228510,Decorrelation fails if query has more than one EXISTS in WHERE clause,"When planning a query with more than one EXISTS in the same WHERE clause, Calcite throws in the {{RelDecorrelator.allLessThan}} method.

Calcite hits an assertion for following query (planning is done by calling {{SubQueryRemoveRule}} followed by {{decorrelateQuery}}):

{code}  select * from emp
where EXISTS (select * from emp e where emp.deptno = e.deptno)
AND NOT EXISTS (select * from emp ee where ee.job = emp.job AND ee.sal=34) {code}

Assertion
{noformat} Caused by: java.lang.AssertionError: out of range; value: 3, limit: 3 {noformat}

This assertion is hit in {{RelDecorrelator's allLessThan}} which is called while registering newly de-correlated {{LogicalAggregate}}.

Plan just before {{SubQueryRemoveRule}}:
{noformat}
LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8])
  LogicalFilter(condition=[AND(EXISTS({
LogicalFilter(condition=[=($cor0.DEPTNO, $7)])
  LogicalTableScan(table=[[CATALOG, SALES, EMP]])
}), NOT(EXISTS({
LogicalFilter(condition=[AND(=($2, $cor0.JOB), =($5, 34))])
  LogicalTableScan(table=[[CATALOG, SALES, EMP]])
})))], variablesSet=[[$cor0]])
    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
{noformat}

Plan just after {{SubQueryRemoveRule}}:
{noformat}
LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8])
  LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8], i=[$9])
      LogicalFilter(condition=[AND(OR(IS NULL($12), =($10, 0)), OR(>=($11, $10), =($10, 0)))])
        LogicalJoin(condition=[true], joinType=[left])
          LogicalJoin(condition=[true], joinType=[inner])
            LogicalCorrelate(correlation=[$cor0], joinType=[INNER], requiredColumns=[{7}])
              LogicalTableScan(table=[[CATALOG, SALES, EMP]])
              LogicalAggregate(group=[{0}])
                LogicalProject(i=[true])
                  LogicalFilter(condition=[=($cor0.DEPTNO, $7)])
                    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
            LogicalAggregate(group=[{}], c=[COUNT()], ck=[COUNT($0, $1, $2, $3, $4, $5, $6, $7, $8)])
              LogicalFilter(condition=[AND(=($2, $cor0.JOB), =($5, 34))])
                LogicalTableScan(table=[[CATALOG, SALES, EMP]])
          LogicalAggregate(group=[{0}])
            LogicalProject(i=[true])
              LogicalFilter(condition=[AND(=($2, $cor0.JOB), =($5, 34))])
                LogicalTableScan(table=[[CATALOG, SALES, EMP]])
{noformat}
",smartshark_2_2,1458,calcite,"""[0.06730342656373978, 0.9326965808868408]"""
252,228511,"Allow ORDER BY aggregate function in SELECT DISTINCT, provided that it occurs in SELECT clause","For example, the query 
{code:sql}
select distinct count(empno) from emp
group by empno
order by 1
{code}

gave:

org.apache.calcite.sql.validate.SqlValidatorException <init>
SEVERE: org.apache.calcite.sql.validate.SqlValidatorException: Expression 'COUNT(`EMP`.`EMPNO`)' is not in the select clause",smartshark_2_2,268,calcite,"""[0.5497610569000244, 0.45023900270462036]"""
253,228512,Window aggregates invalid error/error messages in some cases,"Queries which fail (or fail with inconsistent errors)
Q1:
{code}select count( * ) over () from emp group by deptno 
AssertionError: star should have been expanded.
{code}
Q2:
{code}
SELECT sum(empno), max(empno) OVER (order by deptno) 
FROM emp 
GROUP BY empno 
gives error: Expression 'DEPTNO' is not being grouped

SELECT sum(empno), max(empno) OVER w 
FROM emp 
GROUP BY empno 
WINDOW w as (order by deptno)
gives no error:
{code}
Q3:
{code}
select cume_dist() over w , rank() 
from emp 
window w as (partition by deptno order by deptno)
Assertion Error: Expression 'DEPTNO' is not being grouped
instead of 
Assertion Error: OVER clause is necessary for window functions
{code}",smartshark_2_2,1255,calcite,"""[0.2409655898809433, 0.7590343952178955]"""
254,228513,Fix matching predicate for JdbcProjectRule rule,"InÂ CALCITE-2206 wasÂ prevented pushing windowed aggregates down for the {{SqlDialect}} sÂ which do not support windowed functions.

After these changes, {{JdbcProjectRule}} is matched *only* for the cases when {{SqlDialect}} supports windowed functions and project contains windowed functions:
{code:java}
return out.dialect.supportsWindowFunctions()
    && !RexOver.containsOver(project.getProjects(), null);
{code}

It causes unit tests failures seen on the master.",smartshark_2_2,2395,calcite,"""[0.1142825037240982, 0.8857174515724182]"""
255,228514,Push limit 0 will result in an infinite loop,"We use ""checkInputForCollationAndLimit"" in RelMdUtil.java to check the input #rows. However, it calls RelMetadataQuery.getRowCount which will validate the #rows. The validation will change #row=0 to #row=1. This will result in an infinite loop to push limit. The affected rules include 
SortUnionTransposeRule and any Sort***TransposeRules that call  checkInputForCollationAndLimit.
",smartshark_2_2,934,calcite,"""[0.06267847865819931, 0.9373215436935425]"""
256,228515,Multiple distinct-COUNT query gives wrong results,"The query {code}select ""department_id"" as d, count(distinct ""education_level"") as c1, count(distinct ""gender"") as c2 from foodmart_clone.""employee"" group by ""department_id"";{code} returns 0 rows and should return 12. In the plan {noformat}EnumerableCalc(expr#0..3=[{inputs}], department_id=[$t2], C1=[$t3], C2=[$t1])
  EnumerableJoin(condition=[=($0, $2)], joinType=[inner])
    EnumerableAggregate(group=[{0}], groups=[[{7}]], C2=[COUNT($1)])
      EnumerableAggregate(group=[{7, 15}])
        EnumerableTableScan(table=[[FOODMART_CLONE, employee]])
    EnumerableAggregate(group=[{0}], groups=[[{7}]], C1=[COUNT($1)])
      EnumerableAggregate(group=[{7, 13}])
        EnumerableTableScan(table=[[FOODMART_CLONE, employee]])
{noformat} you can see {noformat}group=[{0}], groups=[[{7}]]{noformat} and this is wrong -- the groups should be made up of the same bits as the group. We should add an assert on this invariant and fixing it will probably cause the plan to return the right results.",smartshark_2_2,353,calcite,"""[0.0784204751253128, 0.9215795993804932]"""
257,228516,Druid adapter: Incorrect result - limit on timestamp disappears,"This can be observed with the following query:

{code:sql}
SELECT DISTINCT `__time`
FROM store_sales_sold_time_subset
ORDER BY `__time` ASC
LIMIT 10;
{code}

Query is translated to Druid _timeseries_, but _limit_ operator disappears.",smartshark_2_2,1518,calcite,"""[0.0672587901353836, 0.9327412247657776]"""
258,228517,Metadata distribution is broken due to last refactor,"Metadata distribution is broken due to last refactor.

Calls of RelMdDistribution will get:

{code}
java.lang.AssertionError: are your methods named wrong?
{code}",smartshark_2_2,1074,calcite,"""[0.16156435012817383, 0.8384356498718262]"""
259,228518,AggregateJoinTransposeRule fails when process aggregate without group keys,"{code}
select sum(sal) from (select * from sales.emp where empno = 10) as e join sales.dept as d on e.job = d.name
{code}
AggregateJoinTransposeRule will broken when process the above sql, thrown exception is 
{code}
java.lang.AssertionError: type mismatch:
aggCall type:
INTEGER
inferred type:
INTEGER NOT NULL

	at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
	at org.apache.calcite.plan.RelOptUtil.eq(RelOptUtil.java:1838)
	at org.apache.calcite.rel.core.Aggregate.typeMatchesInferred(Aggregate.java:428)
	at org.apache.calcite.rel.core.Aggregate.<init>(Aggregate.java:161)
	at org.apache.calcite.rel.logical.LogicalAggregate.<init>(LogicalAggregate.java:65)
	at org.apache.calcite.rel.logical.LogicalAggregate.create_(LogicalAggregate.java:110)
	at org.apache.calcite.rel.logical.LogicalAggregate.create(LogicalAggregate.java:100)
	at org.apache.calcite.rel.core.RelFactories$AggregateFactoryImpl.createAggregate(RelFactories.java:213)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1267)
	at org.apache.calcite.tools.RelBuilder.aggregate(RelBuilder.java:1825)
	at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:260)
{code}
",smartshark_2_2,2312,calcite,"""[0.07504572719335556, 0.9249542355537415]"""
260,228519,JDBC adapter generates wrong SQL if UNION has more than two inputs,"JDBC adapter generates wrong SQL if set operation (UNION, INTERSECT or MINUS) has more than two inputs. In union example, after UnionMergeRule, the union input is convert to three input, and rel-to-sql convert result is wrong.

input sql
 {code:sql}
 SELECT *
FROM (SELECT \""product_id\""
FROM \""foodmart\"".\""product\""
UNION ALL
SELECT \""product_id\""
FROM \""foodmart\"".\""sales_fact_1997\"")
UNION ALL
SELECT \""product_class_id\"" AS \""PRODUCT_ID\""
FROM \""foodmart\"".\""product_class\""
 {code}
output sql
 {code:sql}
SELECT \""product_id\""
FROM \""foodmart\"".\""product\""
UNION ALL
SELECT \""product_id\""
FROM \""foodmart\"".\""sales_fact_1997\""
 {code}
the last union query is lost.
",smartshark_2_2,1639,calcite,"""[0.06825337558984756, 0.9317466020584106]"""
261,228520,Remote fetch in Calcite JDBC driver,"During a CalciteMetaImpl#prepare execute from preparedStatement, the Signature Object created from parseQuery method is not being set into the created StatementHandle Object",smartshark_2_2,339,calcite,"""[0.12909555435180664, 0.8709044456481934]"""
262,228521,Aggregate recommender blows up if row count estimate is too high,"If you run the aggregate recommendation algorithm with a value of rowCountEstimate that is wrong and large, the algorithm runs for a long time and eventually fails with ""OutOfMemoryError: GC overhead limit exceeded"".

I have added LatticeTest.testLatticeWithBadRowCountEstimate as a test case.",smartshark_2_2,1582,calcite,"""[0.13173151016235352, 0.8682685494422913]"""
263,228522,Scan HAVING clause for sub-queries and IN-lists,"This is another variant of CALCITE-516 and CALCITE-614.   

{code}
select count(*) from emp 
  group by emp.deptno
  having sum(case when emp.empno in (1, 2, 3) then emp.sal else 0 end) 
     between 10000.0 and 20000.0
{code}

{code}
java.lang.AssertionError: Internal error: while converting CASE WHEN `EMP`.`EMPNO` IN (1, 2, 3) THEN `EMP`.`SAL` ELSE 0 END
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4058)
	at org.apache.calcite.sql2rel.StandardConvertletTable.convertCase(StandardConvertletTable.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.sql2rel.ReflectiveConvertletTable$1.convertCall(ReflectiveConvertletTable.java:87)
	at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4212)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3668)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:130)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4105)
	at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4483)
	at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4329)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:130)
	at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4528)
	at org.apache.calcite.sql2rel.SqlToRelConverter$AggConverter.visit(SqlToRelConverter.java:4329)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:130)
	at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2573)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2510)
{code}",smartshark_2_2,352,calcite,"""[0.1621428281068802, 0.8378571271896362]"""
264,228523,AVG window function in GROUP BY gives AssertionError,"For example, 
{code}
select avg(deptno) over ()
from emp
group by deptno
{code}

gives 
{code}
java.lang.AssertionError: Internal error: while converting SUM(`EMP`.`DEPTNO`) / COUNT(`EMP`.`DEPTNO`)
	at org.apache.calcite.util.Util.newInternal(Util.java:792)
	at org.apache.calcite.sql2rel.ReflectiveConvertletTable$1.convertCall(ReflectiveConvertletTable.java:96)
	at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4116)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:3566)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:130)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4009)
	at org.apache.calcite.sql2rel.StandardConvertletTable$AvgVarianceConvertlet.convertCall(StandardConvertletTable.java:1121)
	at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:60)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertOver(SqlToRelConverter.java:1786)
	at org.apache.calcite.sql2rel.SqlToRelConverter.access$1000(SqlToRelConverter.java:183)
	at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4002)
	at org.apache.calcite.sql2rel.SqlToRelConverter.createAggImpl(SqlToRelConverter.java:2548)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertAgg(SqlToRelConverter.java:2369)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:620)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:581)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:2768)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:535)
	at org.apache.calcite.test.SqlToRelTestBase$TesterImpl.convertSqlToRel(SqlToRelTestBase.java:487)
	at org.apache.calcite.test.SqlToRelTestBase$TesterImpl.assertConvertsTo(SqlToRelTestBase.java:594)
	at org.apache.calcite.test.SqlToRelTestBase$TesterImpl.assertConvertsTo(SqlToRelTestBase.java:586)
	at org.apache.calcite.test.SqlToRelConverterTest$Sql.convertsTo(SqlToRelConverterTest.java:1421)
	at org.apache.calcite.test.SqlToRelConverterTest.check(SqlToRelConverterTest.java:55)
	at org.apache.calcite.test.SqlToRelConverterTest.test(SqlToRelConverterTest.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{code}
(Below is truncated.)",smartshark_2_2,659,calcite,"""[0.06600009649991989, 0.9339998960494995]"""
265,228524,Metadata provider should not pull predicates up through GROUP BY (),"In org.apache.calcite.rel.metadata.RelMdPredicates.java:

{code}
...
    for (RexNode r : inputInfo.pulledUpPredicates) {
      ImmutableBitSet rCols = RelOptUtil.InputFinder.bits(r);
      if (groupKeys.contains(rCols)) {
        r = r.accept(new RexPermuteInputsShuttle(m, input));
        aggPullUpPredicates.add(r);
      }
    }
...
{code}

The check does not take into account that _rCols_ might be empty, and then _r_ cannot be pulled up e.g. count\(*).",smartshark_2_2,1270,calcite,"""[0.5190810561180115, 0.4809189438819885]"""
266,228525,Infinite loop for JoinPushTransitivePredicatesRule,"Infinite loop is obtained while using FilterIntoJoinRule + JoinPushTransitivePredicatesRule in HEP planner for some correlated queries, for instance:
select d.deptno from sales.emp d where d.deptno IN (select e.deptno from sales.emp e where e.deptno = d.deptno or e.deptno = 4)

{code}
  @Test public void testJoinPushTransitivePredicatesRule() {
    HepProgram preProgram = new HepProgramBuilder()
        .addRuleInstance(FilterJoinRule.FILTER_ON_JOIN)
        .addRuleInstance(FilterJoinRule.JOIN)
        .addRuleInstance(JoinPushTransitivePredicatesRule.INSTANCE)
        .build();

    HepProgramBuilder builder = new HepProgramBuilder();
    HepPlanner hepPlanner = new HepPlanner(builder.build());

    final String sql = ""select d.deptno from sales.emp d where d.deptno\n""
        + ""IN (select e.deptno from sales.emp e ""
        + ""where e.deptno = d.deptno or e.deptno = 4)"";
    TesterImpl tester = new TesterImpl(getDiffRepos(), true, false, true, false, null, null);
    checkPlanning(tester, preProgram, hepPlanner, sql);
  }
{code}

{code}
java.lang.StackOverflowError
	at org.apache.calcite.util.mapping.Mappings$PartialFunctionImpl.<init>(Mappings.java:1568)
	at org.apache.calcite.util.mapping.Mappings.create(Mappings.java:76)
	at org.apache.calcite.rel.metadata.RelMdPredicates.getPredicates(RelMdPredicates.java:185)
	at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:802)
	at org.apache.calcite.rel.metadata.RelMdPredicates.getPredicates(RelMdPredicates.java:344)
	at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source)
	at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:802)
	at org.apache.calcite.rel.metadata.RelMdPredicates.getPredicates(RelMdPredicates.java:318)
	at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source)
...
{code}

Note1: for the same query, but with AND operator the rule works fine.
Note2: the rile works also after firing JoinToCorrelateRule rule.",smartshark_2_2,2287,calcite,"""[0.066628098487854, 0.933371901512146]"""
267,228526,Extend rule to push predicates into CASE statement,"[CaseShuttle|https://github.com/apache/calcite/blob/8139acb78b100483fbafb078fb82d07f921abef0/core/src/main/java/org/apache/calcite/rel/rules/ReduceExpressionsRule.java#L562] may make the optimization but the functions return value doesn't depend on that.

For the following query the =1 is not pushed into the case.
{code}
select empno from emp where case when sal > 1000 then empno else sal end = 1
{code}",smartshark_2_2,2369,calcite,"""[0.9895564913749695, 0.01044345460832119]"""
268,228527,Restore API and Test API links to site,"The site used to have links for API and Test API, but no longer has. It seems to have happened in the [SVN change|http://svn.apache.org/viewvc?view=revision&revision=1739659] to fix CALCITE-1203.

The other non-cosmetic change in that svn change, removing [~michaelmior] from the committers list, was my fault. I had committed to svn but forgot to push to git.",smartshark_2_2,1275,calcite,"""[0.9981727600097656, 0.0018272593151777983]"""
269,228528,Cross-Version Compatibility Test Harness,"One thing that the Protobuf serialization aimed to provide was a library which provides us the tools to make Avatica compatible across versions. However, Protobuf is just a tool and the developers can still misuse protobuf in such a way that breaks compatibility across versions. Not to mention, compatibility isn't even remotely certain without any tests.

Because of Avatica's position as a library less than a product, we have to defer some logic to the concrete product being tested (e.g. Phoenix or Drill). I'm thinking something like the following:

The user provides pairs of client and server ""definitions"" for a given version of compatibility. This would include some version of Avatica and some backing database. For example, Avatica-1.7.1 and Phoenix-4.7.0 or Avatica-1.8.0-SNAPSHOT and HSQLDB-2.3.1.

The client half would be some template for the appropriate JDBC url to use (sans the URL to the Avatica server) and a JAR file containing the necessary classes to run the j.s.Driver. The server half would just be a URL to the Avatica server instance.

The test harness itself can provide the logic to test the remote driver against the other servers, enumerating the possible combinations of client-server communication. Starting the server for each version to test is likely too difficult to automate well given the unknown of what the server will be, so that will be left as a prerequisite.",smartshark_2_2,1142,calcite,"""[0.9985610842704773, 0.001438899664208293]"""
270,228529,Support for grouping sets in AggregateExpandDistinctAggregatesRule,"AggregateExpandDistinctAggregatesRule should support complex aggregations. Tests to check correctness should be added.
",smartshark_2_2,173,calcite,"""[0.9982612729072571, 0.0017387558473274112]"""
271,228530,Replace Closeable with AutoCloseable,"Now Calcite no longer supports JDK 1.6 we can use AutoCloseable if we want to. It is more appropriate interface for releasing resources on close because it is not so tightly associated with I/O. (Closeable.close throws IOException.) Most, if not all, uses of Closeable in Calcite should instead use AutoCloseable.",smartshark_2_2,946,calcite,"""[0.9984117746353149, 0.001588176703080535]"""
272,228531,"Add RAND function, returning DOUBLE values in the range 0..1","Add {{RAND}} function, returning {{DOUBLE}} values in the range 0..1. It has variants {{RAND()}} and {{RAND(seed)}}.

Also, add {{RAND_INTEGER(seed, bound)}}, support for a seed when generating random integers.

To support seeds, the random function needs to have state. The random number generator is initialized, using the seed, on first use, and subsequent calls use the same random number generator. So, we are building on CALCITE-1548.",smartshark_2_2,1441,calcite,"""[0.9982655644416809, 0.0017344638472422957]"""
273,228532,Convert predicates on EXTRACT function calls into date ranges,"We would like to convert predicates on date dimension columns into date ranges. This is particularly useful for Druid, which has a single timestamp column.

Consider the case of a materialized view

{code}
SELECT sales.*, product.*, time_by_day.*
FROM sales
JOIN product USING (product_id)
JOIN time_by_day USING (time_id)
{code}

that corresponds to a Druid table

{noformat}
sales_product_time(
  product_id int not null,
  time_id int not null,
  units int not null,
  the_year int not null,
  the_quarter int not null,
  the_month int not null,
  the_timestamp timestamp not null,
  product_name varchar(20) not null)
{noformat}

And suppose we have the following check constraints:
* {{CHECK the_year = EXTRACT(YEAR FROM the_timestamp)}}
* {{CHECK the_month = EXTRACT(MONTH FROM the_timestamp)}}

Given a query

{code}
SELECT product_id, count(*)
FROM sales
JOIN product USING (product_id)
JOIN time_by_day USING (time_id)
WHERE the_year = 2016
AND the_month IN (4, 5, 6)
{code}

we would like to transform it into the following query to be run against Druid:

{code}
SELECT product_id, count(*)
FROM sales_product_time
WHERE the_timestamp BETWEEN '2016-04-01' AND '2016-06-30'
{code}

Druid can handle timestamp ranges (or disjoint sets of ranges) very efficiently.

I believe we can write a rule that knows the check constraints and also knows the properties of the {{EXTRACT}} function:

1. Apply check constraints to convert {{WHERE year = ...}} to {{WHERE EXTRACT(YEAR FROM the_timestamp) = ...}}, etc.
2. {{EXTRACT(YEAR FROM ...)}} is monotonic, therefore we can deduce the range of the_timestamp values such that {{EXTRACT(YEAR FROM the_timestamp)}} returns 2016.
3. Then we need to use the fact that {{EXTRACT(MONTH FROM the_timestamp)}} is monotonic if {{the_timestamp}} is bounded within a particular year.
4. And we need to merge month ranges somehow.",smartshark_2_2,1260,calcite,"""[0.9984721541404724, 0.0015278575010597706]"""
274,228533,Improve two-level column structure handling,"Calcite now has support for nested column structure in parsing and validation, by representing the inner-level columns as a RexFieldAccess based on a RexInputRef. Meanwhile it does not flatten the inner level structure in wildcard expansion, which would then cause an UnsupportedOperationException in Avatica.
 
The idea is to take into account this nested structure in column resolving, but to flatten the structure when translating to RelNode/RexNode.
For example, if the table structure is defined as
{code}VARCHAR K0,
VARCHAR C1,
RecordType(INTEGER C0, INTEGER C1) F0,
RecordType(INTEGER C0, INTEGER C2) F1{code}
, it should be viewed as a flat type like
{code}VARCHAR K0,
VARCHAR C1,
INTEGER F0.C0,
INTEGER F0.C1,
INTEGER F1.C0,
INTEGER F1.C2{code}
, so that:
1) Column reference ""K0"" is translated as {{$0}}
2) Column reference ""F0.C1"" is translated as {{$3}}
3) Wildcard ""*"" is translated as: {{$0, $1, $2, $3, $4, $5}}
4) Complex-column wildcard ""F1.*"", which is translated as {{$2, $3}}
And we would like to resolve columns based on the following rules (here we only consider the ""suffix"" part of the qualified names, which means the table resolving is already done by this time):
a) A two-part column name is matched with its first-level column name and its second-level column name. For example, ""F1.C0"" corresponds to $4; ""F1,X"" will throw a column not found error.
b) A single-part column name is matched against non-nested columns first, and if no matches, it is then matched against those second-level column names. For example, ""C1"" will be matched as ""$1"" instead of ""$3"", since non-nested columns have a higher priority; ""C2"" will be matched as ""$5""; ""C0"" will lead to an ambiguous column error, since it exists under both ""F0"" and ""F1"".
c) We would also like to have a way for defining ""default first-level column"" so that it has a precedence in column resolving over other first-level columns. For example, if ""F0"" is defined as default, ""C0"" will not cause an ambiguous column error, but instead be matched as ""$2"".
d) Reference to first-level column only without wildcard is not allowed, e.g., ""F1"".",smartshark_2_2,1238,calcite,"""[0.9979928731918335, 0.0020071500912308693]"""
275,228534,Update Kerby dependency to 1.0.0-RC2,"There's at least one fix in Kerby 1.0.0-RC2 that's been bothering me (spam of SocketTimeoutException in the surefire logs).

Update our dependency from 1.0.0-RC1 to 1.0.0-RC2.",smartshark_2_2,1113,calcite,"""[0.9985452890396118, 0.0014547246973961592]"""
276,228535,Create self-contained test-harness for TCK,"Should make a Vagrant VM or a Docker image which is capable of automatically running the Avatica TCK.

Ideally, running the TCK should be as simple as a single command.",smartshark_2_2,2572,calcite,"""[0.9970965385437012, 0.002903418615460396]"""
277,228536,Avatica - Remove JDK 7 and add JDK 9 to .travis.yml,Avatica 0.11.0 dropped support for JDK 7. However it is still in .travis.yml. We should remove JDK 7 and add JDK 9 to .travis.yml for Avatica.,smartshark_2_2,2278,calcite,"""[0.9980959296226501, 0.001904054544866085]"""
278,228537,Configurable SqlToRelConverter.Config in Frameworks,"Like SqlParser.Config, it's useful to be able to configure the SqlToRelConverter.Config. See also CALCITE-1799.",smartshark_2_2,1977,calcite,"""[0.9984422326087952, 0.00155781046487391]"""
279,228538,Further extend simplify for reducing expressions,"We would extend expression simplification, e.g.:
- add walker to perform recursive simplification of expressions for Filter operators, and
- extends simplification for CASE expressions to cover more cases e.g. if all return values are equal (including ELSE), CASE can be removed",smartshark_2_2,1243,calcite,"""[0.9983293414115906, 0.0016706060850992799]"""
280,228539,Annotate user-defined functions as strict and semi-strict,"Annotate user-defined functions as strict and semi-strict.

Definitions:
* A strict function returns null if and only if one or more of its arguments are null.
* A semi-strict function returns null if one or more of its arguments are null.

The code generator should use these annotations to generate more efficient code.",smartshark_2_2,1973,calcite,"""[0.9982965588569641, 0.0017034342745319009]"""
281,228540,Avatica HSQLDB Dockerfile should be bumped to 2.4.0,CALCITE-2013Â bumped the HSQLDB version to 2.4.0. Looks like the Dockerfile was not updated at the same time.,smartshark_2_2,2406,calcite,"""[0.9965586066246033, 0.0034413288813084364]"""
282,228541,Display a list of Avatica clients on the website,"I think it would be really neat to have a well organized list of Avatica clients on the website. This would make clients much more discoverable and make Avatica and Calcite much more easier for people to get started with. For example, Kafka has a wiki page with their clients here: https://cwiki.apache.org/confluence/display/KAFKA/Clients

1. Most of the clients currently available (the .NET and python ones) are currently targeted towards Apache Phoenix. Should we include these clients? If not, is there a process for reaching out to their maintainers to see if they are interested in generalising them to target Avatica?

2. Is there any process for including a client on such a page? Should anyone be able to add the client, or do we need explicit permission from the maintainers?

3. To reduce friction, is it viable to have a wiki page, so that maintainers can easily add their own clients?",smartshark_2_2,1387,calcite,"""[0.9982540011405945, 0.00174595951102674]"""
283,228542,Calcite development under Eclipse IDE,"After ""git clone"" project could not be imported in Eclipse as existing Maven project.",smartshark_2_2,1427,calcite,"""[0.9947793483734131, 0.005220700521022081]"""
284,228543,Request to support of Oracle database as meta data store for Druid,"Currently MySQL, Postgres, and Derby are supported as Druid metadata stores. 

Request to support of Oracle database as meta data store for Druid.",smartshark_2_2,2262,calcite,"""[0.9983018636703491, 0.0016981272492557764]"""
285,228544,"In Druid adapter, push COUNT(__time) as COUNT(*)","Druid Time column is not null by default, thus we can transform {code} select count(__time) from table {code} to {code} select count(*) from table{code}",smartshark_2_2,2122,calcite,"""[0.9516567587852478, 0.048343174159526825]"""
286,228545,Apache Geode adapter,"I've been working on a Calcite adapter for [Apache Geode|http://geode.apache.org]. 
Current implementation uses the plain Geode API and [OQL|http://geode.apache.org/docs/guide/13/developing/querying_basics/chapter_overview.html](Object Query Interface) to push down relational expressions such as projections, filtering, sorting, and grouping . 

Provided functionality can hopefully address certain Geode use cases and will provide a stepping stone for future improvements. 

Here are some remaining tasks as i see it:
* New tests for test suite (and update calcite-test-dataset to support Geode)
* Add Integration tests that use calcite-test-dataset
* Documentation",smartshark_2_2,2336,calcite,"""[0.9983152151107788, 0.0016847507795318961]"""
287,228546,Move populate materializations after sql-to-rel conversion,This is to allow Phoenix to figure out which materializations are concerned with the query and thus have a chance to define them right before materializations are populated.,smartshark_2_2,963,calcite,"""[0.9985169768333435, 0.0014830826548859477]"""
288,228547,Implement logging throughout Avatica server,Right now there's not much to go on regarding the Avatica internals. We should make a decision on a logging library and take a first pass at operational log messages. We should also provide a recommended log configuration for downstreamers.,smartshark_2_2,266,calcite,"""[0.9976105690002441, 0.002389455446973443]"""
289,228548,Create a web site,"The apache web site http://incubator.apache.org/optiq is blank. This task is to create a simple initial web site.

Maybe re-use some content from the old github front page, https://github.com/julianhyde/optiq/tree/master, in files README.md, REFERENCE.md, etc.",smartshark_2_2,489,calcite,"""[0.9972183704376221, 0.0027816682122647762]"""
290,228549,Document protobuf and json REP types with examples,It would be nice to have the documentation for the Rep types here (https://calcite.apache.org/docs/avatica_protobuf_reference.html#rep) documented with examples to show what the serialized representation looks like.,smartshark_2_2,1131,calcite,"""[0.9984186887741089, 0.0015813586069270968]"""
291,228550,Implement planner for converting multiple SQL statements to unified RelNode Tree,"This can be implemented as a separate planner or in {{VolcanoPlanner}} itself. The planner should take multiple SQL statements as input and return a unified {{RelNode}} tree.

Example of above is as follows:

{{SELECT COL1, COL2 FROM TABLE WHERE COL3 > 10;}}
{{SELECT COL1, COL2 FROM TABLE WHERE COL4  = 'abc';}}

The above 2 statements have a common path and hence can provide a unified {{RelNode}} tree as follows:

{noformat}
 [Scan] -> [Project (COL1, COL2)] -> [Filter (COL4 = 'abc')] -> [Delta]
                    |
                    V
            [Filter (COL3 > 10)]
                    |
                    v
                 [Delta]
{noformat}",smartshark_2_2,1402,calcite,"""[0.9983925223350525, 0.0016074696322903037]"""
292,228551,Upgrade to new Apache logo,Upgrade web site to new Apache logo.,smartshark_2_2,1055,calcite,"""[0.9976813793182373, 0.0023186535108834505]"""
293,228552,Unnecessary project expression in multi-sub-query plan,"Query
{code} select sal from emp where empno IN (select deptno from dept where emp.job = dept.name)  AND empno IN (select empno from emp e where emp.ename = e.ename) {code}

Plan just before calling *SubqueryRemoveRule*
{code}
LogicalProject(SAL=[$5])
  LogicalFilter(condition=[AND(IN($0, {
LogicalProject(DEPTNO=[$0])
  LogicalFilter(condition=[=($cor0.JOB, $1)])
    LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
}), IN($0, {
LogicalProject(EMPNO=[$0])
  LogicalFilter(condition=[=($cor0.ENAME, $1)])
    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
}))], variablesSet=[[$cor0]])
    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
{code}

Plan just after *SubqueryRemoveRule*
{code}
LogicalProject(SAL=[$5])
  LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8], DEPTNO0=[$9])
      LogicalJoin(condition=[=($0, $10)], joinType=[inner])
        LogicalFilter(condition=[=($0, $9)])
          LogicalCorrelate(correlation=[$cor0], joinType=[INNER], requiredColumns=[{2}])
            LogicalTableScan(table=[[CATALOG, SALES, EMP]])
            LogicalAggregate(group=[{0}])
              LogicalProject(DEPTNO=[$0])
                LogicalFilter(condition=[=($cor0.JOB, $1)])
                  LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
        LogicalAggregate(group=[{0}])
          LogicalProject(EMPNO=[$0])
            LogicalFilter(condition=[=($cor0.ENAME, $1)])
              LogicalTableScan(table=[[CATALOG, SALES, EMP]])
{code}

Plan just after *decorrelation*
{code}
LogicalProject(SAL=[$5], ENAME0=[$9])
  LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8], ENAME0=[$10])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8], DEPTNO0=[$9], ENAME0=[$12])
      LogicalJoin(condition=[=($0, $11)], joinType=[inner])
        LogicalJoin(condition=[AND(=($2, $10), =($0, $9))], joinType=[inner])
          LogicalTableScan(table=[[CATALOG, SALES, EMP]])
          LogicalAggregate(group=[{0, 1}])
            LogicalProject(DEPTNO=[$0], JOB=[$1])
              LogicalProject(DEPTNO=[$0], JOB=[$2])
                LogicalJoin(condition=[=($2, $1)], joinType=[inner])
                  LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
                  LogicalAggregate(group=[{0}])
                    LogicalProject(JOB=[$2])
                      LogicalTableScan(table=[[CATALOG, SALES, EMP]])
        LogicalAggregate(group=[{0, 1}])
          LogicalProject(EMPNO=[$0], ENAME0=[$1])
            LogicalProject(EMPNO=[$0], ENAME0=[$9])
              LogicalJoin(condition=[=($9, $1)], joinType=[inner])
                LogicalTableScan(table=[[CATALOG, SALES, EMP]])
                LogicalAggregate(group=[{0}])
                  LogicalProject(ENAME=[$1])
                    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
{code}

As you can notice the top *LogicalProject* has unnecessary expression *ENAME0* after decorrelation
",smartshark_2_2,1457,calcite,"""[0.975169837474823, 0.024830181151628494]"""
294,228553,"In Volcano, populate RelOptRuleCall.nodeInputs for operands of type ""any""","If you have an ""any"" operand, it is useful to know all of the siblings of the RelNode that matched the operand. Currently VolcanoRuleCall does not populate RelOptRuleCall.nodeInputs.",smartshark_2_2,944,calcite,"""[0.9739723801612854, 0.026027554646134377]"""
295,228554,Add DESCRIBE SCHEMA/DATABASE/TABLE/query,"Add DESCRIBE
 statement so it can support the following:
{noformat}
DESCRIBE [SCHEMA | DATABASE] name;
DESCRIBE table_name;
{noformat}
Example, [usage in MySql|http://dev.mysql.com/doc/refman/5.7/en/explain.html]
",smartshark_2_2,1885,calcite,"""[0.9983920454978943, 0.0016079365741461515]"""
296,228555,Support for grouping sets in AggregateUnionTransposeRule,AggregateUnionTransposeRule should support complex aggregations. Tests to check correctness should be added.,smartshark_2_2,152,calcite,"""[0.9983161687850952, 0.0016838312149047852]"""
297,228556,Extend Druid Range Rules to extract interval from Floor,"DruidRangeRules currently only extracts intervals from EXTRACT function. 
This task is to enhance Druid Range Rules to also support Floor functions. ",smartshark_2_2,2299,calcite,"""[0.9982377290725708, 0.0017622595187276602]"""
298,228557,JDK 10 builds fail with NPE due to SUREFIRE-1439,SUREFIRE-1439 causes an NPE when using JDK 10. CALCITE-2225 upgraded to Apache parent pom 19 which included Surefire 2.20.1.Â MPOM-184 is tracking the fix for Apache parent pom 20. We should update to Surefire 2.21.0 to ensure that builds function on JDK 10.,smartshark_2_2,2383,calcite,"""[0.8647699952125549, 0.1352299451828003]"""
299,228558,Parameter precision and scale are not returned from Avatica REST API,"Create a table
{noformat}
CREATE TABLE IF DECIMAL_TABLE(
keycolumn VARCHAR(255) PRIMARY KEY,
column1 DECIMAL(38,0));
{noformat}

Prepare a parameterized statement
{noformat}
request:{""request"":""prepare"",""connectionId"":""3c39adc2-d13d-f87a-a63f-4ac6fbaf7CE8"",""sql"":""select * from decimal_table where column1 > ? "",""maxRowCount"":""-1""}
{noformat}

{noformat}
{""response"":""prepare"",""statement"":{""connectionId"":""3c39adc2-d13d-f87a-a63f-4ac6fbaf7CE8"",""id"":1891923767,""signature"":{""columns"":[{""ordinal"":0,""autoIncrement"":false,""caseSensitive"":false,""searchable"":true,""currency"":false,""nullable"":0,""signed"":false,""displaySize"":255,""label"":""KEYCOLUMN"",""columnName"":""KEYCOLUMN"",""schemaName"":"""",""precision"":255,""scale"":0,""tableName"":""DECIMAL_TABLE"",""catalogName"":"""",""type"":{""type"":""scalar"",""id"":12,""name"":""VARCHAR"",""rep"":""STRING""},""readOnly"":true,""writable"":false,""definitelyWritable"":false,""columnClassName"":""java.lang.String""},{""ordinal"":1,""autoIncrement"":false,""caseSensitive"":false,""searchable"":true,""currency"":false,""nullable"":1,""signed"":true,""displaySize"":38,""label"":""COLUMN1"",""columnName"":""COLUMN1"",""schemaName"":"""",""precision"":38,""scale"":0,""tableName"":""DECIMAL_TABLE"",""catalogName"":"""",""type"":{""type"":""scalar"",""id"":3,""name"":""DECIMAL"",""rep"":""OBJECT""},""readOnly"":true,""writable"":false,""definitelyWritable"":false,""columnClassName"":""java.math.BigDecimal""}],""sql"":""select * from decimal_table where column1 > ? "",""parameters"":[{""signed"":false,""precision"":0,""scale"":0,""parameterType"":3,""typeName"":""DECIMAL"",""className"":""java.math.BigDecimal"",""name"":""?1""}],""cursorFactory"":{""style"":""LIST"",""clazz"":null,""fieldNames"":null}}}}
{noformat}
The precision and scale are always returned as '0'.
{noformat}
""parameters"":[{""signed"":false, ""precision"":0, ""scale"":0 ,""parameterType"":3,""typeName"":""DECIMAL"",""className"":""java.math.BigDecimal"",""name"":""?1""}],""cursorFactory"":{""style"":""LIST"",""clazz"":null,""fieldNames"":null}}}}{noformat}",smartshark_2_2,1247,calcite,"""[0.4251689314842224, 0.5748311281204224]"""
300,228559,Using CoreMatchers.containsString causes javadoc errors,"Under JDK 1.7, using the method {{CoreMatchers.containsString}} causes javadoc warnings, similar to https://github.com/junit-team/junit4/issues/872. The cause, I think, is that Avatica uses mockito and it bundles a different version of hamcrest. The solution is to use {{StringContains.containsString}}, which exists in an earlier version of hamcrest.",smartshark_2_2,1327,calcite,"""[0.9804321527481079, 0.01956784911453724]"""
301,228560,Use Jenkins,"Use Jenkins for continuous integration.

See discussion with [~ndimiduk] and [~mujtabachohan] in CALCITE-553.",smartshark_2_2,922,calcite,"""[0.9982403516769409, 0.0017596629913896322]"""
302,228561,Make UnifyRules pluggable for MaterializedViewSubstitutionVisitor,This will allow users to specify customized UnifyRules in materialization substitution. It might be useful for user scenarios like Phoenix's non-covering local index usage which might not look like a general case for other adapters.,smartshark_2_2,594,calcite,"""[0.9981325268745422, 0.0018674663733690977]"""
303,228562,Add new method to ViewExpander interface to allow passing SchemaPlus,"Currently, {{ViewExpander}} interface contains single method {{expandView(RelDataType rowType, String queryString, List<String> schemaPath, List<String> viewPath)}} which allows creating a {{RelNode}} instance that corresponds to the desired view.

Drill supports impersonation for views and it is implemented in such a way, that its configs such as username, etc are stored in {{SchemaPlus}} instance. So currently it is not possible to pass these configs into this method to create {{CatalogReader}} and allow impersonation work.

",smartshark_2_2,2127,calcite,"""[0.9985283613204956, 0.0014716318110004067]"""
304,228563,"Support column reference in ""FOR SYSTEM_TIME AS OF""","As discussed in mailing list[1], the standard says QSTPS canât contain a column reference. So when joining the Orders to the Products table for the price as of the time the order was placed is impossible using ""FOR SYSTEM_TIME AS OF"". But can be expressed using a subquery, such as:

{code}
 SELECT  *
    FROM Orders AS o
    JOIN LATERAL (SELECT * FROM ProductPrices WHERE sysStart <= O.orderTime AND sysEnd > O.orderTime) AS P
      ON o.productId = p.productId
{code}

But subquery is too complex for users. We know that the standard says it canât contain a column reference. We initialize this discuss as we would like to ""extend"" the standard to simplify such query:

{code}
 SELECT  *
    FROM Orders AS o
    JOIN LATERAL ProductPrices FOR SYSTEM_TIME AS OF O.orderTime AS P
      ON o.productId = p.productId
{code}

[1] https://lists.apache.org/thread.html/f877f356a8365bf74ea7d8e4a171224104d653cf73861afb2901a58f@%3Cdev.calcite.apache.org%3E",smartshark_2_2,1901,calcite,"""[0.9886592626571655, 0.011340764351189137]"""
305,228564,Change Avatica dependency of the Pig adapter to avatica-core,Calcite-pig unnecessarily depends on the full {{avatica}} instead of {{avatica-core}}. This causes problems in some environments because {{avatica}} jar contains slf4j classes.,smartshark_2_2,1760,calcite,"""[0.9983886480331421, 0.001611352781765163]"""
306,228565,Generalize p(x) IS TRUE/FALSE/UNKNOWN handling in RexSimplify,"Currently only IS TRUE is handle by the unknownAsFalse field variable.

The main goal would be to extend the logic to also handle IS FALSE cases (and possibly IS UNKNOWN too)
",smartshark_2_2,2600,calcite,"""[0.9984215497970581, 0.0015784860588610172]"""
307,228566,ResultSet.getXxx methods should throw SQLDataException if cannot convert to the requested type,"{{AccessorImpl}} relies on {{cannotConvert(String)}} method to throw a exception when conversion is not possible between the accessor data type, and what the user requested.

The exact class type of the exception is {{RuntimeException}} which is very generic, and not easy for the user to catch, unlike more specific types. Moreover JDBC drivers usually throws a {{SQLException}} instances for these kind of exceptions, which is likely what users are expecting.",smartshark_2_2,1383,calcite,"""[0.9589481353759766, 0.04105185717344284]"""
308,228567,Allow customization for AvaticaServerConfiguration for plugging new authentication mechanisms,"{{AvaticaServerConfiguration}}Â is currently only created if authentication mechanism such as {{BASIC, DIGEST or SPNEGO}} is provided. We can change it to a builder pattern to create this object and provide a way for users to plugin their own security configuration.

An example here can be using it for custom config that supports MTLS.

Thanks [~alexaraujo] for suggesting this approach.",smartshark_2_2,2554,calcite,"""[0.9981244206428528, 0.0018755461787804961]"""
309,228568,Unnecessary maven-remote-resources-plugin configuration,"Just got pinged with a question from someone about something I also noticed during 1.6.0-rc1 testing:

When building from a clean checkout (or deleting the top-level target/ directory), you'll see the calcite build trying to pull down the jars we're trying to build:

{noformat}
mvn package
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Calcite
[INFO] Calcite Avatica
[INFO] Calcite Avatica Server
[INFO] Calcite Linq4j
[INFO] Calcite Core
[INFO] Calcite Examples
[INFO] Calcite Example CSV
[INFO] Calcite Example Function
[INFO] Calcite MongoDB
[INFO] Calcite Piglet
[INFO] Calcite Plus
[INFO] Calcite Spark
[INFO] Calcite Splunk
[INFO] Calcite Ubenchmark
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Calcite 1.7.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-checkstyle-plugin:2.12.1:check (validate) @ calcite ---
[INFO]
[INFO] --- git-commit-id-plugin:2.1.9:revision (default) @ calcite ---
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ calcite ---
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (root-resources) @ calcite ---
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/calcite-avatica-1.7.0-SNAPSHOT.jar
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/calcite-avatica-1.7.0-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-avatica/1.7.0-SNAPSHOT/calcite-avatica-1.7.0-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/calcite-linq4j-1.7.0-SNAPSHOT.jar
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/calcite-linq4j-1.7.0-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-linq4j/1.7.0-SNAPSHOT/calcite-linq4j-1.7.0-SNAPSHOT.jar
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/calcite-core-1.7.0-SNAPSHOT.jar
Downloading: http://conjars.org/repo/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/calcite-core-1.7.0-SNAPSHOT.jar
Downloading: http://repository.apache.org/snapshots/org/apache/calcite/calcite-core/1.7.0-SNAPSHOT/calcite-core-1.7.0-SNAPSHOT.jar
{noformat}

It looks like this was introduced in CALCITE-741. The intent of the change isn't necesarily wrong, but I believe this is just trying to work against Maven.

I think the change to make for Maven to work as it wants is to make a new module dedicated to creating the assembly. That module will depend on the other modules which should properly create the transitive dependency list.

It's also worth noting that the maven-remote-resources-plugin configuration littered everywhere is duplicative (this is already handled in the Apache parent pom).",smartshark_2_2,1082,calcite,"""[0.9965997338294983, 0.003400268731638789]"""
310,228569,Release Calcite 1.9.0,"Release Apache Calcite version 1.9.0. The plan is ""end of August"".

Calcite 1.9 will depend on Avatica 1.8 (Avatica release 1.9 will occur later).",smartshark_2_2,1257,calcite,"""[0.998457670211792, 0.0015423770528286695]"""
311,228570,Bugs in website example code,"On your website's http://calcite.apache.org/docs/ page, the example code is completely broken: it doesn't add() either the ReflectiveSchema or the JdbcSchema to the CalciteConnection's root schema, and the ""name"" and ""hr"" JdbcSchema parameter for MySQL appear to need swapping.

It took me hours to get Calcite working because of this :(.

I am attaching a patch.
",smartshark_2_2,1619,calcite,"""[0.9747817516326904, 0.025218278169631958]"""
312,228571,Planner requires unnecessary collation when using materialized view,"When a query does not have an ORDER BY clause, we should ignore the collation trait of the main table plan and should not request the materialized view plan to have the same collation.

For example, we have a table 'A' sorted by primary key 'id', and we have a materialized view 'V' projected from 'A' which is sorted by column 'col1'. And now we have a query like ""select id, col0, col1, col2 from A where col1 < '10'"".
The main table plan will come out like a Filter on top of a TableScan of 'A', while the materialized view plan should also be something like a Filter on top of a TableScan of 'V' and it should not have a Sort, so that if doing a col1 related filter on col1 ordered table 'V' is cheaper, the materialized view plan will be chosen.",smartshark_2_2,647,calcite,"""[0.972580075263977, 0.02741992473602295]"""
313,228572,"Exclude VolcanoPlanner's ""originalRoot"" from default planning process","The Calcite compilation framework runs a series of Programs for query planning. The default programs consist of some pre-processing HepPrograms, e.g., de-correlation, field-trimming, etc.,  the volcano planning program, and some post-processing HepPrograms. In {{Prepare.optimize()}}, {{planner.setRoot()}} is called before running the programs. As a result, the original rel from sql-to-rel conversion becomes the ""originalRoot"" in the VolcanoPlanner, and the new rel from the pre-processing programs becomes the new ""root"". In some cases, we would only want to run volcano planning on the new root based on the assumption that the new root is the desired form after pre-processing. And if we have an option to turn off the planning of the original root, the planning space can be significantly reduced.",smartshark_2_2,1424,calcite,"""[0.9872328042984009, 0.012767178937792778]"""
314,228573,Add support for aggregates in Calcite Elasticsearch adapter,"Currently, the Calcite Elasticsearch adapter doesn't support aggregates. This issue is created to add aggregate support to Elasticsearch adapter. ",smartshark_2_2,2138,calcite,"""[0.9984166622161865, 0.0015832718927413225]"""
315,228574,Optimize project/filter/calc rules by caching whether an expression contains aggs and multisets,"Optimize project/filter/calc rules by caching whether an expression contains aggs and multisets. EnumerableCalcRule.onMatch calls RexMultisetUtil.containsMultiset and RexProgram.containsAggs. Other rules call similar. I suspect that it would be efficient if the RexProgram computed a bit-set on creation.

RexMultisetUtil.containsMultiset in particular seems to be implemented in an inefficient way. A visitor could make a single pass over the RexNode tree looking for aggs and multiset expressions. Use profiling to make sure this is a net win.

---------------- Imported from GitHub ----------------
Url: https://github.com/julianhyde/optiq/issues/103
Created by: [julianhyde|https://github.com/julianhyde]
Labels: enhancement, 
Created at: Tue Dec 10 21:59:18 CET 2013
State: open
",smartshark_2_2,601,calcite,"""[0.9983395338058472, 0.0016604275442659855]"""
316,228575,Store query results in temporary tables,"REPLs (read-eval-print loops) commonly store the result of each expression evaluated in a variable. This feature would provide similar functionality for interactive SQL.

Add new connection parameter ""autotemp"", values true and false, default false.

If true, Optiq will provide a callback that the specific provider (e.g. Lingual) must call during query execution. The callback provides Schema, Table, and table name. (It is up to the provider which point in the execution -- any time between execute and close it OK.)

If the client has called Statement.setMaxRows, the temp table should nevertheless contain the full result. (Any objections?)

Add a ""table type"" property to the Table interface.

List temporary tables alongside regular tables in all schema requests.

Later we could add a hook that is called when each connection is closed, so that the provider can clean up temporary tables. Add a separate case for that.

---------------- Imported from GitHub ----------------
Url: https://github.com/julianhyde/optiq/issues/28
Created by: [julianhyde|https://github.com/julianhyde]
Labels: enhancement, 
Created at: Tue Mar 12 21:37:50 CET 2013
State: open
",smartshark_2_2,463,calcite,"""[0.9984386563301086, 0.0015613336581736803]"""
317,228576,"Improve handling of ARRAY, MULTISET, STRUCT types","Improve end-to-end handling of STRUCT, MAP, ARRAY and MULTISET types.

Add annotations org.apache.calcite.adapter.java.Array and org.apache.calcite.adapter.java.Map to declare that a member of a class (i.e. a column) is of ARRAY or MAP type, and what the element/key/value types are.

Add metadata so that Avatica JDBC knows that when you have gone into an ARRAY column you are looking at a record type of say (INTEGER, ARRAY DOUBLE). Currently the metadata only goes one level down and after that you are looking at objects.

When an array or multiset is read, we need to know how it is internally represented. Currently it could be of type CUSTOM (e.g. a bean class such as Employee) or LIST or ARRAY. In this change we standardize on LIST. Thus when EnumerableUncollect comes to generate code for {code}select * from unnest(select employees from dept){code} it will know that employees is a list and each employee record is also a list.

To do this we will need to convert (e.g. from an array of Employee objects) on the way in.",smartshark_2_2,713,calcite,"""[0.9983628392219543, 0.0016371695091947913]"""
318,228577,Remove OneRow and Empty relational expressions; Values will suffice,"The OneRow and Empty relational expressions do not give any descriptive power over a Values relational expression with 1 tuple and 0 tuples, respectively. We should remove them (and their sub-classes).

Now that RelOptRuleOperand can take a Predicate, it is easy to write rules that match a Values that has zero tuples.

Benefits include efficiency (the planning process will creature fewer relational expressions) and rule re-use (rules applicable to Values are now automatically applicable to 1- and 0-row relational expressions).",smartshark_2_2,163,calcite,"""[0.9984334111213684, 0.0015666255494579673]"""
319,228578,Add supported SQL Dialects in Calcite website,We can add the SQL dialect section in https://calcite.apache.org/docs/adapter.html.,smartshark_2_2,2391,calcite,"""[0.9983626008033752, 0.0016374050173908472]"""
320,228579,Release Calcite 1.11.0,"Release Apache Calcite 1.11.0. The plan is to release in early January, target code freeze on Wed Dec 28th.",smartshark_2_2,1447,calcite,"""[0.9984549283981323, 0.00154504319652915]"""
321,228580,Suboptimal plan for NOT IN query,"Following query generates sub-optimal plan

{code} explain plan for select * from scott.emp where deptno not in (select deptno from scott.dept where deptno = 20); {code}

Following is the plan
{code}
EnumerableCalc(expr#0..11=[{inputs}], expr#12=[0], expr#13=[=($t8, $t12)], expr#14=[false], expr#15=[IS NOT NULL($t11)], expr#16=[true], expr#17=[IS NULL($t7)], expr#18=[null], expr#19=[<($t9, $t8)], expr#20=[CASE($t13, $t14, $t15, $t16, $t17, $t18, $t19, $t16, $t14)], expr#21=[NOT($t20)], proj#0..7=[{exprs}], $condition=[$t21])
  EnumerableJoin(condition=[=($7, $10)], joinType=[left])
    EnumerableCalc(expr#0..9=[{inputs}], EMPNO=[$t2], ENAME=[$t3], JOB=[$t4], MGR=[$t5], HIREDATE=[$t6], SAL=[$t7], COMM=[$t8], DEPTNO=[$t9], c=[$t0], ck=[$t1])
      EnumerableJoin(condition=[true], joinType=[inner])
        JdbcToEnumerableConverter
          JdbcAggregate(group=[{}], c=[COUNT()], ck=[COUNT($0)])
            JdbcFilter(condition=[=(CAST($0):INTEGER NOT NULL, 20)])
              JdbcTableScan(table=[[SCOTT, DEPT]])
        JdbcToEnumerableConverter
          JdbcTableScan(table=[[SCOTT, EMP]])
    JdbcToEnumerableConverter
      JdbcAggregate(group=[{0, 1}])
        JdbcProject(DEPTNO=[$0], i=[true])
          JdbcFilter(condition=[=(CAST($0):INTEGER NOT NULL, 20)])
            JdbcTableScan(table=[[SCOTT, DEPT]])
{code}

As Julian pointed out in discussion on mailing list instead of two scans for DEPT one is sufficient as clearly DEPTNO is never null.

",smartshark_2_2,2055,calcite,"""[0.9864057898521423, 0.013594210147857666]"""
322,228581,New metadata providers for expression column origin and all predicates in plan,"I am working on the integration of materialized view rewriting within Hive.

Once a view matches an operator plan, rewriting is split vastly in two steps. The first step will verify that the input to the root operator of the matched plan is equivalent or contained within the input to the root operator of the query representing the view. The second step will trigger a _unify_ rule, which tries to rewrite the matched operator tree into a scan on the view and possibly some additional operators to compute the exact results needed by the query (think about Project that alters the column order, additional Filter on the view, additional Join operation, etc.)

If we focus on step 1, checking equivalence/containment, I would like to extend the metadata providers in Calcite to give us more information about the matched (sub)plan. In particular, I am thinking on:
- Expression column origin. Currently Calcite can provide the column origins for a certain column and whether it is derived or not. However, we would need to obtain the expression that generated a certain column. This expression should contain references to the input tables. For instance, given expression column _c_, the new md provider would return that it was generated by expression _A.a + B.b_. 
- All predicates. Currently Calcite can extract predicates that have been applied on an RelNode output (we can think on them as constraints on the output). However, I would like to extract all predicates that have been applied on a given RelNode (sub)plan. Since nodes might not be part of the output, expressions should contain references to the input tables. For instance, the new md provider might return the expressions _A.a + B.b > C.c AND D.d = 100_.
- PK-FK relationship. I do not plan to implement this one immediately. However, exposing this information (given it is provided) can help us to trigger more rewriting containing join operators. Thus, I was wondering if it is worth adding it.

Once this information is available, we can rely on it to implement logic similar to [1] to check whether a given (sub)plan is equivalent/contained within a given view.

One question I have is about representing the table columns as a RexNode, as I think it is the easiest way to be returned by the new metadata providers. I checked _RexPatternFieldRef_ and I think it will meet our requirements: alpha would be the qualified table name, while the index is the column idx for the table. Thoughts?

I have started working on this and will provide a patch shortly; feedback is greatly appreciated.

[1] ftp://ftp10.us.freebsd.org/users/azhang/disc/SIGMOD/pdf-files/331/202-optimizing.pdf",smartshark_2_2,1929,calcite,"""[0.9977715611457825, 0.0022284130100160837]"""
323,228582,Tests to check rules on Aggregate operator w/o grouping sets,Some rules on {{Aggregate}} operators are missing tests e.g. {{AggregateUnionAggregate}}.,smartshark_2_2,150,calcite,"""[0.9975679516792297, 0.002432077657431364]"""
324,228583,Commit functionality not exposed by the RPC server,"It seems that the commit/rollback functionality is not exposed by the RPC server, which means that it's only usable in autocommit mode. Avatica itself doesn't have a concept of commit in the RPC and the remote JDBC connection raises an exception when calling commit() on it, but Phoenix's native JDBC connection does implement commit(), so the RPC needs to be extended to allow calling that remotely.

The easiest way to test this, ""!autocommit off"" and then ""!commit"" fails in ""sqline-thin.py"", but works in ""sqline.py"".",smartshark_2_2,908,calcite,"""[0.707728385925293, 0.2922716438770294]"""
325,228584,Include DB version in SqlDialect,It would be useful to have the DB version # in the SqlDialect for unparsing,smartshark_2_2,1995,calcite,"""[0.9982819557189941, 0.0017180894501507282]"""
326,228585,Use Strong to infer whether a predicate's inputs may be null,"{{RexImplicationChecker}} must use [Strong|https://calcite.apache.org/apidocs/org/apache/calcite/plan/Strong.html] to infer whether a predicate's inputs may be null. In particular the code in {{RexImplicationChecker.implies2}}.

Also {{RelMdPredicates.projectPredicate}}.

[~jcamachorodriguez], Could/should {{RexUtil.ExprSimplifier}} be using {{Strong}}?

Also, maybe, {{RexUtil.simplifyIs}}.

Also, {{LogicVisitor}} might be able to deduce that an expression is never null if certain input fields are not null.",smartshark_2_2,1895,calcite,"""[0.9963609576225281, 0.003639096161350608]"""
327,228586,Remove Project.flags,"Project.flags has no meaning under current code base.

It need to be removed with Project.Flags class",smartshark_2_2,250,calcite,"""[0.9981521964073181, 0.0018477626144886017]"""
328,228587,"Implement queries using an interpreter, as an alternative to code generation & compilation","For small, quick queries, the effort of generating code and compiling using janino is not justified. For these queries an interpreter would be quicker.

We could use the same linq4j methods (e.g. groupBy) but the lambdas that implement expressions would be interpreted. Linq4j's AbstractNode.evaluate(Evaluator) is probably the way to go, but that method needs some work.

To be clear, this change would not remove the code-generation option. It would just be another alternative.

---------------- Imported from GitHub ----------------
Url: https://github.com/julianhyde/optiq/issues/107
Created by: [julianhyde|https://github.com/julianhyde]
Labels: enhancement, 
Created at: Fri Dec 20 20:07:11 CET 2013
State: open
",smartshark_2_2,126,calcite,"""[0.9983843564987183, 0.0016156266210600734]"""
329,228588,Support EXCEPT DISTINCT,"This will rewrite except distinct as below
{code}
Rewrite: (GB-Union All-GB)-GB-UDTF (on all attributes) 
Example: R1 Except All R2
R1 introduce VCol â2â, R2 introduce VCol â1â
R3 = GB(R1 on all attributes + VCol + count(VCol) as c) union all GB(R2 on all attributes + VCol + count(VCol) as c)
R4 = GB(R3 on all attributes + sum(c) as a + sum(VCol*c) as b) 
Note, now we have m+n=a, 2m+n=b where m is the #row in R1 and n is the #row in R2 then
m=b-a, n=2a-b, m-n=2b-3a

R5 = Fil (b-a>0 && 2a-b=0) 
R6 = select only attributes from R5
{code}",smartshark_2_2,1342,calcite,"""[0.9968453049659729, 0.003154715755954385]"""
330,228589,Support NEXT/CURRENT VALUE FOR syntax for sequences usage,Support the ANSI standard NEXT VALUE FOR and CURRENT VALUE FOR syntax to access/increment sequence values. See http://phoenix.apache.org/sequences.html and http://phoenix.apache.org/language/index.html#sequence for examples.,smartshark_2_2,135,calcite,"""[0.9984318614006042, 0.001568067236803472]"""
331,228590,Upgrade org.incava java-diff,"Upgrade org.incava java-diff from 1.1 to 1.1.1.

The license changed from 2-clause BSD to Apache. The package changed from {{org.incava.util.diff}} to {{org.incava.diff}}.

The code is on github at https://github.com/jpace/java-diff.",smartshark_2_2,2281,calcite,"""[0.9982531666755676, 0.0017467926954850554]"""
332,228591,Allow connecting to a single schema without writing a model,"Allow connecting to a single schema without writing a model. Currently, to use a SchemaFactory you need to write a model.json file, but with this feature, you could specify the same information as connect string parameters.

Examples

1. JDBC (built-in). Instead of writing a model with

{code}
 {
    type: 'jdbc',
    name: 'FOODMART',
    jdbcUser: 'FOODMART',
    jdbcPassword: 'FOODMART',
    jdbcUrl: 'jdbc:hsqldb:res:foodmart',
    jdbcSchema: 'foodmart'
  }
{code}

use the connect string {{jdbc:calcite:schema_type=jdbc; schema.jdbcUser=FOODMART; schema.jdbcPassword=FOODMART; schema.jdbcUrl='jdbc:hsqldb:res:foodmart'; schema.jdbcSchema=foodmart}}

2. Mongo (user-defined). Instead of writing a model with

{code}
  {
      type: 'custom',
      name: 'mongo_raw',
      factory: 'org.apache.calcite.adapter.mongodb.MongoSchemaFactory',
      operand: {
        host: 'localhost',
        database: 'test'
      }
    }
{code}

use the connect string {{jdbc:calcite:schemaFactory=org.apache.calcite.adapter.mongodb.MongoSchemaFactory; schema.host=localhost; schema.database=test}}

3. Cassandra (user-defined). Instead of writing a model with

{code}
  {
      type: 'custom',
      name: 'twissandra',
      factory: 'org.apache.calcite.adapter.cassandra.CassandraSchemaFactory',
      operand: {
        host: 'localhost',
        keyspace: 'twissandra'
      }
    }
{code}

use the connect string {{jdbc:calcite:schemaFactory=org.apache.calcite.adapter.cassandra.CassandraSchemaFactory; schema.host=localhost; schema.keyspace=twissandra}}

You can supply any operand supported by the adapter, as long as you prefix its name with ""schema."".",smartshark_2_2,1206,calcite,"""[0.9985028505325317, 0.0014971621567383409]"""
333,228592,"When ReduceExpressionRule simplifies a nullable expression, allow the result to change type to NOT NULL","In some cases, the user needs to select whether or not to add casts that match nullability.
One of the motivations behind this is to avoid unnecessary casts like the following example.
original filter 
{code}
OR(AND(>=($0, CAST(_UTF-16LE'2010-01-01 00:00:00 UTC'):TIMESTAMP_WITH_LOCAL_TIME_ZONE(15)), <=($0, CAST(_UTF-16LE'2012-03-01 00:00:00 UTC'):TIMESTAMP_WITH_LOCAL_TIME_ZONE(15))))
{code}
the optimized expression with matching nullability
{code}
OR(AND(CAST(>=($0, 2010-01-01 00:00:00)):BOOLEAN, CAST(<=($0, 2012-03-01 00:00:00)):BOOLEAN))
{code}
As you can see this extra cast gets into the way of following plan optimization steps.
The desired expression can be obtained by turning off the nullability matching.
{code}
OR(AND(>=($0, 2010-01-01 00:00:00), <=($0, 2012-03-01 00:00:00)))
{code}
",smartshark_2_2,2086,calcite,"""[0.9984112977981567, 0.0015887090703472495]"""
334,228593,"Avatica should sent payload in message body, not header","This one is pretty surprising; noticed while playing around with the server with curl. We're reading the request payload off of the {{request}} header rather than from the request payload. For example, this is required:

{noformat}
curl -XPOST \
  -H 'request: {""request"":""prepareAndExecute"",""connectionId"":""3b0f3231-47d2-45f0-9aba-1f317955beaa"",""sql"":""select * from WEB_STAT"",""maxRowCount"":-1}' \
  http://localhost:8765/
{noformat}",smartshark_2_2,434,calcite,"""[0.5744578242301941, 0.4255421757698059]"""
335,228594,Allow user-defined SqlGroupedWindowFunction,"In Flink we want to create additional group auxiliary functions (such as {{TUMBLE_ROWTIME(), TUMBLE_PROCTIME()}}). 

Unfortunately, {{SqlGroupFunction}} and its methods are package-private which prevents us from adding custom functions. Also {{AuxiliaryConverter}} limits because it is statically defined and not pluggable.",smartshark_2_2,2062,calcite,"""[0.9986407160758972, 0.0013593011535704136]"""
336,228595,Additional metrics instrumentation,I've found that instrumenting ExecuteRequest and CommitRequest are really nice to see latencies on the write side.,smartshark_2_2,1037,calcite,"""[0.9983946681022644, 0.0016053466824814677]"""
337,228596,Upgrade Apache parent POM to version 18,"Upgrade Apache parent POM to version 18, which was released in May; we are currently on version 17.

The [differences|https://svn.apache.org/viewvc/maven/pom/tags/apache-18/pom.xml?r1=HEAD&r2=1675930&diff_format=h] seem to include:
* upgrade various plugins
* change {{http:}} to {{https:}} in URLs
* exclude {{DEPENDENCIES}} from release (related to CALCITE-741)

",smartshark_2_2,1480,calcite,"""[0.9983635544776917, 0.0016364854527637362]"""
338,228597,Revert temporary API changes introduced in [CALCITE-575],The API changes introduced in CALCITE-575 are temporary. We should remove them before 1.1.,smartshark_2_2,238,calcite,"""[0.9959771037101746, 0.004022936802357435]"""
339,228598,Avatica - Remove unused shaded jar,"Currently shaded/core/pom.xmlÂ and standalone-server/pom.xmlÂ files shade jar whichÂ was not declared in the pom files:
 - org.apache.commons

It may cause problems for projects which shade Avatica and have dependencies to the specified library.",smartshark_2_2,2531,calcite,"""[0.998332679271698, 0.001667345641180873]"""
340,228599,Intermittent test failures,"A few tests are currently failing intermittently.

1. UdfTest

{noformat}
UdfTest.testUserDefinedFunction:162 
Expected: is <9>
    but: was <10>
{noformat}

2. StreamTest.testInfiniteStreamsDoNotBufferInMemory

{noformat}
FAILURE! - in org.apache.calcite.test.StreamTest
testInfiniteStreamsDoNotBufferInMemory(org.apache.calcite.test.StreamTest)
Time elapsed: 0.218 sec  <<< ERROR!
java.lang.RuntimeException: exception while executing [select stream * from
orders]
at
org.apache.calcite.test.StreamTest.testInfiniteStreamsDoNotBufferInMemory(StreamTest.java:237)
Caused by: java.util.NoSuchElementException
at
org.apache.calcite.test.StreamTest.testInfiniteStreamsDoNotBufferInMemory(StreamTest.java:237)
{noformat}

I don't consider any of them serious -- they are test issues, not product correctness issues -- but they are inconvenient because false negatives waste developer time.",smartshark_2_2,1825,calcite,"""[0.9713397026062012, 0.0286603681743145]"""
341,228600,Push condition of non-ansi join into join operator,"I've tested two plans and it turns out the query with non-ansi joins has extremely bad plan (note {{EnumerableJoinRel(condition=\[true\]}}):
{code:sql}
explain plan for select d.""deptno"", e.""empid""
from ""hr"".""emps"" as e
  , ""hr"".""depts"" as d
where e.""deptno"" = d.""deptno""+0

PLAN=EnumerableCalcRel(expr#0..2=[{inputs}], expr#3=[CAST($t1):INTEGER NOT NULL], expr#4=[0], expr#5=[+($t2, $t4)], expr#6=[=($t3, $t5)], deptno=[$t2], empid=[$t0], $condition=[$t6])
  EnumerableJoinRel(condition=[true], joinType=[inner])
    EnumerableCalcRel(expr#0..4=[{inputs}], proj#0..1=[{exprs}])
      EnumerableTableAccessRel(table=[[hr, emps]])
    EnumerableCalcRel(expr#0..2=[{inputs}], deptno=[$t0])
      EnumerableTableAccessRel(table=[[hr, depts]])
{code}

Same works fine with ANSI style:
{code:sql}
explain plan for select d.""deptno"", e.""empid""
from ""hr"".""emps"" as e
  join ""hr"".""depts"" as d
 on (e.""deptno"" = d.""deptno""+0)

PLAN=EnumerableCalcRel(expr#0..3=[{inputs}], deptno=[$t2], empid=[$t0])
  EnumerableJoinRel(condition=[=($1, $3)], joinType=[inner])
    EnumerableCalcRel(expr#0..4=[{inputs}], expr#5=[CAST($t1):INTEGER NOT NULL], empid=[$t0], $f5=[$t5])
      EnumerableTableAccessRel(table=[[hr, emps]])
    EnumerableCalcRel(expr#0..2=[{inputs}], expr#3=[0], expr#4=[+($t0, $t3)], deptno=[$t0], $f3=[$t4])
      EnumerableTableAccessRel(table=[[hr, depts]])
{code}

The query that does not use calculations works fine even with non-ansi style:
{code:sql}
explain plan for select d.""deptno"", e.""empid""
from ""hr"".""emps"" as e
  , ""hr"".""depts"" as d
where e.""deptno"" = d.""deptno""

PLAN=EnumerableCalcRel(expr#0..2=[{inputs}], deptno=[$t2], empid=[$t0])
  EnumerableJoinRel(condition=[=($1, $2)], joinType=[inner])
    EnumerableCalcRel(expr#0..4=[{inputs}], proj#0..1=[{exprs}])
      EnumerableTableAccessRel(table=[[hr, emps]])
    EnumerableCalcRel(expr#0..2=[{inputs}], deptno=[$t0])
      EnumerableTableAccessRel(table=[[hr, depts]])
{code}",smartshark_2_2,430,calcite,"""[0.955485463142395, 0.044514596462249756]"""
342,228601,Git test fails when run from source distro,"Git test fails when run from source distro, because there the test is run in a directory that is not under git.

{code}
Failed tests:
 OsAdapterTest.testGitCommitsTop:177
Expected: ""author=Julian Hyde <julianhyde@gmail.com>""
    but: was """"
Tests in error:
 OsAdapterTest.testFiles:110 Â» Runtime exception while executing [select
distin...
{code}",smartshark_2_2,2108,calcite,"""[0.9885332584381104, 0.01146672572940588]"""
343,228602,Make RelCollation trait and AbstractRelNode.getCollationList consistent,"Currently {{getCollationList}} is not consistent with {{RelCollation}} trait since a node can have only one collation trait, thus you cannot express the node that is sorted on multiple collations at the same time.

We should either drop {{getCollationList}} in favour of {{getCollation}} or make collation trait support multiple ""collation lists"".",smartshark_2_2,229,calcite,"""[0.9968319535255432, 0.0031681032851338387]"""
344,228603,Aggregate Join Push-down on a Single Side,"WhileÂ investigatingÂ https://issues.apache.org/jira/browse/CALCITE-2195,Â it's apparent that aggregation can be pushed on on a single side (either side), and leave the other side non-aggregated, regardless of whether grouping columns are unique on the other side. My analysis â [http://zhong-j-yu.github.io/aggregate-join-push-down.pdf]Â .

This mayÂ be useful when the metadata is insufficient; in any case, we may try to provide all 3 possible transformations (aggregate on left only; right only; both sides) to the cost based optimizer, so that the cheapest one can be chosen based on stats.Â 

Does this make any sense, anybody? If it sounds good, I'll implement it and offerÂ a PR.Â ",smartshark_2_2,2344,calcite,"""[0.9480312466621399, 0.051968708634376526]"""
345,228604,Define initial list of topics,"a. Overview of Calcite
b. Pushing down operations, manipulating the relational expression tree
c. The standard relational expressions
d. Overview of planning concepts (RelNode, rule, trait, convention, metadata)
e. The lifecycle of a query, from parsing to execution
f. Enumerable convention (expands on e)
g. Bindable convention and the interpreter (expands on e)
h. Writing an adapter
i. Testing an adapter
j. Example for extending the language. ",smartshark_2_2,255,calcite,"""[0.9984931945800781, 0.001506761065684259]"""
346,228605,Need Detailed Documentation for HTTP Avatica Support,"I was unable to find any documentation on how to use the Avatica features.  I was not even able to find them on the Avatica site.  Would be great if Phoenix had a least a few examples of how to us the feature such as the one I wrote below.  It took me 2-3 days to figure it out with the help of user@phoenix.apache.org .  Should have only taken about 15 minutes.  Granted, the JSON format is outdated in recent versions, but you get the idea.  I couldn't even figure out which requests were required and what sequence they had to be in.

{code:title=example_upsert.sh|borderStyle=solid}
#!/bin/bash

curl -XPOST -H 'Content-Type application/json; charset=UTF-8' -d '{""connectionId"": ""00000000-0000-0000-0000-000000000000"", ""request"": ""openConnection""}' http://10.0.100.28:8765/

curl -XPOST -d '{""connectionId"": ""00000000-0000-0000-0000-000000000000"", ""request"": ""connectionSync"", ""connProps"": {""autoCommit"": null, ""connProps"": ""connPropsImpl"", ""transactionIsolation"": null, ""catalog"": null, ""readOnly"": null, ""dirty"": null, ""schema"": null}}' http://10.0.100.28:8765/

curl -XPOST -d '{""connectionId"": ""00000000-0000-0000-0000-000000000000"", ""request"": ""connectionSync"", ""connProps"": {""autoCommit"": true, ""connProps"": ""connPropsImpl"", ""transactionIsolation"": null, ""catalog"": null, ""readOnly"": null, ""dirty"": null, ""schema"": null}}' http://10.0.100.28:8765/

curl -XPOST -d ""{\""request\"":\""createStatement\"",\""connectionId\"":\""00000000-0000-0000-0000-000000000000\""}"" http://10.0.100.28:8765/

echo
echo Enter the statementId:
read statement_id

echo
echo Enter the value:
read value

curl -XPOST -d ""{\""request\"":\""prepareAndExecute\"",\""connectionId\"":\""00000000-0000-0000-0000-000000000000\"",\""sql\"":\""upsert into CAT_MAP(CHA, CAT, MAP) values('TEST_RECORD_CHANNEL', 'Test::Record::Channel::Category', '$value')\"", \""statementId\"":$statement_id}"" http://10.0.100.28:8765/

curl -XPOST -d ""{\""request\"":\""prepareAndExecute\"",\""connectionId\"":\""00000000-0000-0000-0000-000000000000\"",\""sql\"":\""upsert into CAT_MAP(CHA, CAT, MAP) values('TEST_RECORD_CHANNEL2', 'Test::Record::Channel::Category', '$value')\"", \""statementId\"":$statement_id}"" http://10.0.100.28:8765/

curl -XPOST -d ""{\""request\"":\""closeStatement\"",\""connectionId\"":\""00000000-0000-0000-0000-000000000000\"", \""statementId\"":$statement_id}"" http://10.0.100.28:8765/

curl -XPOST -d '{""connectionId"": ""00000000-0000-0000-0000-000000000000"", ""request"": ""closeConnection""}' http://10.0.100.28:8765/
{code}",smartshark_2_2,2233,calcite,"""[0.9984676241874695, 0.001532332506030798]"""
347,228606,Cost estimation for expression evaluate,"Expression evaluate performance is different one to another based on its operator type and underlying execution engine. For example, a regex pattern matching (e.g., col like 'foo%bar%') may requires 100 times more cost compares to a simple comparison (e.g., col > 3). Currently we don't have this kind of differentiation.

Expression cost will grant following applications:

1. Improved predicate push down: predicates can be pushed down to the right position based on row count and its own cost.
2. Predicate ordering rule: predicate with different order, e.g.,

{code:sql}
col1 like 'foo%bar%' and col2 > 3
col2 > 3 and col1 like 'foo%bar%'
{code}
 
may have different performance, introducing a predicate ordering rule will help find a better one.

The cost of each RexCall / RexAggCall is provided by Schema, stored in an external metadata service.",smartshark_2_2,1936,calcite,"""[0.9972115159034729, 0.002788454992696643]"""
348,228607,Update site to adhere to a `baseurl` property,"Noticed while working CALCITE-866, the trick to deploy a Jekyll site to a subdirectory (instead of the root of some vhost) doesn't work because Calcite has hard-coded anchors at ""/"".

We can work around this by using 

{noformat}
{{site.baseurl}}
{noformat}

There's a description on how to do this at https://jekyllrb.com/docs/github-pages/#project-page-url-structure. Would be convenient to be able to deploy at any location, not just the root of some vhost.

",smartshark_2_2,737,calcite,"""[0.9983737468719482, 0.0016262470744550228]"""
349,228608,Simplify CASE P1 THEN <boolean> P@ THEN <booleans> ...  ELSE TRUE/FALSE,"In HIVE-14431 [~jcamachorodriguez] proposed a simplification for CASE when all branches are not nullable boolean expression into an alternative AND/OR/NOT based expression. This allows for more aggressive reductions and split/push-down on the whole.  Meantime the simplifier code migrated to Calcite so I'm reviving this here.

The proposed simplification is:

{code}
CASE
WHEN p1 THEN ex1
WHEN p2 THEN ex2
...
WHEN pn THEN exn
ELSE TRUE/FALSE
END
{code}

to be transformed into:
{code}
(p1 AND ex1)
OR (not(p1) AND p2 AND x2)
...
OR (not(p1) AND not(p2) ... AND not(pn-1) AND Pn AND exn)
[OR (not(p1) AND not(p2) ... AND not(pn))]
{code}

The last OR is depending on the ELSE branch being TRUE/FALSE.",smartshark_2_2,1729,calcite,"""[0.9982839226722717, 0.0017160336719825864]"""
350,228609,LogicalAggregate public constructor arguments are undocumented,LogicalAggregate constructor arguments traitSet and indicator are undocumented in Javadoc. Documentation is missing in the corresponding factories too.,smartshark_2_2,314,calcite,"""[0.9964078068733215, 0.003592228516936302]"""
351,228610,Push OVER Clause to underlying SQL via JDBC adapter,"The jdbc adapter adapter should push down the OVER clause  for all dialects that support window functions. 

At the moment the Rel to SQL conversion ignores the 'OVER(...)'. The RexOver expression is treated as a plain RexCall and the RexOver#window attribute is not converted into SQL. 

For example if the following sql query (using Postgres dialect): 
{code:sql}
SELECT ""id"", ""device_id"", ""transaction_value"", ""account_id"", ""ts_millis"", MAX(""ts_millis"") OVER(partition by ""device_id"") as ""last_version_number"" 
FROM ""HAWQ"".""transaction""
WHERE ""device_id"" = 1445
{code}
is pushed down to the jdbc like this:
{code:sql}
SELECT ""id"", ""device_id"", ""transaction_value"", ""account_id"", ""ts_millis"", MAX(""ts_millis"") AS ""last_version_number""
FROM ""transaction""
WHERE ""device_id"" = 1445
{code}
The OVER clause is completely dropped!  Here is the plan:
{code}
JdbcToEnumerableConverter
  JdbcProject(id=[$0], device_id=[$1], transaction_value=[$2], account_id=[$3], ts_millis=[$4], last_version_number=[MAX($4) OVER (PARTITION BY $1 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)])
    JdbcFilter(condition=[=($1, 1445)])
      JdbcTableScan(table=[[HAWQ, transaction]])
{code}
",smartshark_2_2,1465,calcite,"""[0.9034712910652161, 0.09652876108884811]"""
352,228611,README.md instruction for compile in example for csv doesn't work,"mvn compile doesn't put the required files in the expected place for the tutorial. Change to 'mvn clean install'.

I will provide the oneliner patch. :)",smartshark_2_2,179,calcite,"""[0.9933538436889648, 0.006646172609180212]"""
353,228612,"Avatica - Apache Parent POM 19, JDK 10 surefire, and JDK 10 javadoc",Avatica is still using Apache Parent POM 18. Apache Parent POM 19 is out. This will also fix surefire and Javadoc generation with JDK 10+ for Avatica.,smartshark_2_2,2430,calcite,"""[0.9981974959373474, 0.00180248508695513]"""
354,228613,Simpler SPI to query Table,"Currently, the easiest way for a Table to return results is to implement TranslatableTable, and in the toRel method return a RelNode. That RelNode is typically a sub-class of TableAccessRelBase and implements itself by generating Java code, which must be compiled an executed.

We propose an interfaces so that a Table can return results directly.

{code}
import net.hydromatic.avatica.Cursor;
import org.eigenbase.util.Pair;
import net.hydromatic.optiq.Table;
import org.eigenbase.rex.RexNode;
import org.eigenbase.reltype.RelDataTypeField;

interface CursorableTable extends Table {
  /** Returns a cursor over the rows in this Table. */
  Cursor scan();
}

interface ProjectableCursorableTable extends Table {
  /** Given a list of filters and a list of fields to project, returns a
   * cursor over the rows in this Table and a list of filters that it
   * could not implement.
   *
   * <p>The cursor must implement the projects and any filters not
   * in the list. */
  Pair<Cursor, List<RexNode>> projectFilterScan(List<RexNode> filters, List<RelDataTypeField> projects);
}
{code}

Cursor is a simple API to implement.

A table that implements the CursorableTable SPI does not need to implement TranslatableTable, and so can return its results without doing any code generation.

The ProjectableCursorableTable SPI is also optional and goes further; it allows Calcite to push down simple projects and filters without the Table implementor writing any rules or code-generation code. If the implementation of the SPI cannot handle a particular filter, it just returns it, and Calcite will add its own filter to implement it.",smartshark_2_2,233,calcite,"""[0.9984244108200073, 0.0015756147913634777]"""
355,228614,Upgrade Jackson,"Upgrade Jackson to latest stable (2.6.3); current is 2.1.1 (released 2012).

Before fixing, make a list in this JIRA case of places that Jackson classes appear in Calcite public APIs.

See [email thread|https://mail-archives.apache.org/mod_mbox/calcite-dev/201512.mbox/%3CCA%2BAsLaO%2BSgXg4he1ez7z05QeeDODraAoRd%3D6GfDU_oB2tMOYKw%40mail.gmail.com%3E].

This issue is marked ""avatica"" because JSON is one of the message encodings uses by Avatica uses but also affects Calcite core.",smartshark_2_2,905,calcite,"""[0.9984311461448669, 0.0015688295243307948]"""
356,228615,Tweak cost of BindableTableScan to make sure Project is pushed through Aggregate,"Similar to [CALCITE-1876].
Projects are not pushed to BindableTableScan when using ProjectableFilterableTable with aggregate functions.
The reason is that the cost of BindableTableScan does not use projects (and filters), so the planner chooses a plan with Project node removed by ProjectRemoveRule.
By tweaking the cost to use the number of used projects solved the problem.
Any suggestion on the cost formula to take both projects and filters into account?",smartshark_2_2,2066,calcite,"""[0.9559182524681091, 0.04408177733421326]"""
357,228616,"Implement ""connectionSync"" RPC","Per thread on dev list titled ""Avatica handling of connection state"", this ticket implements such an rpc.",smartshark_2_2,299,calcite,"""[0.9984336495399475, 0.0015663832891732454]"""
358,228617,"Support ""FOR SYSTEM_TIME AS OF"" in regular queries","As discussed in mailling list[1], we hope to support ""FOR SYSTEM_TIME AS OF"" in Calcite to retrieve table contents as of a given point in time. 

[1] https://lists.apache.org/thread.html/f877f356a8365bf74ea7d8e4a171224104d653cf73861afb2901a58f@%3Cdev.calcite.apache.org%3E",smartshark_2_2,1900,calcite,"""[0.9984733462333679, 0.0015266372356563807]"""
359,228618,Implement SUBSTRING function for BINARY and VARBINARY,"Calling SUBSTRING(binary FROM int) and SUBSTRING(binary FROM int FOR int) is failing. While failing it complains that there're no matching SqlFunctions.substring(ByteString, int) and SqlFunctions.substring(ByteString, int, int).

Could be simple fix.",smartshark_2_2,1381,calcite,"""[0.781923234462738, 0.21807673573493958]"""
360,228619,Add more types to Pig adapter,"Currently the Pig adapter only supports VARCHAR columns. Add support for other popular types, such as numeric and date-time ones.",smartshark_2_2,1776,calcite,"""[0.9984865188598633, 0.0015134585555642843]"""
361,228620,2 copies of avatica docs on the website,"There appears to be 2 different copies of the avatica docs on the website.

For example, for the protobuf docs, there are 2 copies:

- https://calcite.apache.org/avatica/docs/protobuf_reference.html
- https://calcite.apache.org/docs/avatica_protobuf_reference.html

The ones hosted directly at https://calcite.apache.org/docs do not appear to be identical to the ones at https://calcite.apache.org/avatica/docs.

It might be better to 1 copy of the docs and point the links to them.",smartshark_2_2,2227,calcite,"""[0.9976333379745483, 0.0023666282650083303]"""
362,228621,remove or relocate shaded jackson in Avatica,"The avatica jar includes a shaded version of jackson under the original package path.

This shaded version interferes with other jackson versions in the classpath.

Currently this prevents us from using jackson functionality which is only implemented in newer versions.

https://mail-archives.apache.org/mod_mbox/calcite-dev/201602.mbox/%3C977C7450-18F4-48E2-A970-69B7E3E1BD2C%40apache.org%3E
{quote}
Is it reasonable to have a maven profile that uses jackson as âprovidedâ[1] rather than shading? This would not be the default â the default would be continue to use a shaded version of jackson (relocated to org.apache.calcite.jackson, as Josh suggests) â but folks looking to embed calcite/avatica in a container might appreciate a lighter weight option.
{quote}",smartshark_2_2,1379,calcite,"""[0.9984645843505859, 0.0015354831703007221]"""
363,228622,Unicode character seems to be handled incorrectly in Avatica,"This was discovered with Apache Phoenix (4.4 and 4.5) while using the thin JDBC client to connect to Phoenix Query Server.

This can be reproduced using a CREATE TABLE statement with non-ASCII characters in column name:

create table colUnicode (""ÐÐ¾Ð¼ÐµÑÐ¢ÐµÐ»ÐµÑÐ¾Ð½Ð°"" integer not null primary key, col2 varchar)

When executing the above statement using the thin client and retrieving the table using either the thin or the thick client the column name shows up as Ãï¿½ÃÂ¾ÃÂ¼ÃÂµÃï¿½ÃÂ¢ÃÂµÃÂ»ÃÂµÃï¿½ÃÂ¾ÃÂ½ÃÂ°. Executing the same CREATE table statement using the thick client seems to work fine, the column name shows up correctly when using either the thin or the thick client to retrieve the table.

The same behavior can also be observed when when inserting non-ASCII data into a varchar column in a table.

When using WireShark to observe the network traffic, we can see the non-ASCII character sent from the thin client to the server has been encoded into UTF-8. ",smartshark_2_2,1105,calcite,"""[0.1299554705619812, 0.8700445294380188]"""
364,228623,Make NonReservedKeyWord List Extendable,"Per conversation on DRILL-1065 (or see [pull request|https://github.com/apache/drill/pull/159]), the keyword list can be extended but non-reserved keywords cannot be. Also, we don't need NonReservedKeyWord() and CommonNonReservedKeyWord() to be separate anymore.",smartshark_2_2,565,calcite,"""[0.9985577464103699, 0.0014423062093555927]"""
365,228624,Change return type of JoinFactory::create*Join(),"This provides tighter type checking than RelNode and is consistent with RelFieldTrimmer, only consumer of this factory which assumes these nodes to be of type {{JoinRelBase}}",smartshark_2_2,96,calcite,"""[0.9984311461448669, 0.0015688386047258973]"""
366,228625,"Need ""normal"" policy for the issues","I wish there was ""Normal"" priority for optiq issues.
I struggle using ""Major"" priority (it seems way too high) and I struggle using ""Minor"" (it seems way too low).

For instance, for this issue I would use ""Normal"" priority if I could.

I guess rename Major->Normal, Critical->Major would do the trick.",smartshark_2_2,872,calcite,"""[0.9984227418899536, 0.0015772486804053187]"""
367,228626,Support view partial rewriting in aggregate materialized view rewriting,"Simple extension for AbstractMaterializedViewRule similar to CALCITE-1791, however for views/queries rooted with an Aggregate node.",smartshark_2_2,1853,calcite,"""[0.9980447292327881, 0.0019552779849618673]"""
368,228627,"Allow table, column and field called ""*""","Currently if we see an asterisk in an identifier, we translate it to an identifier name segment called ""\*"". If there happens to be a table, column or field called ""*"" we're sunk.

Star can only validly occur in the SELECT clause (not in FROM or WHERE). Examples: {{select emp.\* from emp}}, {{select x from sales.\*}}, {{select 1 from emp where emp.\* is not null}}.

This task is to test with tables, columns, fields called ""*"", and use a special value in SqlIdentifier. Maybe the empty string would work. Names cannot be empty.",smartshark_2_2,787,calcite,"""[0.9953069090843201, 0.004693048074841499]"""
369,228628,Detect transitive join conditions via expressions,"Given table aliases ta, tb column names ca, cb, and an arbitrary (deterministic) expression expr then calcite should be capable to infer join conditions by transitivity:

{noformat}
ta.ca = expr AND tb.cb = expr -> ta.ca = tb.cb
{noformat}

The use case for us stems from SPARQL to SQL rewriting, where SPARQL queries such as

{code:java}
SELECT {
  dbr:Leipzig a ?type .
  dbr:Leipzig dbo:mayor ?mayor
}
{code}
result in an SQL query similar to

{noformat}
SELECT s.rdf a, s.rdf b WHERE a.s = 'dbr:Leipzig' AND b.s = 'dbr:Leipzig'
{noformat}

A consequence of the join condition not being recognized is, that Apache Flink does not find an executable plan to process the query.

Self contained example:
{code:java}
package my.package;

import org.apache.calcite.adapter.java.ReflectiveSchema;
import org.apache.calcite.plan.RelOptUtil;
import org.apache.calcite.rel.RelNode;
import org.apache.calcite.rel.RelRoot;
import org.apache.calcite.schema.SchemaPlus;
import org.apache.calcite.sql.SqlNode;
import org.apache.calcite.sql.parser.SqlParser;
import org.apache.calcite.tools.FrameworkConfig;
import org.apache.calcite.tools.Frameworks;
import org.apache.calcite.tools.Planner;
import org.junit.Test;


public class TestCalciteJoin {
    public static class Triple {
        public String s;
        public String p;
        public String o;

        public Triple(String s, String p, String o) {
            super();
            this.s = s;
            this.p = p;
            this.o = o;
        }

    }

    public static class TestSchema {
        public final Triple[] rdf = {new Triple(""s"", ""p"", ""o"")};
    }


    @Test
    public void testCalciteJoin() throws Exception {
        SchemaPlus rootSchema = Frameworks.createRootSchema(true);

        rootSchema.add(""s"", new ReflectiveSchema(new TestSchema()));

        Frameworks.ConfigBuilder configBuilder = Frameworks.newConfigBuilder();
        configBuilder.defaultSchema(rootSchema);
        FrameworkConfig frameworkConfig = configBuilder.build();

        SqlParser.ConfigBuilder parserConfig = SqlParser.configBuilder(frameworkConfig.getParserConfig());
        parserConfig
            .setCaseSensitive(false)
            .setConfig(parserConfig.build());

        Planner planner = Frameworks.getPlanner(frameworkConfig);

        // SELECT s.rdf a, s.rdf b WHERE a.s = 5 AND b.s = 5
        SqlNode sqlNode = planner.parse(""SELECT * FROM \""s\"".\""rdf\"" \""a\"", \""s\"".\""rdf\"" \""b\"" WHERE \""a\"".\""s\"" = 5 AND \""b\"".\""s\"" = 5"");
        planner.validate(sqlNode);
        RelRoot relRoot = planner.rel(sqlNode);
        RelNode relNode = relRoot.project();
        System.out.println(RelOptUtil.toString(relNode));
    }
}
{code}



Actual plan:
{code:java}
LogicalProject(s=[$0], p=[$1], o=[$2], s0=[$3], p0=[$4], o0=[$5])
  LogicalFilter(condition=[AND(=($0, 5), =($3, 5))])
    LogicalJoin(condition=[true], joinType=[inner])
      EnumerableTableScan(table=[[s, rdf]])
      EnumerableTableScan(table=[[s, rdf]])
{code}

Expected Plan fragment:
{code:java}
    LogicalJoin(condition=[=($0, $3)], joinType=[inner])
{code}

",smartshark_2_2,1931,calcite,"""[0.9959937334060669, 0.004006249364465475]"""
370,228629,Take advantage of Calcite DDL,In Calcite 1.15 abstract support for DDL moved into the calcite core. We should take advantage of that.,smartshark_2_2,2378,calcite,"""[0.9984092116355896, 0.0015908745117485523]"""
371,228630,Modeler: text fields discard input unless you press enter,"In Modeler, if you enter text in a text field, like the DbAttribute name or the DbAttribute maxLength and then select a different entity in the sidebar, or click the ""new attribute"" button, then the text you entered is lost.  Those are just two examples of common situations when this occurs; there are certainly more.  The text entered should be pushed to the model object backing the text field when the field loses focus.

Since this is essentially a ""data loss"" issues I would consider this high priority.  It really need to just work.",smartshark_2_2,1024,cayenne,"""[0.31900808215141296, 0.6809918880462646]"""
372,228631,Oracle - passing blobs as stored procedure parameters,"Currently if we try to pass Blob as stored procedure parameter using Oracle database, we will get ClassCastException: cannot cast org.apache.cayenne.util.MemoryBlob to oracle.sql.BLOB.",smartshark_2_2,801,cayenne,"""[0.08927760273218155, 0.9107223749160767]"""
373,228632,problems with relationships when using nested contexts and ROP,The problem reveals itself in several ways. I'll attach a junit case to show one of the problems. ,smartshark_2_2,290,cayenne,"""[0.10576490312814713, 0.8942350745201111]"""
374,228633,Maven cgen: all and datamap modes can not be activated,"This was reported on the user list, and my own tests seem to confirm that setting <mode>datamap</mode> or <mode>all</mode> don't result in class generation for a DataMap. Setting <mode>entity</mode> or omitting ""mode"" tag, results in correct generation of classes for all entities.",smartshark_2_2,736,cayenne,"""[0.13527412712574005, 0.8647258877754211]"""
375,228634,reverse engineering partially changed schema shows empty error messge window,"I get an error in the CayenneModeler 1.2.4  when reverse engineering my schema partially.

CayenneModeler INFO  [Thread-13 04-27 15:02:50] org.objectstyle.cayenne.modeler.dialog.db.DbLoaderHelper: Exception on reverse engineering
java.lang.NullPointerException
        at org.objectstyle.cayenne.access.DbLoader.loadDbRelationships(DbLoader.java:661)
        at org.objectstyle.cayenne.access.DbLoader.loadDbEntities(DbLoader.java:488)
        at org.objectstyle.cayenne.access.DbLoader.loadDataMapFromDB(DbLoader.java:775)
        at org.objectstyle.cayenne.access.DbLoader.loadDataMapFromDB(DbLoader.java:758)
        at org.objectstyle.cayenne.modeler.dialog.db.DbLoaderHelper$LoadDataMapTask.execute(DbLoaderHelper.java:388)
        at org.objectstyle.cayenne.modeler.util.LongRunningTask.internalExecute(LongRunningTask.java:251)
        at org.objectstyle.cayenne.modeler.util.LongRunningTask$1.run(LongRunningTask.java:139)
        at java.lang.Thread.run(Unknown Source)


I have updated a portion of the schema in  the database. I delete the DbEntities and ObjEntities of the tables that were changed.

I then click reverse Engineer from database. I get popups asking whether to overwrite existing Objects. I click ""remember my decision"" and NO.

at the end I get an error popup without a message. It is just containing the red error icon and a title reading ""Error Reengineering Database"".


This error does occur also when I select not to remember my decision and click through each popup one by one.

When reverse engineering the whole database, removing all ObjEntities and all DBEntities, before reverse engineering the error does not occur.
",smartshark_2_2,321,cayenne,"""[0.10195165872573853, 0.8980484008789062]"""
376,228635,SelectQuery 'fetchSize' setting is not propagated to disjoint prefetches,"When setting statementFetchSize property on select query with disjoint prefetches, it only uses it for the root query, while this setting must be copied to all the child prefetch queries.",smartshark_2_2,410,cayenne,"""[0.10836151242256165, 0.891638457775116]"""
377,228636,cgen: meaningful PK with boxed type ends up with primitive type in generated source,"If you have: 
1) modeled a primary key as an ObjAttribute, like customerPK 
2) modeled it as an object type, like java.lang.Long 
3) are using the Cayenne-Generated (Default) PK generation strategy
Then Cayenne won't generate any PKs for the entity and will just use zero over and over. This can be worked around by switching the attribute's type to a primitive (like long).",smartshark_2_2,1852,cayenne,"""[0.19740092754364014, 0.8025990724563599]"""
378,228637,cayenne-lifecycle: UuidBatchFault concurrency issues,"In case of UuidRelationship, when one thread is fetching the source entity, it may override UuidBatchFault of the existing objects. Then when another thread tries to read object relationship, it triggers fault resolution, all in parallel with the first thread, so ConcurrentModificationException may happen between UuidBatchFault.addUuid() and UuidBatchFault.fetchObjects().",smartshark_2_2,855,cayenne,"""[0.07569807767868042, 0.9243018627166748]"""
379,228638,Null value in subclass's field.,"For example have next objEntities: 

Super :
 - globalAttribute

Sub1 : 
 - sub1Attribute

Sub2 : 
 - sub2Attribute

where Sub1 and Sub2  are sub entities of Super entity, and table : 

super: 
 - ID
 - GLOBAL_ATTRIBUTE 
 - SUB_ATTRIBUTE

Then take records from db by entity ""Super"". Select Query filled ""sub2Attribute"" with null values in all objects of Sub2, when ""sub1Attribute"" filled with values from db.",smartshark_2_2,1311,cayenne,"""[0.20210900902748108, 0.7978910207748413]"""
380,228639,Error on Save after importing database,"Precondition: create db with one/or several tables
1. create new project in CayenneModeler (create only dataNode /or dataNode and dataMap). Save
2. Select Tools -> Reengineer Database Schema
3. Import tables from earlier created derby database -> OK
4. Click Save button
-> error appears

Exception: 
=================================
java.lang.NullPointerException
at org.apache.cayenne.project.FileProjectSaver.createSaveUnit(FileProjectSaver.java:125)
at org.apache.cayenne.project.FileProjectSaver.save(FileProjectSaver.java:75)
at org.apache.cayenne.modeler.action.SaveAction.saveAll(SaveAction.java:70)
at org.apache.cayenne.modeler.action.SaveAsAction.performAction(SaveAsAction.java:190)
at org.apache.cayenne.modeler.action.SaveAsAction.performAction(SaveAsAction.java:179)
at org.apache.cayenne.modeler.util.CayenneAction.actionPerformed(CayenneAction.java:162)
at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2028)
at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2351)
at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:387)
at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:242)
at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:236)
at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:272)
at java.awt.Component.processMouseEvent(Component.java:6348)
at javax.swing.JComponent.processMouseEvent(JComponent.java:3267)
at java.awt.Component.processEvent(Component.java:6113)
at java.awt.Container.processEvent(Container.java:2085)
at java.awt.Component.dispatchEventImpl(Component.java:4714)
at java.awt.Container.dispatchEventImpl(Container.java:2143)
at java.awt.Component.dispatchEvent(Component.java:4544)
at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4618)
at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4282)
at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4212)
at java.awt.Container.dispatchEventImpl(Container.java:2129)
at java.awt.Window.dispatchEventImpl(Window.java:2475)
at java.awt.Component.dispatchEvent(Component.java:4544)
at java.awt.EventQueue.dispatchEvent(EventQueue.java:635)
at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:296)
at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:211)
at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:201)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:196)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:188)
at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)",smartshark_2_2,929,cayenne,"""[0.09608462452888489, 0.9039154052734375]"""
381,228640,Modeler is trying to load data object classes when selecting a query,"1. Open our test cayenne.xml (1.5-unpublished/src/test/resources/)
2. Select testmap->ParameterizedQueryWithLocalCache. It is not selected, and you can see exceptions in log. CM is trying to load org.apache.artist.Artist at modeling stage",smartshark_2_2,390,cayenne,"""[0.1972704976797104, 0.8027294874191284]"""
382,228641,Garbage SQL generated for EJBQL subqueries,"Since EJBQL translator incorrectly reuses DISTINCT and WHERE buffers between the main query and subqueries, subqueries may end up generated incorrectly. E.g.:

""SELECT p FROM Painting p""
                + "" WHERE p.estimatedPrice = ALL (""
                + "" SELECT MAX(p1.estimatedPrice) FROM Painting p1""
                + "")""

SELECT t0.PAINTING_TITLE AS ec0_0, t0.ARTIST_ID AS ec0_1, t0.PAINTING_DESCRIPTION AS ec0_2, t0.ESTIMATED_PRICE AS ec0_3, t0.GALLERY_ID AS ec0_4, t0.PAINTING_ID AS ec0_5 FROM PAINTING t0 WHERE t0.ESTIMATED_PRICE = ALL (SELECT MAX(t1.ESTIMATED_PRICE) FROM PAINTING t1 WHERE)

(notice trailing WHERE...)",smartshark_2_2,111,cayenne,"""[0.07101929187774658, 0.9289806485176086]"""
383,228642,"Error resolving fault, no matching row exists in the database","Following testcase fail with error:

org.apache.cayenne.FaultFailureException: [v.3.0RC1 Ã.ÃÂ½ÃÂ² 27 2010 15:28:50] Error resolving fault, no matching row exists in the database for ObjectId: <ObjectId:GroupProperties, id=201>
        at org.apache.cayenne.BaseContext.prepareForAccess(BaseContext.java:152)
        at org.apache.cayenne.access.DataContextDeleteAction.performDelete(DataContextDeleteAction.java:86)
        at org.apache.cayenne.access.DataContextDeleteAction.processDeleteRules(DataContextDeleteAction.java:236)
        at org.apache.cayenne.access.DataContextDeleteAction.deletePersistent(DataContextDeleteAction.java:107)
        at org.apache.cayenne.access.DataContextDeleteAction.performDelete(DataContextDeleteAction.java:92)
        at org.apache.cayenne.access.DataContext.deleteObject(DataContext.java:931)
        at org.apache.cayenne.UserManagerTest.testGroupActions(UserManagerTest.java:53)

",smartshark_2_2,541,cayenne,"""[0.14992138743400574, 0.8500785827636719]"""
384,228643,NPE when callbacks invoked on null objects,Null values in object select via {{EJBQL}} query with registered callback result in {{NullPointerException}} thrown in {{LifecycleCallbackEventHandler.performCallbacks}}.,smartshark_2_2,1818,cayenne,"""[0.06829379498958588, 0.9317062497138977]"""
385,228644,ROP: CayenneContext does not unregister deleted nodes,"This test fail:

public class CayenneContextDeletionTest extends RemoteCayenneCase {
    public void test() {
        ClientMtTable1 obj = context.newObject(ClientMtTable1.class);
        context.commitChanges();
        
        context.deleteObject(obj);
                
        context.commitChanges();

        //now check that the object is unregistered
        assertNull(context.getGraphManager().getNode(obj.getObjectId()));
    }
}

BTW same thing with DataContext works",smartshark_2_2,238,cayenne,"""[0.11508070677518845, 0.8849192261695862]"""
386,228645,[PATCH] PersistentObjectList.setValue does not work,PersistentObjectList.setValue does not work - it ignores the new value passed to the function.,smartshark_2_2,813,cayenne,"""[0.10317385941743851, 0.8968261480331421]"""
387,228646,ROP: All entity's to-many relationships getting faulted from database when using it as a parameter in qualifier expression.,"If entity is used as a parameter in qualifier expression all its unfaulted to-many relationships are faulted prior to actual query sent to server. For example:

Expression exp = ExpressionFactory.matchExp(Painting.ARTIST_PROPERTY, artist);
context.performQuery(new SelectQuery(Painting.class, exp));

Before actual query will be sent to server all artist's to-many relationships will be faulted. What I see is that in the process of query serialization on client side PersistentObjectList.size() is invoked which triggers relationships' faulting from database.",smartshark_2_2,1315,cayenne,"""[0.12240782380104065, 0.8775922060012817]"""
388,228647,Db-generated PK type mismatch,"java class of a DB-generated PK turns out to be undefined and generally not matching the type of column it is used for. E.g. for a JDBC int column, the default Java mapping is  java.lang.Integer. When a generated key is configured for this column all db's return different values:

MySQL 5/Connector J 3.1.13 BigInteger
MySQL 5/Connector J 5.0.6   Long
Derby 10.2                            BigDecimal

To be fair JDBC spec does not specify what should be returned, so the vendors are doing what they want. 

This causes a number of problems in Cayenne. One specific exception - although we've hacked ObjectId comparison algorithm to unwrap the numbers and compare their numeric values, IncrementalFaultList compares DataRows and fails to match Objects against the id datarows:

/-- Encapsulated exception ------------\
org.apache.cayenne.CayenneRuntimeException: [v.3.0-SNAPSHOT Jun 28 2007 08:48:22] Can't find id for {<ObjectId:BlaBla, ID=85>; committed; [...]}
	at org.apache.cayenne.access.IncrementalFaultList$IncrementalListHelper.updateWithResolvedObjectInRange(IncrementalFaultList.java:779)
	at org.apache.cayenne.access.IncrementalFaultList.resolveInterval(IncrementalFaultList.java:427)
	at org.apache.cayenne.access.IncrementalFaultList.get(IncrementalFaultList.java:613)


I think we need to explicitly pass the Java type we expect for the ID inside the BatchAction",smartshark_2_2,224,cayenne,"""[0.11673947423696518, 0.8832605481147766]"""
389,228648,NPE's in the Modeler for incomplete relationships,"Getting various NPE's when mapping partial DbRelationships to ObjRelationships:

Exception in thread ""AWT-EventQueue-0"" java.lang.NullPointerException
	at org.apache.cayenne.modeler.dialog.objentity.EntityRelationshipsModel.displayName(EntityRelationshipsModel.java:73)
	at org.apache.cayenne.modeler.dialog.objentity.EntityRelationshipsModel.displayName(EntityRelationshipsModel.java:66)
	at org.apache.cayenne.modeler.dialog.objentity.EntityRelationshipsModel.getRelationshipNames(EntityRelationshipsModel.java:104)
	at org.apache.cayenne.modeler.dialog.objentity.ObjRelationshipInfoDialog$RelationshipPicker.getTableCellEditorComponent(ObjRelationshipInfoDialog.java:273)
	at javax.swing.JTable.prepareEditor(JTable.java:3983)
	at javax.swing.JTable.editCellAt(JTable.java:2688)
	
	
	
	
	
	pr 11, 2008 4:21:37 PM org.scopemvc.view.swing.STableModel setValueAt
WARNING: Can't set column 0 from row 0
java.lang.NullPointerException
	at org.apache.cayenne.modeler.dialog.objentity.ObjRelationshipInfoModel.connectEnds(ObjRelationshipInfoModel.java:379)
	at org.apache.cayenne.modeler.dialog.objentity.ObjRelationshipInfoModel.relationshipChanged(ObjRelationshipInfoModel.java:249)
	at org.apache.cayenne.modeler.dialog.objentity.ObjRelationshipInfoModel.modelChanged(ObjRelationshipInfoModel.java:231)
	at org.scopemvc.model.basic.ModelChangeEventSupport.fireModelChange(Unknown Source)
	at org.scopemvc.model.basic.BasicModel.fireModelChange(Unknown Source)
	at org.scopemvc.model.basic.BasicModel.modelChanged(Unknown Source)
	at org.scopemvc.model.basic.ModelChangeEventSupport.fireModelChange(Unknown Source)
	at org.scopemvc.model.basic.BasicModel.fireModelChange(Unknown Source)
	at org.apache.cayenne.modeler.dialog.objentity.EntityRelationshipsModel.setRelationshipDisplayName(EntityRelationshipsModel.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.scopemvc.model.beans.BeansPropertyManager.set(Unknown Source)",smartshark_2_2,231,cayenne,"""[0.06794469058513641, 0.9320553541183472]"""
390,228649,cdbimport do not create xml file in resource folder,"1. Create project using maven archetype:generate
2. Add to pom.xml cayenne maven plugin configuration
3. Run mvn install

Actual: xml file do not saved in main/java/resource
Expected: file with project and map must be created",smartshark_2_2,1086,cayenne,"""[0.9374114274978638, 0.06258852779865265]"""
391,228650,POST_LOAD is not called on prefetched objects,"Noticed that POST_LOAD is not invoked when an object was prefetched with another object. Here is a test case:


+    public void testPostLoad_Prefetch() throws Exception {
+        LifecycleCallbackRegistry registry = getDomain()
+                .getEntityResolver()
+                .getCallbackRegistry();
+
+        ObjectContext context = createDataContext();
+
+        registry.addListener(LifecycleEvent.POST_LOAD, Artist.class, ""postLoadCallback"");
+        MockCallingBackListener listener = new MockCallingBackListener();
+        registry.addListener(
+                LifecycleEvent.POST_LOAD,
+                Artist.class,
+                listener,
+                ""publicCallback"");
+
+        Artist a1 = context.newObject(Artist.class);
+        a1.setArtistName(""XX"");
+        Painting p1 = context.newObject(Painting.class);
+        p1.setToArtist(a1);
+        p1.setPaintingTitle(""XXX"");
+        context.commitChanges();
+
+        // reset context and read related object
+        context = createDataContext();
+
+        SelectQuery q = new SelectQuery(Painting.class);
+        q.addPrefetch(Painting.TO_ARTIST_PROPERTY);
+        p1 = (Painting) context.performQuery(q).get(0);
+
+        // artist is prefetched here, and a callback must have been invoked
+        a1 = p1.getToArtist();
+        assertEquals(PersistenceState.COMMITTED, a1.getPersistenceState());
+        assertEquals(1, a1.getPostLoaded());
+        assertSame(a1, listener.getPublicCalledbackEntity());
+    }",smartshark_2_2,751,cayenne,"""[0.10280738770961761, 0.8971925973892212]"""
392,228651,NPE with commitChangesToParent() after deleting object with child and cascade delete rule ,"Steps to reproduce:

1) Create object A
2) Add child object B (A has a cascade delete rule for B)
3) Create a child context
4) Delete A in the child context
5) commitChangesToParent in the child context
6) NPE:
java.lang.NullPointerException
	at org.apache.cayenne.access.DataContextDeleteAction.performDelete(DataContextDeleteAction.java:59)
	at org.apache.cayenne.access.DataContext.deleteObject(DataContext.java:946)
	at org.apache.cayenne.graph.ChildDiffLoader.nodeRemoved(ChildDiffLoader.java:127)
	at org.apache.cayenne.graph.NodeDeleteOperation.apply(NodeDeleteOperation.java:37)
	at org.apache.cayenne.graph.CompoundDiff.apply(CompoundDiff.java:91)
	at org.apache.cayenne.access.ObjectStoreGraphDiff.apply(ObjectStoreGraphDiff.java:134)
	at org.apache.cayenne.access.DataContext.onContextFlush(DataContext.java:1074)
	at org.apache.cayenne.BaseContext.onSync(BaseContext.java:298)
	at org.apache.cayenne.access.DataContext.flushToParent(DataContext.java:1121)
	at org.apache.cayenne.access.DataContext.commitChangesToParent(DataContext.java:1051)

Analysis:

The issue appears to be that the graph diff to be committed to the parent context records that both A and B are to be deleted, however the delete rules for A also cause B to deleted:

1) Child diff causes A to be deleted
2) A's delete rules cause B to deleted
3) Child diff causes B to be deleted, but B no longer exists

Possible fix (not tested in any way):

In ChildDiffLoader.nodeRemoved(), check if the result of findObject() is null before attempting to delete it.

Workaround (works for me):

In the child context, delete B before deleting A.",smartshark_2_2,821,cayenne,"""[0.0654359683394432, 0.9345639944076538]"""
393,228652,Reverse relationships may not be correctly set if inheritance is used.,"Given two entities, Employee and Address, such that there is a one-to-many relationship between the two, it may be possible that reverse relationships are not fully set if inheritance is used.

For example, let's say that HomeAddress extends Address, then the following fails:

Employee e = context.newObject(Employee.class);

        Address a = context.newObject(Address.class);
        a.setToEmployee(e);

        assertEquals(1, e.getAddresses().size());
        assertEquals(0, e.getHomeAddresses().size());

        HomeAddress ha = context.newObject(HomeAddress.class);
        ha.setToEmployee(e);

        assertEquals(2, e.getAddresses().size());
        assertEquals(1, e.getHomeAddresses().size());

The last assertion fails as e.getHomeAddresses() will return an empty list.

On the face of it, the problem is that the ObjRel ""addresses"" is being set rather than ""homeAddresses"".  This is due to how ObjRelationship#getReverseRelationship() determines reverse relationships.  It does so by inspecting the relationship structure and if there's a match, returns it.  ""addresses"" and ""homeAddresses"" have the same structure and ""addresses"" is the first match found and returned.

Simply reversing order or other similar tricks won't really do anything more for us though.  The real issue seems to be how to deal with multiple ObjRels that match to the same DbRel.  Each ObjRel does need to be updated in order for the graph to remain consistent.",smartshark_2_2,760,cayenne,"""[0.07524479180574417, 0.9247552156448364]"""
394,228653,Local jNDI hack breaks when running with Jetty6-Maven,"http://objectstyle.org/cayenne/lists/cayenne-user/2007/02/0125.html

Local JNDI hack (JNDIDataSourceFactory) loads Cayenne Modeler classes via reflection to read local preferences DB. In any non-trivial ClassLoader situation (Maven being among the more obscure ones), this may not work with  a simple Class.forName() call. The exception details are described in the mailing list message above. The fix is easy - use thread class loader",smartshark_2_2,202,cayenne,"""[0.34827110171318054, 0.6517288684844971]"""
395,228654,Bogus runtime relationships can mess up commit.,"I'm still the process of investigating this one, so more details to come.  Basically what I'm seeing is that if two ObjRels map to the same DbRel somehow and only one is mapped explicitly, the runtime one can fail validation while the specified one passes.  I'm observing this in a relationship that involves inheritance.  It looks like I've explicitly mapped an ObjRel for the subclass type and Cayenne is creating one for the base class.  The DbAttr is marked as required.  Since I never specify a value in the runtime relationship, since it's not mapped or useful to me, validation fails.  If I don't mark the attribute as mandatory, everything commits just fine.  So, I suspect Cayenne resolves the conflict somewhere down the line.",smartshark_2_2,772,cayenne,"""[0.08171243220567703, 0.9182875156402588]"""
396,228655,Deleting an object with a read-only flattened relationship fails,"When commiting a delete for an object that has a read-only flattened relationship with NO_ACTION as the delete rule, that relationship still seems to be processed causing the following exception to be thrown:

org.apache.cayenne.CayenneRuntimeException:[v.2.0.2 January 14 2007] Cannot unset the read-only flattened relationship fullTextLanguagesArray
org.apache.cayenne.access.DataDomainIndirectDiffBuilder.arcDeleted(DataDomainIndirectDiffBuilder.java:128)



After doing some debugging, it seems that problem is coming from the DataContextDeleteAction class, because it is setting the processFlattened as true. It only takes a look at whether it is a flattened relationship and if the relationship is to a dependent entity, but doesn't look at whether it is read only or not. This causes the objectStore.recordArcDeleted() to be called for the object related through this flattened read-only relationship.

In our case, this flattened relationship (fullTextLanguagesArray) has a relationship path that takes it through 4 relationships, so it's definitely a read-only flattened relationship. I can also say this for sure, because the DataDomainIndirectDiffBuilder.arcDeleted checks whether the relationship is flattened and read-only, and that's when the exception is thrown.

I would think that changing:

     boolean processFlattened = relationship.isFlattened() && relationship.isToDependentEntity();

to 

    boolean processFlattened = relationship.isFlattened() && relationship.isToDependentEntity() && !relationship.isReadOnly();

would fix the issue.

Please let me know if you need any details. I would be really interested to have this fixed in a patch release of 2.0 soon, since it is preventing us from using one part of our system.",smartshark_2_2,172,cayenne,"""[0.08374132961034775, 0.9162586331367493]"""
397,228656,HTTP connections aren't always closed in new ROP implementation,"The new ROP implementation appears to have a bug which results in HTTP connections aren't closing properly. 

The fix is to always close input stream object we get from URLConnection after we done reading and deserializing it.",smartshark_2_2,1539,cayenne,"""[0.09121447056531906, 0.9087855815887451]"""
398,228657,Modeler. Exception in save action after reverse engineering some complex DB schema,"No test schema available for this exception only stack trace was reported.

CayenneModeler Info
Version: 4.0.M4-SNAPSHOT
Build Date: Nov 14 2016 06:17:32
Exception: 
=================================
org.apache.cayenne.exp.ExpressionException: [v.4.0.M4-SNAPSHOT Nov 14 2016 06:17:32] Can't resolve path component: [test_item.testDbPath].
    at org.apache.cayenne.map.Entity$PathIterator.next(Entity.java:428)
    at org.apache.cayenne.map.Entity$PathIterator.next(Entity.java:375)
    at org.apache.cayenne.map.ObjAttribute.getDbAttribute(ObjAttribute.java:180)
    at org.apache.cayenne.project.validation.ObjAttributeValidator.validate(ObjAttributeValidator.java:131)
    at org.apache.cayenne.project.validation.DefaultProjectValidator$ValidationVisitor.visitObjAttribute(DefaultProjectValidator.java:199)
    at org.apache.cayenne.project.validation.DefaultProjectValidator$ValidationVisitor.visitObjEntity(DefaultProjectValidator.java:209)
    at org.apache.cayenne.project.validation.DefaultProjectValidator$ValidationVisitor.visitDataMap(DefaultProjectValidator.java:127)
    at org.apache.cayenne.project.validation.DefaultProjectValidator$ValidationVisitor.visitDataChannelDescriptor(DefaultProjectValidator.java:110)
    at org.apache.cayenne.project.validation.DefaultProjectValidator$ValidationVisitor.visitDataChannelDescriptor(DefaultProjectValidator.java:92)
    at org.apache.cayenne.configuration.DataChannelDescriptor.acceptVisitor(DataChannelDescriptor.java:111)
    at org.apache.cayenne.project.validation.DefaultProjectValidator.validate(DefaultProjectValidator.java:89)
    at org.apache.cayenne.modeler.action.SaveAsAction.performAction(SaveAsAction.java:164)
    at org.apache.cayenne.modeler.action.SaveAsAction.performAction(SaveAsAction.java:158)
    at org.apache.cayenne.modeler.util.CayenneAction.actionPerformed(CayenneAction.java:162)
    at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2022)
    at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2348)
    at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402)
    at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:259)
    at javax.swing.AbstractButton.doClick(AbstractButton.java:376)
    at com.apple.laf.ScreenMenuItem.actionPerformed(ScreenMenuItem.java:125)
    at java.awt.MenuItem.processActionEvent(MenuItem.java:669)
    at java.awt.MenuItem.processEvent(MenuItem.java:628)
    at java.awt.MenuComponent.dispatchEventImpl(MenuComponent.java:351)
    at java.awt.MenuComponent.dispatchEvent(MenuComponent.java:339)
    at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:761)
    at java.awt.EventQueue.access$500(EventQueue.java:97)
    at java.awt.EventQueue$3.run(EventQueue.java:709)
    at java.awt.EventQueue$3.run(EventQueue.java:703)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:76)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:86)
    at java.awt.EventQueue$4.run(EventQueue.java:731)
    at java.awt.EventQueue$4.run(EventQueue.java:729)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:76)
    at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
    at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
    at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
    at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
    at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
    at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)",smartshark_2_2,1596,cayenne,"""[0.12815043330192566, 0.8718495965003967]"""
399,228658,NPE on commit when the new object was deleted before commit,"Affects 3.0M3 and probably 2.0 as well.  The following code causes the exception below:
 Painting p1 = child.newObject(Painting.class);
        p1.setPaintingTitle(""P1"");
        a.addToPaintingArray(p1);

        Painting p2 = child.newObject(Painting.class);
        p2.setPaintingTitle(""P2"");
        a.addToPaintingArray(p2);

        a.removeFromPaintingArray(p2);

        // this causes an error on commit
        child.deleteObject(p2);

        child.commitChangesToParent();

This is because artist -> painting diff is preserved for the deleted painting and parent context fails to process it. The fix is rather trivial

Caused by: java.lang.NullPointerException: Attempt to add null target DataObject.
	at org.apache.cayenne.CayenneDataObject.addToManyTarget(CayenneDataObject.java:262)
	at
org.apache.cayenne.reflect.generic.DataObjectToManyProperty.addTarget(DataObjectToManyProperty.java:72)
	at org.apache.cayenne.access.ChildDiffLoader$1.visitToMany(ChildDiffLoader.java:131)
	at
org.apache.cayenne.reflect.generic.DataObjectToManyProperty.visit(DataObjectToManyProperty.java:112)
	at org.apache.cayenne.access.ChildDiffLoader.arcCreated(ChildDiffLoader.java:119)
	at org.apache.cayenne.access.ObjectDiff$ArcOperation.apply(ObjectDiff.java:445)
	at org.apache.cayenne.graph.CompoundDiff.apply(CompoundDiff.java:92)
	at org.apache.cayenne.access.ObjectStoreGraphDiff.apply(ObjectStoreGraphDiff.java:135)
	at org.apache.cayenne.access.DataContext.onContextFlush(DataContext.java:1135)
	at org.apache.cayenne.access.DataContext.onSync(DataContext.java:1117)
	at org.apache.cayenne.access.DataContext.flushToParent(DataContext.java:1175)
	... 29 more",smartshark_2_2,120,cayenne,"""[0.06329789757728577, 0.9367021322250366]"""
400,228659,Not-Escaping <> during serialization to *.map.xml,"Object entity -> attributes tab -> type column
E.g. when using custom types (extended types) you specify custom class name in the type column. Sometimes it needs to be parametrized type, e.g. Validator<Integer>. During save the modeler fails to escape <> and thus the serialized *.map.xml becomes invalid (and subsequently can't be read by modeler). When escaped as &lt; and &gt; , modeler loads the project successfully. 
",smartshark_2_2,1239,cayenne,"""[0.10731818526983261, 0.8926818370819092]"""
401,228660,Row index out of range exception when selecting a relationship ,"I don't think I can reproduce this anymore, but the original scenario was a rather common sequence of events:  add a new ObjRelationship and then click on it. So likely a race condition.

CayenneModeler Info
Version: cayenne.version
Build Date: cayenne.build.date
Exception: 
=================================
java.lang.IllegalArgumentException: Row index out of range
	at javax.swing.JTable.boundRow(JTable.java:1392)
	at javax.swing.JTable.addRowSelectionInterval(JTable.java:1442)
	at org.apache.cayenne.modeler.util.CayenneTable$CayenneListSelectionModel.setSelection(CayenneTable.java:178)
	at org.apache.cayenne.modeler.util.CayenneTable.select(CayenneTable.java:102)
	at org.apache.cayenne.modeler.editor.ObjEntityRelationshipTab.selectRelationships(ObjEntityRelationshipTab.java:232)
	at org.apache.cayenne.modeler.editor.ObjEntityTabbedView.currentObjRelationshipChanged(ObjEntityTabbedView.java:157)
	at org.apache.cayenne.modeler.ProjectController.fireObjRelationshipDisplayEvent(ProjectController.java:1482)
	at org.apache.cayenne.modeler.editor.ObjEntityRelationshipTab.processExistingSelection(ObjEntityRelationshipTab.java:272)
	at org.apache.cayenne.modeler.editor.ObjEntityRelationshipTab$2.valueChanged(ObjEntityRelationshipTab.java:206)
	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:187)
	at org.apache.cayenne.modeler.util.CayenneTable$CayenneListSelectionModel.fireValueChanged(CayenneTable.java:189)
	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:167)
	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:214)
	at javax.swing.DefaultListSelectionModel.changeSelection(DefaultListSelectionModel.java:408)
	at javax.swing.DefaultListSelectionModel.changeSelection(DefaultListSelectionModel.java:417)
	at javax.swing.DefaultListSelectionModel.setSelectionInterval(DefaultListSelectionModel.java:441)
	at org.apache.cayenne.modeler.util.CayenneTable.select(CayenneTable.java:81)
	at org.apache.cayenne.modeler.editor.ObjEntityRelationshipTab.objRelationshipAdded(ObjEntityRelationshipTab.java:333)
	at org.apache.cayenne.modeler.ProjectController.fireObjRelationshipEvent(ProjectController.java:1437)
	at org.apache.cayenne.modeler.action.CreateRelationshipAction.fireObjRelationshipEvent(CreateRelationshipAction.java:94)
	at org.apache.cayenne.modeler.action.CreateRelationshipAction.createObjRelationship(CreateRelationshipAction.java:86)
	at org.apache.cayenne.modeler.action.CreateRelationshipAction.performAction(CreateRelationshipAction.java:66)
	at org.apache.cayenne.modeler.util.CayenneAction.actionPerformed(CayenneAction.java:163)
	at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:1882)
	at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2202)
	at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:420)
	at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:258)
	at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:236)
	at java.awt.AWTEventMulticaster.mouseReleased(AWTEventMulticaster.java:231)
	at java.awt.Component.processMouseEvent(Component.java:5602)
	at javax.swing.JComponent.processMouseEvent(JComponent.java:3135)
	at java.awt.Component.processEvent(Component.java:5367)
	at java.awt.Container.processEvent(Container.java:2010)
	at java.awt.Component.dispatchEventImpl(Component.java:4068)
	at java.awt.Container.dispatchEventImpl(Container.java:2068)
	at java.awt.Component.dispatchEvent(Component.java:3903)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4256)
	at java.awt.LightweightDispatcher.processMouseEvent(Container.java:3936)
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:3866)
	at java.awt.Container.dispatchEventImpl(Container.java:2054)
	at java.awt.Window.dispatchEventImpl(Window.java:1801)
	at java.awt.Component.dispatchEvent(Component.java:3903)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:463)
	at java.awt.EventDispatchThread.pumpOneEventForHierarchy(EventDispatchThread.java:269)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:190)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:184)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:176)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:110)
",smartshark_2_2,316,cayenne,"""[0.0688953846693039, 0.9311046004295349]"""
402,228661,Bug in CayenneRuntimeException using wrong specified string in Formatter,"INFO: Exception processing message org.apache.cayenne.remote.QueryMessage of type Query
java.util.UnknownFormatConversionException: Conversion = '""'
	at java.util.Formatter.checkText(Formatter.java:2579)
	at java.util.Formatter.parse(Formatter.java:2565)
	at java.util.Formatter.format(Formatter.java:2501)
	at java.util.Formatter.format(Formatter.java:2455)
	at java.lang.String.format(String.java:2928)
        at org.apache.cayenne.access.ClientServerChannelQueryAction.interceptSinglePageQuery(ClientServerChannelQueryAction.java:99)
        at org.apache.cayenne.access.ClientServerChannelQueryAction.execute(ClientServerChannelQueryAction.java:64)
        at org.apache.cayenne.access.ClientServerChannel.onQuery(ClientServerChannel.java:51)
        at org.apache.cayenne.remote.service.DispatchHelper.dispatch(DispatchHelper.java:39)
        at org.apache.cayenne.remote.service.BaseRemoteService.processMessage(BaseRemoteService.java:135)
        at org.apache.cayenne.rop.ROPServlet.doPost(ROPServlet.java:122)

invalid format specifiers
String.format(messageFormat, messageArgs)
For example , Painting/name likeIgnoreCase ""gi%""/l2 can't be used as messageFormat
",smartshark_2_2,1647,cayenne,"""[0.09399552643299103, 0.9060045480728149]"""
403,228662,SelectQuery with prefetches and cache bring data losing,"SelectQuery with prefetches and cache bring data losing

Object model looks like:

Â 
{code:java}
COURSE OneToMany COURSE_CLASSES
COURSE_CLASSES ManyToOneÂ COLLEGE
COURSE_CLASSES OneToMany SESSIONS
COURSE_CLASSES ManyToOne ROOM
ROOM ManyToOne SITEÂ 
{code}
Â 

Select query looks like:
{code:java}
List<Course> courses = ObjectSelect.query(Course.class)
.where(Course.CODE.eq(code)) 
.prefetch(Course.COURSE_CLASSES.joint()) 
.prefetch(Course.COURSE_CLASSES.dot(CourseClass.COLLEGE).joint()) 
.prefetch(Course.COURSE_CLASSES.dot(CourseClass.SESSIONS).joint())
.prefetch(Course.COURSE_CLASSES.dot(CourseClass.ROOM).joint()) 
.prefetch(Course.COURSE_CLASSES.dot(CourseClass.ROOM).dot(Room.SITE).joint()) 
.cacheStrategy(LOCAL_CACHE) 
.cacheGroup(Course.class.getSimpleName()) 
.selectOne(context);

{code}
Â whereÂ 

Â 
{code:java}
context
{code}
Â 

is shared readonlyÂ  instance of cayenne context. It is also used for other queries inside our application.

Â 

When IÂ iterate through result listÂ 
{code:java}
courses.get(i).getCourseClasses(){code}
I see that list of related CourseClassesÂ isÂ trimmed (doesn't contains full set of records which expected, some records just lost).

I am not sure who is blame on it

Â - is it prefetch semantic (I have found similar but not exact the same bugÂ CAY-2257)

Â - some concurrent modifications of related objects list (since we load it in shared context)

Â - any cachingÂ  issues (we are using JCache/HECache implementation)

In additional I can say that we are not limited number of cache entries for any cache group, just use 10 min expiry timeout.

But the main difficulties that I can not reproduce such behaviour locally (reproducible on production environment only) so I can not write clear test for this issue.

Â 

Â 

Â ",smartshark_2_2,1887,cayenne,"""[0.08727823942899704, 0.9127216935157776]"""
404,228663,LOCAL_CACHE policy fails when query is paged,"The same query which works when cache policy is set to SHARED_CACHE or NO_CACHE fails to execute with LOCAL_CACHE, exception:     

[java] 11:57:32,230 [AWT-EventQueue-0] ERROR ish.oncourse.controller.ListController :335 - Error performing update query (qualifier: 'org.apache.cayenne.query.SelectQuery@6afa2[root=class ish.oncourse.cayenne.Room,name=<null>]')
     [java] java.lang.IllegalArgumentException: Returned page size (40) exceeds requested page size (25)
     [java] 	at org.apache.cayenne.remote.RemoteIncrementalFaultList.<init>(RemoteIncrementalFaultList.java:127)
     [java] 	at org.apache.cayenne.CayenneContextQueryAction.interceptPaginatedQuery(CayenneContextQueryAction.java:47)
     [java] 	at org.apache.cayenne.CayenneContextQueryAction$1.createObject(CayenneContextQueryAction.java:66)
     [java] 	at org.apache.cayenne.cache.MapQueryCache.get(MapQueryCache.java:74)
     [java] 	at org.apache.cayenne.util.ObjectContextQueryAction.interceptLocalCache(ObjectContextQueryAction.java:258)
     [java] 	at org.apache.cayenne.util.ObjectContextQueryAction.execute(ObjectContextQueryAction.java:82)
     [java] 	at org.apache.cayenne.CayenneContext.onQuery(CayenneContext.java:364)
     [java] 	at org.apache.cayenne.CayenneContext.performQuery(CayenneContext.java:352)
     [java] 	at ish.oncourse.cayenne.CayenneContext.performQuery(CayenneContext.java:227)

I can do some more debug/testing in later time, very busy now...

",smartshark_2_2,104,cayenne,"""[0.07947191596031189, 0.9205281138420105]"""
405,228664,Treating TIME jdbc-type as datetime db-type for MS SQL Server,"Relates to the failed test DataContextEJBQLDateTimeFunctionalExpressionsIT.testCURRENT_TIME:85

We treats TIME jdbc-type as datetime db-type for MS SQL Server:
{code:xml}
   <jdbc-type name=""TIME"">
       <db-type name=""datetime""/>
   </jdbc-type>
{code}

For testCURRENT_TIME we have the following insertion query:
{noformat}
INSERT INTO DATE_TEST (DATE_COLUMN, DATE_TEST_ID, TIMESTAMP_COLUMN, TIME_COLUMN) VALUES (?, ?, ?, ?)
[batch bind: 1->DATE_COLUMN:NULL, 2->DATE_TEST_ID:220, 3->TIMESTAMP_COLUMN:NULL, 4->TIME_COLUMN:'2016-02-01 23:59:59.753']
[batch bind: 1->DATE_COLUMN:NULL, 2->DATE_TEST_ID:221, 3->TIMESTAMP_COLUMN:NULL, 4->TIME_COLUMN:'2016-02-01 00:00:00.753']
{noformat}

But actually in the database we have:
{noformat}
TIME_COLUMN (datetime)
1970-01-01 00:00:00.383
1970-01-01 23:59:59.383
{noformat}

Then testCURRENT_TIME compare time column with current time and fails.
{noformat}
SELECT t0.DATE_COLUMN AS ec0_0, t0.TIME_COLUMN AS ec0_1, t0.TIMESTAMP_COLUMN AS ec0_2, t0.DATE_TEST_ID AS ec0_3 FROM DATE_TEST t0 WHERE t0.TIME_COLUMN < {fn CURTIME()}
{noformat}


Treating TIME jdbc-type as time db-type solves this problem, but it also breaks backward compatibility with 2005 version. MS SQL Server has supported time db-type since 2008 version.",smartshark_2_2,1638,cayenne,"""[0.4037628173828125, 0.5962371826171875]"""
406,228665,"Modeler misses new renamed attributes, relationship mappings","Something else is badly broken on the trunk compared to M4. 

Scenario 1:

1. Add a new DbAttribute to an existing DbEntity. 
2. Rename it from default ""untitledXXX"" to something else.
3. Go the ObjEntity of this DbEntity and try to add a new ObjAttribute

At this point the list of DbAttributes still lists ""untitledXXX"" , not the new name of the new DbAttribute. 

Scenario 2:

1. Add a new DbRelationship between 2 existing DbEntities
2. Open join mapping dialog and select source and target keys
3. Close join mapping dialog
4. Open it again

At this point join target is blank",smartshark_2_2,70,cayenne,"""[0.09824821352958679, 0.9017517566680908]"""
407,228666,Cayenne ROP server resets session on every request if BASIC auth is used,"Per http://stackoverflow.com/questions/12314857/apache-cayenne-rop-server-no-session-associated-with-request-on-tomcat-7 Tomcat 7 resets HTTP session on every ROP request resulting in a loss of state on the client. 

I reproduced that on Tomcat 7 and Jetty 8. Jetty 6 works correctly. 

Debugging on Jetty shows that if BASIC auth is present, container invalidates the existing session and creates a new one during auth credentials checking phase. So it goes like this:

1. Connect ... session1 is established
2. Bootstrap ... session1 cookie is accepted, but session is immediately invalidated and session2 is created
3. Commit ... Client still sends session1 cookie, while the server expects session2, causing an exception:

org.apache.cayenne.remote.service.MissingSessionException: [v.3.2M1-SNAPSHOT Sep 10 2012 23:14:19] No session associated with request.
	at org.apache.cayenne.remote.service.BaseRemoteService.processMessage(BaseRemoteService.java:127)

I wonder if the new servlet spec is specifying this behavior (?).

A possible fix is to read the session cookie on the client and reset session ID on every request. 

A hideous workaround for the users is to remove BASIC auth.
",smartshark_2_2,1035,cayenne,"""[0.10114814341068268, 0.8988518714904785]"""
408,228667,Modeler: Unable to set enum:value as Entity qualifier,"Modeler fail to parse Entity qualifier like this:
{{enumAttribute = enum:org.apache.cayenne.testdo.enum_test.Enum1.two}}

The reason is that parser throws {{ClassNotFoundException}} because Enum class is unknown to the Modeler.
This can be fixed by some sort of lazy resolving of enum value.

P.S. For the history: this bug surfaced from other end, e.g. in runtime. New code generated by {{cgen}} in *4.1.M1* throws ClassCastException when trying to provide field of Enum type with initial value base on qualifier expression where String scalar is used. ",smartshark_2_2,1863,cayenne,"""[0.07760932296514511, 0.9223906397819519]"""
409,228668,"EJBQL Query with relational queries (<, <=, >, >=) are throwing exceptions","When using a relational operator in an EJBQLQuery it is throwing a ParseException. Additionally shouldn't this return a data row instead a list of an Object[] array?  That is assuming that one can alias the selected attributes.  However, it appears that you cannot specify an alias.

Source:

	EJBQLQuery q = new EJBQLQuery(""SELECT at.isUsage, sum(at.amount), sum(at.hours) FROM AccrualTransaction at WHERE at.accrualCode = :accrualCode and at.employeeID = :employeeID and effectiveDate >= :payrollDate GROUP BY at.isUsage"");
	q.setParameter(""accrualCode"", bal.getCode());
	q.setParameter(""employeeID"", emp.getEmployeeID());
	q.setParameter(""effectiveDate"", balanceDate);
	List results = emp.getObjectContext().performQuery(q);  <<== throws exception here
	System.out.println(results);

Error follows:

org.apache.cayenne.ejbql.parser.ParseException: Encountered ""effectiveDate >="" at line 1, column 149.
Was expecting one of:
    ""("" ...
    ""NOT"" ...
    ""EXISTS"" ...
    ""+"" ...
    ""-"" ...
    <DECIMAL_LITERAL> ...
    <INTEGER_LITERAL> ...
    "":"" ...
    ""?"" ...
    <IDENTIFIER> ...
    <IDENTIFIER> ""."" ...
    ""LENGTH"" ...
    ""LOCATE"" ...
    ""ABS"" ...
    ""SQRT"" ...
    ""MOD"" ...
    ""SIZE"" ...
    ""AVG"" ...
    ""MAX"" ...
    ""MIN"" ...
    ""SUM"" ...
    ""COUNT"" ...
    ""SELECT"" ...
    <STRING_LITERAL> ...
    ""CONCAT"" ...
    ""SUBSTRING"" ...
    ""TRIM"" ...
    ""LOWER"" ...
    ""UPPER"" ...
    <BOOLEAN_LITERAL> ...
    ""CURRENT_DATE"" ...
    ""CURRENT_TIME"" ...
    ""CURRENT_TIMESTAMP"" ...
    ""NEW"" ...
    ""ALL"" ...
    ""ANY"" ...
    ""SOME"" ...
    ""EMPTY"" ...
    ""ASC"" ...
    ""DESC"" ...
    ""ORDER"" ...
    ""IS"" ...
    ""MEMBER"" ...
    ""OF"" ...
    ""LIKE"" ...
    ""ESCAPE"" ...
    ""BETWEEN"" ...
    ""NULL"" ...
    ""OR"" ...
    ""AND"" ...
    ""LEADING"" ...
    ""TRAILING"" ...
    ""BOTH"" ...
    ""DISTINCT"" ...
    ""FROM"" ...
    ""UPDATE"" ...
    ""DELETE"" ...
    ""WHERE"" ...
    ""GROUP"" ...
    ""BY"" ...
    ""HAVING"" ...
    ""AS"" ...
    ""LEFT"" ...
    ""OUTER"" ...
    ""INNER"" ...
    ""JOIN"" ...
    ""FETCH"" ...
    ""IN"" ...
    ""SET"" ...
    ""OBJECT"" ...
    <IDENTIFIER> ""="" ...
    <IDENTIFIER> ""<>"" ...
    <IDENTIFIER> ""NOT"" ...
    <IDENTIFIER> ""MEMBER"" ...
    ",smartshark_2_2,50,cayenne,"""[0.19914761185646057, 0.8008524179458618]"""
410,228669,Support native PK generation using sequences for H2 databases,Add support for native PK generation using sequences for H2 databases.,smartshark_2_2,1309,cayenne,"""[0.9982054233551025, 0.001794620300643146]"""
411,228670,EOModel import fails to determine type for prototyped columns,"For prototypes that map to a custom Java class (or enum) that is not known to the model importer, there is no way to determine the sql type from that alone. But usually the {{externalType}} is mentioned in the model plist of the prototype, so we can use that as a hint.

I'll submit a pull request doing this shortly.",smartshark_2_2,1548,cayenne,"""[0.13798294961452484, 0.8620170950889587]"""
412,228671,Add cwiki url to incubator status page,"Small item. Some chatting on #asfinfra and they requested you add the url to cwiki (http://cwiki.apache.org/CAY/) to your incubator status page ( 
http://incubator.apache.org/projects/cayenne.html) ",smartshark_2_2,2367,cayenne,"""[0.9983404874801636, 0.0016594690969213843]"""
413,228672,Deprecate org.apache.cayenne.xml,"Here is a thread on motivation behind deprecating XML servialization package:

http://markmail.org/message/uffbbzahbzskpvne

and one asking who's using it:

http://markmail.org/thread/y2jwyksa2oltvgdi

A quote: ""I suggest to deprecate XML serialization package in 3.1 and remove it in 3.2. There were past discussions why it is not a working solution for real life
applications (DOM, singletons, etc.). Also real life serialization/deserialization patterns are different from what we provide there (at least I couldn't adopt it to any of my own needs). And finally there's nobody around willing to address these deficiencies. ""

If somebody wants to revive XML serialization work as a part of Cayenne, it would be cool to build it on JAXB or XStream and address all current deficiencies.",smartshark_2_2,837,cayenne,"""[0.9984194040298462, 0.0015806283336132765]"""
414,228673,CayenneCase unit tests conversion(5),Continuing conversion to ServerCase unit tests.,smartshark_2_2,883,cayenne,"""[0.998272180557251, 0.0017277891747653484]"""
415,228674,Use standard JDBC API for writing Oracle LOBs,"Currently OracleAdapter uses reflection on the Oracle driver classes to access Writers and OutputStreams for populating CLOB/BLOB's. Aside from dependency on the vendor API, this may even be casuing bugs in some application servers: http://markmail.org/message/hclsphw5ptxfy6u2

So I am going to change that to standard JDBC methods (Clob.setCharacterStream(..) / Blob.setBinaryStream(..)), that work the same on Oracle 10 in my tests.",smartshark_2_2,623,cayenne,"""[0.9983062744140625, 0.0016936571337282658]"""
416,228675,cdbimport improvements,"So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


",smartshark_2_2,1064,cayenne,"""[0.9983159303665161, 0.0016841189935803413]"""
417,228676,Add nvarchar mapping for cayenne,"Cayenne modeler currently supports varchar in the drop down list and it maps to varchar for viarious DB types.
However, when storing unicode, it's probably better to use nvarchar DB type. It's good to support the nvarchar type too.

Current workaround is to manually do a global replace for the generated DDL (varchar to nvarchar).",smartshark_2_2,1248,cayenne,"""[0.9984930753707886, 0.0015069512883201241]"""
418,228677,Local cache doesn't work with paginated queries and DataContext,"Per CAY-570, paginated queries and caching should work on remote clients (although see CAY-643), however doing it in the DataContext results in cache settings being ignored (for the same reason as CAY-570) -

From DataContextTst - the last assertion fails:

    public void testPerformPaginatedQueryLocalCache() throws Exception {
        SelectQuery query = new SelectQuery(""Artist"");
        query.setPageSize(5);
        query.setCachePolicy(QueryMetadata.LOCAL_CACHE);
        
        List objects = context.performQuery(query);
        assertNotNull(objects);
        assertTrue(objects instanceof IncrementalFaultList);

        assertTrue(((IncrementalFaultList) objects).elements.get(0) instanceof Artist);
        assertTrue(((IncrementalFaultList) objects).elements.get(7) instanceof Map);
        assertTrue(objects.get(7) instanceof Artist);
        
        // do it from cache
        List objects1 = context.performQuery(query);
        assertNotNull(objects1);
        assertSame(objects, objects1);
    }",smartshark_2_2,250,cayenne,"""[0.14441289007663727, 0.8555871248245239]"""
419,228678,Allow setting of --userdir for CM Windows,"Please allow the setting of the --userdir parameter for CayenneModeler for windows to be able to store the preferences somewhere else than the default location.

The ""cayenne.userdir"" seems to work only when starting CayenneModeler from the JAR. The EXE is not using it :(.",smartshark_2_2,1221,cayenne,"""[0.9985670447349548, 0.0014329758705571294]"""
420,228679,More control at creating AUTO_PK_SUPPORT table,"When running DbGenerator with:

        generator.setShouldCreatePKSupport(true);

it would be GREAT if one could control if the DELETE and INSERT statements should be executed (maybe even automatically based on error at CREATE). The method could be:

        generator.setShouldCreatePKSupport(true, false);

where second false would suppress the execution of DELETE FROM AUTO_PK_SUPPORT and INSERT INTO AUTO_PK_SUPPORT if CREATE fails.

What do you think?


Sample output:

 INFO [14:49:11.712] CREATE TABLE AUTO_PK_SUPPORT (  TABLE_NAME CHAR(100) NOT NULL,  NEXT_ID BIGINT NOT NULL, UNIQUE (TABLE_NAME))
 INFO [14:49:11.712] *** error.
com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Table 'AUTO_PK_SUPPORT' already exists
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1026)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3491)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3423)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1936)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2060)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2536)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2465)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:734)
        at org.apache.cayenne.access.DbGenerator.safeExecute(DbGenerator.java:352)
        at org.apache.cayenne.access.DbGenerator.runGenerator(DbGenerator.java:330)
        at com.interseek.portal.server.Bootstrap.createTables(Bootstrap.java:155)
        at com.interseek.portal.server.Bootstrap.<init>(Bootstrap.java:58)
        at com.interseek.portal.server.Bootstrap.main(Bootstrap.java:71)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:271)
        at java.lang.Thread.run(Thread.java:595)
 INFO [14:49:11.712] DELETE FROM AUTO_PK_SUPPORT WHERE TABLE_NAME IN ('table1', 'table2', 'table3')
 INFO [14:49:11.728] INSERT INTO AUTO_PK_SUPPORT (TABLE_NAME, NEXT_ID) VALUES ('table1', 200)
 INFO [14:49:11.728] INSERT INTO AUTO_PK_SUPPORT (TABLE_NAME, NEXT_ID) VALUES ('table2', 200)
 INFO [14:49:11.728] INSERT INTO AUTO_PK_SUPPORT (TABLE_NAME, NEXT_ID) VALUES ('table3', 200)


",smartshark_2_2,422,cayenne,"""[0.9985253214836121, 0.0014746941160410643]"""
421,228680,"When adding a filter, auto-register it as a listener","Most filters declare listener methods in them. It should do no harm and save us some code if we automatically process filter listener annotations, thus implicitly registering filter as a listener.",smartshark_2_2,1199,cayenne,"""[0.9983195662498474, 0.0016803559847176075]"""
422,228681,Tooltips for all toolbar buttons in Modeler,"Tooltips may be annoying for the seasoned user of any application, but are helpful to newcomers.  And, since in a number of places Cayenne Modeler uses tooltips, it would seem to be of some value to implement tooltips for all of the toolbar buttons, and in general, any CayenneAction (or subclass thereof).

To effect this improvement, the following small changes need to be made to org.objectstyle.cayenne.modeler.util.CayenneAction:

1. Constructor enhancements:instead of offering just the single two argument constructor, do the following: 

  /**
   * Creates a named CayenneAction.
   */
  public CayenneAction(String name, Application application) {
    this(name, application, name);
  }

  /**
   * Creates a named CayenneAction.
   */
  public CayenneAction(String name, Application application, String longDescription) {
    super(name);
    super.putValue(Action.DEFAULT, name);

    this.application = application;

    Icon icon = createIcon();
    if (icon != null) {
      super.putValue(Action.SMALL_ICON, icon);
    }

    KeyStroke accelerator = getAcceleratorKey();
    if (accelerator != null) {
      super.putValue(Action.ACCELERATOR_KEY, accelerator);
    }
    
    if ( longDescription != null && longDescription.length() > 0 ) { 
      super.putValue(Action.LONG_DESCRIPTION, longDescription);
    }

    setEnabled(false);
  }

That is, move the actual construction to a new three parameter
constructor, adding the longDescription parameter as show.  The 
Action.LONG_DESCRIPTION value is used later on to assign text to the button's tooltip text.

2. Assign tooltip text: in the constructor for the nested class ""CayenneAction.CayenneToolbarButton"", enhance the constructor to 
read as follows:

   /**
    * Constructor for CayenneMenuItem.
    */
   public CayenneToolbarButton(Action a) {
     super();
      setAction(a);            
      setToolTipText((String)a.getValue(Action.LONG_DESCRIPTION));
   }

",smartshark_2_2,2246,cayenne,"""[0.998396098613739, 0.0016038432950153947]"""
423,228682,Add support for start index/offset of queries,"Cayenne already allows us to programatically set a query fetch limit.
It would be nice if we could also specify a fetch index/fetch offset into a query, to make it simple to fetch, eg, the 100th through the 150th results from the database.

",smartshark_2_2,616,cayenne,"""[0.9984136819839478, 0.0015862610889598727]"""
424,228683,Streamlining LOCAL_CACHE handling,"Per CAY-1013 I created a  fix for a LOCAL_CACHE refreshing issue. Now trying to look at the bigger picture and address a related problem:

LOCAL_CACHE and multiple levels of nesting.

When we have a DataContext connected to the DataDomain, LOCAL_CACHE is what's cached at the DC level, shared - at the DD. If we have an ROP situation (CayenneContext -> DataContext -> DataDomain) or a nested DC situation (DataContext -> [DataContext]+ -> DataDomain), LOCAL_CACHE setting causes caching to occur all the way down to the last DC in a chain... This behavior is clearly wrong. Only the starting DC should cache query results.",smartshark_2_2,633,cayenne,"""[0.987863302230835, 0.012136773206293583]"""
425,228684,cayenne-crypto: add optional compression to the encryption pipeline,"This is about transparently compressing encrypted data. Compression can be optionally enabled via CryptoModuleBuilder. By default we'll use GZIP compression, but as with everything in cayenne-crypto, this is a pluggable strategy.  

If compression is enabled, we should distinguish between small and large values. For small values compression only adds overhead and should be skipped. Per  [1] Google recommends not compressing files smaller than 150 - 1000 bytes (this is of course for the web content). We'll use an internal threshold somewhere in this range, and then we should just do some performance tests.

Compression will happen *before* encryption, as it is likely to be more efficient on plaintext than randomized encrypted data [2]. 

We'll need to add compression flag to the ""header"" block of the ciphertext. To do that we'll reorganize the header to allow storing more data, and expand dynamically as needed:

* byte 0..2: ""magic"" number identifying the format as Cayenne-crypto encrypted sequence.
* byte 3: header length N, i.e. how many bytes the header contains, including magic number and the length indicator. N can be 0..127.
* byte 4: a bit String representing various flags, such as compression.
* byte 5..N: UTF8-encoded symbolic name of the encryption key.

[1] https://developers.google.com/speed/docs/best-practices/payload#GzipCompression
[2] http://superuser.com/questions/257782/compress-and-then-encrypt-or-vice-versa",smartshark_2_2,1500,cayenne,"""[0.9984349608421326, 0.0015650830464437604]"""
426,228685,Some changes to LogDialog,"Delete the buttons (""Clear"", ""Copy"", ""Dock"") from the panel. Create new Button "">"", it show menu, and  JPopupMenu in the EditorPane. (The menu contains next Items: ""Clear"", ""Copy"", ""Dock"".)",smartshark_2_2,1430,cayenne,"""[0.9806894659996033, 0.019310465082526207]"""
427,228686,Deprecate derived dbentity,"As per recent conversation, derived db entities need to be marked as deprecated and removed from Cayenne modeler. I've already marked references in the documentation as deprecated. It is unknown what will replace them, but better to warn people not to use the current implementation.",smartshark_2_2,19,cayenne,"""[0.9984110593795776, 0.0015889034839347005]"""
428,228687,CM Usability: JComboBox Autocompletion,"Please use JComBox with autocompletion in CM at least in places that are very repetitive like selecting Field Types.
It is very simple to implement it and there are several extremly well documented examples how to do it. E.g.:
http://www.orbital-computer.de/JComboBox/
",smartshark_2_2,1148,cayenne,"""[0.9984229803085327, 0.0015770114259794354]"""
429,228688,Support for the scalar and aggregate SQL functions in ObjectSelect API,"For now only options to use SQL functions (including aggregate) are:
 # EJBQL
 # plain SQL

Both of it lack any type-checking or support from the API side as user needs to wright and debug queries with plain text.

So Cayenne needs API for that functionality in order to have more control.
It is suggested to add following features and new API in ObjectSelect query:
# New expressions for function calls and new factory as convenient method to create them: {code} 
        Expression substrExp = FunctionExpressionFactory.substringExp(Artist.ARTIST_NAME.path(), 10, 15);     
        Expression modExp = FunctionExpressionFactory.modExp(Artist.ARTIST_SALARY.path(), 10);
{code}
# Enhanced Property class with Expression support and explicit type,
they can be declared in persistent object like normal Cayenne properties or adhoc for specific query: {code}
        Property<String> namePart = Property.create(""namePart"", substrExp, String.class);
        Property<Long> artistCount = Property.create(""artistCount"", countExp, Long.class);
        Property<Integer> minSalary = Property.create(""minSalary"", minExp, Integer.class);
{code}
# New methods in ObjectSelect for specifying return columns for the query: {code}           
        // Single column
        long totalCount = ObjectSelect.query(Artist.class)
                            .column(Artist.ARTIST_COUNT)
                            .selectOne(context);
                            
        // Several columns
        List<Object[]> result = ObjectSelect.query(Artist.class)
                                    .columns(artistCount, minSalary, namePart)
                                    .select(context);
        for(Object[] r : result) {
            long count = (long)r[0];
            int min = (int)r[1];
            String namePart = (String)r[2];
        }
{code}
# New having() method in ObjectSelect: {code}
        // Full query example:
        List<Object[]> result = ObjectSelect.query(Artist.class)
                // result
                .columns(artistCount, minSalary, namePart)
                // WHERE clause
                .where(Artist.DATE_OF_BIRTH.lt(new Date())) 
                // additional condition in WHERE clause
                .or(...)                      
                // HAVING clause
                .having(namePart.like(""P%""))                
                // additional condition in HAVING clause
                .or(...)                                    
                .select(context);

        for(Object[] r : result) {
            long count = (long)r[0];
            int min = (int)r[1];
            String name = (String)r[2];
        }

        // Aggregate on toMany relationship:
        Property<Long> paintingCountProperty = Artist.PAINTING_ARRAY.count();
        List<Object[]> result2 = ObjectSelect.query(Artist.class)
                .columns(Artist.ARTIST_NAME, paintingCountProperty)
                .having(paintingCountProperty.gt(10L))
                .select(context);
        for(Object[] r : result2) {
            String artistName = (String)r[0];
            long paintingCount = (long)r[1];
        }
{code}",smartshark_2_2,1625,cayenne,"""[0.9985392093658447, 0.0014607870252802968]"""
430,228689,Sometimes PropertyUtilsTest failed,"Sometimes PropertyUtilsTest failed. Something like 1 per 10 runs.
{code}
org.apache.cayenne.CayenneRuntimeException: [v.3.2M2-SNAPSHOT Oct 09 2013 09:32:17] Error setting property segment 'dateField' in path 'dateField' to value '0' for object 'org.apache.cayenne.reflect.TestJavaBean@32bd8b9c'
	at org.apache.cayenne.reflect.PropertyUtils.setProperty(PropertyUtils.java:169)
	at org.apache.cayenne.reflect.PropertyUtilsTest.testSetConverted(PropertyUtilsTest.java:196)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at com.intellij.junit3.JUnit3IdeaTestRunner.doRun(JUnit3IdeaTestRunner.java:139)
	at com.intellij.junit3.JUnit3IdeaTestRunner.startRunnerWithArgs(JUnit3IdeaTestRunner.java:52)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: org.apache.cayenne.CayenneRuntimeException: [v.3.2M2-SNAPSHOT Oct 09 2013 09:32:17] Unable to convert '0' to a Date
	at org.apache.cayenne.reflect.PropertyUtilsTest$1.convert(PropertyUtilsTest.java:224)
	at org.apache.cayenne.reflect.PropertyUtilsTest$1.convert(PropertyUtilsTest.java:209)
	at org.apache.cayenne.reflect.PropertyUtils.setSimpleProperty(PropertyUtils.java:234)
	at org.apache.cayenne.reflect.PropertyUtils.setProperty(PropertyUtils.java:166)
	... 24 more
{code}",smartshark_2_2,1297,cayenne,"""[0.9490758180618286, 0.05092413350939751]"""
431,228690,Cache velocity SQL parsing results to boost it's performance,"We need to think about caching velocity parsing results.
There is corresponding pull request (https://github.com/apache/cayenne/pull/174) but it can't be applied as is.",smartshark_2_2,1866,cayenne,"""[0.9983803033828735, 0.0016197275836020708]"""
432,228691,Replace Query objects in DataMap with query descriptors,This will help to build cleaner named query API and untie query metadata from the actual query object instance.,smartshark_2_2,1537,cayenne,"""[0.9982684850692749, 0.00173143798019737]"""
433,228692,Upload build jar to http://www.ibiblio.org/,"Could the cayenne.jar and cayenne-nodeps.jar files be uploaded to the Ibibilo respository so that it is available for automated build systems.

http://maven.apache.org/reference/repository-upload.html

regards Malcolm Edgar",smartshark_2_2,2195,cayenne,"""[0.9979643821716309, 0.00203558593057096]"""
434,228693,Multiple Domains Having DataMap with the same name,"DataDomain intention is to provide a namespace for entities.

Still there is one issue where namespace abstraction breaks - if two domains in the same project have DataMaps (or JDBC DataNodes for that matter) that coincidentally have the same name, there is a conflict in saving these maps, as such maps are stored in the same directory under the same filename. 

A solution for the future release is to prepend DataDomain name to the map name (e.g. MyDomain.MyMap.map.xml).

Current solution maybe to generate the default map name not just as ""UntitledDataMap"", but ""MyDomain.UntitledDataMap"", and then also do the validation on save for any name conflicts. At least this will make users aware of the problem.",smartshark_2_2,2053,cayenne,"""[0.16648398339748383, 0.8335160613059998]"""
435,228694,"Remove "":sync w/DbEntity"" button from ObjEntity","Remove "":sync w/DbEntity"" button from ObjEntity - it is redundant and already present on the entity toolbar.",smartshark_2_2,1272,cayenne,"""[0.998422384262085, 0.001577659510076046]"""
436,228695,Allow to exclude DataMap java class from Modeler class generation,"When generating classes from CayenneModeler, it includes a class pair per entity, and an extra pair for each DataMap. DataMap class that was intended to keep query static methods is not very useful as of yet (we'll need to do more work to make it useful), but is generated never the less.

Let's add a checkbox to the ""standard"" and ""client"" modeler class generation modes to include or exclude DataMap class generation. Exclude should be the default.",smartshark_2_2,1436,cayenne,"""[0.9985266923904419, 0.0014732314739376307]"""
437,228696,javax.persistence.spi.ClassTransformer to turn POJO into DataObject,"While switching the entire Cayenne stack to use new ClassDescriptor interfaces, and thereby going straight to the POJO ""heaven"", will take us some time, we should be able to turn POJO's to DataObjects on the fly, JDO-style using standard JPA interface: javax.persistence.spi.ClassTransformer

Need to implement DataObject ClassTransformer.",smartshark_2_2,446,cayenne,"""[0.9984338879585266, 0.0015661581419408321]"""
438,228697,JPA Spec Compatibility Tests,"Create a set of integration tests that follow the JPA spec features in a provider agnostic manner. I started this work here:

http://svn.apache.org/repos/asf/incubator/cayenne/sandbox/itest-unit1/

Once we resolve the Maven issues with -javaagent (http://objectstyle.org/cayenne/lists/cayenne-devel/2006/08/0059.html) we can find a permanent location for the test suite.

This suite will be overlapping with the offical TCK, but since access to the TCK is private, we still need it (related prior issue is CAY-458).",smartshark_2_2,424,cayenne,"""[0.9985203146934509, 0.001479727914556861]"""
439,228698,Change CayenneModeler new object naming strategy,"Default modeler object naming (UntitledDomain, UntiltledDomainMap) trickles to the XML file naming, resulting in mixed case names that look rather ugly. A suggested new naming strategy is the following:
* ""project"" for domains (there can be only 1 per project in 3.1), 
* ""datamap"", ""datamap1"", etc. for DataMaps.
* ""datanode"", ""datanode1"", ""datanode2"", etc. for DataNodes
* ""dbentity"", ""dbentity1"", etc. for DbEntities
* ""objentity"", ""objentity1"", etc. for ObjEntities
* Same for procedures, queries and embeddables.

These will result in cleaner file names like cayenne-project.xml, datamap.map.xml etc.

Also IIRC the same strategy is used in places outside the Modeler (cdbimport?) so we'll need to check what gets affected by this change, and whether we can place the strategy in a corresponding DI module (not the main ServerModule)??",smartshark_2_2,782,cayenne,"""[0.9984608888626099, 0.001539102173410356]"""
440,228699,Split DataChannel filter into two independent filters,"Split {{DataChannelFilter}}Â filter into two independent filters - {{DataChannelQueryFilter}} and {{DataChannelSyncFilter}}

This will likely introduce minor incompatibility:Â contribution of custom filters that are linked with {{TransactionFIlter}} will likely need change. Independent filters should be fully functional (except for deprecation warning).",smartshark_2_2,1931,cayenne,"""[0.9985639452934265, 0.0014361030189320445]"""
441,228700,Deprecate EntityResolver.indexedByClassProperty,This is a part of EntityResolver refactoring.  'indexedByClassProperty' property is meaningless these days and should be deprecated and should no longer be checked for by the code.,smartshark_2_2,1215,cayenne,"""[0.9983953833580017, 0.0016046568052843213]"""
442,228701,"Convert EventManager to an interface, make it injectable","To simplify mocking up the event manager, will convert the EM class to an interface/implementation pair. Will create a DI provider so that EM could be injected. ",smartshark_2_2,576,cayenne,"""[0.9974015951156616, 0.002598383231088519]"""
443,228702,Modeler must show progress bar for long running tasks,"CayenneModeler experience can be somewhat confusing - long running tasks can go on background without any visual indication. There are two main examples:

1. database reengineering

2. opening a project

3. class generation

etc...

Case (2) also has a pretty evil behavior when the project is specified in the command line. In this case UI is not locked, so the user can do other things (e.g. open another project manually), thus messing up things. ",smartshark_2_2,2057,cayenne,"""[0.9736694097518921, 0.026330536231398582]"""
444,228703,PATCH: Ingres limit query support,"Adds support for Ingres's FIRST keyword to limit select query results.  Code is based on existing implementation for Microsoft SQLServer.  The behavior is similar although the name of the keyword is FIRST rather than TOP.  Note that unlike SQL Server, DISTINCT must come after the limit.  

I also updated the URL to Ingres as it's no longer a CA product.  

Index: src/main/java/org/apache/cayenne/dba/ingres/IngresSelectTranslator.java
===================================================================
--- src/main/java/org/apache/cayenne/dba/ingres/IngresSelectTranslator.java	(revision 0)
+++ src/main/java/org/apache/cayenne/dba/ingres/IngresSelectTranslator.java	(working copy)
@@ -0,0 +1,46 @@
+/*****************************************************************
+ *   Licensed to the Apache Software Foundation (ASF) under one
+ *  or more contributor license agreements.  See the NOTICE file
+ *  distributed with this work for additional information
+ *  regarding copyright ownership.  The ASF licenses this file
+ *  to you under the Apache License, Version 2.0 (the
+ *  ""License""); you may not use this file except in compliance
+ *  with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing,
+ *  software distributed under the License is distributed on an
+ *  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ *  KIND, either express or implied.  See the License for the
+ *  specific language governing permissions and limitations
+ *  under the License.
+ ****************************************************************/
+package org.apache.cayenne.dba.ingres;
+
+import org.apache.cayenne.access.trans.SelectTranslator;
+import org.apache.cayenne.query.QueryMetadata;
+
+public class IngresSelectTranslator extends SelectTranslator {
+	
+    @Override
+    protected void appendLimitAndOffsetClauses(StringBuilder buffer) {
+        QueryMetadata metadata = getQuery().getMetaData(getEntityResolver());
+        
+        int limit = metadata.getFetchLimit();
+        int offset = metadata.getFetchOffset();
+        
+        if (limit > 0) {
+        	String sql = buffer.toString();
+        	
+        	// If contains distinct insert first limit before
+        	if (sql.startsWith(""SELECT DISTINCT "")) {
+        		buffer.replace(0, 15, ""SELECT FIRST "" + (offset + limit) + "" DISTINCT "");	
+        		
+        	} else {
+        		buffer.replace(0, 6, ""SELECT FIRST "" + (offset + limit));	
+        	}
+        }
+    }
+
+}
\ No newline at end of file
Index: src/main/java/org/apache/cayenne/dba/ingres/IngresSelectAction.java
===================================================================
--- src/main/java/org/apache/cayenne/dba/ingres/IngresSelectAction.java	(revision 0)
+++ src/main/java/org/apache/cayenne/dba/ingres/IngresSelectAction.java	(working copy)
@@ -0,0 +1,47 @@
+/*****************************************************************
+ *   Licensed to the Apache Software Foundation (ASF) under one
+ *  or more contributor license agreements.  See the NOTICE file
+ *  distributed with this work for additional information
+ *  regarding copyright ownership.  The ASF licenses this file
+ *  to you under the Apache License, Version 2.0 (the
+ *  ""License""); you may not use this file except in compliance
+ *  with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing,
+ *  software distributed under the License is distributed on an
+ *  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ *  KIND, either express or implied.  See the License for the
+ *  specific language governing permissions and limitations
+ *  under the License.
+ ****************************************************************/
+package org.apache.cayenne.dba.ingres;
+
+import java.sql.Connection;
+
+import org.apache.cayenne.access.jdbc.SelectAction;
+import org.apache.cayenne.access.trans.SelectTranslator;
+import org.apache.cayenne.dba.JdbcAdapter;
+import org.apache.cayenne.map.EntityResolver;
+import org.apache.cayenne.query.SelectQuery;
+
+public class IngresSelectAction extends SelectAction {
+
+    public IngresSelectAction(SelectQuery query, JdbcAdapter adapter,
+            EntityResolver entityResolver) {
+        super(query, adapter, entityResolver);
+    }
+
+    @Override
+    protected SelectTranslator createTranslator(Connection connection) {
+        SelectTranslator translator = new IngresSelectTranslator();
+        translator.setQuery(query);
+        translator.setAdapter(adapter);
+        translator.setEntityResolver(getEntityResolver());
+        translator.setConnection(connection);
+        translator.setJdbcEventLogger(adapter.getJdbcEventLogger());
+        return translator;
+    }
+
+}
Index: src/main/java/org/apache/cayenne/dba/ingres/IngresAdapter.java
===================================================================
--- src/main/java/org/apache/cayenne/dba/ingres/IngresAdapter.java	(revision 1240211)
+++ src/main/java/org/apache/cayenne/dba/ingres/IngresAdapter.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.cayenne.access.types.ExtendedType;
 import org.apache.cayenne.access.types.ExtendedTypeFactory;
 import org.apache.cayenne.access.types.ExtendedTypeMap;
+import org.apache.cayenne.access.DataNode;
 import org.apache.cayenne.configuration.RuntimeProperties;
 import org.apache.cayenne.dba.JdbcAdapter;
 import org.apache.cayenne.dba.PkGenerator;
@@ -37,9 +38,11 @@
 import org.apache.cayenne.di.Inject;
 import org.apache.cayenne.map.DbAttribute;
 import org.apache.cayenne.map.DbEntity;
+import org.apache.cayenne.query.Query;
+import org.apache.cayenne.query.SQLAction;
 
 /**
- * DbAdapter implementation for <a href=""http://opensource.ca.com/projects/ingres/"">Ingres</a>.
+ * DbAdapter implementation for <a href=""http://www.actian.com/products/ingres"">Ingres</a>.
  * Sample connection settings to use with Ingres are shown below:
  * 
  * <pre>
@@ -59,6 +62,17 @@
             @Inject(EXTENDED_TYPE_FACTORY_LIST) List<ExtendedTypeFactory> extendedTypeFactories) {
         super(runtimeProperties, defaultExtendedTypes, userExtendedTypes, extendedTypeFactories);
     }
+
+    /**
+     * Uses IngresActionBuilder to create the right action.
+     * 
+     * @since 1.2
+     */
+    @Override
+    public SQLAction getAction(Query query, DataNode node) {
+        return query
+                .createSQLAction(new IngresActionBuilder(this, node.getEntityResolver()));
+    }
     
     @Override
     public QualifierTranslator getQualifierTranslator(QueryAssembler queryAssembler) {
Index: src/main/java/org/apache/cayenne/dba/ingres/IngresActionBuilder.java
===================================================================
--- src/main/java/org/apache/cayenne/dba/ingres/IngresActionBuilder.java	(revision 0)
+++ src/main/java/org/apache/cayenne/dba/ingres/IngresActionBuilder.java	(working copy)
@@ -0,0 +1,45 @@
+/*****************************************************************
+ *   Licensed to the Apache Software Foundation (ASF) under one
+ *  or more contributor license agreements.  See the NOTICE file
+ *  distributed with this work for additional information
+ *  regarding copyright ownership.  The ASF licenses this file
+ *  to you under the Apache License, Version 2.0 (the
+ *  ""License""); you may not use this file except in compliance
+ *  with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing,
+ *  software distributed under the License is distributed on an
+ *  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ *  KIND, either express or implied.  See the License for the
+ *  specific language governing permissions and limitations
+ *  under the License.
+ ****************************************************************/
+
+package org.apache.cayenne.dba.ingres;
+
+import org.apache.cayenne.access.jdbc.BatchAction;
+import org.apache.cayenne.dba.JdbcActionBuilder;
+import org.apache.cayenne.dba.JdbcAdapter;
+import org.apache.cayenne.map.EntityResolver;
+import org.apache.cayenne.query.BatchQuery;
+import org.apache.cayenne.query.ProcedureQuery;
+import org.apache.cayenne.query.SelectQuery;
+import org.apache.cayenne.query.SQLAction;
+
+
+/**
+ * @since 1.2
+ */
+public class IngresActionBuilder extends JdbcActionBuilder {
+
+    public IngresActionBuilder(JdbcAdapter adapter, EntityResolver resolver) {
+        super(adapter, resolver);
+    }
+    
+    @Override
+    public SQLAction objectSelectAction(SelectQuery query) {
+        return new IngresSelectAction(query, adapter, entityResolver);
+    }    
+}
",smartshark_2_2,899,cayenne,"""[0.9961544871330261, 0.003845551051199436]"""
445,228704,maven2 plugin to reverse engineer database,"Besides

   1. maven2-cgen
   2. maven2-cdbgen
   3. maven2-cdeploy
   4. maven2-cdataport

it would be greate if we had something like
    5. maven2-creverse

This plugin would reverse engineer the given database and create a mapping file. I guess all the classes exist in the modeler, so it would not be too hard to create a mojo.

What do you think?",smartshark_2_2,1151,cayenne,"""[0.9983291029930115, 0.0016709580086171627]"""
446,228705,EJBQL: Implement support for relationship-ending paths in GROUP BY clause,"Test case to cause the error:

        ObjectContext context = createDataContext();
        
        Expression exp = ExpressionFactory.matchExp(Painting.TO_GALLERY_PROPERTY, null);
        EJBQLQuery query = new EJBQLQuery(""select count(p), p.toArtist from Painting p where "" + exp.toEJBQL(""p"") + "" group by p.toArtist"");
    
        context.performQuery(query);

From the test I get this message:
java.sql.SQLException: Not in aggregate function or group by clause: org.hsqldb.Expression@83b18f in statement [SELECT COUNT(*) AS sc0,  t1.ARTIST_ID AS ec1_2,  t1.ARTIST_NAME AS ec1_1,  t1.DATE_OF_BIRTH AS ec1_0 FROM PAINTING t0 LEFT OUTER JOIN ARTIST t1 ON (t0.ARTIST_ID = t1.ARTIST_ID) GROUP BY t0.ARTIST_ID]",smartshark_2_2,535,cayenne,"""[0.6856269240379333, 0.31437304615974426]"""
447,228706,Improve support for meaningful primary keys,"I would like the ability (as a preference, particularly when reverse engineering?) to treat string ids as meaningful attributes. I know from my EOF days that meaningful ids are to be avoided, but sometimes they just make life easier (or you have no choice :-). All the numeric ids I deal with are meaningless, but the string ids are meaningful. 

For example, our database has a number of key ""type"" lookup tables - having a string as the PK saves lots of lookups on the DB for reports, summary list displays etc. The type table in turn can contain data that aids certain business processing logic based around the type (at which point the lookup is not a big hit in the scheme of things). It also enforces simple referential integrity.

A possible extention would be to have a button that makes an object's primary key ""visible""/""invisible"", with the option to ask that the attributes in other objects that are fk links to this object become ""visible""/""invisible"" as well.
",smartshark_2_2,1058,cayenne,"""[0.9983769655227661, 0.0016229684697464108]"""
448,228707,Generate explicit error message instead of silently ignoring lack of support for Generated columns,"There are lots of things that can go wrong when trying to get the generated columns support to work. Good error messages would be effective in putting the developer on the right track.

Examples of mistakes:

- Incorrectly select org.objectstyle.cayenne.dba.JdbcAdapter 
  instead of org.objectstyle.cayenne.dba.sqlserver.SQLServerAdapter 

- Accidentlly leaving the old JDBC drivers around. Often the
  developer environment is different than the production environment,
  so silent handling and subsequent cryptic error messages result.

Basically there is a need to distinguish between two cases:

- The developer MUST have generated column support(e.g. legacy
  databases). Here error messages are approperiate.
- The developer wants generated column support *if it is available*.

How to fix? 

Unpondered suggestion #1: Perhaps switch generated columns flag in the XML file to be a tristate choice? 

- Do not use generated column support
- Always use generated column support and generate error message if
  it is not supported by either the JDBC driver, adapter or 
  database
- Use generated column support if available in database AND
  

Unpondered suggestion #2: add flags to Cayenne XML model to handle various error situations in different ways. ""Silently fall back to Cayenne primary key support when generated columns are not supported by database/JDBC driver/adapter""",smartshark_2_2,2221,cayenne,"""[0.9985454082489014, 0.0014546425081789494]"""
449,228708,Save additional metadata as separate files,"New extension mechanics can be pushed a little bit further if it can support transparent loading of data from several files.
One possible way to do so is to use XInlcude feature that is supported by JDK's SAX parser Cayenne use.",smartshark_2_2,1805,cayenne,"""[0.9981831908226013, 0.0018167555099353194]"""
450,228709,Change CREATE statement for HSQLDB when Generating SQL,"When generating SQL for HSQLDB in the Modeler, table creation is specified as:

CREATE TABLE ...

when it should most likely be:

CREATE CACHED TABLE ...

In HSQLDB, the default version is a ""memory"" table, which means all data for the table is in memory (even if it is writing through to the disk as INSERT statements).  The ""cached"" table only keeps a portion of the table/indexes in memory, which allows for much larger tables (yes, the terminology is confusing since it seems that the cached table would be kept in memory).

To quote the documentation:

Memory tables are the default type when the CREATE TABLE command is used.  Their data is held entirely in memory but any change to their structure or contents is written to the <dbname>.script file.  The script file is read the next time the database is opened, and the MEMORY tables are recreated with all their contents. So unlike TEMP table, the default, MEMORY tables are persistent.

CACHED tables are created with the CREATE CACHED TABLE command.  Only part of their data or indexes is held in memory, allowing large tables that would otherwise take up to several hundred megabytes of memory. Another advantage of cached tables is that the database engine takes less time to start up when a cached table is used for large amounts of data.  The disadvantage of cached tables is a reduction in speed.  Do not use cached tables if your data set is relatively small.  In an application with some small tables and some large ones,  it is better to use the default, MEMORY mode for the small tables.
",smartshark_2_2,2093,cayenne,"""[0.9888284802436829, 0.011171502992510796]"""
451,228710,SelectDescriptor concept,"The goal is to create an abstract SelectDescriptor interface that would define what columns a select query contains and how to read them from result set. SelectDescriptor will support stacks of columns (needed in UNION queries with horizontal inheritance), it will also support complex Object[] results, mixing scalars and entities. This is a major refactoring effort that would tie together SELECT clause creating and coding of the DataRow keys (and hopefully will pave a way to future DataRow optimization/removal process). ",smartshark_2_2,360,cayenne,"""[0.998362123966217, 0.0016379094449803233]"""
452,228711,CayenneContext Serializability,"CayenneContext implements Serializable, but it is not in reality. I have a patch for this problem, but will defer it until 3.0.",smartshark_2_2,2350,cayenne,"""[0.9575400352478027, 0.04246002063155174]"""
453,228712,CayenneCase unit tests conversion(3),Continuing conversion to ServerCase unit tests.,smartshark_2_2,871,cayenne,"""[0.9982439279556274, 0.001756028039380908]"""
454,228713,Stripping common name prefixes on reverse engineering,"Often table namespacing in a database is done by prepending a common prefix to table names. Would be nice to be able to clean up the names before we make java names for them. E.g. transform ""myt_table1"" into ""Table1"", ""XyzTable1"" also into ""Table1"". Looks like what we need is a sed-like expression. E.g. in maven config form:

{noformat}
<tableNameFilter>s/^myt_//i</tableNameFilter>
{noformat}

The problem is that Java Pattern can't directly parse such expressions. May need to use  a third-party library like https://github.com/tools4j/unix4j or similar.",smartshark_2_2,1580,cayenne,"""[0.9983854293823242, 0.0016145756235346198]"""
455,228714,Remove everything deprecated in 3.0,Remove all API that was marked as deprecated in 3.0,smartshark_2_2,371,cayenne,"""[0.9969999194145203, 0.0030000440310686827]"""
456,228715,true/false in expression,"Sometimes having boolean true/false in an expression are needed. See discussion:

Example from Ãyvind Harboe:
Expression e=ExpressionFactory.expTrue();
if (xxx)
{
  e=e.andExp(....);
}
if (yyy)
{
  e=e.andExp(....);
}
return e;

http://mail-archives.apache.org/mod_mbox/incubator-cayenne-user/200608.mbox/%3cAB568C55-2716-4F5A-B498-1F0CBDC71BE6@objectstyle.org%3e

http://mail-archives.apache.org/mod_mbox/incubator-cayenne-user/200608.mbox/%3cc09652430608161347ka3eac59y3803eb1c80322f13@mail.gmail.com%3e",smartshark_2_2,679,cayenne,"""[0.9374048113822937, 0.0625951737165451]"""
457,228716,Modeler: Visualization issues with undo/redo actions for attributes and relationships,This is a parent task for attributes and relationship delete undo/redo related tasks,smartshark_2_2,1850,cayenne,"""[0.9842225909233093, 0.015777453780174255]"""
458,228717,Another bogus test issue,still testing....,smartshark_2_2,2407,cayenne,"""[0.8568222522735596, 0.14317771792411804]"""
459,228718,remove light-superclass.vm class template,remove light-superclass.vm class template until CAY-1230 is ready for prime time. see http://markmail.org/message/pe2rua6jwicxfnkl,smartshark_2_2,1051,cayenne,"""[0.9985055923461914, 0.0014943946152925491]"""
460,228719,Update Cayenne Modeler's validation errors panel to resize correctly.,"When validating a project (especially with a lot of errors) in Cayenne Modeler, the validation errors panel does not resize correctly.  You can make the window larger expecting to see more of the validation errors, but the errors portion stays constant while the window is enlarged.
",smartshark_2_2,590,cayenne,"""[0.9733118414878845, 0.02668813243508339]"""
461,228720,custom exception class for ROP missing session,"Currently BaseRemoteService are throwing a CayenneRuntimeException if the session are missing. It would be better if this was a subclass of CRE as it would make it easier for the ROP client to detect this situation and perhaps reconnect.

My proposal is to add org/apache/cayenne/remote/MissingSessionException as a subclass of CRE and throw it from BaseRemoteService.",smartshark_2_2,618,cayenne,"""[0.9984647035598755, 0.0015352950431406498]"""
462,228721,Add varargs addPrefetches(String...) and removePrefetches(String...) to SelectQuery,"If adding multiple prefetches to a query, it would be more convenient (often, anyway) to specify them in one line than multiple calls.
",smartshark_2_2,1178,cayenne,"""[0.9984970092773438, 0.0015029716305434704]"""
463,228722,Cayenne JDK 10 compatibility,"Ok, JDK 10 is already in RC state, need to check build and runtime compatibility.

Known issues so far:
# {{maven-surefire-plugin}} compatible version not published yet, see SUREFIRE-1439
# Modeler has problem with custom UI component on Windows",smartshark_2_2,1914,cayenne,"""[0.992341160774231, 0.007658898830413818]"""
464,228723,cdbimport: add option to create project file,"*cdbimport* tools are already pretty advanced and stable, but you still need Modeler to create new project. It is really slows down start of new project, and moreover complicates new users transition into Cayenne world.

New option in *cbimport* config can be like this:
{code:xml}
<configuration>
    <cayenneProject>${project.basedir}/src/main/resources/cayenne/cayenne-project.xml</cayenneProject>
    <map>${project.basedir}/src/main/resources/cayenne/datamap.map.xml</map>
    <cdbimport>
        <!-- ... -->
    <cdbimport>
</configuration>
{code}
And the logic should be like this:
 * without {{cayenneProject}} option result will be same as now
 * if {{cayenneProject}} is set but no file exists it will be created and DataMap linked to it
 * it {{cayenneProject}} is and file already exists then DataMap should be linked to it (if it is new) or update existing one",smartshark_2_2,1860,cayenne,"""[0.9980118274688721, 0.001988097792491317]"""
465,228724,Group Generated Fields by a Choosen Strategy,"Could be useful to have the ability to select the Field Grouping desired, in the generation of Tables (in SQL Scripts) and in Java Classes, by:

- none (or by inserting order)
- sort by field name, default, like now
- sort by attribute type (first all PK fields, then all FK fields, then other NOT NULL fields, then others
- other options ...
",smartshark_2_2,1098,cayenne,"""[0.9983990788459778, 0.0016008636448532343]"""
466,228725,Implement Protostuff as serialization service for Cayenne ROP,"Source: http://www.protostuff.io/
It based on protobuf but has some optimizations and some cool things like runtime serialization graph of objects (like Hessian). So we shouldn't define proto files although it might increase efficiency. And it work well with Java8 Date and Time types. It is also an alive project.

Here is some benchmarks. Take a look at Full Object Graph Serializers section. 
http://hperadin.github.io/jvm-serializers-report/report.html 
https://github.com/eishay/jvm-serializers/wiki

I've also forked one repo and added Hessian to comparison https://github.com/thinline72/protostuff-example 
Here is my results: 
Benchmark for 1000000 objects (includes both serialization and deserialization) 
jdk : 3,428 s 
jackson : 575,2 ms 
fasterxml : 503,5 ms 
protobuf : 257,9 ms 
protostuff : 215,5 ms 
kryo : 390,5 ms
hessian : 3,234 s",smartshark_2_2,1538,cayenne,"""[0.9984201192855835, 0.0015798596432432532]"""
467,228726,[PATCH] Use entrysets when key and value are desired,"Whenever you want to get the value of a map when iterating, it is more efficient to use an entrySet iterator rather than using a keySet iterator and using map.get(key). The time difference when done in a loop is (n) vs. (n)log(n). This patch fixes this.",smartshark_2_2,2183,cayenne,"""[0.9982497096061707, 0.0017502617556601763]"""
468,228727,Improve QueryLogger to log more information,"Since I have to support lot's of applications using Cayenne and usually Cayenne logs is only information I have about some bugs, I wish to have more information in that logs.

I think it will be nice to have:

1) Information of query that was executed (e.g. name of NamedQuery). To find right query in thousands...
2) Result of Select query.
3) As much information as it could be collected while logging errors.Like:  What query? Params? What Query String was build? Connection pool State? Driver version? Cayenne version? Better more than less.... 

Last one can be very useful since on production I have only ERROR logging level. And sometimes it is not possible to enable INFO level to find out what happen. ",smartshark_2_2,1384,cayenne,"""[0.998412013053894, 0.001587934559211135]"""
469,228728,support for Ingres database,"It would be nice if cayenne supported the Ingres database.

I have started to create a custom adapter.",smartshark_2_2,1001,cayenne,"""[0.9979471564292908, 0.0020528698805719614]"""
470,228729,Hudson automated testing against multiple databases,I need to get a variety of databases installed in the Cayenne Solaris zone so we can run Hudson tests against them all for each commit.,smartshark_2_2,355,cayenne,"""[0.9983558058738708, 0.0016441566403955221]"""
471,228730,A notion of default node,"Per CAY-943, Cayenne can merge multiple configurations into a single runtime. Now using that in a real system, I ran into a scenario that I have to define DataNode over and over again with the same name across multiple projects. It would be nice to have DataMap-only projects that would link to a ""default"" node.

Will add a ""default node name"" property to DataChannelDescriptor (so that DataChannelDescriptorMerger could set that). Since 99% of projects are single node, if ""default node name""  is null, that single node will be used as default (the merger probably can take care of that). DataDomain.lookupDataNode would route all unlinked maps to the default node.

Also DataDomain.lookupDataNode should probably throw on NULL.",smartshark_2_2,911,cayenne,"""[0.9985519051551819, 0.0014481126563623548]"""
472,228731,Vertical inheritance,Implement vertical inheritance as per http://cayenne.apache.org/doc/inheritance-overview.html,smartshark_2_2,697,cayenne,"""[0.9984322190284729, 0.0015677916817367077]"""
473,228732,"Make ""deleteObject"" atomic with respect to DeleteDenyException","http://objectstyle.org/cayenne/lists/cayenne-user/2007/09/0062.html

DeleteDenyException down the object tree should abort the delete op without doing any deletion. We will need to change the algorithm to process DENY rule recursively before doing actual deletion or relationship breaking.",smartshark_2_2,247,cayenne,"""[0.9752556681632996, 0.02474432997405529]"""
474,228733,"Found one unresolved task: DeduplicationVisitor ""swap inner classes for lambdas when we are on java 8""","We found this task where the condition satisfied but action was not taken:

cayenne-dbsync/src/main/java/org/apache/cayenne/dbsync/naming/DeduplicationVisitor.java:43
{code:java}
// TODO: swap inner classes for lambdas when we are on java 8{code}
Â 
 - The trigger: ""when we are on java 8"" happened around commit <b332610abdc3e4f0ec7743982c6f3c84b3436dde> (Aug 18 2017).

 - But the action: ""swap inner classes for lambdas"" never happened since then.

 - What need to be done: replace the inner classes, mostly
{code:java}
new Predicate(){...}{code}
with lambdas introduced in Java 8.",smartshark_2_2,1883,cayenne,"""[0.946311891078949, 0.05368811637163162]"""
475,228734,don't support multibyte,"project encoding is utf-8
HTML page encoding is utf-8
request.setCharacterEncoding(""utf-8"");
MySQL database encoding is utf-8
JDBC URL is ""jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf-8""

code:
String name = request.getParameter(""name"");    //name value is Chinese string ""æ½é¾å¿ç¨""
ObjectContext context = DataContext.createDataContext();
 	Artist artist = context.newObject(Artist.class);
    	artist.setName(name);
    	out.println(name);    //output string is ""æ½é¾å¿ç¨"",is normal.
        context.commitChanges();    //after insert into database,view table record is ""????""

get table record from database and view at page,view ""????"" too.",smartshark_2_2,1240,cayenne,"""[0.7284817695617676, 0.2715182602405548]"""
476,228735,Migrate ROP server-side components to DI-based stack,"Change the bootstrap procedure of the ROP server-side components to use CayenneServerRuntime, maybe with an addition of a special ROP server module. We may actually implement this as a new servlet, with parameters being the same as the ones used by CayenneFilter (""configuration-location"", ""extra-modules""), and using servlet name as a default configuration name. HessianServlet should be deprecated (see WebApplicationContextFilter for how it was done).",smartshark_2_2,1184,cayenne,"""[0.9984695315361023, 0.0015304541448131204]"""
477,228736,Runtime relationships leak into CayenneModeler,"This affects 3.0 M3 and causes lots of user confusion and potential modeling errors.... In *runtime* Cayenne creates missing reverse relationships to have a consistent mapping graph internally. This should be invisible to the Modeler... however it is not, as when a project is loaded, the ""defaults"" are applied and runtime relationships are shown to the user. Luckily they are not saved to XML , but the whole things is very confusing.",smartshark_2_2,106,cayenne,"""[0.16545754671096802, 0.834542453289032]"""
478,228737,MyISAM (and others?) don't support FKs; DbMerger should be aware,"MySQL only supports FKs in InnoDB, so the DbMerger should not attempt to reconstruct relationships",smartshark_2_2,494,cayenne,"""[0.8832388520240784, 0.11676115542650223]"""
479,228738,Delete Rule Cascade with Deny on reverse relationship always fails.,"Delete Rule Cascade with Deny on reverse relationship always fails.

On 7/27/06, Mike Kienenberger <mkienenb@gmail.com> wrote:
> Does the Deny still happen if the only
> reverse-relationship value is the cascading original object to be
> deleted?


On 7/27/06, Andrus Adamchik <andrus@objectstyle.org> wrote:
> Doesn't look like DataContextDeleteAction is smart about it:
> 
>             if (relatedObjects.size() == 0) {
>                  continue;
>              }
> 
>              // process DENY rule first...
>              if (relationship.getDeleteRule() == DeleteRule.DENY) {
>                  object.setPersistenceState(oldState);
> 
>                  String message = relatedObjects.size() == 1
>                          ? ""1 related object""
>                          : relatedObjects.size() + "" related objects"";
>                  throw new DeleteDenyException(object,
> relationship.getName(), message);
>              }
> 
> To handle cascade/deny combination we may need to change this code to
> check that at least one object in 'relatedObjects' collection is
> itself not deleted.
",smartshark_2_2,1291,cayenne,"""[0.08916934579610825, 0.9108306169509888]"""
480,228739,Cayenne JDK 11 compatibility,"JDK 11 early access builds already available, so we can start experiments.",smartshark_2_2,1934,cayenne,"""[0.9982228875160217, 0.0017771061975508928]"""
481,228740,lifecycle events docs,"Per doc @PreUpdate is called ""prior to validateForUpdate()"" but really validation is called before @PreUpdate

Doc needs review & further update improvements.
https://cayenne.apache.org/docs/3.1/cayenne-guide/lifecycle-events.html",smartshark_2_2,1485,cayenne,"""[0.9960850477218628, 0.003914922010153532]"""
482,228741,Unexpected read-only relationships in vertical inheritance mapping ,While we are investigating getting rid of read-only relationships completely (per CAY-1743) we should probably fix ObjRelationship.recalculateReadOnlyValue(..) to stop treating 1..1 / N..1 flattened chain (common in vertical inheritance mapping) from being treated as read-only. Should be a simple fix. ,smartshark_2_2,1043,cayenne,"""[0.9317018389701843, 0.06829822063446045]"""
483,228742,Model marked as dirty when leaving DataMap name field,"Trunk build of the modeler (post 3.0M5), when I click on ""DataMap Name"" field, and then click on any other text field (without changing the name), the model is marked as dirty.",smartshark_2_2,286,cayenne,"""[0.1852382868528366, 0.8147616982460022]"""
484,228743,Limit input into numeric fields to 10 digits,"This issue affects the following fields:
||Where||What||
|DbEntity > Properties tab|Max Length, Scale|
|Procedure > Parameters tab|Max Length, Precision|

Currently, if you enter more than 10 digits, a message appears that has an incorrect meaning for this situation. *See 1.png*",smartshark_2_2,1774,cayenne,"""[0.9686095118522644, 0.03139043599367142]"""
485,228744,Untangle HttpRemoteService from ServiceContext thread local setup,We need to untangle HttpRemoteService from dependence on Hessian's ServiceContext thread local setup.,smartshark_2_2,1540,cayenne,"""[0.9983401298522949, 0.0016598855145275593]"""
486,228745,Clean up OS X Modeler,"OS X assembly of CayenneModeler has a few issues:

* ""About"" menu reads ""About OSXMain"" instead of ""About CayenneModeler"". 
* There are a few deprecated API calls to com.apple classes. 

Perhaps Java 7 or 8 allows platform integration without com.apple hacks?",smartshark_2_2,1741,cayenne,"""[0.9982472658157349, 0.0017527260351926088]"""
487,228746,Cayenne Modeler Eclipse Plugin: one instance of Modeler must be running for single Cayenne project at one time,"Now on every click on Cayenne project file new instance of Modeler is created, which is probably not very comfortable.
I suggest that we must run only one instance of Modeler for single Cayenne project. I think we may do it by keeping own injector for each project, so we get fully independent Modeler instances for each project.",smartshark_2_2,793,cayenne,"""[0.9980157613754272, 0.001984266098588705]"""
488,228747,Add support for Microsoft Access Adapter so as to support isGenerated,"Microsoft Access sucks :-), hence it is often encountered in applications that are in great need of being rewritten from scratch.

Having Cayenne support for Microsoft Access allows running the new improved Cayenne powered application alongside the sucky old .asp spagetti code.

Referring to:

http://www.objectstyle.org/confluence/display/CAY/Custom+DbAdapter

I'm opening a new feature request which where I can place the results of my efforts to write a Microsoft Access adapter.",smartshark_2_2,1104,cayenne,"""[0.9984630346298218, 0.0015370005276054144]"""
489,228748,Better logging for delete cascading,Add logging which shows which objects delete cascading deletes (i.e. who causes who to get deleted),smartshark_2_2,2175,cayenne,"""[0.9984958171844482, 0.001504182699136436]"""
490,228749,Modeler: Validate case when dependent PK is marked as âgeneratedâ,Should we just ignore generated?,smartshark_2_2,1731,cayenne,"""[0.9502312541007996, 0.049768757075071335]"""
491,228750,Link Cayenne callback API with JPA provider,Per CAY-660 Cayenne 3.0 provides a JPA-compilant callback mechanism. We need to update the JPA provider to take advantage of it.,smartshark_2_2,443,cayenne,"""[0.9983150959014893, 0.0016849151579663157]"""
492,228751,SQLTemplate data row query of PostgreSQL Blob,"Using PostgreSQL oid blob, the PostgreSQL jdbc driver does not know if the oid is a blob or not. The user must issue getBlob to return the blob. getObject will return an Integer with the oid value.

select imagedata from person (where imagedata is a oid blob column)

does return an Integer. I guess this is okay as the jdbc driver are not able to provide the Blob as the jdbc type and ResultSet.getObject returns an Integer.

select #result('imagedata' 'java.sql.Blob' 'imagedata') from person

does also return an Integer, but should return an Blob. 

",smartshark_2_2,681,cayenne,"""[0.32162973284721375, 0.6783702969551086]"""
493,228752,Reserverd words mapping in WHERE clause,"I need to map a database schema which has reserverd words in column names. For example, the USER table has a column called GROUP.

I solved partially the problem, by setting Cayenne to quote identifiers:

DataMap dataMap = new DataMap(DUMMY_NAME);
dataMap.setQuotingSQLIdentifiers(true);

but this just quotes columns in the SELECT section. This generated query fails:

SELECT [t0].[NAME], [t0].[GROUP], [t0].[ID] FROM [USER] [t0] WHERE t0.GROUP = ? [bind: 1->GROUP:29]

because the GROUP column is used in the WHERE clause and it is not quoted. 
",smartshark_2_2,845,cayenne,"""[0.5388386845588684, 0.461161345243454]"""
494,228753,Investigate needed delegation methods for PK generator customization,"It should be helpful to have a delegate method (not sure if this should be DataContextDelegate, or something below it) that can customize PK value generation on top of any adaptor specific mechanism. Interesting example is described here:

http://objectstyle.org/cayenne/lists/cayenne-user/2003/11/0081.html",smartshark_2_2,2219,cayenne,"""[0.9984021782875061, 0.0015978029696270823]"""
495,228754,Support prototype entities and attributes,"EOModeler supports the creation of prototype entities and attributes, which are useful for quickly changing what database is used (OpenBase, Oracle, etc.) either through the GUI or programmatically, and also for promoting consistency of datatype definitions across the entire model.  The lack of this feature in Cayenne is the only thing keeping my group from dropping EOModeler in favor of Cayenne, since we have defined prototypes which we depend on.  Please consider adding support for this feature in Cayenne in the near future!",smartshark_2_2,1059,cayenne,"""[0.9984160661697388, 0.001583923352882266]"""
496,228755,Exclude asm jars from compile maven dependencies,"Just noticed that when I add Cayenne to the project via Maven, three asm-xyz.jar files are added (see screenshot of the pom graph). Need to make those ""provided"" I guess.",smartshark_2_2,401,cayenne,"""[0.9984132051467896, 0.00158679427113384]"""
497,228756,Simplify (junit) testing in cayenne,"Junit tests are becoming very important once the project reaches a certain point. Cayenne has dozens of junit tests but writing a junit test for cayenne based application is not easy at all. 

For me the main trouble is when there is no need to fetch or commit something (like testing GUI or lifecycle events). I tried to reproduce the tests found in cayenne,but always ended up with problems with mocking up the context, datachannel, entityResolver,  altering the configuration to point to different db etc. 

To solve that my idea was that one might specify a package in the CayenneModeler, this package will than be populated with generated a set of _MockupXXX extends XXX (like _MockupArtist extends Artist, _MockupPainting extends Painting etc.) and a MockupDataContext etc. There could be a second set of _MockupEntities for ROP client. 

Another thing is to specify the testing environment with ease. I think there should be also a possibility to create a ""testing"" DataNode pointing to a different database than deployment, and for the DataMap could be related to the real or testing DataNode at the same time. To choose the testing environment a system param like -Dcayenne.testing=TRUE could be utilised.
I might have missed something here: is there a simply way of having two DataNodes for one DataMap ?

I think that simplified testcase writing feature would be a great advantage for Cayenne over any other ORM.",smartshark_2_2,1123,cayenne,"""[0.9984884262084961, 0.0015115723945200443]"""
498,228757,Stress Testing Cayenne,"A few months ago we had a discusion about building a ""regression profiling"" framework [1]. Would be nice to start this while 1.2 is in Beta. In addition to the initial benchmarking goal, it would be nice to test various scenarios that can potentially have race conditions (such as [2]). Putting it on Jira in hope that it will get attention someday :-)

[1] http://objectstyle.org/cayenne/lists/cayenne-devel/2006/02/0006.html
[2] http://objectstyle.org/cayenne/lists/cayenne-user/2006/04/0207.html",smartshark_2_2,2330,cayenne,"""[0.9984556436538696, 0.0015443430747836828]"""
499,228758,Deprecate EventManager.getDefaultManager() and stop using it,EventManager.getDefaultManager()  is used by the map package to update mapping on dependent mapping object changes. The rest of Cayenne runtime uses EventManager that belongs to Configuration. We should deprecate 'getDefaultManager' and stop using the singleton in runtime.,smartshark_2_2,635,cayenne,"""[0.9984949827194214, 0.0015049814246594906]"""
500,228759,Move Attribute up/down to change it's order in list,"Please allow in CM to move up and down an attribute in the attribute table, to change it's order in the list.

This issue is related to CAY-910
(right now cayenne is forcing an alphabetic order :( )

Thank you,
A.
P.S. To make the Modeler even more user friendly, this order change could be mirrored automatically to the corresponding DBEntity too.",smartshark_2_2,292,cayenne,"""[0.998477041721344, 0.0015229785349220037]"""
501,228760,Update web app tutorial - JettyLauncher plugin does not work with Eclipse Ganymede release or newer,"http://cayenne.apache.org/doc/tutorial-webapp.html

JettyLauncher stopped working with Eclipse Ganymede builds since ~ January 2008. Will need to update the tutorial either with the ""mainstream"" but less user friendly WTP instructions, or run-jetty-run plugin (a descendant of JettyLauncher)",smartshark_2_2,317,cayenne,"""[0.9985411167144775, 0.0014588518533855677]"""
502,228761,Cayenne should support enum types in qualifier statements/expression handling,"Currently, cayenne doesn't support java enums in its expression syntax and, by extension, in qualifier statements.  This makes it impossible to use an enum type as a class descriminator for inheritance.

Enum expressions would look like:

propertyName = some.package.Type.CONSTANT

to make it consistent with JPA.",smartshark_2_2,730,cayenne,"""[0.9984713196754456, 0.001528708846308291]"""
503,228762,Delete Rules in remote CayenneContext,CayenneContext should support model delete rules. Probably on the client side; a less likely alternative is that delete rules will be applied on the server and then sent back to the client in a retrun diff.,smartshark_2_2,2275,cayenne,"""[0.9927254915237427, 0.007274441886693239]"""
504,228763,MergeToken sorting is highly unstable now,"MergeToken sorting is actually depends on many factors and thus highly unstable even with minor changes. 
Need to fix this probably by introducing some sort of explicit weight in every token.",smartshark_2_2,1619,cayenne,"""[0.9742357134819031, 0.02576429210603237]"""
505,228764,Improving Modeler preferences DB concurrency and consistency,"Embedded HSQLDB creates a number of concurrency problems - two or more instances of the Modeler can't use the same DB; when a Modeler is killed via kill -9, the DB lock is not cleaned; fresh preferences updates are not picked up by the local apps using cayenne ""JNDI emulation"" mode, etc.

Suggested improvements:

* Issue SHUTDOWN periodically (most likely by closing the connection pool and using ""shutdown=true"" URL property, so it will be transparent to Cayenne)

* When a DB is locked we create a copy of it so that the Modeler can still run. But then if the preferences are saved to this copy, they never make it to the master instance. Improve that by presenting ""unlock preferences"" dialog when a user attempts to edit preferences on the copy. If a users selects ""yes, unlock"", an attempt will be made to delete the lock on the master DB and take it over.

",smartshark_2_2,404,cayenne,"""[0.9966523051261902, 0.0033477318938821554]"""
506,228765,Problem with hotkeys,On pressing combination Ctrl+D with selected attribute/rel/callback fires a removing of selected ProjectTree element instead of selected item.,smartshark_2_2,1277,cayenne,"""[0.12048333138227463, 0.8795166611671448]"""
507,228766,Modeler: improve Java package refactoring,"1.  When clicking ""Update.."" on the DataMap ""Java package"", we provide a choice of updating a package of all ObjEntities/Embeddables or updating only entities that have an empty package. This doesn't make much sense... Instead of the radio buttons let's add two checkboxes - ""Update ObjEntities"", ""Update Embeddables"". Both should be checked by default. The action should respect the choices made by the user... Also these choices should be stored in the Modeler preferences.

2. Add ""Listener Java Package"" field below ""Java Package"", with an update button that would update all DataMap and Entity listener packages. Additionally iof a package is entered, creating a new listener should prepopulate the listener creation dialog with the package name. The package setting should be persistent, just like java package. It will saved in DataMap XML  as a property. E.g.:

<property name=""defaultListenerPackage"" value=""com.foo""/>

",smartshark_2_2,1387,cayenne,"""[0.9984831213951111, 0.0015168496174737811]"""
508,228767,Interceptable ExtendedTypes,"We need to build a mechanism that allows external code to intercept all calls to ExtendedType.setJdbcObject and ExtendedType.materializeObject to e.g. do some data modification. 

Ideally that code should also know what DbAttribute is being bound/read, so we may need to pass extra metadata to the ET API (or build another interceptable API on top of it).

",smartshark_2_2,1328,cayenne,"""[0.9984343647956848, 0.0015656434698030353]"""
509,228768,Pluggable Query Cache,"It would be nice to make query cache an independent module that can be managed externally. There have been some discussions on that in the past [1], and now I have a real-life case where this can be tested. 

[1] http://objectstyle.org/cayenne/lists/cayenne-devel/2006/04/0250.html

Here is the scope of this feature:

* (DONE - LRUMap and OSCache implementations) Pluggable cache for queries (local and shared) that can be managed externally
* determine cache key automatically. I.e. if a query has cache enabled, but no ""name"" specifiied, cache key should be build on the fly.
* OSCache provider - support mapping of cache groups (entities are groups) to cache policies in OSCache 
* OSCache provider - support mapping individual query names to cache policies.
* hookup ObjectStore and DataRowStore *object* caches with shared cache.
",smartshark_2_2,657,cayenne,"""[0.998331606388092, 0.0016683920985087752]"""
510,228769,Move cayenne-modeler module out of framework/ and out of the assembly binary,"In addition to runnable CayenneModeler, we ship in 3.0 cayenne-modeler.jar in the distro lib folder. This was to enable a JNDI hack that is no longer applicable in 3.1. So I am going to move cayenne-modeler out of framework to modeler fiolder, and prevent its inclusion in the assembly.",smartshark_2_2,722,cayenne,"""[0.9985284805297852, 0.0014715084107592702]"""
511,228770,cayenne-rop: initial submission of code v2,"Two branches: cws, containing the Cayenne Modeller files, the generated Java classes and other files needed to generate a WAR file for the server side of Cayenne ROP; rcp, containing the GUI client.",smartshark_2_2,1011,cayenne,"""[0.9981154203414917, 0.0018846222665160894]"""
512,228771,Slow performance of DbMerger,"This is a consequence of CAY-1946 refactoring. DbMerger applies ""includeTableName"" only after full reverse-engineering of the DB. So on databases with lots of system tables invoking DbMerger.createMergeTokens(..) becomes prohibitively slow. This is visible with MergeCase subclasses when running unit tests on PostgreSQL and Oracle.

E.g. on PostgreSQL 'mvn clean verify' that previously took ~2 min on my laptop, is now taking 1 hour. On Oracle I could not even finish the test run. 

There is actually a TODO in DbMerger, line 178:

if (!includeTableName(tableName)) {
    // TODO we have to cut this entities in db loader
   // TODO log
   continue;
}

We need to address this one.",smartshark_2_2,1425,cayenne,"""[0.9977796673774719, 0.002220314694568515]"""
513,228772,[cm] Display entity properties in the tree too.,"In the actual cm it's impossible to to see all the entities and attributes in order to get a quick idea about the entire structure.
The user must click each entity to see it's properties - this is very user unfriendly and it's impossible to get an overview since when the user has selected one entity the other is not displayed anymore.
The simplest solution is to display as subnodes the properties too (under each entity). ",smartshark_2_2,1127,cayenne,"""[0.9983775615692139, 0.001622476615011692]"""
514,228773,Formattable exceptions,"I'd like to make Cayenne exception classes easier to use internally. Namely add Object... vararg to constructors taking String message. Then exceptions can be built using String.format capability, e.g.:

throw new CayenneRuntimeException(""The entity '%s' is not found in DataMap '%s'"", entityName, mapName);

",smartshark_2_2,373,cayenne,"""[0.9983707070350647, 0.0016293227672576904]"""
515,228774,Sorting DB table columns in DBEntity,"If I insert a new column in DBEntry screen, there seems no way to determine the sort order.
It would be helpful, also for keeping the overview, to have some up/down-buttons for moving a column.",smartshark_2_2,276,cayenne,"""[0.9983590245246887, 0.0016410179669037461]"""
516,228775,Import EOModel ignores optimistic locking,"When doing an ""Import EOModel"", Cayenne Modeler ignores optimistic locking attributes in the EOModel.

The import should probably default to optimistic locking in Cayenne (since EOF uses optimistic locking by default) and set locks on the appropriate attributes.
",smartshark_2_2,471,cayenne,"""[0.5118597745895386, 0.4881402552127838]"""
517,228776,Need to provide ability to specify DataContextFactory to allow subclassing of DataContext,Need to provide ability to specify DataContextFactory to allow subclassing of DataContext.,smartshark_2_2,989,cayenne,"""[0.9980623126029968, 0.0019376591080799699]"""
518,228777,Refactor EJBQL-related translators to a standalone 'org.apache.cayenne.access.translator.ejbql' package,"Similar to procedure, select, batch translator packages, create one for EJBQL - org.apache.cayenne.access.translator.ejbql",smartshark_2_2,1333,cayenne,"""[0.9975674152374268, 0.002432593610137701]"""
519,228778,Add .zip generation to Windows profile.,Windows users tend to expect .zip files rather than .tar.gz.  We should add a new assembly descriptor for the Windows profile that will build the ZIP up for us.,smartshark_2_2,1125,cayenne,"""[0.9974504113197327, 0.0025495707523077726]"""
520,228779,Update function support in expression parser,"We need following enhancements in expression parser:
- add missing support for current date/time/timestamp functions
- java-style ""camelCase"" naming for expression functions
- make functions' names case sensitive (this one can break a little some code)",smartshark_2_2,1708,cayenne,"""[0.9983453750610352, 0.001654574298299849]"""
521,228780,Implement pluggable strategy for fault resolving,"As discussed in this thread http://markmail.org/message/5tn3gxruak6flwn4 on databases with lax referential integrity Cayenne would not only throw FaultFailureException, but may misbehave in other ways. It would be nice to have an ability to redefine fault resolving strategies for such databases.",smartshark_2_2,1388,cayenne,"""[0.9984328150749207, 0.0015672246227040887]"""
522,228781,One way to-many support,"This is somewhere between a bu and a new feature. Normally Cayenne allows uni-directional mapping of ObjRelationships, as long as underlying DbRelationships
are bi-directional.

Problem:

Uni-directional to-many do not work. Symptom of this is saving a FK column value of the ""to-many"" side object as NULL, when it was added to the parent object via ""parent.addToXyzArray(child)"".

This was originally reported by Scott Merritt. His example is attached to the bug report.

[LATER COMMENT BY ARNDT]

we just stumbled over this same problem, went all the way through debugging it and finally found the unit-test that is supposed
to test this.

OneWayOneToManyTst.testReadList()

However, the assertion assertNotNull(a2) is really misleading - it suggests that this is a successfull test, but the result in fact is totallay wrong! (empty list instead of
2 entries)

so I have difficulties qualifiyng this as a ""missing feature"", and I would like to see at least to have this constraint added
to the datamap-validation to see a warning
about this.

Or , of course, even better - make it work :-)

regards,

Arndt
",smartshark_2_2,960,cayenne,"""[0.8355388641357422, 0.16446109116077423]"""
523,228782,rop-browser update 3,"Changes:
- Expand/contract relationship
- Hide elements
- Insert/delete objects
- Toggle between auto/manual layout
- Cayenne 1.2 libraries",smartshark_2_2,2377,cayenne,"""[0.9981902241706848, 0.001809788984246552]"""
524,228783,Modeler: support optional meaningful PK mapping during reverse engineering,"It would be nice to have an option to map PK columns as ObjAttributes during reverse engineering. Let's add a check box to reveng dialog saying ""Meaningful PK"", and handle it in the backend.",smartshark_2_2,597,cayenne,"""[0.9983581900596619, 0.0016418471932411194]"""
525,228784,Database Schema Migration should sort tokens,"The merger stuff should sort the diff tokens before issuing them to the db and/or model. Sorting must be done by entity, but also token type.",smartshark_2_2,303,cayenne,"""[0.9723970890045166, 0.027602965012192726]"""
526,228785,cannot build cayenne against jdk 1.6 due to changes in javax.sql package classes/interfaces,"kevin menard noticed in a beta of java 6 some time ago ( CAY-549 ) that the interface definitions for some classes/interfaces in javax.sql have changed to also include a new class, javax.sql.Wrapper, in the implements clause, and as a result, the cayenne codebase does not compile against a java 6 sdk.

i've tapped out a rough patch that sets up a shallow hierarchy for the relevant cayenne classes which are affected by this, and provides some simple abstract implementations for Connection, DataSource, PooledDataSource and ResultSetMetaData in order to satisfy the new interface requirements that include Wrapper in their implements declarations.

( theres also a couple of other classes which get some new abstract/interface methods, check the patch for more details )",smartshark_2_2,26,cayenne,"""[0.9864107966423035, 0.013589184731245041]"""
527,228786,Automatic AUTO_PK_SUPPORT row creation,"When a user of Cayenne adds new tables to their schema, they must currently remember to add a new row to the AUTO_PK_SUPPORT table for  primary keys managed by Cayenne. This should not be necessary if Cayenne could at startup read all rows of that table and automatically add any rows missing, or else capture the appropriate SQL error when trying to update the PK (and failing due to the missing row) and then automatically add the row and try again.",smartshark_2_2,241,cayenne,"""[0.9984874725341797, 0.001512527815066278]"""
528,228787,Switch usage of SelectQuery to ObjectSelect internally,"Recommended select query since 4.0 is {{ObjectSelect}}, that is converted internally to {{SelectQuery}}.
We can gain some performance and maintainability by inverting this, i.e. using {{ObjectSelect}} directly in Cayenne stack and converting {{SelectQuery}} to it.

Important note: this should not affect any public API.",smartshark_2_2,1925,cayenne,"""[0.9981743097305298, 0.0018256423063576221]"""
529,228788,Create a SPEC file to build RPM distributions of cayenne,Create a SPEC file to produce an RPM distribution of cayenne. future compatibility with the JPackage repository would be nice,smartshark_2_2,558,cayenne,"""[0.9965798258781433, 0.0034202246461063623]"""
530,228789,"cayenne-modeler:run plugin should open a (configured) project file, with suitable defaults","To increase the utility of the cayenne-modeler plugin, it should allow users to configure a ""model file"" to be opened when the plugin is run.  Should no file be specified, the plugin should search through the top level of non-test resource directories for cayenne.xml, using the first one found.  Failing that, src/main/webapp/WEB-INF/cayenne.xml should also be tried before falling back to opening the modeler with no project specified.",smartshark_2_2,596,cayenne,"""[0.9985986351966858, 0.001401340588927269]"""
531,228790,Implement support for auto-pks propagation to dependent entities,"Support for auto-pks propagation to dependent entities is the last thing we need to do for ""generated"" PK columns released in M2 to become first class citizens in Cayenne... 

I started working on EntityDescriptor class that would allow propagation of pk values that were generated in the same transaction (this is done via deferred PK maps in InsertBatches)... Need to finish it.",smartshark_2_2,2216,cayenne,"""[0.9985190033912659, 0.0014809959102422]"""
532,228791,Support for mapping to-many as Maps and Sets and Collections,"Per JPA spec we should support mapping to-many relationships as Lists, Collections, Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:

* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)
* In the Modeler allow to specify the choices in ObjRelationship Inspector
* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)
* Runtime support, including reverse relationships
* Support for prefetching
* Testing
* Bridging JPA mapping 
",smartshark_2_2,1139,cayenne,"""[0.9984902143478394, 0.0015097518917173147]"""
533,228792,Provider implementation (javax.persistence.spi interfaces),Implement javax.persistence.spi.PersistenceProvider and PersistenceUnitInfo interfaces.,smartshark_2_2,441,cayenne,"""[0.9979819059371948, 0.002018043538555503]"""
534,228793,Add deleteObjects() to ObjectContext,"DataContext has a deleteObjects() method, but ObjectContext doesn't.

The documentation shows using deleteObjects:

http://cayenne.apache.org/doc/deleting-objects.html

This is more of a hassle when using cayenneObject.getObjectContext().delete... instead of cayenneObject.getDataContext() because getDataContext() is deprecated in 3.0.
",smartshark_2_2,585,cayenne,"""[0.9971983432769775, 0.002801678143441677]"""
535,228794,Migrate HSQLDB modeler preferences to Java preferences API,"Per CAY-1314 the old file-based preferences were switched to Java preferences API. The new preferences store is much more user friendly, at least on Linux and OS X. It doesn't seem to have concurrency issues, can be viewed and edited using OS specific tools. So switching the rest of the preferences using HSQLDB is a logical next step.

",smartshark_2_2,716,cayenne,"""[0.9983742237091064, 0.001625798991881311]"""
536,228795,"Add method that lists ""bad names"" and show warning in Cayenne modeler","When creating database models directly in Cayenne, there are names for fields which are incompatible with databases. E.g. naming a field ""file"" works fine in Derby, but fails in MS SQL server.

It would be nice if the modeler gave a warning about such bad namechoices. 

In addition to a global list of bad names, each adapter could ammend names that probably shouldn't be used.

The main problem is that such bad name choices will only be deteced upon deployment, so testing against Derby and deploying against MS SQL/Oracle can cause unecessary headaches.",smartshark_2_2,1019,cayenne,"""[0.9985276460647583, 0.0014723604544997215]"""
537,228796,Support JDBC 4,"Support new data types in JDBC 4. http://jcp.org/aboutJava/communityprocess/final/jsr221/index.html

This will probably involve creating a new maven build target for Java 6 and adding specific support there.

http://markmail.org/message/zfdrqz6rnnjkdsrr",smartshark_2_2,1401,cayenne,"""[0.998417854309082, 0.001582195982336998]"""
538,228797,Dialog Cancel/Close button positioning,"Hi,

I have notice that in some of the Cayenne Modeller dialogs the ""Close"" and ""Cancel"" buttons are not always the right most button.

This is generally the convention with Microsoft Windows (cant remember if this is also the case with MacOS). 

While this may sound trivial issue, when I cancel or close dialogs I will often hit the right hand button without even reading the label. In the ""Generate DB Schema: Options"" I actually dropped the existing tables when I hit the ""Generate"" button by accident. This was not what I was expecting.

Dialogs which do not have the Cancel/Close on the right hand side include:
""Reengineer DB Schema: Connect to Database""
""Reengineer DB Schema: Options""
""Generate Java Classes""
""Generate DB Schema: Connect to Database""
""Generate DB Schema: Options""
""Edit Preferences""

regards Malcolm Edgar'",smartshark_2_2,473,cayenne,"""[0.9501261711120605, 0.04987379163503647]"""
539,228798,Support for DBCP2,A new Cayenne module that provides DBCP2 support.,smartshark_2_2,1472,cayenne,"""[0.9980395436286926, 0.0019604358822107315]"""
540,228799,ObjRelationship Mapping Dialog Improvements,"[This is a GSoC 2008 task]

The biggest complaint about the ObjRelationship mapping dialog is that it is often unclear how it operates, especially to the new users. I.e. empty list of DbRelationships is displayed as a white area, not giving any hints on what needs to be done to map a relationship. So that's confusing. Same thing when you add 1 path component, there is no hint that you can chain more path components for the flattened relationship.

At the minimum we may just add some hint text (""Select next DbRelationship"" in grey over the next available dropdown), but ideally we should implement a path browser, similar to how the SelectQuery prefetch and ordering browsers operate (and similar to how OS X Finder does).
",smartshark_2_2,612,cayenne,"""[0.9984015822410583, 0.0015983626944944263]"""
541,228800,Create a share distributed cache for DataRows.,"In 1.1 Cayenne will implement smart caching of DataRows, either as a local cache for each ObjectStore, or as a single common cache.

MOTIVATION: 
1. Lower memory footprint.
2. Improved performance for objects retrieval.
3. Synchronization of objects state between DataContexts in the same VM and cross-VM

STATUS: 
  - Current implementation does not support updates of objects that have other objects added to their to-many relationships. To-many relationships are refreshed when an object is deleted, but this is not very reliable either (e.g. if a deleted object is in HOLLOW state, relationships using it are not handled properly)

  - There is a bug in sending snapshot events. When an object is updated via a remote event, and then the same object is modified locally, no event is sent out on commit of this object. When an object is modified and committed for the second time, everything starts to work as expected.

TODO: 
  - multuthreaded/multiprocess testing.
  - documentation",smartshark_2_2,1122,cayenne,"""[0.9983879327774048, 0.001612093998119235]"""
542,228801,Contribute lifecycle events listeners via DI,"Contribute lifecycle events listeners via DI, just like filters:

{code}
Module m = binder -> {
    ServerModule.contributeDomainListeners(binder).add(myListener);
}
{code}",smartshark_2_2,1707,cayenne,"""[0.9984620809555054, 0.0015379440737888217]"""
543,228802,MethodParameters should read 1 byte not two for parameter count,Originally reported against Apache Tomcat's cut-down version of BCEL as https://github.com/apache/tomcat/pull/12,smartshark_2_2,180,commons-bcel,"""[0.3727579414844513, 0.6272420287132263]"""
544,228803,IINC does not handle -128 properly,src/main/java/org/apache/bcel/generic/IINC.java is only allowing -127 to +127 as an argument.  it should allow -128 as well.  I have attached a patch.,smartshark_2_2,279,commons-bcel,"""[0.15557923913002014, 0.8444207906723022]"""
545,228804,Problem with JAXB if the bcel classloader is used,"When I try to run program with a JAXB code I got an exception if the bcel
classloader is used.

Exception in thread ""main"" java.lang.LinkageError: Class org/xml/sax/InputSource
violates loader constraints
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(Unknown Source)
	at java.lang.ClassLoader.defineClass(Unknown Source)
	at org.apache.bcel.util.ClassLoader.loadClass(ClassLoader.java:127)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at java.lang.ClassLoader.loadClassInternal(Unknown Source)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(Unknown
Source)
	at
advdebug.config.parser.impl.runtime.UnmarshallerImpl.unmarshal(UnmarshallerImpl.java:140)
	at
javax.xml.bind.helpers.AbstractUnmarshallerImpl.unmarshal(AbstractUnmarshallerImpl.java:131)
	at
javax.xml.bind.helpers.AbstractUnmarshallerImpl.unmarshal(AbstractUnmarshallerImpl.java:136)
	at
javax.xml.bind.helpers.AbstractUnmarshallerImpl.unmarshal(AbstractUnmarshallerImpl.java:145)
	at
javax.xml.bind.helpers.AbstractUnmarshallerImpl.unmarshal(AbstractUnmarshallerImpl.java:163)
	at test.Test.main(Test.java:47)",smartshark_2_2,158,commons-bcel,"""[0.1658824235200882, 0.834117591381073]"""
546,228805,JustIce verifier does not check correctly the returned value of a method,"Defining a {{areturn}} opcode (return object) in a void method is accepted by the bytecode verifier.
This must not be allowed.

Here is an example of a non-valid bytecode for a void method:
{code}
       BB 0003      // 0   : new java/lang/Object
       59           // 3   : dup 
       B7 0008      // 4   : invokespecial java/lang/Object.<init>()V
       00           // 7   : nop 
       B0           // 8   : areturn (Not allowed in a void method)
{code}

",smartshark_2_2,215,commons-bcel,"""[0.08734510093927383, 0.9126548767089844]"""
547,228806,StackMapTable[Entry] should be removed and improvements merged into StackMap[Entry],StackMapTableEntry is a much better implementation of the same object as StackMapEntry.  The later should be removed.,smartshark_2_2,230,commons-bcel,"""[0.9983240962028503, 0.0016758856363594532]"""
548,228807,add support for TypeVariables to signatureToString,BCEL currently throws an Invalid signature exception for a TypeVariableSignature - designated with the letter 'T'.  This patch adds support for these.,smartshark_2_2,178,commons-bcel,"""[0.9984363913536072, 0.001563618890941143]"""
549,228808,ClassPath.getClassFile() and friends do not work with JRE 9 and higher,See BCEL-304,smartshark_2_2,301,commons-bcel,"""[0.8170915842056274, 0.18290835618972778]"""
550,228809,small typo in ConstantLong.java,fix misspelling,smartshark_2_2,267,commons-bcel,"""[0.9921633005142212, 0.007836614735424519]"""
551,228810,Enable loading ConstantPool from a byte array,"In some cases such as JDI API we have the class constant pool content only in form of separate byte array. Currently there is no way to load constant pool from byte array.

The proposed patch enable  loading ConstantPool from custom DataInput stream by making ConstantPool(DataInput) constructor  public.

GitHub pull request: https://github.com/apache/commons-bcel/pull/1",smartshark_2_2,234,commons-bcel,"""[0.9984196424484253, 0.001580344745889306]"""
552,228811,Added a functionality to throw a ClassNotfoundException,"I am a senior student of computer science at the University of
 Maryland. As one of my project, I am required to contribute to an open
 source  project.
  I came upon one error in your Apache BCEL program, more specifically
 in the file 
  Passe1Verifier. java. I have added the functionnality to allow the private 
  class JavaClass getJavaClass()to throw  ClassNotFoundException 
  whenever such an exception is caught.
  Originally the catch block was empty.",smartshark_2_2,40,commons-bcel,"""[0.9984198808670044, 0.0015801364788785577]"""
553,228812,NullPointerException at org.apache.bcel.classfile.FieldOrMethod.dump(),"NullPointerException at org.apache.bcel.classfile.FieldOrMethod.dump()

{noformat}
java.lang.NullPointerException
	at org.apache.bcel.classfile.FieldOrMethod.dump(FieldOrMethod.java:142)
	at org.apache.bcel.classfile.JavaClass.dump(JavaClass.java:311)
	at org.apache.bcel.classfile.JavaClass.dump(JavaClass.java:287)
	at org.apache.xalan.xsltc.compiler.XSLTC.dumpClass(XSLTC.java:856)
	at org.apache.xalan.xsltc.compiler.Stylesheet.translate(Stylesheet.java:738)
	at org.apache.xalan.xsltc.compiler.XSLTC.compile(XSLTC.java:371)
	at org.apache.xalan.xsltc.compiler.XSLTC.compile(XSLTC.java:446)
	at org.apache.xalan.xsltc.trax.TransformerFactoryImpl.newTemplates(TransformerFactoryImpl.java:799)
{noformat}",smartshark_2_2,315,commons-bcel,"""[0.06946302205324173, 0.9305369257926941]"""
554,228813,Verifier does not report the signature of methods not found,"When JustICE finds that INVOKEXXX instruction references non-existing method, it reports the method name, but not the method signature: (Pass 3a)
""constraint violated: Referenced method 'XXXX' with expected signature not found in class 'my.package.Clazz'. The native verifier possibly allows the method to be declared in some superclass or implemented interface, which the Java Virtual Machine Specification, Second Edition does not.""

It would be mutch better if along with the method name, its arguments and return types (signature) is reported.

Regards, 
Cheffo",smartshark_2_2,53,commons-bcel,"""[0.7731620669364929, 0.22683794796466827]"""
555,228814,Add Automatic-Module-Name MANIFEST entry for Java 9 compatibility,See LANG-1338,smartshark_2_2,288,commons-bcel,"""[0.9972148537635803, 0.0027851888444274664]"""
556,228815,WeakHashmap enters into infinite loop in WrapDynaClass.java,"We noticed that our application was using too much of CPU , all the 6 cores were used.Â 

On capturing the thread dump we saw that large number of threads were in the running state and in :

at java.util.WeakHashMap.get(WeakHashMap.java:403)
 at org.apache.commons.beanutils.WrapDynaClass.createDynaClass(WrapDynaClass.java:425)

Â 

So we are suspecting that the thread has entered into indefinite while loop and hogging all the CPU resources.

I have attached the thread dump for reference.Â Â 

Â 

what is the solution for this issue?",smartshark_2_2,517,commons-beanutils,"""[0.2039804458618164, 0.7960195541381836]"""
557,228816,PropertyUtilsBean isReadable() and isWriteable() methods do not work correctly for WrapDynaBean,"Try this: given any pojo class MyBean.java,

MyBean orig = new MyBean();
MyBean dest = new MyBean();
DynaBean db = new WrapDynaBean(dest);
PropertyUtils.copyProperties(db, orig);

You'll see an exception like:
java.lang.IllegalArgumentException: Property 'class' has no write method
        at org.apache.commons.beanutils.WrapDynaBean.set(WrapDynaBean.java:266)

This surprised me because 'copyProperties()' does an 'isWritable()' check before
it sets properties. However, what PropertyUtilsBean.isWritable actually does is
this:
if (bean instanceof DynaBean) {
            // All DynaBean properties are writeable
            return (((DynaBean) bean).getDynaClass().getDynaProperty(name) != null);
} ...

That's plainly wrong for a WrapDynaBean, since pretty much every wrapped object
has a non-writable 'class' property. 

A workaround for the immediate problem:
if (db instanceof WrapDynaBean) {
  PropertyUtils.copyProperties(((WrapDynaBean) db).getInstance(), src);
} else {
  PropertyUtils.copyProperties(dest, src);
}

... the problem affects isReadable and isWritable, and hence copyProperties; to
fix it for all cases would (I think) require unwrapping any WrapDynaBeans passed
into those calls.

I guess having a situation where you'd use PropertyUtils /and/ WrapDynaBeans is
fairly unusual so this is just a minor bug.",smartshark_2_2,79,commons-beanutils,"""[0.11779990047216415, 0.8822001218795776]"""
558,228817,Method createDynaProperty of JDBCDynaClass should first look for column label instead of column name in ResultSetMetadata object..,"Method  *createDynaProperty( ResultSetMetaData metadata, int i)*  of class *JDBCDynaClass* should read *column label* instead of *column name* from ResultSetMetadata object.

For example..
If query is something like *'select user_id as userId, user_name as userName....'* than *columName* variable should have the value *userId* instead of *user_id*

Fix is ..

*Original*
{code:title=JDBCDynaClass.java|borderStyle=solid}
protected DynaProperty createDynaProperty(
                                    ResultSetMetaData metadata,
                                    int i)
                                    throws SQLException {
//This is reading ColumnName but not ColumnLabel which returns value 'user_id' ..not 'userId' 
String columnName = metadata.getColumnName(i); 
...
...
...
}
{code} 

*It should be like following*

{code:title=JDBCDynaClass.java|borderStyle=solid}
protected DynaProperty createDynaProperty(
                                    ResultSetMetaData metadata,
                                    int i)
                                    throws SQLException {
 //First read ColumnLabel ..which sould be 'userId' in our case
 String columnName = metadata.getColumnLabel(i);  

 // If ColumnLabel is 'null' or 'empty' read ColumnName...
 if(columnName == null || columnName.trim().equals(""""))  {  
     columnName = metadata.getColumnName(i);
 }
...
...
...
{code} 

Let me know if this is not how it suppose to be or I am missing something here.

Thanks,
Viral Patel

",smartshark_2_2,377,commons-beanutils,"""[0.533947765827179, 0.46605220437049866]"""
559,228818,Many threads are stuck in infinite loops in MethodUtils  because static WeakHashMap is not thread safe,"The problem lies in the class org.apache.commons.beanutils.MethodUtils. This class is keeping a global cache in a non synchronized WeakHashMap. WeakHashMap is not thread safe and required external synchronization. The lack of synchronization can cause corruption and the WeakHashMap.get() method to go into an infinite loop.

Googling ""WeakHashMap infinite loop"" returns many cases of similar problem with WeakHashMap. The solution is to decorate the WeakHashMap in a synchronized Map as described in this thread:
http://dev.eclipse.org/mhonarc/lists/aspectj-users/msg08824.html

The modification to make the MethodUtils cache thread safe is a one line change. 
Before: 
private static WeakHashMap cache = new WeakHashMap();

After: 
private static Map cache = Collections.synchronizedMap(new WeakHashMap());

Example of thread dump

""ExecuteThread: '0' for queue: 'weblogic.kernel.Default'"" id=13 idx=0x3c tid=5905 prio=5 alive, daemon
    at org/apache/commons/beanutils/MethodUtils$MethodDescriptor.equals(MethodUtils.java:828)[optimized]
    at java/util/WeakHashMap.eq(WeakHashMap.java:254)[inlined]
    at java/util/WeakHashMap.get(WeakHashMap.java:345)[optimized]
    at org/apache/commons/beanutils/MethodUtils.getMatchingAccessibleMethod(MethodUtils.java:530)[optimized]
    at org/apache/commons/beanutils/MethodUtils.invokeMethod(MethodUtils.java:209)[inlined]
::::

""ExecuteThread: '1' for queue: 'weblogic.kernel.Default'"" id=14 idx=0x40 tid=5906 prio=5 alive, daemon
    at org/apache/commons/beanutils/MethodUtils$MethodDescriptor.equals(MethodUtils.java:833)[optimized]
    at java/util/WeakHashMap.eq(WeakHashMap.java:254)[inlined]
    at java/util/WeakHashMap.get(WeakHashMap.java:345)[optimized]
    at org/apache/commons/beanutils/MethodUtils.getMatchingAccessibleMethod(MethodUtils.java:530)[optimized]
    at org/apache/commons/beanutils/MethodUtils.invokeMethod(MethodUtils.java:209)[inlined]
:::
""ExecuteThread: '2' for queue: 'weblogic.kernel.Default'"" id=15 idx=0x44 tid=5907 prio=5 alive, daemon
    at org/apache/commons/beanutils/MethodUtils$MethodDescriptor.equals(MethodUtils.java:833)[optimized]
    at java/util/WeakHashMap.eq(WeakHashMap.java:254)[inlined]
    at java/util/WeakHashMap.get(WeakHashMap.java:345)[optimized]
    at org/apache/commons/beanutils/MethodUtils.getMatchingAccessibleMethod(MethodUtils.java:530)[optimized]
    at org/apache/commons/beanutils/MethodUtils.invokeMethod(MethodUtils.java:209)[inlined]


",smartshark_2_2,518,commons-beanutils,"""[0.09732033312320709, 0.9026796221733093]"""
560,228819,Converters can return an invalid result object if a default value is set,"For BeanUtils converters derived from {{AbstractConverter}} it is possible to set a default value. If the default flag is set, this value is returned for *null* input, but also if a conversion to an unsupported type is attempted (in this case, a conversion of the default value to the target class is tried, but if this fails, the default value is returned directly). This causes the converter to return an object of another class than requested.

IMHO this is a source of ClassCastExceptions and should be changed. A converter should never return an object of a different type than the requested target class. So either perform a successful conversion (if necessary, convert the default value to the target class) or throw an exception.",smartshark_2_2,437,commons-beanutils,"""[0.10999715328216553, 0.8900028467178345]"""
561,228820,"BeanUtilsBean: Set a mapped/indexed property, for example ""property(time)[0]""","From line 1014 ff there is the following piece of code:

        try {
            if (index >= 0) {
                getPropertyUtils().setIndexedProperty(target, propName,
                                                 index, newValue);
            } else if (key != null) {
                getPropertyUtils().setMappedProperty(target, propName,
                                                key, newValue);
            } else {
                getPropertyUtils().setProperty(target, propName, newValue);
            }
...

That's good for mapped OR indexed properties, but unfortunatly i have both at
the same time. So index is >= 0 and key is also != null.

My property-name looks like this one here: 
property(time)[0]",smartshark_2_2,175,commons-beanutils,"""[0.8117586374282837, 0.18824133276939392]"""
562,228821,Provide a BeanIntrospector implementation which supports properties in a fluent API,"BEANUTILS-425 was about adding a mechanism for customizing bean introspection. This ticket is a request for a concrete new {{BeanIntrospector}} implementation which detects writable bean properties even if the return value of the set method is not *void*.

The _builder_ pattern and fluent API definitions become more and more popular. Here methods are offered for setting properties that return the current instance of a builder object for method chaining, for instance:

{code}
public class FooBuilder {
    public FooBuilder setFooProperty1(String value) {
        ...
        return this;
    }

    public FooBuilder setFooProperty2(int value) {
        ...
        return this;
    }
}
{code}

Standard bean introspection will not recognize set methods like that because they do not conform to the Java Beans specification. A new {{BeanIntrospector}} implementation can be created which ignores the return type of set methods and create corresponding {{PropertyDescriptor}} objects with a correct write method.",smartshark_2_2,343,commons-beanutils,"""[0.9984298348426819, 0.0015701565425843]"""
563,228822,Provide facility to configure and clear MethodUtils cache,Provide the faciltity to configure whether MethodUtils caches methods (currently always does) and to clear the method cache.,smartshark_2_2,238,commons-beanutils,"""[0.9984772801399231, 0.0015226781833916903]"""
564,228823,[beanutils] Add nullate utilities,"It's a common task to reset all properties of an object to null (or to default
values for primitives), for instance in Struts in the reset method of subclasses
of ActionForm.

It would be usefull to have something like
- nullateProperties(Object _o)
 to nullate the properties of _o declared by the class of _o

- nullateProperties(Object _o, Class _clazz)
 to nullate the properties of _o declared by _clazz

- nullateProperties(Object _o, Class _clazz, boolean _bAncestorsToo)
 to nullate the properties of _o declared by _clazz and its ancestors",smartshark_2_2,347,commons-beanutils,"""[0.9984082579612732, 0.0015917798737064004]"""
565,228824,[beanutils] Change propertyUtilsBean.findNextNestedIndex(String) modifier to protected ?,"Could be be changed findNextNestedIndex modifier from ""private"" to ""protected""
?
It's to avoid to senselessly copy out this algo in custom PropertyUtilsBean
subclasses to get exactly same feature.
Thanks.",smartshark_2_2,192,commons-beanutils,"""[0.9983921647071838, 0.00160783261526376]"""
566,228825,There is no way to get a value from the map which has a key type of enum,"There is no way to get a value from the map which has a key type of Enum. So I solve the problem and I will upload my patch (with Junit test cases).

",smartshark_2_2,346,commons-beanutils,"""[0.8966481685638428, 0.103351891040802]"""
567,228826,AbstractArrayConverter.strings should be final,"AbstractArrayConverter.strings is used as a constant, but is not marked final",smartshark_2_2,381,commons-beanutils,"""[0.9981561303138733, 0.0018439034465700388]"""
568,228827,Add ClassConverter to standard converters,"A converter for String to Class conversions. I like it, and would like it to be
added to the standard BeanUtils converters.",smartshark_2_2,205,commons-beanutils,"""[0.9982702732086182, 0.0017297512385994196]"""
569,228828,[beanutils] Enhancement to ResultSetDynaClass to allow case-insensitive and missing parameter bean mapping,"##This is a 1 file patch for JDBCDynaClass and ResultSetDynaClass to allow more
##flexibility in the mapping of variable names to bean attributes.  
##e.g. PROJNBR in resultset will automatically get mapped to ProjNbr or PrOjNbR
##in bean.  class returned by iterator will not fail if there are fewer fields
##in resultset than in input bean.
--- JDBCDynaClass.orig	2003-10-14 11:40:20.000000000 -0400
+++ JDBCDynaClass.java	2003-10-13 11:39:49.000000000 -0400
@@ -69,11 +69,13 @@
 import java.util.HashMap;
 import java.util.Map;
 
+
 /**
  * <p>Provides common logic for JDBC implementations of {@link DynaClass}.</p>
  *
  * @author   Craig R. McClanahan
  * @author   George Franciscus
+ * @author   Michael Mainguy
  * @version $Revision: 1.3 $ $Date: 2003/10/09 20:43:15 $
  */
 
@@ -256,6 +258,48 @@
 
     }
 
-}
 
+	/**
+	 * <p>Introspect the metadata associated with our result set, and 
populate
+	 * the <code>properties</code> and <code>propertiesMap</code> instance
+	 * variables.</p>
+	 *
+	 * @param resultSet The <code>resultSet</code> whose metadata is to
+	 *  be introspected
+	 * @param resultSet The <code>DynaClass</code> whose properties we want
+	 *  to use
+	 * 
+	 * @exception SQLException if an error is encountered processing the
+	 *  result set metadata
+	 * 
+	 */
+	protected void introspect(ResultSet resultSet, DynaClass dynaClass) 
throws SQLException {
+		HashMap propmap = new HashMap();
+		DynaProperty[] props = dynaClass.getDynaProperties();
+		for (int i = 0; i< props.length; i++) {
+			propmap.put(props[i].getName().toLowerCase(), props[i]);
+		
+		}
+		// Accumulate an ordered list of DynaProperties
+		ArrayList list = new ArrayList();
+		ResultSetMetaData metadata = resultSet.getMetaData();
+		int n = metadata.getColumnCount();
+		for (int i = 1; i <= n; i++) { // JDBC is one-relative!
+			DynaProperty dynaProperty = (DynaProperty)propmap.get
(metadata.getColumnName(i).toLowerCase());
+			if (dynaProperty != null) {
+					list.add(dynaProperty);
+			}
+		}
+
+		// Convert this list into the internal data structures we need
+		properties =
+			(DynaProperty[]) list.toArray(new DynaProperty[list.size
()]);
+		for (int i = 0; i < properties.length; i++) {
+			propertiesMap.put(properties[i].getName(), properties
[i]);
+		}
+
+	}
+
+
+}
--- ResultSetDynaClass.orig	2003-10-14 11:39:15.000000000 -0400
+++ ResultSetDynaClass.java	2003-10-13 11:39:47.000000000 -0400
@@ -123,145 +123,171 @@
  * </pre>
  *
  * @author Craig R. McClanahan
+ * @author Michael Mainguy
  * @version $Revision: 1.13 $ $Date: 2003/10/09 20:43:15 $
  */
 
 public class ResultSetDynaClass extends JDBCDynaClass implements DynaClass {
 
 
-	// ----------------------------------------------------------- 
Constructors
+    // ----------------------------------------------------------- Constructors
 
 
-	/**
-	 * <p>Construct a new ResultSetDynaClass for the specified
-	 * <code>ResultSet</code>.  The property names corresponding
-	 * to column names in the result set will be lower cased.</p>
-	 *
-	 * @param resultSet The result set to be wrapped
-	 *
-	 * @exception NullPointerException if <code>resultSet</code>
-	 *  is <code>null</code>
-	 * @exception SQLException if the metadata for this result set
-	 *  cannot be introspected
-	 */
-	public ResultSetDynaClass(ResultSet resultSet) throws SQLException {
 
-		this(resultSet, true);
 
-	}
+    /**
+     * <p>Construct a new ResultSetDynaClass for the specified
+     * <code>ResultSet</code>.  The property names corresponding
+     * to column names in the result set will be lower cased.</p>
+     *
+     * @param resultSet The result set to be wrapped
+     *
+     * @exception NullPointerException if <code>resultSet</code>
+     *  is <code>null</code>
+     * @exception SQLException if the metadata for this result set
+     *  cannot be introspected
+     */
+    public ResultSetDynaClass(ResultSet resultSet) throws SQLException {
+
+        this(resultSet, true);
+
+    }
+
+
+    /**
+     * <p>Construct a new ResultSetDynaClass for the specified
+     * <code>ResultSet</code>.  The property names corresponding
+     * to the column names in the result set will be lower cased or not,
+     * depending on the specified <code>lowerCase</code> value.</p>
+     *
+     * <p><strong>WARNING</strong> - If you specify <code>false</code>
+     * for <code>lowerCase</code>, the returned property names will
+     * exactly match the column names returned by your JDBC driver.
+     * Because different drivers might return column names in different
+     * cases, the property names seen by your application will vary
+     * depending on which JDBC driver you are using.</p>
+     *
+     * @param resultSet The result set to be wrapped
+     * @param lowerCase Should property names be lower cased?
+     *
+     * @exception NullPointerException if <code>resultSet</code>
+     *  is <code>null</code>
+     * @exception SQLException if the metadata for this result set
+     *  cannot be introspected
+     */
+    public ResultSetDynaClass(ResultSet resultSet, boolean lowerCase)
+        throws SQLException {
+
+        if (resultSet == null) {
+            throw new NullPointerException();
+        }
+        this.resultSet = resultSet;
+        this.lowerCase = lowerCase;
+        introspect(resultSet);
+
+    }
+
 
+    // ----------------------------------------------------- Instance Variables
 
 	/**
 	 * <p>Construct a new ResultSetDynaClass for the specified
 	 * <code>ResultSet</code>.  The property names corresponding
-	 * to the column names in the result set will be lower cased or not,
-	 * depending on the specified <code>lowerCase</code> value.</p>
-	 *
-	 * <p><strong>WARNING</strong> - If you specify <code>false</code>
-	 * for <code>lowerCase</code>, the returned property names will
-	 * exactly match the column names returned by your JDBC driver.
-	 * Because different drivers might return column names in different
-	 * cases, the property names seen by your application will vary
-	 * depending on which JDBC driver you are using.</p>
+	 * will be case insensitive compared to the properties on the 
+	 * input dynaClass and mapped appropriately.</p>
 	 *
 	 * @param resultSet The result set to be wrapped
-	 * @param lowerCase Should property names be lower cased?
+	 * @param dynaClass The DynaClass containing the properties we want to 
map 
 	 *
 	 * @exception NullPointerException if <code>resultSet</code>
 	 *  is <code>null</code>
 	 * @exception SQLException if the metadata for this result set
 	 *  cannot be introspected
 	 */
-	public ResultSetDynaClass(ResultSet resultSet, boolean lowerCase)
-		throws SQLException {
+	public ResultSetDynaClass(ResultSet resultSet, DynaClass dynaClass) 
throws SQLException {
 
 		if (resultSet == null) {
 			throw new NullPointerException();
 		}
 		this.resultSet = resultSet;
-		this.lowerCase = lowerCase;
-		introspect(resultSet);
+		introspect(resultSet, dynaClass);
 
 	}
 
 
-	// ----------------------------------------------------- Instance 
Variables
+    /**
+     * Flag defining whether column names should be lower cased when
+     * converted to property names.
+     */
+    protected boolean lowerCase = true;
 
 
-	/**
-	 * Flag defining whether column names should be lower cased when
-	 * converted to property names.
-	 */
-	protected boolean lowerCase = true;
+    /**
+     * The set of dynamic properties that are part of this DynaClass.
+     */
+    protected DynaProperty properties[] = null;
 
 
-	/**
-	 * The set of dynamic properties that are part of this DynaClass.
-	 */
-	protected DynaProperty properties[] = null;
+    /**
+     * The set of dynamic properties that are part of this DynaClass,
+     * keyed by the property name.  Individual descriptor instances will
+     * be the same instances as those in the <code>properties</code> list.
+     */
+    protected HashMap propertiesMap = new HashMap();
 
 
-	/**
-	 * The set of dynamic properties that are part of this DynaClass,
-	 * keyed by the property name.  Individual descriptor instances will
-	 * be the same instances as those in the <code>properties</code> list.
-	 */
-	protected HashMap propertiesMap = new HashMap();
+    /**
+     * <p>The <code>ResultSet</code> we are wrapping.</p>
+     */
+    protected ResultSet resultSet = null;
 
 
-	/**
-	 * <p>The <code>ResultSet</code> we are wrapping.</p>
-	 */
-	protected ResultSet resultSet = null;
+    // --------------------------------------------------------- Public Methods
 
 
-	// --------------------------------------------------------- Public 
Methods
+    /**
+     * <p>Return an <code>Iterator</code> of {@link DynaBean} instances for
+     * each row of the wrapped <code>ResultSet</code>, in ""forward"" order.
+     * Unless the underlying result set supports scrolling, this method
+     * should be called only once.</p>
+     */
+    public Iterator iterator() {
 
+        return (new ResultSetIterator(this));
 
-	/**
-	 * <p>Return an <code>Iterator</code> of {@link DynaBean} instances for
-	 * each row of the wrapped <code>ResultSet</code>, in ""forward"" order.
-	 * Unless the underlying result set supports scrolling, this method
-	 * should be called only once.</p>
-	 */
-	public Iterator iterator() {
+    }
 
-		return (new ResultSetIterator(this));
 
-	}
+    // -------------------------------------------------------- Package Methods
 
 
-	// -------------------------------------------------------- Package 
Methods
+    /**
+     * <p>Return the result set we are wrapping.</p>
+     */
+    ResultSet getResultSet() {
 
+        return (this.resultSet);
 
-	/**
-	 * <p>Return the result set we are wrapping.</p>
-	 */
-	ResultSet getResultSet() {
-
-		return (this.resultSet);
-
-	}
+    }
 
 
-	// ------------------------------------------------------ Protected 
Methods
+    // ------------------------------------------------------ Protected Methods
     
-	/**
-	 * <p>Loads the class of the given name which by default uses the class 
loader used 
-	 * to load this library.
-	 * Dervations of this class could implement alternative class loading 
policies such as
-	 * using custom ClassLoader or using the Threads's context class loader 
etc.
-	 * </p>
-	 */        
-	protected Class loadClass(String className) throws SQLException {
-
-		try {
-			return getClass().getClassLoader().loadClass(className);
-		} 
-		catch (Exception e) {
-			throw new SQLException(""Cannot load column class '"" +
-								   className 
+ ""': "" + e);
-		}
-	}
-}
-
+    /**
+     * <p>Loads the class of the given name which by default uses the class 
loader used 
+     * to load this library.
+     * Dervations of this class could implement alternative class loading 
policies such as
+     * using custom ClassLoader or using the Threads's context class loader 
etc.
+     * </p>
+     */        
+    protected Class loadClass(String className) throws SQLException {
+
+        try {
+            return getClass().getClassLoader().loadClass(className);
+        } 
+        catch (Exception e) {
+            throw new SQLException(""Cannot load column class '"" +
+                                   className + ""': "" + e);
+        }
+    }
+}
\ No newline at end of file",smartshark_2_2,387,commons-beanutils,"""[0.9984558820724487, 0.001544110826216638]"""
570,228829,[beanutils] Dynabeans are not cloneable,"Currently, calling clone() on any DynaBean implementation raises a
CloneNotSupportedException.

As property holders, it would be convenient that DynaBeans support be Cloneable. 

A usage example of such cloneability is the need for memorizing previous bean's
states, either for logging or transactional (memory rollback) purposes.",smartshark_2_2,418,commons-beanutils,"""[0.9982819557189941, 0.0017180705908685923]"""
571,228830,BeanUtils.populate does not skip zero lenght String,"Whi Beanutils populate method does not skip zero lenght string? 
I i have a Bean with a property of Type BigDecimal and i don't want to have a 
BigDecimalConverter default value, when I submit a Form , leaving blank the 
input, i have an exception because BigDecimalConverter can't conver """" to a 
BigDecimal. 
The Requestprocessor calls the populate funcion  before the validate function 
and so i can't validate my property (required). All we be work correct if 
beanUtils.populate skip zero-lenght string if Class is not a String",smartshark_2_2,191,commons-beanutils,"""[0.6165152788162231, 0.3834846615791321]"""
572,228831,[beanutils] MethodUtils.invoke for static methods,"I modified the MethodUtils class and added 6 new methods to invoke static 
methods.

invokeExactMethod (Class, String, Object [])
invokeExactMethod (Class, String, Object [], Class [])
invokeExactMethod (Class, String, Object)
invokeMethod (Class, String, Object [])
invokeMethod (Class, String, Object [], Class [])
invokeMethod (Class, String, Object)",smartshark_2_2,19,commons-beanutils,"""[0.9983415603637695, 0.0016583942342549562]"""
573,228832,copyProperties should have optional flag for including transient member variables,"The EqualsBuilder, HashCodeBuilder, and ToStringBuilder all have a flag for
allowing you to use only the non-transient member variables on an object.  The
BeanUtils.copyProperties method should have a similar such variable (which can
default to false) so you can designate only some variables to copy across domains.",smartshark_2_2,373,commons-beanutils,"""[0.9984978437423706, 0.001502111554145813]"""
574,228833,Remove copied Collections classes,"In BeanUtils 1.7 four classes were copied from Commons Collections with the aim of removing that dependency:
    * ArrayStack
    * Buffer
    * BufferUnderflowException
    * FastHashMap

However there are other classes within BeanUtils that depend on Commons Collections (e.g. BeanMap)  - so three flavours of jars were created:
    * commons-beanutils.jar (all classes)
    * commons-beanutils-core.jar (core beantils which doesn't require Commons Collections)
    * commons-beanutils-bean-collections.jar (classes dependant on Commons Collections)

This situation is a bit of a mess an is causing issues such as BEANUTILS-378

I suggest the following action is taken in the next 1.x BeanUtils release:
    * remove the copied Commons Collections classes
    * add Commons Collections as a dependency
    * Only produce the one commons-beanutils.jar

Removing the dependency on the four collections classes is more difficult as some of them are in BeanUtils API",smartshark_2_2,330,commons-beanutils,"""[0.9983212351799011, 0.001678720349445939]"""
575,228834,[beanutils] LazyDynaBean and LazyDynaClass,"I developed ""Lazy"" implementations of DynaBean and DynaClass and they have been 
available for people to download from my web site for a while - would there be 
any interest in having them in beanutils?

LazyDynaClass is an implementation of the MutableDynaClass interface that 
allows the properties of a DynaClass to be changed.

LazyDynaBean must be used in conjunction with a MutableDynaClass and 
automatically adds properties when its set() methods are called if the property 
doesn't exist. The versions available on my web site only dealt with ""simple"" 
properties, but I now have a version that caters for indexed and mapped 
properties as well.

This version automatically instantiates a Map when a mapped property is set 
that doesn't exist.

It also instantiates either a List or Array when an indexed property is set 
that doesn't exist and automatically ""grows"" the List or Array so that it is 
big enough to cater for the index being set.

Quite a few people seem to be using them now and I've had good feedback.

I'm not sure of the Commons ""etiquette"" here - I was recently voted a Commons 
committer for the Validation project - but I guess I don't have karma for 
beanutils and even if I have, shouldn't just plough in adding stuff without the 
existing Beanutils commiters say so?

Niall",smartshark_2_2,177,commons-beanutils,"""[0.9984741806983948, 0.0015258495695888996]"""
576,228835,"BeanComparator(String, Comparator) should check the comparator for null and default to ComparableComparator.getInstance()","
The way the BeanComparator(String, Comparator) constructor is implemented is inconvenient.  I've got code that passes in a comparator.  This comparator may be null.  I assumed that the 2-args constructor would sanely ignore a null comparator argument and use a default like ReverseComparator does in commons-collections, but alas, no.  I have to do the null check before I pass it in  

For instance, here's the constructor for ReverseComparator, which takes a Comparator argument...

    public ReverseComparator(Comparator comparator) {
        if(comparator != null) {
            this.comparator = comparator;
        } else {
            this.comparator = ComparableComparator.getInstance();
        }
    }

The null check and provided default is convenient and reasonable.  

Here's the current constructor for BeanComparator that can only end in a NullPointerException if provided  null comparator....

    public BeanComparator( String property, Comparator comparator ) {
        setProperty( property );
        this.comparator = comparator;
    }

Why not?....

    public BeanComparator( String property, Comparator comparator ) {
        setProperty( property );
        if(comparator != null) {
            this.comparator = comparator;
        } else {
            this.comparator = ComparableComparator.getInstance();
        }
    }

The fact that BeanComparator allows itself to be put in a bad state by storing a null comparator which it later tries to use with no null check, guaranteeing a NullPointerException, probably should be considered a bug.  However, since it works just fine when provided a non-null comparator, I consider this more of an ""Improvement"" opportunity than a bug, thus the reported Issue Type.  Hopefully this can be applied to the next release.


Jake",smartshark_2_2,219,commons-beanutils,"""[0.9713088870048523, 0.0286911278963089]"""
577,228836,Don't try parsing values that are already Dates/Numbers in Date/Number locale Converters,"DateLocaleConverter shouldn't try and parse values that are already Date or Calendar objects
DecimalLocaleConverter shouldn't try and parse values that are alreadyjava.lang.Number objects",smartshark_2_2,230,commons-beanutils,"""[0.9600009322166443, 0.03999911993741989]"""
578,228837,Handling exceptions during BeanUtils.populate(),"Hi,

I know this has been asked already before but could there be a way to handle 
exceptions that occur during population? The populate() function could either 
return a map(property, exception), take that kind of map as argument or -even 
better- take a PopulateExceptionHandler as argument.

The reason I would like to see this feature implemented is to allow struts to 
use this mechanism to convert parameters from the request to actionform's 
properties without *falling apart* when encountering one that is not well-
formed. 

It would be nice too if we were not *forced* to use string-only properties for 
actionforms (which in fact is a way to circumvent this conversion problem). I 
would like my ActionForm or DynaActionForm declare strongly-typed properties 
(maybe custom classes), register proper Converters into ConvertUtils in the 
ActionServlet.initServlet() for example, and then maybe get back conversion 
errors from within my action (maybe the PopulateExceptionHandler could add 
some ActionErrors to the request).

What do you think? I know this issue is tightly coupled to struts but 
well... ;-)

Thanks a lot,

Xavier",smartshark_2_2,389,commons-beanutils,"""[0.9983944296836853, 0.0016055515734478831]"""
579,228838,Upgrade from Apache Commons Collections 3 to 4,"uptake commons-collections4.

The main difference is the removal of 'FastHashMap', and replacement with ConcurrentHashMap.

There are a few breaking changes for deprecated methods that return a FastHashMap, that are exposed.

I made them package private, and undeprecated them, thus 2.0",smartshark_2_2,505,commons-beanutils,"""[0.9982411861419678, 0.0017588069895282388]"""
580,228839,java.util.List to be permitted for indexed properties.,"Indexed properties are of the following formats as per the spec.

public Object getIndexedProperty(int i) {}
or 
public Object[] getIndexedProperty() {}

It's a big help when using indexed properties to be able to return an
implementation of java.util.List rather than return an array primitive all the time.
The BeanUtils package already uses implementations of Map, so using List
implementations isn't stepping out of line.
To fix this, open PropertyUtils.java and replace lines 418 to 423 with the code
below (line numbers are inclusive, and as of 30th Dec. 2001)...

Code start --==>>

   // Call the property getter and return the value
   Object value = readMethod.invoke(bean, new Object[0]);
   if (!value.getClass().isArray()) {
     if (!(value instanceof java.util.List)) {
       throw new IllegalArgumentException(""Property '"" + name
         + ""' is not indexed"");
     } else {
       return ((java.util.List)value).get(index);
     }
   }
   return (Array.get(value, index));

<<==-- code finish. 
If the code has changed since then, it's at the end of the getIndexedProperty
method, the comment and the next line in the code block above is the same.",smartshark_2_2,171,commons-beanutils,"""[0.9984285235404968, 0.0015714924084022641]"""
581,228840,Copy Properties with source and destination attribute mapping,"At present copyProperties are copying attributes when name matches. In most of the conversions, its required map source attribute name with destination attribute name. It will be really helpful for developers if we can provide this functionality. copyProperties can take either map as input or xml file name as input and by using that map it can copy properties to destination object",smartshark_2_2,345,commons-beanutils,"""[0.9984904527664185, 0.001509477267973125]"""
582,228841,[beanutils] defaultTransformers is public in BeanMap class,"The BeanMap class in the ""optional"" section has:
    public static HashMap defaultTransformers = new HashMap();

I think this is a *really* bad idea. 

Firstly, because it's static, various parts of an application can interact in
unexpected ways. For example, some library your code is calling might,
unexpectedly, add a transformer to the default transformers causing surprising
effects.

That's even more interesting if the library (commons-collections or
commons-beanutils) is deployed via a shared webapp in a container. In that case,
one webapp can have side-effects on other webapps.

And because it's a public member, there is no way to control access to this field.

This class was in commons-collections since 1.0. It was copied into
commons-beanutils before the 1.7.0 release, and deprecated in commons-collections.

This class is only in the ""optional"" section, and is not used by the core
beanutils code.",smartshark_2_2,290,commons-beanutils,"""[0.9912159442901611, 0.008784031495451927]"""
583,228842,Encoding data using Base64OutputStream omits part of the input in some cases,"In case Base64OutputStream is used for encoding byte array produced from some strings, an incomplete result (that is inconsistent with both other Base64 implementations and Base64InputStream implementation) is produced. It appears that ""s"" character is getting omitted if it's in the end of the string to be encoded; there may be other cases as well.

Here is the test that allows to reproduce the problem: http://kiberion.net/kibertoad/temp/codec-base64-error.zip",smartshark_2_2,181,commons-codec,"""[0.12963785231113434, 0.8703621625900269]"""
584,228843,"StringUtils.newStringxxx(null) should return null, not NPE","Method calls such as StringUtils.newStringIso8859_1(null) should return null, not NPE.

It looks like this capability was lost with the fix for CODEC-136, i.e.
http://svn.apache.org/viewvc?rev=1306366&view=rev

Several methods were changed from

{code}
return StringUtils.newString(bytes, CharEncoding.xxx);
to
return new String(bytes, Charsets.xxx);
{code}

The new code should have been:

{code}
return newString(bytes, Charsets.xxx);
{code}

The newString method handles null input.

There were no tests for null input so the change in behaviour was missed.
",smartshark_2_2,227,commons-codec,"""[0.44792699813842773, 0.5520730018615723]"""
585,228844,RefinedSoundex creates instance before al fields have been initialized,"The RefinedSoundex code creates an instance of itself in the variable US_ENGLISH; however this appears before some of the other static final variables.

The variable US_ENGLISH needs to be moved after the other variables.

See patch to follow.",smartshark_2_2,40,commons-codec,"""[0.12152568250894547, 0.8784743547439575]"""
586,228845,Allow the build to run with Maven 2 and Maven 3 with commons-parent 17,Allow the build to run with Maven 2 and Maven 3,smartshark_2_2,110,commons-codec,"""[0.9954959154129028, 0.004504078533500433]"""
587,228846,"Use Charset objects when possible, create Charsets class for required character encodings","Use Charset objects when possible, create Charsets for required character encodings.",smartshark_2_2,226,commons-codec,"""[0.998101532459259, 0.0018984816269949079]"""
588,228847,Character set used by Base64 not documented,"The Javadoc for the Base64 class does not document which character set is returned by encode() and expected by decode(). The RFC specifies ""characters"", not ""bytes"" as the result of the encoding, and yet Base64 returns bytes. It should provide complete information as to how to convert these bytes to and from Strings. I assume the character set used is ASCII, but that should be made explicit in the Javadoc.",smartshark_2_2,119,commons-codec,"""[0.990880012512207, 0.00911997351795435]"""
589,228848,Enhance documentation for Language Encoders,"The current userguide (http://commons.apache.org/codec/userguide.html) just lists four Language Encoders, but there are five at the moment. CODEC-106 implements a sixth one.
Would be a good idea, to complete documentation.

Additionally, I suggest to extent the userguide in order to show a simple performance measurement:

_SNIP_

org.apache.commons.codec.language.Metaphone encodings per msec: 327
org.apache.commons.codec.language.DoubleMetaphone encodings per msec: 224
org.apache.commons.codec.language.Soundex encodings per msec: 904
org.apache.commons.codec.language.RefinedSoundex encodings per msec: 637
org.apache.commons.codec.language.Caverphone encodings per msec: 5
org.apache.commons.codec.language.ColognePhonetic encodings per msec: 289

So, Soundex is the fastest encoder. Caverphone is much slower than any other algorithm. All others show off nearly the same performance.

Checked with the following code:

{code:java}
  private static final int REPEATS = 1000000;

  public void checkSpeed() throws Exception {
	  checkSpeedEncoding(new Metaphone(), ""easgasg"", REPEATS);
	  checkSpeedEncoding(new DoubleMetaphone(), ""easgasg"", REPEATS);
	  checkSpeedEncoding(new Soundex(), ""easgasg"", REPEATS);
	  checkSpeedEncoding(new RefinedSoundex(), ""easgasg"", REPEATS);
	  checkSpeedEncoding(new Caverphone(), ""Carlene"", 100000);
	  checkSpeedEncoding(new ColognePhonetic(), ""Schmitt"", REPEATS);
  }
  
  private void checkSpeedEncoding(Encoder encoder, String toBeEncoded, int repeats) throws Exception {
	  long start = System.currentTimeMillis();
	  for ( int i=0; i<repeats; i++) {
		    encoder.encode(toBeEncoded);
	  }
	  long duration = System.currentTimeMillis()-start;
	  System.out.println(encoder.getClass().getName() + "" encodings per msec: ""+(repeats/duration));
  }
{code}

_SNAP_",smartshark_2_2,85,commons-codec,"""[0.9984147548675537, 0.001585205551236868]"""
590,228849,[Codec] Add MD5 and SHA digest algorithms,"MD5 and SHA encryption algorithms might be useful.  I noticed that Turbine has
some of this, but I ended up rolling my own.",smartshark_2_2,0,commons-codec,"""[0.9982813596725464, 0.001718673505820334]"""
591,228850,Remove deprecated code for 2.0.,Remove deprecated code for 2.0.,smartshark_2_2,98,commons-codec,"""[0.9981012940406799, 0.001898728427477181]"""
592,228851,DoubleMetaphone.maxCodeLen should probably be private,DoubleMetaphone.maxCodeLen should probably be private - it has public getter and setter anyway.,smartshark_2_2,104,commons-codec,"""[0.998389482498169, 0.001610553590580821]"""
593,228852,Base64.encodeBase64String could better use newStringUsAscii,"{{org.apache.commons.codec.binary.Base64.encodeBase64String}} currently uses {{StringUtils.newStringUtf8}}. But the text should be in fact be in ASCII, so {{newStringUsAscii}} would be clearer and perhaps faster.",smartshark_2_2,171,commons-codec,"""[0.9982727766036987, 0.0017271682154387236]"""
594,228853,DigestUtils: Add MD2 APIs,Add MD2 support. It is a standard algorithm per http://docs.oracle.com/javase/6/docs/technotes/guides/security/StandardNames.html,smartshark_2_2,135,commons-codec,"""[0.9983816146850586, 0.0016183300176635385]"""
595,228854,2 Test failures in SoundexTest,"Testsuite: org.apache.commons.codec.language.SoundexTest
Tests run: 25, Failures: 2, Errors: 0, Time elapsed: 0.907 sec

Testcase: testUsMappingOWithDiaeresis(org.apache.commons.codec.language.SoundexTest):   FAILED
expected:<?000> but was:<>
junit.framework.ComparisonFailure: expected:<?000> but was:<>
        at org.apache.commons.codec.language.SoundexTest.testUsMappingOWithDiaeresis(SoundexTest.java:349)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


Testcase: testUsMappingEWithAcute(org.apache.commons.codec.language.SoundexTest):       FAILED
expected:<?000> but was:<>
junit.framework.ComparisonFailure: expected:<?000> but was:<>
        at org.apache.commons.codec.language.SoundexTest.testUsMappingEWithAcute(SoundexTest.java:364)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",smartshark_2_2,43,commons-codec,"""[0.9544565677642822, 0.04554349556565285]"""
596,228855,Typo in DecoderException message thrown from Hex.decodeHex,"There is a typo in the exception message thrown when the string to be decoded in Hex.decodeHex() has invalid characters:

Illegal hexadecimal charcter J at index 2

The word ""character"" is misspelled.",smartshark_2_2,79,commons-codec,"""[0.9861828088760376, 0.013817192986607552]"""
597,228856,Support JEP 287: SHA-3 Hash Algorithms,"Support JEP 287: SHA-3 Hash Algorithms: SHA3-224, SHA3-256, SHA3-384, SHA3-512.",smartshark_2_2,223,commons-codec,"""[0.9981428384780884, 0.0018571217078715563]"""
598,228857,Complete FilterInputStream interface for BaseNCodecInputStream,"Small patch to implement mark and reset in a safe manner.

markSupported is already implemented, but the other two methods are inherited from the default FilterInputStream implementation, which calls the corresponding methods of the underlying stream. The patch provides a noop implementation for mark, and throws an IOException when reset is called.",smartshark_2_2,122,commons-codec,"""[0.9984643459320068, 0.0015357292722910643]"""
599,228858,Charsets Javadoc breaks build when using Java 8,"Some Javadocs in {{Charsets}} use a period instead of a hash to separate class and field in {{@link}} references.

Specifically:

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.598 s
[INFO] Finished at: 2016-05-13T13:33:29-07:00
[INFO] Final Memory: 67M/658M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.4:site (default-site) on project commons-codec: Error generating maven-javadoc-plugin:2.10.3:javadoc:
[ERROR] Exit code: 1 - E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:96: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.ISO_8859_1} instead
[ERROR] ^
[ERROR] E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:107: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.US_ASCII} instead
[ERROR] ^
[ERROR] E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:119: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.UTF_16} instead
[ERROR] ^
[ERROR] E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:130: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.UTF_16BE} instead
[ERROR] ^
[ERROR] E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:141: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.UTF_16LE} instead
[ERROR] ^
[ERROR] E:\vcs\svn\apache\commons\trunks-proper\codec\src\main\java\org\apache\commons\codec\Charsets.java:152: error: reference not found
[ERROR] * @deprecated Use Java 7's {@link java.nio.charset.StandardCharsets.UTF_8}
[ERROR] ^
[ERROR]
[ERROR] Command line was: ""C:\Program Files\Java\jdk1.8.0_91\jre\..\bin\javadoc.exe"" @options @packages
[ERROR]
[ERROR] Refer to the generated Javadoc files in 'E:\vcs\svn\apache\commons\trunks-proper\codec\target\site\apidocs' dir.
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{noformat}",smartshark_2_2,198,commons-codec,"""[0.9786842465400696, 0.021315764635801315]"""
600,228859,"Add convenience API org.apache.commons.codec.binary.Hex.encodeHexString(byte[]|ByteBuffer, boolean)","Add convenience APIs:

{{org.apache.commons.codec.binary.Hex.encodeHexString(byte[], boolean)}}:

{code:java}
    /**
     * Converts an array of bytes into a String representing the hexadecimal values of each byte in order. The returned
     * String will be double the length of the passed array, as it takes two characters to represent any given byte.
     *
     * @param data
     *            a byte[] to convert to Hex characters
     * @param toLowerCase
     *            <code>true</code> converts to lowercase, <code>false</code> to uppercase
     * @return A String containing lower-case hexadecimal characters
     * @since 1.11
     */
    public static String encodeHexString(final byte[] data, boolean toLowerCase)
{code}

{{org.apache.commons.codec.binary.Hex.encodeHexString(ByteBuffer, boolean)}}:

{code:java}
    /**
     * Converts a byte buffer into a String representing the hexadecimal values of each byte in order. The returned
     * String will be double the length of the passed array, as it takes two characters to represent any given byte.
     *
     * @param data
     *            a byte buffer to convert to Hex characters
     * @param toLowerCase
     *            <code>true</code> converts to lowercase, <code>false</code> to uppercase
     * @return A String containing lower-case hexadecimal characters
     * @since 1.11
     */
    public static String encodeHexString(final ByteBuffer data, boolean toLowerCase)
{code}

",smartshark_2_2,238,commons-codec,"""[0.9985155463218689, 0.0014843958197161555]"""
601,228860,DualTreeBidiMap.readObject() uses wrong comparator to create reverseMap,"DualTreeBidiMap.readObject() uses the wrong comparator to create reverseMap. The code reads:

bq. reverseMap = new TreeMap(comparator);

it should read:

bq. reverseMap = new TreeMap(valueComparator);

Note: this was found when trying to fix generics warnings.",smartshark_2_2,508,commons-collections,"""[0.0630679652094841, 0.9369320273399353]"""
602,228861,"TreeBag allows uncomparable item to be added, breaking toString","The following code throws an exception not when the Object is added, but when toString is called:

		TreeBag bag = new TreeBag();
		bag.add(new Object());
		bag.toString();

Trace:

java.lang.ClassCastException: java.lang.Object
	at java.util.TreeMap.compare(TreeMap.java:1093)
	at java.util.TreeMap.getEntry(TreeMap.java:347)
	at java.util.TreeMap.get(TreeMap.java:265)
	at org.apache.commons.collections.bag.AbstractMapBag.getCount(AbstractMapBag.java:116)
	at org.apache.commons.collections.bag.AbstractMapBag.toString(AbstractMapBag.java:581)
[...]

In a client program, toString should never throw an exception--it makes debugging much harder, for one thing.  I believe that TreeBag should defend against the addition of uncomparable objects, so that toString will never throw an exception.",smartshark_2_2,349,commons-collections,"""[0.09294305741786957, 0.907056987285614]"""
603,228862,equals/hashCode mismatch,"We used Randoop on the collection classes, which found several test cases where two objects are equal but their hash code differs.

I will attach a file containing two test cases that are different; the other tests seem to be longer versions showing the same issue.",smartshark_2_2,495,commons-collections,"""[0.1947287619113922, 0.8052712678909302]"""
604,228863,SetUniqueList may become inconsistent,"I found this bug during my work on issue COLLECTIONS-310 : 

When you 'set' an element to a position that contains this element, it is removed from the internal set. This leads to the situation that 
- invocing get() returns the element
- invocing contains() returns false.

Extending the existing test method for set:
{code}
   public void testSet() {
        final SetUniqueList<E> lset = new SetUniqueList<E>(new ArrayList<E>(), new HashSet<E>());

        // Duplicate element
        final E obj1 = (E) new Integer(1);
        final E obj2 = (E) new Integer(2);
        final E obj3 = (E) new Integer(3);

        lset.add(obj1);
        lset.add(obj2);
        lset.set(0, obj1);
        assertEquals(2, lset.size());
        assertSame(obj1, lset.get(0));
        assertSame(obj2, lset.get(1));

        assertTrue(lset.contains(obj1));  // fails !
        assertTrue(lset.contains(obj2));

{code}


",smartshark_2_2,301,commons-collections,"""[0.07919952273368835, 0.9208004474639893]"""
605,228864,PatriciaTrie bugs when only a few bits change,"I have a bug report for you, for the class AbstractPatriciaTrie.  
It has to do with how you handle bits when they are very close to each other.  
For example, some of your methods seem to think that if the only difference between a prefix and a longer string, is a single additional bit, then they are actually the same data.  Or if the only difference is some number of zero bits, then it also thinks they are the same data.  
There are also MANY situations where the prefixMap does not return all the strings that start with the prefix.

Can you also make AbstractPatriciaTrie public, and your other package level methods into protected level, that way I don't have to copy the entire class and subclasse's code out into another class just to extend it?

thank you,
Chris Duncan (github user: VEQRYN)
",smartshark_2_2,529,commons-collections,"""[0.06635531783103943, 0.933644711971283]"""
606,228865,[collections] Map that returns a default value if key is not present,"By definition if a map does not contain a determined key it returns null. But 
sometimes may be desirable that it returns a not-null value for not present 
keys. My propose is create a Decorated Map that extends 
org.apache.commons.collections.map.AbstractMapDecorator class. In constructor 
it receives the original Map and a default value to be returned is for a 
inexistent key. This class will override get() method like this:

public Object get(Object key) {
	Object value = super.get(key);
	if(value == null) {
		value = this.defaultValue;
	}
	return value;
}",smartshark_2_2,195,commons-collections,"""[0.9924843907356262, 0.00751556595787406]"""
607,228866,[collections] [REF] TransformedPredicate,"There is predicate that tranforme its input before calling another predicate. 
If I write proper javadoc and test case, could it be included in commons
collections.",smartshark_2_2,137,commons-collections,"""[0.9983737468719482, 0.0016262618592008948]"""
608,228867,Satisfies utility method,"I recently needed to use something like the CollectionUtils.exists method, but I wanted to know if a given predicate was true for ALL members of a collection, rather than just one.  I cooked up a quick method called satisfies (help would be appreciated on the name - it's the best that I could come up with) that determines whether a given predicate is true for all members of a collection.

Been using this library a good deal recently - hoping to get involved in its development!",smartshark_2_2,266,commons-collections,"""[0.9984682202339172, 0.001531711663119495]"""
609,228868,TransformerClosure should implement Transformer and allow extension.,"TransformerClosure currently decorates a transformer.  However, in the interests of non-verbose code, it makes sense to be able to subclass it for a class that implements both interfaces.

I propose the addition of the following constructor and method

protected TransformerClosure(){
     iTransformer = this;
}

public O transform(I in){
    return iTransformer.transform();
}",smartshark_2_2,239,commons-collections,"""[0.9983918070793152, 0.0016081470530480146]"""
610,228869,Undocumented performance issue in the removeAll method in CollectionUtils,"This bug is analogous to https://issues.apache.org/jira/browse/COLLECTIONS-544

The method removeAll in CollectionUtils is inefficient when the second parameter collection has a slow containment method.

The following is the current implementation with its documentation:
============================
     /**
     * Removes the elements in <code>remove</code> from <code>collection</code>. That is, this
     * method returns a collection containing all the elements in <code>c</code>
     * that are not in <code>remove</code>. The cardinality of an element <code>e</code>
     * in the returned collection is the same as the cardinality of <code>e</code>
     * in <code>collection</code> unless <code>remove</code> contains <code>e</code>, in which
     * case the cardinality is zero. This method is useful if you do not wish to modify
     * the collection <code>c</code> and thus cannot call <code>collection.removeAll(remove);</code>.
     *
     * @param <E>  the type of object the {@link Collection} contains
     * @param collection  the collection from which items are removed (in the returned collection)
     * @param remove  the items to be removed from the returned <code>collection</code>
     * @return a <code>Collection</code> containing all the elements of <code>collection</code> except
     * any elements that also occur in <code>remove</code>.
     * @throws NullPointerException if either parameter is null
     * @since 4.0 (method existed in 3.2 but was completely broken)
     */
    public static <E> Collection<E> removeAll(final Collection<E> collection, final Collection<?> remove) {
        return ListUtils.removeAll(collection, remove);
    }


=======================================

We can notice the inefficiency by looking at the removeAll method in ListUtils.
The removeAll method from ListUtils is implemented and documented as follows:

=======================================

     /**
     * Removes the elements in <code>remove</code> from <code>collection</code>. That is, this
     * method returns a list containing all the elements in <code>collection</code>
     * that are not in <code>remove</code>. The cardinality of an element <code>e</code>
     * in the returned collection is the same as the cardinality of <code>e</code>
     * in <code>collection</code> unless <code>remove</code> contains <code>e</code>, in which
     * case the cardinality is zero. This method is useful if you do not wish to modify
     * <code>collection</code> and thus cannot call <code>collection.removeAll(remove);</code>.
     * <p>
     * This implementation iterates over <code>collection</code>, checking each element in
     * turn to see if it's contained in <code>remove</code>. If it's not contained, it's added
     * to the returned list. As a consequence, it is advised to use a collection type for
     * <code>remove</code> that provides a fast (e.g. O(1)) implementation of
     * {@link Collection#contains(Object)}.
     *
     * @param <E>  the element type
     * @param collection  the collection from which items are removed (in the returned collection)
     * @param remove  the items to be removed from the returned <code>collection</code>
     * @return a <code>List</code> containing all the elements of <code>c</code> except
     * any elements that also occur in <code>remove</code>.
     * @throws NullPointerException if either parameter is null
     * @since 3.2
     */
    public static <E> List<E> removeAll(final Collection<E> collection, final Collection<?> remove) {
        final List<E> list = new ArrayList<E>();
        for (final E obj : collection) {
            if (!remove.contains(obj)) {
                list.add(obj);
            }
        }
        return list;
    }

=======================================

In the case of ListUtils:removeAll, the inefficiency is properly documented.

Perhaps the disclaimer about potential inefficiencies depending on the type 
of the parameter collection in ListUtils:removeAll should also be included in CollectionUtils:removeAll.",smartshark_2_2,487,commons-collections,"""[0.9805561900138855, 0.019443802535533905]"""
611,228870,[collections] Implement a BoundedBuffer class,"Please implement a BoundedBuffer wrapper class which makes another buffer bound
(maximum size).  An optional timeout value can be supplied which allows it to
wait until an item can be added to the enclosed Buffer.",smartshark_2_2,159,commons-collections,"""[0.9983910322189331, 0.0016089949058368802]"""
612,228871,Satisfies CollectionUtils Method,"I recently needed to use something like the CollectionUtils.exists method, but I wanted to know if a given predicate was true for ALL members of a collection, rather than just one.  I cooked up a quick method called satisfies (help would be appreciated on the name - it's the best that I could come up with) that determines whether a given predicate is true for all members of a collection.

Been using this library a good deal recently - hoping to get involved in its development!",smartshark_2_2,264,commons-collections,"""[0.9984672665596008, 0.0015326555585488677]"""
613,228872,Proposal for a new List implementation: BackedList,"Here is a new List implementation named BackedList, to use for transparently accessing values of a generic data source as it were a normal List.
This List implementation dynamically accesses data source values through a proper strategy object, keeping in memory only a limited, configurable, buffer of values.
So, it is useful for:
1) Providing a List-like view of different data source values.
2) Avoiding to pre-load all values in a standard list, providing better performances and less memory consumption.

As a side note, this list implementation is already used in production for implementing pagination over a relational data source.",smartshark_2_2,534,commons-collections,"""[0.998299777507782, 0.0017002788372337818]"""
614,228873,Easey Factory for ListUtils.lazyList() method,"I created this class to cut down on having to write a class Factory for every 
class object i wanted to create a LazyList for. It uses reflection to create 
the object rather than a hardcoded class. The syntax for its' usage is as 
follows: List myList = ListUtils.lazyList(new ArrayList(),new GenericFactory
(""com.foo.MyClass"")); This calls the GenericFactory which will be used to pass 
back com.foo.MyClass objects to the calling method. Please visit the about url 
to download it. It should be ready to plug right in to commons. I have compiled 
it into the nightly build and it works fine. For some reason the docs didn't 
generate. But, i wasn't sure if that was an issue of the Ant script or my 
javadoc. Anyways i hope to see this soon in the nightly. Also, I hope this is 
the right place to submit this.",smartshark_2_2,155,commons-collections,"""[0.9983464479446411, 0.0016535460017621517]"""
615,228874,Remove deprecated classes from generics version,The generics version should have no deprecations,smartshark_2_2,426,commons-collections,"""[0.9967261552810669, 0.0032738458830863237]"""
616,228875,Proposal for change in MultiHashMap,"Current implementation of  MultiHashMap has one problem with notion of equality.
Namely, when for some key some value is added, and then removed from MultiMap,
and this value was only value, that was put for this key
than multimap wouldn't be equal to other multimap with same values of key/value
pairs, in which this new value was not added.

This can be demonstrated by following test (can be added to
TestMultiHashMap.testMapEquals):

public void testMapEquals() {
       MultiHashMap one = new MultiHashMap();
       Integer value = new Integer(1);
       one.put(""One"", value);
       one.remove(""One"", value);

       MultiHashMap two = new MultiHashMap();
       assertEquals(two, one);
   }

Suggested fix for this problem is the following change in code of
MultiHashMap.remove(Object key, Object value).

public Object remove( Object key, Object item )
   {
       ArrayList valuesForKey = (ArrayList) super.get( key );
             if ( valuesForKey == null )
           return null;
           valuesForKey.remove( item );
//start of change
       if(valuesForKey.isEmpty()){
           remove(key);
       }
//end of change
       return item;
   }",smartshark_2_2,140,commons-collections,"""[0.992228090763092, 0.007771867793053389]"""
617,228876,Replace use of deprecated Class#newInstance() #49,"Java 9 deprecates [SetUniqueList] Class#newInstance().

Fix two call sites:

- {{org.apache.commons.collections4.map.MultiValueMap.ReflectionFactory.create()}}
- {{org.apache.commons.collections4.list.SetUniqueList.createSetBasedOnList(Set<E>, List<E>)}}

The patch https://github.com/apache/commons-collections/pull/49 is incomplete and incorrect:

- Fix to {{org.apache.commons.collections4.map.MultiValueMap.ReflectionFactory.create()}} is missing
- Patch to {{org.apache.commons.collections4.list.SetUniqueList.createSetBasedOnList(Set<E>, List<E>)}} performs extra work by adding the initial values to the new set twice.

Â ",smartshark_2_2,648,commons-collections,"""[0.9980078339576721, 0.0019922414794564247]"""
618,228877,Unused variable in TreeBidiMap.java,"Twice in TreeBidiMap there is an entrySet variable that is not used. Rather the entrySet() method returns a new TreeView every time.

We should either:

a) Delete the variable.
b) Use the variable and always return the same TreeView.

I'm thinking a).

[Found via http://opensource.fortifysoftware.com/ ]",smartshark_2_2,422,commons-collections,"""[0.9972033500671387, 0.002796669490635395]"""
619,228878,commons-collections integrated into OpenSolaris,"I have integrated commons-collections version 3.2.1 into Solaris Nevada build 119

The code review webrev is located here:
http://cr.opensolaris.org/~bjwancn/commons-collections/

The integrated code is here:
http://src.opensolaris.org/source/xref/sfw/usr/src/lib/commons-collections/
http://src.opensolaris.org/source/xref/sfw/usr/src/pkgdefs/SUNWcommons-collections/

I have also added a man page for this package, it's in the package too.

commons-collections is one of the standard jar library available on Solaris now.

You can get an OpenSolaris IPS package for commons-collections at:
http://pkg.opensolaris.org/dev/en/index.shtml

Can somebody can help to add the link to the commons project download page for those users who want to use it in OpenSolaris. Thanks very much!",smartshark_2_2,237,commons-collections,"""[0.998019814491272, 0.0019801603630185127]"""
620,228879,[contribution] PassiveTimeOutMap,"This is a Map decorator which passively evicts expired keys once their expiry time has been reached.

When putting a key-value pair in the map, this decorator calls expiryTime(key, value), passing the key and the value as parameters, and uses the returned value as the expiry time for that key.

When getting the value for a key, its expiry time is checked, and if it's greater than the current time, the value is returned. Otherwise, the key is removed from the decorated map, and null is returned.

Doing so, there's no need to have a separate, active thread (hence the name 'passive') to check expiry times - the check is performed on demand.
",smartshark_2_2,416,commons-collections,"""[0.9971582889556885, 0.0028417459689080715]"""
621,228880,Expand LoopingListIterator,"Please enhance {{LoopingListIterator}} to accept a starting offset and a number to indicate the number of loops.

https://docs.oracle.com/javase/7/docs/api/java/util/List.html#listIterator(int)

{code:java}
public LoopingListIterator(List<E> list, int offset, int loops);
{code}

As I imagine it, if a list has 3 items (1,2,3) then {{LoopingListIterator(list, 1, 1)}} would iterate: (2,3,1)",smartshark_2_2,657,commons-collections,"""[0.9983822107315063, 0.0016178417718037963]"""
622,228881,Change to HashEntry inner class of AbstractHashedMap,"The way the AbstractHashedMap class is currently written it is not as extensible as it could be.
The HashEntry static inner class is abstract. By slightly refactoring it to an inner interface one can get a 
lot more mileage out of it.

This change has minimal impact on four other classes from the map package.

Two extra classes which I will submit as a seperate patch (should I do this before this one is accepted?)
that use this inner interface are a HashedSet and a WeakHashedSet. Both of these classes are adapters 
of a subclass of the changed AbstractHashedMap.

The HashEntry interface essentially hides the representation of the key and value objects. In a HashSet 
the key is the value, so this avoids the duplication - not of objects, but of references. More importantly 
it also allows a HashEntry to extend a WeakReference Object, which cannot be done as the code 
presently stands.

Here is the new interface:

    protected static interface HashEntry  extends Map.Entry, KeyValue {
         public HashEntry getNext();

         public void setNext(HashEntry next);

         public int getHashCode();

         public void setHashCode(int hashCode);

        /**
          * @param key raw key (ie, no interpretation for special cases like NULL
         */

        public void setRawKey(Object key);

        /**
         *
         * @return the raw key
         */
        public Object getRawKey();


    }

This allows the implementation to decide how they can refer to the key and the values. Essentially we 
remove all reference in the code to the variables 'key' and 'value' and replace them with the (set/
get)rawkey (set/get)value methods. The raw key method is necessary as the setKey method is often 
overridden to do special null substition work.

I have also created a more interesting NULL object, that can better describe itself. When debugging it is 
often helpful to know that one is looking at a NULL object.

Finally I also - and debatably wrongly  - changed the Iterator from  static inner classes to real inner 
classes. This is how it is meant to work anyway. An inner class just has a reference to its enclosing 
class. But perhaps there is something here that I have not understood. My other contributions don't 
hang on this change. They just make the code a little simpler. 
If the intention is to extract these classes to an external package, then the current solution of having a 
static inner class makes sense.

I will attach class diagrams and diffs to this request to clarify the changes.",smartshark_2_2,440,commons-collections,"""[0.9982740879058838, 0.0017258854350075126]"""
623,228882,ListUtils.subtract is very slow ,"Hi,

ListUtils.subtract is very slow when subtracting two large lists.  The
root cause of this problem is similar to the root cause of the
previously fixed COLLECTIONS-328 in ListUtils, i.e., quadratic
complexity instead of linear complexity.

I am encountering this problem in version 3.2.1 and also in revision
1342815 (May 25th 2012).  I have attached a test that exposes this
problem and a simple patch.  On my machine, for the attached test,
this patch provides a 95X speedup.

To run the test, just do:

$ java Test

Currently, the code for ListUtils.subtract is:

final ArrayList<E> result = new ArrayList<E>(list1);
for (E e : list2) {
    result.remove(e);
}
return result;

which is quadratic, because result.remove(e) has linear complexity.

The attached patch makes the remove() be constant complexity by
removing from an org.apache.commons.collections.bag.HashBag.  I use
HashBag and not HashSet because ListUtils.subtract needs to respect
the cardinality when there are repeated objects in the list.

As mentioned in COLLECTIONS-328 discussion, for small lists, there is
some overhead for creating the HashBag.  This can be fixed with a
threshold, but I did not implement it in my patch because the
COLLECTIONS-328 patch does not implement it.

Unlike the patch for COLLECTIONS-328, my patch does not choose the
list to iterate over based on size, because of the cardinality
requirement in subtract.  This means the code could be made even
faster if we could use something like a LinkedHashBag but neither
Apache Collections nor standard Java libraries have such a class.
Even so, this patch is still a lot faster than the original version.

Is this truly a bug, or am I missing something here?  If so, can you
please confirm if the patch is correct?

Thanks,

Adrian


",smartshark_2_2,367,commons-collections,"""[0.9785117506980896, 0.02148822695016861]"""
624,228883,"AbstractUntypedCollectionDecorator<E, D>  is not used","The public ctor for AbstractUntypedCollectionDecorator<E, D> takes no argument and so collection = null; however the protected ctor checks for collection parameter != null.

The decorated() method says that all access to collection goes through it, and there is no setter.

At present the only way to recover from calling the public ctor is for the sub-class to set collection directly.

This is inconsistent.

The class is abstract and there appear to be no concrete subclasses. Looks like the class might be superfluous, but if it is required, it should ideally have a private final collection field.",smartshark_2_2,489,commons-collections,"""[0.8851566910743713, 0.11484324932098389]"""
625,228884,Improve thread-safety of ExtendedProperties,"It looks as though ExtendedProperties is intended to be thread-safe, otherwise why bother synchronizing load() and save()?

If so, then ExtendedProperties field ""isInitialized"" should be made volatile to ensure the variable is correctly published.

Likewise, the field ""includePropertyName"" needs to be volatile or synchronised.

Also, the following protected variables could be made final to improve thread-safety:

defaults
file
basePath
fileSeparator - this could perhaps be static too?
keysAsListed

Regardless of thread-safety issues, does it make sense for these variables to be changed once initialised?
",smartshark_2_2,568,commons-collections,"""[0.9971091151237488, 0.00289091351442039]"""
626,228885,"Implementation of SynchronizedBidiMap and SynchronizedMap, with tests","I've implemented SynchronizedBidiMap which was listed in the task list for
Commons Collections. Implementing this required that I also implement
SynchronizedMap. As usual, I tried to keep the format of the code similar to
other code in Commons Collections. I've also included JUnit tests for both of
these new classes and have included the .obj files suitable for placement in
data/test so you don't have to bother generating them yourself.",smartshark_2_2,203,commons-collections,"""[0.9982593655586243, 0.0017406090628355742]"""
627,228886,[Collection|List]Utils.[intersection|union] should accept vararg parameters,"Would be convenient if intersection/union methods accepted arbitrary number of arguments. Two-arg methods could be retained in order to avoid compiler warnings (https://issues.apache.org/jira/browse/COLLECTIONS-481?focusedCommentId=13762173&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13762173). Alternatively, method could be overloaded to accept a single Iterable of Lists/Sets. I could create a patch for this if it's deemed worthwhile.",smartshark_2_2,468,commons-collections,"""[0.9984362721443176, 0.001563666621223092]"""
628,228887,"CollectionUtils.bisect(...), this would be a combination of Select and SelectRejected","I recently needed a way to use a Predicate to select things from a list, but I also wanted to know which ones failed the predicate test.

I wanted the following, but with one iteration instead of two:
{code}
List originalList = (...)

Predicate p = (...)

Collection selected = CollectionUtils.select(originalList, p);
Collection rejected = CollectionUtils.selectRejected(originalList, p);
// handle the selected cases (save them or whatnot)
// now throw an error message or handle the rejected cases
{code}

This is what I came up with based on the CollectionUtils.select(...) method:
{code:java}
	public static <O, R extends Collection<? super O>> void bisect(
			final Iterable<? extends O> inputCollection,
			final Predicate<? super O> predicate, 
			final R selectedCollection,
			final R rejectedCollection) {

		if (inputCollection != null && predicate != null) {
			for (final O item : inputCollection) {
				if (predicate.evaluate(item)) {
					selectedCollection.add(item);
				}else{
					rejectedCollection.add(item);
				}
			}
		}
	}
	
	public static void main(String[] args){
		// this will test the bisection code
		List<String> original = Arrays.asList(
				""testString1"",
				""testString2"",
				""testString3"",
				""String1"",
				""String2"",
				""String3"",
				""testString4"",
				""String4"",
				""testString5""
				);
		
		List<String> selected = new ArrayList<String>();
		List<String> rejected = new ArrayList<String>();
		
		Predicate<String> beginsWithTestPredicate =
		new Predicate<String>() {
			public boolean evaluate(String object) {
				return object.startsWith(""test"");
			}
		};
		
		bisect(original, beginsWithTestPredicate, selected, rejected);
		
		System.out.println(""Size of selected (should be 5):"" 
				+ selected.size());
		System.out.println(""Size of rejected (should be 4):""
				+ rejected.size());
	}
{code}

This will of course throw a NullPointerException if either output collection is null.  This seems appropriate since we need to return two outputs anyway.

Not sure if *bisect* is the best name, but this method will split the original into two pieces. https://www.google.com/#q=define+bisect",smartshark_2_2,472,commons-collections,"""[0.9978184700012207, 0.0021815462969243526]"""
629,228888,Clarify JavaDoc of MultiKey getKey() and size(),Placeholder for PR https://github.com/apache/commons-collections/pull/4,smartshark_2_2,558,commons-collections,"""[0.9984925985336304, 0.0015073752729222178]"""
630,228889,TransformingComparator should have a constructor with just a Transformer,"With generics, it's possible to have a constructor like:

TransformingComparator(Transformer<I, O extends Comparable> transformer) {
        this(transformer, new ComparableComparator());
    }

*NB: Pseudo code - that will not work as is",smartshark_2_2,392,commons-collections,"""[0.9985664486885071, 0.0014335975283756852]"""
631,228890,Contribution: GeneralizedSuffixTree,"Hello,
a while ago I wrote a GeneralizedSuffixTree (based on the Ukkonen's paper ""On-line construction of suffix trees"" http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf), that I later open sourced at https://github.com/abahgat/suffixtree.

Thomas Neidhart did several improvements here: https://github.com/netomi/suffixtree and I merged them back in the 'commons' branch of my own repository.

As per Thomas's suggestion, I am submitting the library for evaluation, as it may be a good fit for this project.

Regards,
Alessandro",smartshark_2_2,620,commons-collections,"""[0.9984180927276611, 0.0015819172840565443]"""
632,228891,performance problem in ListOrderedSet.retainAll(),"Hi,

I am encountering a performance problem in ListOrderedSet.retainAll().
It appears in version 3.2.1 and also in revision 1365132.  I attached
a test that exposes this problem and a patch that fixes it.  On my
machine, for this test, the patch provides a 263X speedup.

To run the test, just do:

$ java Test

The output for the un-patched version is:
Time is 3682

The output for the patched version is:
Time is 14

The problem is that the code for method
""ListOrderedSet.retainAll(Collection<?> coll)"" is:

{code:java|borderStyle=solid}
public boolean retainAll(Collection<?> coll) {
    boolean result = collection.retainAll(coll);
....
}
{code}

because ""collection.retainAll(coll)"" has quadratic complexity if
parameter ""coll"" is a List.  Conceptually, the solution is to call
""coll.retainAll(collection)"" which has linear complexity (not
quadratic), because ""collection"" is always a HashSet (we know this,
because ""collection"" is an inherited field of ListOrderedSet, and thus
cannot have another type).  See the
""java.util.AbstractCollection.retainAll()"" code inlined below for why
one call has quadratic complexity, and the other has linear
complexity.

Directly calling ""coll.retainAll(collection)"" would change the
behavior of ListOrderedSet.retainAll(), which is why the patch seems a
bit involved.  In reality, the patch simulates the behavior of
""coll.retainAll(collection)"" (which is just the intersection of two
collections).  You can easily see this by looking at the patch and at
the ""java.util.AbstractCollection.retainAll()"" code inlined below.

""collection.retainAll(coll)"" is ""java.util.HashSet.retainAll()"", which
is ""java.util.AbstractCollection.retainAll()"", which has the code:

{code:java|borderStyle=solid}
public boolean retainAll(Collection<?> c) {
    boolean modified = false;
    Iterator<E> e = iterator();
    while (e.hasNext()) {
        if (!c.contains(e.next())) {
            e.remove();
            modified = true;
        }
    }
    return modified;
}
{code} 


Is this a bug, or am I misunderstanding the intended behavior?  If so,
can you please confirm if the patch is correct?

Thanks,

Adrian
",smartshark_2_2,504,commons-collections,"""[0.990349292755127, 0.009650727733969688]"""
633,228892,TestMultiKeyMap  new test case for Serialization and Deserialization ,"We should add a serialzation and deserialzation test case for the class MultiKeyMap since there is a testCone() method it would be good to have the below test case in the TestMultiKeyMap

  public void testSerializationDeserialization() {
    	MultiKeyMap serialized = new MultiKeyMap();
    	serialized.put(new MultiKey(I1, I2), ""1-2"");
        try {
        ObjectOutputStream out =  new ObjectOutputStream(new FileOutputStream(""map.ser""));
        out.writeObject(serialized);
        ObjectInputStream in = new ObjectInputStream(new FileInputStream(""map.ser""));
        MultiKeyMap deserialzed = (MultiKeyMap)in.readObject();
        assertEquals(serialized.size(), deserialzed.size());
        }catch(Exception ex){
        	fail(""Test failed due to exception"" + ex.getMessage() );
        }
        
    }        
    
I have checked the latest class at http://svn.apache.org/viewvc/commons/proper/collections/trunk/src/java/org/apache/commons/collections/map/MultiKeyMap.java?revision=560660
I could not find the test method

thanks 
Alan Mehio
London, UK",smartshark_2_2,228,commons-collections,"""[0.9983223080635071, 0.0016777032287791371]"""
634,228893,[collections] [RFE] ListToArrayTransformer,"There is transformer received a List and return a array after apply a
transformation on all the elements.  If I write proper
javadoc and test case, could it be included in commons collections.",smartshark_2_2,185,commons-collections,"""[0.9984435439109802, 0.0015565212815999985]"""
635,228894,CollectionUtils.sizeIsEmpty(null) should return true,"The API would look inconsistent if
- CollectionUtils.isEmpty(null) == true
- CollectionUtils.size(null) == 0
but
- CollectionUtils.sizeIsEmpty(null) throws IAE
",smartshark_2_2,281,commons-collections,"""[0.9913986921310425, 0.008601328358054161]"""
636,228895,Add a NavigableTrie interface which implements NavigableMap,Trie should implement NavigableMap,smartshark_2_2,619,commons-collections,"""[0.997097373008728, 0.002902678679674864]"""
637,228896,Update JavaDoc in ListUtils to discribe generified methods,The JavaDoc of {{ListUtils.lazyList()}} and {{ListUtils.fixedSizeList}} still refer to the old methods prior to generics. This should be updated.,smartshark_2_2,400,commons-collections,"""[0.9983489513397217, 0.0016511180438101292]"""
638,228897,Add indexed get to CircularFifoQueue,"Since the BoundedFifoBuffer is implemented as an array, it should be simple to implement random access to any element of the buffer.

[This functionality was requested on the Commons user list]

For example:

// get nth entry; i.e. get(0) is the same as get()
// NoSuchElementException if i >= numEntries
// if i < 0, could either throw an Exception or treat as reverse index from end of buffer
E get(int i)

This would also work for the sub-class CircularFifoBuffer.",smartshark_2_2,362,commons-collections,"""[0.998456597328186, 0.0015433656517416239]"""
639,228898,UnmodifiableBoundedCollection does not implement Unmodifiable marker interface,The UnmodifiableBoundedCollection does not implement the Unmodifiable interface.,smartshark_2_2,359,commons-collections,"""[0.8937428593635559, 0.1062571331858635]"""
640,228899,TreeBidiMap should implement Serializable,"TreeBidiMap does not implement Serializable. DualTreeBidiMap does.

It's just a matter of checking which fields should be transient and done.

In fact, all Maps should be Serializable.",smartshark_2_2,314,commons-collections,"""[0.9983855485916138, 0.0016145011177286506]"""
641,228900,splitmap.TransformedMap is not really a Map,"splitmap.TransformedMap is part of the Get/Put hierarchy, but it does not behave like a proper Java Map. 

In particular, java.util.Map.put(K, V) returns V.
However the collections Put interface returns Object.
As far as I can tell, this was done in order to be able to include TransformedMap in the hiearchy. But the side effect is to break the generics for all the non-transformer maps in the hierarchy.

Maybe there should be a separate PutTransformed interface which has the appropriate generic types, i.e.

public T put(K key, V value)

",smartshark_2_2,372,commons-collections,"""[0.7086207866668701, 0.29137924313545227]"""
642,228901,"Rename MultiMap.remove(K, V) to avoid clashes with newly introduced default methods in Java 8","Java 8 will introduce new default methods for the Map interface which clash with the existing method ""V remove(K key, V value)"" in our MultiMap interface.

To avoid future problems rename the method to a more specific version and change the return type to be more logical. Brief discussion on the ml favored either:

 * boolean removeMapping(K key, V value)
 * boolean removeValue(K key, V value)",smartshark_2_2,311,commons-collections,"""[0.998333752155304, 0.0016662712441757321]"""
643,228902,Add a fixed order Comparator,"Among the simple classes I use frequently is a Comparator which imposes a fixed
and arbitrary order on a group of objects.  As with so many things, it's not
hard to coerce this behavior, but it's really convenient to have a class which
does it for you.  It's also something that can be hard to live without once you
have it, assuming the appropriate problem space.  (Then again, what isn't?)

I wrote such a class for my current employer (http://www.vocalabs.com) which has
graciously given me permission to donate it and its unit tests to Commons, as
well as spend work hours on this project.  It will need some cleaning up (add
the Apache copyright, probably change the name, and tighten error handling),
which I am prepared to do.

If others consider this valuable, I'd like some advice on how to proceed to best
integrate it into Commons.  But first some details.

The class is currently named MapOrderComparator, since it uses a map to enforce
the underlying order.  I don't like the name because it obscures its purpose. 
I'm still fishing for a better name; FixedOrderComparator and/or
ArbitraryComparator don't get the point across, though.  A sample usage is as
follows:

    String[] days = { ""Monday"", ""Tuesday"", ""Wednesday"", /* etc...*/ };

    /* Construct a Comparator which uses the order of the array */
    Comparator dayComparator = new MapOrderComparator(days);

    TreeMap stuffThatHappensDuringTheWeek = new TreeMap(dayComparator);

    myThingie.populate(stuffThatHappensDuringTheWeek);
    myViewer.displaySomeMappedData(stuffThatHappensDuringTheWeek);


I wrote the class to do arbitrary order comparisons, using a Map implementation,
but included some general-purpose Map functionality, including a constructor
which takes a Map.  I've never populated the map values with anything other than
Integers to keep track of the initial order.  So one way to proceed would be to
hide the Map as an implementation detail.

Another way to proceed would be to treat it as an 
org.apache.commons.collections.comparators.TransformingComparator.  In fact,
when I first saw the TransformingComparator, I thought it would be a good
replacement for the MapOrderComparator.  Unfortunately, usage is more awkward
and it doesn't do the right thing out-of-the-box.

The TransformingComparator takes an org.apache.commons.collections.Transformer
and a Comparator (typically a ComparableComparator).  Transformer is an
interface, and other than org.apache.cocoon.transformation.TraxTransformer
 I can't find any classes which implement it.  What you'd want is a
CountingTransformer, which transfomrs Objects into Integers.

Given a CountingTransformer, the above code could be rewritten as:

<   Comparator dayComparator = new MapOrderComparator(days);
-
>   Transformer daysToIntegers = new CountingTransformer(days);
>   Comparator dayComparator 
>       = new TransformingComparator(daysToIntegers);

This approach gets the job done just as well, if slightly more verbosely.  It's
 the best integrated with the existing classes, but harder to find.  (I wouldn't
go looking for a CountingTransformer if I needed a Comparator.)  The classes I'm
propopsing are trivial to code, so their value depends on (1) being easier to
find than to write, and (2) inspiring simple solutions which are only obvious
from awareness of the class.

Thus the second way to proceed would be for me to submit a CountingTransformer,
perhaps noting its existance in the TransformingComparator javadoc.

I welcome advice and opinions on how to proceed.",smartshark_2_2,168,commons-collections,"""[0.998418927192688, 0.0015811132034286857]"""
644,228903,ZipException on reading valid zip64 file,"ZipFile zip = new ZipFile(new File(""ordertest-64.zip"")); throws ZipException ""central directory zip64 extended information extra field's length doesn't match central directory data.  Expected length 16 but is 28"".

The archive was created by using DotNetZip-WinFormsTool uzing zip64 flag (forces always to make zip64 archives).

Zip file is tested from the console: $zip -T ordertest-64.zip

Output:
test of ordertest-64.zip OK

I can open the archive with FileRoller without problem on my machine, browse and extract it.
",smartshark_2_2,216,commons-compress,"""[0.13779883086681366, 0.8622012138366699]"""
645,228904,ArchiveStreamFactory does not recognise tar files created by Ant,"ArchiveStreamFactory does not recognise tar files created by Ant.

These appear to have magic of  ""ustar\0"" (i.e. same as MAGIC_POSIX) followed by ""\0\0"".

Note that Compress can process the files OK.

Patch to follow after checking what Ant writes as the signature.",smartshark_2_2,88,commons-compress,"""[0.17863915860652924, 0.8213608860969543]"""
646,228905,ZipEncodingHelper.isUTF8(String) does not check all UTF-8 aliases,ZipEncodingHelper.isUTF8(String) does not check all UTF-8 aliases.,smartshark_2_2,231,commons-compress,"""[0.1146349161863327, 0.8853651285171509]"""
647,228906,ArArchiveInputStream doesn't make use of internal offset,"When entering the ArchiveInputStream, the header will be investigated. If offset = 0, then for <arch> will be searched with a read method from InputStream. The offset var will not be updated of course and stays 0. Next time an invalid header exception is thrown.

Solution: all read operations must update the offset field in ArchiveInputStream",smartshark_2_2,3,commons-compress,"""[0.08512849360704422, 0.9148715138435364]"""
648,228907,Permanent Hang creating ZipFile(File),"While using Tika-1.15 and 1.17 to detect file types, it hangs when creating an instance of ZipFile with the attached corrupted zip file. Taking a thread dump shows the app is looping at the following point:
{code}
java.lang.Thread.State: RUNNABLE
        at org.apache.commons.compress.archivers.zip.X0017_StrongEncryptionHeader.parseCentralDirectoryFormat(X0017_StrongEncryptionHeader.java:313)
        at org.apache.commons.compress.archivers.zip.X0017_StrongEncryptionHeader.parseFromCentralDirectoryData(X0017_StrongEncryptionHeader.java:380)
        at org.apache.commons.compress.archivers.zip.ExtraFieldUtils.parse(ExtraFieldUtils.java:181)
        at org.apache.commons.compress.archivers.zip.ZipArchiveEntry.setCentralDirectoryExtra(ZipArchiveEntry.java:598)
        at org.apache.commons.compress.archivers.zip.ZipFile.readCentralDirectoryEntry(ZipFile.java:706)
        at org.apache.commons.compress.archivers.zip.ZipFile.populateFromCentralDirectory(ZipFile.java:618)
        at org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:290)
        at org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:213)
        at org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:196)
        at org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:157)
        at org.apache.tika.parser.pkg.ZipContainerDetector.detectZipFormat(ZipContainerDetector.java:132)
        at org.apache.tika.parser.pkg.ZipContainerDetector.detect(ZipContainerDetector.java:88)
{code}",smartshark_2_2,450,commons-compress,"""[0.11702115088701248, 0.8829788565635681]"""
649,228908,TarArchiveInputStream fails to read PAX header from InputStream,"We have a scenario with a ""slow"" {{InputStream}} and are facing {{IOExceptions}} with {{TarArchiveEntry#getNextTarEntry()}}.

If the {{InputStream}} does not deliver fast enough, {{TarArchiveEntry#parsePaxHeaders(InputStream i)}} fails at this location:

{code:title=TarArchiveInputStream.java|borderStyle=solid}
// Get rest of entry
byte[] rest = new byte[len - read];
int got = i.read(rest);
if (got != len - read){
	throw new IOException(""Failed to read ""
		+ ""Paxheader. Expected ""
		+ (len - read)
		+ "" bytes, read ""
		+ got);
}
{code}

We would suggest to change the code to something like this:

{code:title=TarArchiveInputStream.java|borderStyle=solid}
// Get rest of entry
byte[] rest = new byte[len - read];
int got = 0;
while((ch = i.read()) != -1) {
	rest[got] = (byte) ch;
	got++;
	if(got == len - read) {
		break;
	}
}
if (got != len - read){
	throw new IOException(""Failed to read ""
		+ ""Paxheader. Expected ""
		+ (len - read)
		+ "" bytes, read ""
		+ got);
}
{code}

This would make sure, that it gets all bytes of the PAX header value.",smartshark_2_2,269,commons-compress,"""[0.07505296915769577, 0.9249470233917236]"""
650,228909,Retore immutability/thread-safety to ArchiveStreamFactory,"COMPRESS-180 added support for encoding.
Unfortunately this was done in a way that broke immutability.
Also the factory is no longer thread-safe as the encoding field is not synch/volatile.

Consider whether to restore immutability, e.g. by adding a constuctor which takes the encoding setting. The setEntryEncoding method could be deprecated intitially and eventually dropped.

One way to support immutability now would be to add a second final encoding field which is set by a new ctor. See patch to follow.",smartshark_2_2,297,commons-compress,"""[0.29551810026168823, 0.704481840133667]"""
651,228910,Decompress tar.gz file failed. java.io.Exception:Error detected parsing the header,"{code:java}
    public static void deGzipArchive(String filepath,String dir)
            throws Exception {
        final File input = new File(filepath);
        final InputStream is = new FileInputStream(input);
        final CompressorInputStream in = new GzipCompressorInputStream(is, true);
        TarArchiveInputStream tin = new TarArchiveInputStream(in);
        TarArchiveEntry entry = tin.getNextTarEntry();
        while (entry != null) {
            File archiveEntry = new File(dir, entry.getName());
            archiveEntry.getParentFile().mkdirs();
            if (entry.isDirectory()) {
                archiveEntry.mkdir();
                entry = tin.getNextTarEntry();
                continue;
            }
            OutputStream out = new FileOutputStream(archiveEntry);
            IOUtils.copy(tin, out);
            out.close();
            entry = tin.getNextTarEntry();
        }
        in.close();
        tin.close();
    }

    public static void main(String[] args) throws Exception {
        Gztest.deGzipArchive(""D:/20000102.0000+0800-20000102.0015+0800_0.tar.gz"",""D:/"");
    }
{code}

the tar.gz file can be decompressed in linux environment use 'tar' command.

The error log:

Exception in thread ""main"" java.io.IOException: Error detected parsing the header
	at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:286)
	at gztest.deGzipArchive(gztest.java:23)
	at gztest.main(gztest.java:149)
Caused by: java.lang.IllegalArgumentException: Invalid byte 100 at offset 0 in 'dos{NUL}{NUL}{NUL}{NUL}{NUL}' len=8
	at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:141)
	at org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:171)
	at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:1128)
	at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:1091)
	at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:368)
	at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:284)
	... 2 more
	
",smartshark_2_2,418,commons-compress,"""[0.36400604248046875, 0.6359939575195312]"""
652,228911,ZipArchiveInputStream may read 0 bytes when reading from a nested Zip file,"When the following code is run an error ""Underlying input stream returned zero bytes"" is produced. If the commented line is uncommented it can be seen that the ZipArchiveInputStream returned 0 bytes. This only happens the first time read is called, subsequent calls work as expected i.e. the following code actually works correctly with that line uncommented!

The zip file used to produce this behavious is available at http://wwmm.ch.cam.ac.uk/~dl387/test.ZIP

If this is not the correct way of processing a zip file of zip files please let me know. Also I believe whilst ZipFile can iterate over entries fast due to being able to look at the master table whilst ZipArchiveInputStream cannot. Is there anyway of instantiating a ZipFile from a zip file inside another zip file without first extracting the nested zip file?

    ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
      ZipArchiveEntry entry = iterator.nextElement();
      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
      ZipArchiveEntry innerEntry;
      while ((innerEntry = zipInput.getNextZipEntry()) != null){
        if (innerEntry.getName().endsWith(""XML"")){
          //zipInput.read();
          System.out.println(IOUtils.toString(zipInput));
        }
      }
    }",smartshark_2_2,169,commons-compress,"""[0.08660124987363815, 0.913398802280426]"""
653,228912,duplicate entries may let ZipFile#getInputStream return null,"Found while investigating https://issues.apache.org/bugzilla/show_bug.cgi?id=54967

If an archive contains two entries for the same file readCentralDirectory in ZipFile will only add the first entry to the entries map and only the last to the nameMap map - this is because entries is a LinkedHashMap and nameMap is a ""plain"" HashMap.

Normally this wouldn't matter since both ZipArchiveEntry instances are equal and thus it is irrelevant which of the two is used as a key when obtaining the InputStream.

Things get different, though, if the entry has data inside the local file header as only the first entry is dealt with in resolveLocalFileHeaderData - after this the two instances are no longer equal and nameMap.get(NAME) will return an instance that can no longer be found.

I intend to modify readCentralDirectory to only add the first entry to nameMap as well and document the behavior.  I'll also start a discussion thread on the dev list on whether we need to provide an additional API with multiple entries per name.",smartshark_2_2,217,commons-compress,"""[0.08364935219287872, 0.9163506031036377]"""
654,228913,Cannot Read Winzip Archives With Unicode Extra Fields,"I have a zip file created with WinZip containing Unicode extra fields. Upon attempting to extract it with org.apache.commons.compress.archivers.zip.ZipFile, ZipFile.getInputStream() returns null for ZipArchiveEntries previously retrieved with ZipFile.getEntry() or even ZipFile.getEntries(). See UTF8ZipFilesTest.patch in the attachments for a test case exposing the bug. The original test case stopped short of trying to read the entries, that's why this wasn't flagged up before. 

The problem lies in the fact that inside ZipFile.java entries are stored in a HashMap. However, at one point after populating the HashMap, the unicode extra fields are read, which leads to a change of the ZipArchiveEntry name, and therefore a change of its hash code. Because of this, subsequent gets on the HashMap fail to retrieve the original values.

ZipFile.patch contains an (admittedly simple-minded) fix for this problem by reconstructing the entries HashMap after the Unicode extra fields have been parsed. The purpose of this patch is mainly to show that the problem is indeed what I think, rather than providing a well-designed solution.

The patches have been tested against revision 1210416.",smartshark_2_2,131,commons-compress,"""[0.09985925257205963, 0.9001407623291016]"""
655,228914,TarUtils.parseName does not properly handle characters outside the range 0-127,"if a tarfile contains files with special characters, the names of the tar entries are wrong.

example:
correct name: 0302-0601-3Â±Â±Â±F06Â±W220Â±ZBÂ±LALALAÂ±Â±Â±Â±Â±Â±Â±Â±Â±Â±CANÂ±Â±DCÂ±Â±Â±04Â±060302Â±MOE.model
name resolved by TarUtils.parseName: 0302-0101-3ï¾±ï¾±ï¾±F06ï¾±W220ï¾±ZBï¾±HECKMODULï¾±ï¾±ï¾±ï¾±ï¾±ï¾±ï¾±ï¾±ï¾±ï¾±ECEï¾±ï¾±DCï¾±ï¾±ï¾±07ï¾±060302ï¾±DOERN.model

please use: 
result.append(new String(new byte[] { buffer[i] }));

instead of: 
result.append((char) buffer[i]);

to solve this encoding problem.",smartshark_2_2,96,commons-compress,"""[0.07119163870811462, 0.9288083910942078]"""
656,228915,Remove src/main/resources,"The src/main/resources directory currently contains a copy of the bla.zip file from src/test/resources. The test file most likely should not be included in the resulting commons-compress jar file. In fact it doesn't since the commons-parent:11 parent POM overrides the default resources settings, but it's still confusing to have the test file under src/main.

I would simply remove the entire src/main/resources directory as we're not using it for anything and the current Maven settings won't use the directory in any case.",smartshark_2_2,193,commons-compress,"""[0.9978910088539124, 0.0021090577356517315]"""
657,228916,TarArchiveOutputStream#failForBigNumber could be more helpful,"We have now ported all of maven to use commons compress. When a user is running tar compression, we may sometimes fail because the selected tar format does not support big numbers.

In this context we'd like to help our users by informing them that there may be other tar formats than can be more suitable. I would request that this method throw some specialized subclass of RuntimeException that we can catch to identify this particular error condition, so we can talk to our users about their options :)",smartshark_2_2,288,commons-compress,"""[0.9984560012817383, 0.0015439357375726104]"""
658,228917,ZipArchiveInputStream does not show location in file where a problem occurred,"See COMPRESS-62 - if ExtraFieldUtils.parse() detects an error, it only shows the offset within the local buffer, which is fairly useless in tracking down a problem.

Somehow the current location in the file needs to be determined and added to the Exception message.

The count/bytesRead field would help, but that actually points to the next available byte, i.e. after the problem area.
Also, the internal data may have been expanded.",smartshark_2_2,91,commons-compress,"""[0.6116361618041992, 0.38836389780044556]"""
659,228918,bz2 stream decompressor is 10x slower than it could be,"This is related to COMPRESS-291. In short: decompressing 7z archives was an order of magnitude slower in Java than with native tooling.

My investigation showed that the problematic archive used bz2 streams inside. I then did a quick hack-experiment which took bz2 decompressor from the Apache Hadoop project (the Java version, not the native one) and replaced the default one used for bz2 stream decompression of the 7z archiver in commons.

I then ran a quick benchmark on this file:
{code}
https://archive.org/download/stackexchange/english.stackexchange.com.7z
{code}

The decompression speeds are (SSD, the file was essentially fully cached in memory, so everything is CPU bound):
{code}
native {{7za}}: 13 seconds
Commons (original): 222 seconds
Commons (patched w/Hadoop bz2): 30 seconds
Commons (patched w/BufferedInputStream): 28 seconds
{code}

Yes, it's still 3 times slower than native code, but it's no longer glacially slow... 

My patch is a quick and dirty proof of concept (not committable, see [1]), but it passes the tests. Some notes:

- Hadoop's stream isn't suited for handling concatenated bz2 streams, it'd have to be either patched in the code or (better) decorated at a level above the low-level decoder,
- I only substituted the decompressor in 7z, but obviously this could benefit in other places (zip, etc.); essentially, I'd remove BZip2CompressorInputStream entirely.
- while I toyed around with the above idea I noticed a really annoying thing -- all streams are required to extend {{CompressorInputStream}}, which only adds one method to count the number of consumed bytes. This complicates the code and makes plugging in other implementations of InputStreams more cumbersome. I could get rid of CompressorInputStream entirely with a few minor changes to the code, but obviously this would be backward incompatible (see [2]).

References:
[1] GitHub fork, {{bzip2}} branch: https://github.com/dweiss/commons-compress/tree/bzip2
[2] Removal and cleanup of CompressorInputStream: https://github.com/dweiss/commons-compress/commit/6948ed371e8ed6e6b69b96ee936d1455cbfd6458",smartshark_2_2,356,commons-compress,"""[0.9728087782859802, 0.02719126269221306]"""
660,228919,While Archiving the empty directories are not archived to the archive.,"I am trying to create a zip archive. I am having a empty directory ""test"" . When i try to create it to archive . The archive created but the test folder is created as file. And also I got the below exception.

Temp file created at C:\Users\Soundar\AppData\Local\Temp\sample5278197872513888977.zip
Exception in thread ""main"" java.io.IOException: This archives contains unclosed entries.
	at org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.finish(ZipArchiveOutputStream.java:343)
	at org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream.close(ZipArchiveOutputStream.java:550)
	at com.tcs.tools.archive.test.ArchiveTest.createArchive(ArchiveTest.java:64)
	at com.tcs.tools.archive.test.ArchiveTest.main(ArchiveTest.java:119)

Code used
-------------

	protected File createArchive(String archivename) throws Exception {
		ArchiveOutputStream out = null;
		OutputStream stream = null;
		ArchiveStreamFactory factory = new ArchiveStreamFactory();
		try {
			File archive = File.createTempFile(""sample"", ""."" + archivename);
			System.out.println(""Temp file created at "" + archive.getAbsolutePath());
			List archiveList = new ArrayList();

			stream = new FileOutputStream(archive);
			out = factory.createArchiveOutputStream(archivename, stream);
			final File file7 = new File(""C:\\Users\\Soundar\\Desktop\\test"");

			addArchiveEntry(out, ""test"", file7);
			System.out.println(""Going to process directory"");
			System.out.println(""Directory Name ="" + file7.getName());
			ArchiveEntry entry = out.createArchiveEntry(file7, ""test"");
			out.putArchiveEntry(entry);
			IOUtils.copy(new FileInputStream(file7), out);
			out.closeArchiveEntry();

			out.finish();
			return archive;
		} finally {
			if (out != null) {
				out.close();
			} else if (stream != null) {
				stream.close();
			}
		}
	}

Note
------

I tried the same archiving using the compressed zip of Windows Vista default feature. It creates the Sample.zip which contains the empty test folder as archived.

I know there is a work around  using java.util.zip.zipEntry. But is there a  work around in our Compress itself?",smartshark_2_2,82,commons-compress,"""[0.9464088082313538, 0.05359114706516266]"""
661,228920,Move acknowledgements from NOTICE to README,"The NOTICE.txt file in commons-compress contains the following entries:

{noformat}
Original BZip2 classes contributed by Keiron Liddle
<keiron@aftexsw.com>, Aftex Software to the Apache Ant project

Original Tar classes from contributors of the Apache Ant project

Original Zip classes from contributors of the Apache Ant project

Original CPIO classes contributed by Markus Kuss and the jRPM project
(jrpm.sourceforge.net)
{noformat}

It's good that we acknowledge contributions, but having those entries in the NOTICE file is not appropriate unless the licensing of the original contributions explicitly required such attribution notices.

I suggest that we move these entries to a README.txt file.",smartshark_2_2,94,commons-compress,"""[0.9982499480247498, 0.0017500568646937609]"""
662,228921,Cleaner way to catch/detect Seven7 files which are password protected,"Currently, if we open a password protected 7z file and call {{getNextEntry()}} on it, it will blow up with an IOException with a specific string:

{code}
java.io.IOException: Cannot read encrypted files without a password
	at org.apache.commons.compress.archivers.sevenz.AES256SHA256Decoder$1.init(AES256SHA256Decoder.java:56)
	at org.apache.commons.compress.archivers.sevenz.AES256SHA256Decoder$1.read(AES256SHA256Decoder.java:112)
	at java.io.DataInputStream.readUnsignedByte(DataInputStream.java:288)
	at org.tukaani.xz.rangecoder.RangeDecoderFromStream.<init>(Unknown Source)
	at org.tukaani.xz.LZMAInputStream.initialize(Unknown Source)
	at org.tukaani.xz.LZMAInputStream.initialize(Unknown Source)
	at org.tukaani.xz.LZMAInputStream.<init>(Unknown Source)
	at org.apache.commons.compress.archivers.sevenz.Coders$LZMADecoder.decode(Coders.java:113)
	at org.apache.commons.compress.archivers.sevenz.Coders.addDecoder(Coders.java:77)
	at org.apache.commons.compress.archivers.sevenz.SevenZFile.buildDecoderStack(SevenZFile.java:853)
	at org.apache.commons.compress.archivers.sevenz.SevenZFile.buildDecodingStream(SevenZFile.java:820)
	at org.apache.commons.compress.archivers.sevenz.SevenZFile.getNextEntry(SevenZFile.java:151)
{code}

It would be good if either a specific subtype of IOException could be thrown (which could then be caught to differentiate this from other kinds of IO problems), or if a method could be added to SevenZFile which could be called to see if a password is needed / given password is correct

(If implemented, this would help make the code in Tika dealing with 7z files cleaner)",smartshark_2_2,290,commons-compress,"""[0.9877744317054749, 0.012225568294525146]"""
663,228922,Document null return value of ArchiveInputStream.getNextEntry,"The ArchiveInputStream.getNextEntry method should mention that the return value will be null when there are no more entries in the archive stream.

{noformat}
Index: src/main/java/org/apache/commons/compress/archivers/ArchiveInputStream.java
===================================================================
--- src/main/java/org/apache/commons/compress/archivers/ArchiveInputStream.java (revision 760154)
+++ src/main/java/org/apache/commons/compress/archivers/ArchiveInputStream.java (working copy)
@@ -43,8 +43,10 @@
     private static final int BYTE_MASK = 0xFF;

     /**
-     * Returns the next Archive Entry in this Stream.
-     * @return the next entry
+     * Returns the next Archive Entry in this Stream.
+     *
+     * @return the next entry,
+     *         or <code>null</code> if there are no more entries
      * @throws IOException if the next entry could not be read
      */
     public abstract ArchiveEntry getNextEntry() throws IOException;
{noformat}

",smartshark_2_2,39,commons-compress,"""[0.992195725440979, 0.007804335094988346]"""
664,228923,Missing OSGI Import-Package,"The configuration of the maven-bundle-plugin overrides the <Import-Package> directive : ""*"" is not present anymore, and severals packages do not resolve correctly.

For example, ""javax.crypto"", which is used by 7z encryption, is not resolved and usingÂ this featureÂ fails with a ClassNotFoundException on javax.crypto.SecretKey.

ahma I think that the correct Import-Package directive should be :
{code:java}
<Import-Package>*;resolution:=optional</Import-Package>{code}
this directive will import everything commons-compress depends on, but with an optional resolution (and so every dependencies will be optionals)",smartshark_2_2,466,commons-compress,"""[0.9216356873512268, 0.0783642828464508]"""
665,228924,Suport reading of DEFLATE64 streams in 7z archives,With the introduction of the deflate64 package via COMPRESS-380 it should be possible ti support DEFLATE64 in 7z as well.,smartshark_2_2,439,commons-compress,"""[0.9983742237091064, 0.001625773380510509]"""
666,228925,Better support for encrypted ZIP files,"Currently when the ZipArchiveInputStream or ZipFile encounters an encrypted zip it bails out with cryptic exceptions like: 'invalid block type'. I would like to have two things:

1. an 'encrypted' flag in the ZipArchiveEntry class. It would be taken from the first bit of the 'general purpose flag'
2. more descriptive error messages, both in ZipFile and ZipArchiveInputStream

It might be useful in case someone wants to implement proper support for encrypted zips, with methods to supply passwords/encryption keys and proper encryption/decryption algorithms.

For the time being I just need to know if a file is encrypted or not. 

I will post a patch with a proposal of a solution in near future.",smartshark_2_2,64,commons-compress,"""[0.9984697699546814, 0.0015302269021049142]"""
667,228926,Expose zip stream offset and size via API,"In certain cases it may be useful to get information about where in the archive the stream starts and ends. Typically when zip is used as resource container and the resources are then mapped directly into memory, but not only.

The size and compressed size are already available but not the stream offset.

This can be applied to other archive types as well, therefore it would make sense to put this into basic interface - ArchiveEntry. But not necessarily all of them have to support it.
",smartshark_2_2,398,commons-compress,"""[0.998428463935852, 0.0015715340850874782]"""
668,228927,Opening of a very large zip file is extremely slow compared to java.util.zip.ZipFile,"We have a quite large zip file 35 gb and try to open this with ZipFile.Â 

{code:java}
        try (ZipFile zf = new ZipFile(new File(""35gb.zip""))) {
            System.out.println(""File opened..."" + (System.currentTimeMillis() - start));
        }
{code}

This code takes about 300 000 - 400 000 ms (5-6 minutes).
If I run this with JDK-builtin java.util.zip.ZipFile, same code takes 300 ms (less than a second). 

I'm not totally sure what it is the problem but I did some debugging and basically all time is spent in
{code:java}
    private void resolveLocalFileHeaderData(final Map<ZipArchiveEntry, NameAndComment> entriesWithoutUTF8Flag)
{code}

Anything that can be done to improve this?",smartshark_2_2,477,commons-compress,"""[0.935206413269043, 0.06479353457689285]"""
669,228928,show how one large of e.g. 8MB can be zipped into 3 zip-parts of e.g. compressed 1 MB each,"examples like 
http://stackoverflow.com/questions/243992/how-to-split-a-huge-zip-file-into-multiple-volumes

only show how multiple small files can be zipped into a few compressed parts",smartshark_2_2,156,commons-compress,"""[0.9984731078147888, 0.0015268363058567047]"""
670,228929,Documentation should provide example of CompressorStreamFactory.createCompressorInputStream(),"The documentation found at http://commons.apache.org/compress/examples.html should explain the difference between Archive and Compressor classes. Currently the only example for making a ""factory guess the input format for a given stream"" uses ArchiveStreamFactory but in actuality CompressorStreamFactory.createCompressorInputStream() will do the same for file formats like gzip. Without further clarification it sounds like ArchiveStreamFactory is only used for decompressing and CompressorStreamFactory is only used for compressing.

# Please clarify the difference between Archive and Compressor classes
# Please mention that CompressorStreamFactory.createCompressorInputStream() can also be used to auto-detect the input format of a stream.",smartshark_2_2,182,commons-compress,"""[0.9984343647956848, 0.001565589103847742]"""
671,228930,Expose whether ZIP entry name & comment come from Unicode extra field,"It is known fact that detecting the encoding of the name/comment of ZIP entries is a messy process. And that the general purpose bit 11 is often unreliable.
Only the so-called Unicode extra field (if present) can be trusted to reliably determine a ZIP entry name & comment, as far as I understand.

But the current API of Commons Compress doesn't (easily) expose in which situation the ZIP archive reader is.
That's why I propose to add a couple of new getter/setter-exposed fields to {{ZipArchiveEntry}}, e.g.:

{noformat}
boolean hasUnicodeName
boolean hasUnicodeComment
{noformat}

This way it can be easily determined if the value returned by {{ZipArchiveEntry::getName}} or {{ZipArchiveEntry::getComment}} can be trusted. Or if it needs some ""character encoding sniffing"" of sorts.

What do you think?",smartshark_2_2,436,commons-compress,"""[0.9985228180885315, 0.001477230922318995]"""
672,228931,Uncorrect string terminal symbol in entry when creating Tar archive using TarArchiveOutputStream,"I create Tar archive using TarArchiveOutputStream. And I've got one contradiction in output achive file with Tar-file format specification.

https://www.gnu.org/software/tar/manual/html_node/Standard.html#SEC186 

Specification has next sentention for entry header fields:

""The name, linkname, magic, uname, and gname are null-terminated character strings. All other fields are zero-filled octal numbers in ASCII. Each numeric field of width w contains w minus 1 digits, and a null.""

But in file generated TarArchiveOutputStream octal numbers fields (mode, size, checksum etc) has terminated character not NULL (0x00) but SPACE (0x20), so some programs have problems while work with this files.",smartshark_2_2,417,commons-compress,"""[0.5987403392791748, 0.40125972032546997]"""
673,228932,Fix threading issue in X5455_ExtendedTimestampTest test class (a test for COMPRESS-210),"The test for COMPRESS-210 is currently failing when Compress is built at Apache Gump.
http://vmgump.apache.org/gump/public/commons-compress/commons-compress-test/index.html

It says that the last success was on 2015-10-06T00:00:09,
it started failing at  2015-10-06T12:00:09
and as of now the failure state is persistent for 22 runs (which means ~11 days).

The failure:
{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Commons Compress 1.11-SNAPSHOT
[INFO] ------------------------------------------------------------------------

<...>

Running org.apache.commons.compress.archivers.zip.X5455_ExtendedTimestampTest
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.054 sec <<< FAILURE! - in org.apache.commons.compress.archivers.zip.X5455_ExtendedTimestampTest
testSampleFile(org.apache.commons.compress.archivers.zip.X5455_ExtendedTimestampTest)  Time elapsed: 0.027 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<2105-01-01/0[0:00:02] +0000> but was:<2105-01-01/0[8:00:01] +0000>
	at org.junit.Assert.assertEquals(Assert.java:116)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.apache.commons.compress.archivers.zip.X5455_ExtendedTimestampTest.testSampleFile(X5455_ExtendedTimestampTest.java:171)
{noformat}

Reviewing rhe code of the test class, its usage of `SimpleDateFormat DATE_FORMAT` field is wrong. The field is declared as ""static"". A SimpleDateFormat is not thread-safe, so it must not be shared between tests, as some testing configurations may run several tests in parallel.

A simple fix will be to remove ""static"" from declaration of that field and its initialization block, so that each running instance of the test gets its own copy of SimpleDateFormat class.

(I am not sure whether this bug is the actual cause of the test failure. I do not see any reconfigurations of test environment on 2015-10-06. *Update*: according to gump-general mailing list \[1], on Oct 06 the Java runtime used to run Gump was updated to the latest Java 8 release)

\[1] http://mail-archives.apache.org/mod_mbox/gump-general/201510.mbox/%3C5613A448.8040109%40apache.org%3E",smartshark_2_2,318,commons-compress,"""[0.9538252353668213, 0.0461747869849205]"""
674,228933,checkstyle.xml is missing,Checkstyle.xml is missing and mvn site cannot run successfully.,smartshark_2_2,34,commons-compress,"""[0.9965406060218811, 0.0034593609161674976]"""
675,228934,ArArchiveInputStream does not handle GNU extended filename records (//),"ArArchiveInputStream does not handle GNU extended filename records (//).

The // record data area contains a list of long filenames, terminated by /<LF>.

The file entries refer to them using the file name

/nnn

where nnn=decimal offset into the // data record.

",smartshark_2_2,90,commons-compress,"""[0.10440145432949066, 0.8955985903739929]"""
676,228935,Add DEFLATE support,"GZIP is not a compression algorithm ""as such"". The de facto (and currently the only supported) compression algorithm it uses is DEFLATE.
GZIP adds a header of minimum 10 bytes and a footer of 8 bytes to a ""deflated"" data stream. Find out more here: http://en.wikipedia.org/wiki/Gzip#File_format

I have no problem with the current GZIP support, but it would be nice if CommonsCompress would also have compression and decompression support for ""raw"" DEFLATE streams and DEFLATE streams with the zlib header.

Similarly to the GZIP support in CommonsCompress these functionality can be implemented very easily using the standard java.util.zip package, as done in the provided patch.",smartshark_2_2,283,commons-compress,"""[0.998426079750061, 0.001573930261656642]"""
677,228936,BZip2 Compressor count bug,"BZIp2CompressorInputStream

            count(r < 0 ? -1 : 1);

Do you mean

   count(r < 0 ? 0 : 1);",smartshark_2_2,285,commons-compress,"""[0.09186297655105591, 0.9081370234489441]"""
678,228937,add notifier support for new block in BZip2CompressorInputStream,"hi,

attached patch enables an program to add a listener when a new bzip2
block is detected.

The notifier is called with:
 - xxx.newBlock(this, currBlockPosition)

- this = the current BZip2CompressorInputStream object
- currBlockPosition = The offset (i.e. start position) in the compressed
input stream of the current block
",smartshark_2_2,379,commons-compress,"""[0.9984104633331299, 0.0015894640237092972]"""
679,228938,ZipArchiveInputStream should throw an exception if a data descriptor is used for STORED entries,"There is no reliable way to determine the end of data when the method is STORE and in fact the current code relies on the method to be DEFLATE in that case.

I propose to make this figure into the canReadEntryData method and to throw an exception if any attempt is made
to read the data.
",smartshark_2_2,71,commons-compress,"""[0.9889195561408997, 0.011080465279519558]"""
680,228939,Allow setting of the zip encoding in ArchiveStreamFactory,When using the ArchiveStreamFactory it's currently not possible to control the encoding used by zip archive streams. Having that feature available in ArchiveStreamFactory would be useful for TIKA-936.,smartshark_2_2,213,commons-compress,"""[0.9984310269355774, 0.0015689573483541608]"""
681,228940,Using load(InputStream) on a new XMLPropertyListConfiguration results in a thrown ConfigurationException,"I'm trying to use XMLPropertyListConfiguration to load a property-list from a byte array.  My code looks like:

     XMLPropertyListConfiguration plist = new XMLPropertyListConfiguration();
     //	plist.setRootNode(new PListNode());	// HACK to work around internal cast exception
     plist.load(new ByteArrayInputStream(bytes));

This causes a ConfigurationException caused by a class exception (reproduced below).  The exception can be avoided by uncommenting the second line.

org.apache.commons.configuration.ConfigurationException: Unable to parse the configuration file
	at org.apache.commons.configuration.plist.XMLPropertyListConfiguration.load(XMLPropertyListConfiguration.java:250)
	at org.apache.commons.configuration.AbstractHierarchicalFileConfiguration$FileConfigurationDelegate.load(AbstractHierarchicalFileConfiguration.java:449)
	at org.apache.commons.configuration.AbstractFileConfiguration.load(AbstractFileConfiguration.java:358)
	at org.apache.commons.configuration.AbstractFileConfiguration.load(AbstractFileConfiguration.java:324)
	at org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.load(AbstractHierarchicalFileConfiguration.java:184)
	at testcc.TestCC.main(TestCC.java:29)
Caused by: java.lang.ClassCastException: org.apache.commons.configuration.HierarchicalConfiguration$Node cannot be cast to org.apache.commons.configuration.plist.XMLPropertyListConfiguration$PListNode
	at org.apache.commons.configuration.plist.XMLPropertyListConfiguration$XMLPropertyListHandler.endElement(XMLPropertyListConfiguration.java:534)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:601)
	at com.sun.org.apache.xerces.internal.impl.dtd.XMLDTDValidator.endNamespaceScope(XMLDTDValidator.java:2077)
	at com.sun.org.apache.xerces.internal.impl.dtd.XMLDTDValidator.handleEndElement(XMLDTDValidator.java:2028)
	at com.sun.org.apache.xerces.internal.impl.dtd.XMLDTDValidator.endElement(XMLDTDValidator.java:901)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanEndElement(XMLDocumentFragmentScannerImpl.java:1774)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2930)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
	at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
	at org.apache.commons.configuration.plist.XMLPropertyListConfiguration.load(XMLPropertyListConfiguration.java:246)
	... 5 more
",smartshark_2_2,352,commons-configuration,"""[0.10312362015247345, 0.8968763947486877]"""
682,228941,SubsetConfiguration ignores local StrLookups,For an AbstractConfiguration it is normally possible to register local StrLookup instances. These are simply ignored by the SubsetConfiguration.,smartshark_2_2,349,commons-configuration,"""[0.09608174860477448, 0.9039182066917419]"""
683,228942,Regression with SystemProperties in 1.8 and 1.9,"The attached test case succeeds in commons configuration 1.7 but fails in 1.8 and 1.9. It does succeed on trunk again, probably due to the ""use properties directly"" changes in SystemProperties.

As 2.0 is not yet released, it would be nice to get a 1.9.1 that is 1.9 + this bug fixed.

(The test uses the old junit framework style to be drop in compatible with 1.7, 1.8 and 1.9).",smartshark_2_2,472,commons-configuration,"""[0.11914239078760147, 0.8808576464653015]"""
684,228943,ConfigurationUtils.fileFromURL mangles files with '+' (plus sign) in the name,"Attached is a sample program that demonstrates the problem.  The following is the method in ConfigurationUtils in question:

public static File fileFromURL(URL url)
{
...
        return new File(URLDecoder.decode(url.getPath()));
...
}

URLDecoder (poorly named) decodes data encoded in the application/x-www-form-urlencoded MIME format.  This format is commonly used to encode HTML form data.  It is not intended for encoding URLs, though the formats are similar.

The operative difference is that the MIME format allows the use of the plus sign ('+') to represent spaces, whereas URLs must have spaces hex encoded ('%20').  Files may have plus signs in the name, and therefore, decoding the plus sign as a space produces a different path.

See attached code demonstrating the problem.

Reference:  http://www.w3.org/MarkUp/html-spec/html-spec_8.html#SEC8.2.1
and http://www.ietf.org/rfc/rfc1738.txt",smartshark_2_2,361,commons-configuration,"""[0.09052765369415283, 0.9094723463058472]"""
685,228944,XMLBeanDeclaration.getNestedBeanDeclarations() doesn't escape the node name when query the child node,"When query the nested bean declaration, the child name may contain the special character. So when call ""HierarchicalConfiguration.configurationsAt()"", the node name should be escaped by the expression engine first as the following

{code:java}
getConfiguration().getExpressionEngine().nodeKey(node,"""")
{code}

Call sequence to the problem code:
{noformat}
getNestedBeanDeclarations()
->nested.put(child.getName(), createBeanDeclaration(child));
   ->List<HierarchicalConfiguration> list = getConfiguration().configurationsAt(node.getName());
{noformat}",smartshark_2_2,620,commons-configuration,"""[0.1057601273059845, 0.8942398428916931]"""
686,228945,PropertiesConfigurationLayout.getCanonicalComment throws StringIndexOutOfBoundsException when line before property has a single space,"When a properties file has a line with a single space in it, calling getCanonicalComment(<propertyKey>, false) on the PropertiesConfigurationLayout will throw a StringIndexOutOfBoundsException. Below is the stack trace:

{code}
java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.charAt(String.java:686)
	at org.apache.commons.configuration.PropertiesConfigurationLayout.stripCommentChar(PropertiesConfigurationLayout.java:768)
	at org.apache.commons.configuration.PropertiesConfigurationLayout.trimComment(PropertiesConfigurationLayout.java:741)
	at org.apache.commons.configuration.PropertiesConfigurationLayout.constructCanonicalComment(PropertiesConfigurationLayout.java:900)
	at org.apache.commons.configuration.PropertiesConfigurationLayout.getCanonicalComment(PropertiesConfigurationLayout.java:212)
{code}
",smartshark_2_2,602,commons-configuration,"""[0.06304281949996948, 0.9369572401046753]"""
687,228946,Deadlock during refresh properties,"Hi
Commons configurations get itself stuck in deadlock when refreshing
properties using Managed reloading strategy. It seems to me it get stuck
because of fireEvent in reload method. Another access grabs lock on
synchronized (getNodeCombiner()) when trying to rebuild but Combined
configuration is one of the listeners for event es well and it gets
stuck when processing invalidate. Can anyone suggest quick fix please?
Relevant information follows.
Thanks
Pavel

Configuration:
{code:xml}
<configuration> 
  <override>
    <system/>    
    <properties fileName=""gsxweb.properties"" throwExceptionOnMissing=""false""
       config-name=""gsxweb"" config-optional=""false"" listDelimiter=""|"">
       <reloadingStrategy config-class=""org.apache.commons.configuration.reloading.ManagedReloadingStrategy""/>      
    </properties>    
  </override> 
</configuration>
{code}

Our Reload code:

{code:java}
        int ln = combinedConfiguration.getNumberOfConfigurations();
        int reloaded = 0;
        for (int i = 0; i < ln; i++) {
            Configuration conf = combinedConfiguration.getConfiguration(i);
            if (conf instanceof PropertiesConfiguration) {
                ManagedReloadingStrategy strat = null;
                ReloadingStrategy strategy = ((PropertiesConfiguration) conf).getReloadingStrategy();
                //refresh if managed strategy
                if (strategy instanceof ManagedReloadingStrategy) {
                    ((ManagedReloadingStrategy) strategy).refresh();
                //reload if file changed strategy    
                } else if (strategy instanceof FileChangedReloadingStrategy) {                    
                    ((PropertiesConfiguration) conf).reload();
                }
                reloaded++;
            }
        }
{code}

{code}
Stack trace of deadlock threads
Name: http-10980-1
State: BLOCKED on
org.apache.commons.configuration.tree.OverrideCombiner@8511bb owned by:
http-10980-6
Total blocked: 154  Total waited: 2

Stack trace: 
org.apache.commons.configuration.CombinedConfiguration.invalidate(CombinedConfiguration.java:474)
org.apache.commons.configuration.CombinedConfiguration.configurationChanged(CombinedConfiguration.java:488)
org.apache.commons.configuration.event.EventSource.fireEvent(EventSource.java:249)
org.apache.commons.configuration.AbstractFileConfiguration.fireEvent(AbstractFileConfiguration.java:911)
org.apache.commons.configuration.AbstractFileConfiguration.reload(AbstractFileConfiguration.java:828)
   - locked java.lang.Object@127e34c
org.apache.commons.configuration.AbstractFileConfiguration.isEmpty(AbstractFileConfiguration.java:927)
org.apache.commons.configuration.reloading.ManagedReloadingStrategy.refresh(ManagedReloadingStrategy.java:91)
com.gsx.properties.PropertyProviderImpl.reset(PropertyProviderImpl.java:203)
   - locked java.lang.Class@109bcda
org.apache.jsp.test.testPropertyProvider_jsp._jspService(testPropertyProvider_jsp.java:60)
org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:374)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:342)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:267)
javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)


Name: http-10980-6
State: BLOCKED on java.lang.Object@127e34c owned by: http-10980-1
Total blocked: 115  Total waited: 2

Stack trace: 
org.apache.commons.configuration.AbstractFileConfiguration.reload(AbstractFileConfiguration.java:814)
org.apache.commons.configuration.AbstractFileConfiguration.getKeys(AbstractFileConfiguration.java:939)
org.apache.commons.configuration.ConfigurationUtils.copy(ConfigurationUtils.java:139)
org.apache.commons.configuration.ConfigurationUtils.convertToHierarchical(ConfigurationUtils.java:199)
org.apache.commons.configuration.CombinedConfiguration
$ConfigData.getTransformedRoot(CombinedConfiguration.java:794)
org.apache.commons.configuration.CombinedConfiguration.constructCombinedNode(CombinedConfiguration.java:653)
org.apache.commons.configuration.CombinedConfiguration.getRootNode(CombinedConfiguration.java:504)
   - locked
org.apache.commons.configuration.tree.OverrideCombiner@8511bb
org.apache.commons.configuration.HierarchicalConfiguration.fetchNodeList(HierarchicalConfiguration.java:925)
org.apache.commons.configuration.HierarchicalConfiguration.getProperty(HierarchicalConfiguration.java:327)
org.apache.commons.configuration.CombinedConfiguration.getProperty(CombinedConfiguration.java:578)
org.apache.commons.configuration.AbstractConfiguration.resolveContainerStore(AbstractConfiguration.java:1155)
org.apache.commons.configuration.AbstractConfiguration.getString(AbstractConfiguration.java:1034)
org.apache.jsp.test.testPropertyProvider_jsp._jspService(testPropertyProvider_jsp.java:69)
org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:374)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:342)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:267)
javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
{code}",smartshark_2_2,325,commons-configuration,"""[0.07074570655822754, 0.9292542934417725]"""
688,228947,XMLPropertyListConfiguration doesn't handle empty dictionary correctly,"Empty ""dict"" elements are not handled properly.   During printing of a configuration, configuration nodes that have no children and no value are assumed to be ""strings"" (see XMLPropertyListConfiguration.java:printValue()'s last ""else"" clause).   <dict/> is parsed in such a way (see startElement() and endElement() of that same file) that it ends up creating a configuration node with no children and no value.  Thus printing out a parsed property list will print <string>null</string> everywhere an empty dictionary was in the input.
",smartshark_2_2,395,commons-configuration,"""[0.06868968158960342, 0.9313102960586548]"""
689,228948,PropertiesConfiguration does not use the default encoding to load files,"The piece of code

{code:title=PropertiesConfiguration.java}
    // initialization block to set the encoding before loading the file in the constructors
    {
        setEncoding(DEFAULT_ENCODING);
    }
{code}

seems to set correctly the default encoding, but this block is called after ""super()"" in constructors.

So when using either PropertiesConfiguration(java.io.File file), PropertiesConfiguration(java.lang.String fileName) or PropertiesConfiguration(java.net.URL url), the super() statement is called, and it loads the file without the default encoding.",smartshark_2_2,319,commons-configuration,"""[0.13580016791820526, 0.8641998171806335]"""
690,228949,XMLConfiguration does not fully support disabled delimiter parsing,"A call to setDelimiterParsingDisabled(true) should completely turn off the mechanism for searching for list delimiters and splitting property values.

However XMLConfiguration.save() escapes list delimiters even in this mode. When later such a configuration file is loaded and delimiter parsing is turned off, the values of affected properties will contain the escape character.",smartshark_2_2,257,commons-configuration,"""[0.1623072326183319, 0.8376927375793457]"""
691,228950,ClassCastException in BeanHelper constructing beans with a list of child beans,"If you wish to create a bean that has a collection as a property where that collection contains multiple beans, the code crashes as it assumes that you can only ever have single bean declarations as children. 

For example the following brief construct is not possible to build

class A {
   String name;
}

class B {
   List<A> children;
}

The result is the following stack trace 

Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.commons.configuration.beanutils.BeanDeclaration
	at org.apache.commons.configuration.beanutils.BeanHelper.initBean(BeanHelper.java:212)
	at org.apache.commons.configuration.beanutils.TestBeanHelper$TestBeanFactory.createBean(TestBeanHelper.java:579)
	at org.apache.commons.configuration.beanutils.BeanHelper.createBean(BeanHelper.java:342)
	... 33 more


It's pretty easy to see why when you look at the code beginning line 208:

  for (Map.Entry<String, Object> e : nestedBeans.entrySet())
  {
     String propName = e.getKey();
     Class<?> defaultClass = getDefaultClass(bean, propName);
     initProperty(bean, propName, createBean(
 	   (BeanDeclaration) e.getValue(), defaultClass));
  }
 
Note the silly assumption that e.getValue() only ever has BeanDeclaration instances, where it could be BeanDeclaration[], or Collection<BeanDeclaration> as other possible options - which is what the above example show. 

Extended version of the existing unit test to follow that illustrates the problem.",smartshark_2_2,467,commons-configuration,"""[0.08383491635322571, 0.9161650538444519]"""
692,228951,Inconsistency with configurationAt method.,"After some testing, I found some inconsistency with the use of {{configurationAt()}}, to simplify it, this version is working correctly, returning the {{HierarchicalConfiguration}} pointing to the right property:
{code:java}
config.configurationAt(""property.indexedProperty(0)"", false);
{code}
While this version, instead of returning a {{HierarchicalConfiguration}} with the root tracking node pointing to {{indexedProperty(0)}}, it returns the tracking node pointing to the root of the whole tree of properties.
{code:java}
config.configurationAt(""property.indexedProperty(0)"", true);
{code}

I'm trying to implement a support to a new format called [SURF|https://urf.io/surf/] created by GlobalMentor, but I didn't override this method, so I think it's coming from the original implementation.

More informations may be found at [https://globalmentor.atlassian.net/browse/URF-34].",smartshark_2_2,672,commons-configuration,"""[0.6163951754570007, 0.3836047947406769]"""
693,228952,interpolator for reading environment variables,"Hello there,

This is an extension of issue Id CONFIGURATION-284

The ""env"" interpolator prefix still does not seem to work. On investigation, I noticed that we have a new java class to support reading environment variables from different OSes, but there is no interpolator class (extending StrLookup) that supports the ""env"" prefix.
Could someone look into this? For the moment, I have put in a class myself locally. Will add to the repository once I find some time.. 
Regards.
",smartshark_2_2,382,commons-configuration,"""[0.9951900243759155, 0.004809941630810499]"""
694,228953,The Windows file path cannot be saved correctly as expected in XMLConfiguration,"I want to generate a XML as:
{code:xml}
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>
<Test>
    <Cluster>
        <Server location=""C:\Server92""/>
    </Cluster>
</Test>
{code}

Java Code:

{code:title=Test.java|borderStyle=solid}
XMLConfiguration config = new XMLConfiguration();
config.setRootElementName(""Test"");
config.addProperty(""Cluster.Server[@location]"",  ""C:\\Server92"");
config.save(""C:\\NEW.xml"");
{code}

BUT after running the Java Code, the generated XML looks like:
{code:xml}
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>
<Test>
<Cluster>
<Server location=""C:\\Server92""/>
</Cluster>
</Test>
{code}
You will find that the location is ""C:\ \Server92"", BUT what I expected is ""C:\Server92"".",smartshark_2_2,379,commons-configuration,"""[0.13071398437023163, 0.8692860007286072]"""
695,228954,Formatter for properties file,"It would be cool to retain all formatting of the original properties file when writing back some new entry. If properties file is large and have many commented entries which are there as a remainder of all possible options that can be set, those commented entries should noot be deleted when saving configuration.

As mentioned in http://issues.apache.org/jira/browse/CONFIGURATION-170, a formatter class of some kind would be the solution.",smartshark_2_2,717,commons-configuration,"""[0.9983824491500854, 0.0016176106873899698]"""
696,228955,[configuration] Messed file on saving an XMLConfiguration,"The XMLConfiguration is a configuration based on XML. But when the 
configuration is used to save config properties back into the XML file from 
code, its makes a total mess of the file and doesnt follow basic XML 
conventions. 

If a particular key group already exists, rather than insert new keys there, 
it always just appends new properties to the end of the file. After repeated 
modification, the file becomes an unstructured, unreadable mess. What should 
happen is illustrated below. Starting file:

<configuration>
  <a>
     <key1>value1</key1>
  </a>
</configuration>

Configuration.addProperty(""a.key2"", ""value2"") should result in:

<configuration>
  <a>
     <key1>value1</key1>
     <key2>value2</key2>
  </a>
</configuration>

Because of the way setProperty() works, (remove then add), this behaviour also 
results in the same property being moved, it if is modified. So in the example 
above, modifying <key1> would cause it to be moved to the end of file, leaving 
an empty <a> group at the start. Its just unworkable! 


Also, a seperate but releated issue is that a ""pretty-print"" output format 
would greatly improve developer and user ease-of-use. Currently, there is no 
indenting, so files that are modified by the XMLConfiguration become very hard 
to read/maintain.",smartshark_2_2,149,commons-configuration,"""[0.38845735788345337, 0.6115425825119019]"""
697,228956,Update optional Spring dependency from 4.2.5.RELEASE to 4.3.9.RELEASE,Update optional Spring dependency from 4.2.5.RELEASE to 4.3.9.RELEASE.,smartshark_2_2,669,commons-configuration,"""[0.9981265664100647, 0.0018734202021732926]"""
698,228957,"After a few unit tests, a few newly created directories not cleaned completely.","After a few unit tests, created folders and files are not deleted. It's a bad practice in software engineering, because disk shouldn't be polluted.

After execution of the test class 'org.apache.commons.configuration2.TestDynamicCombinedConfiguration' ,  'target/test-classes/testwrite' folder isn't deleted recursively.
After execution of the test class 'org.apache.commons.configuration2.TestPropertiesConfiguration',  'target/sharp#1.properties' and 'target/testsave-custom-url.properties' files aren't deleted.
After execution of the class 'org.apache.commons.configuration2.TestXMLConfiguration',  'target/testsample2.xml' file isn't deleted.",smartshark_2_2,652,commons-configuration,"""[0.9948038458824158, 0.005196147132664919]"""
699,228958,Include forceReload or refresh method in AbstractFileConfiguration ,"Consider an enhancement to AbstractFileConfiguration, where in a new method called forceReload() or refresh(), which is similar to the existing reload() method, except that strategy.reloadingRequired() is ignored and so a clear() and load() always occur when this new method is called. I will upload changes to AbstractFileConfiguration with this new method in a bit, for your review.",smartshark_2_2,364,commons-configuration,"""[0.9984831213951111, 0.0015168749960139394]"""
700,228959,Binary property list format,"With OS X 10.4 Apple introduced a variant of the plist format that is compressed using a binary format. That would be nice to support this format with a BinaryPropertyListConfiguration class.

I haven't found the specification of this format. 

The plutil tool can be used to convert between xml and binary plist files.",smartshark_2_2,633,commons-configuration,"""[0.9982993006706238, 0.0017007726710289717]"""
701,228960,DatabaseConfiguration support in DefaultConfigurationBuilder,"DefaultConfigurationBuilder doesn't support the declaration of a DatabaseConfiguration. This could be implemented by specifying the JNDI name of the datasource to use in the descriptor file. The declaration would look like this :

\\{code:xml}
<configuration>
    <database jndi=""java:comp/env/jdbc/myapp"" table=""configuration"" keyColumn=""key"" valueColumn=""value""/>
</configuration>
{code}",smartshark_2_2,610,commons-configuration,"""[0.9981740713119507, 0.0018259360222145915]"""
702,228961,Update optional dependency Spring from 4.3.13.RELEASE to 4.3.14.RELEASE.,Update optional dependency Spring from 4.3.13.RELEASE to 4.3.14.RELEASE.,smartshark_2_2,693,commons-configuration,"""[0.9980461597442627, 0.0019538509659469128]"""
703,228962,Add support to CompositeConfiguration to allow Configurations to be prepend to the LinkedList,Create methods like addFirstConfiguration with similar signatures asÂ addConfiguration but instead of appending them to the LinkedList add them to the start of the list. This would allow for support of inserting new override configs post initial construction without needing to rebuild a newÂ CompositeConfiguration.,smartshark_2_2,707,commons-configuration,"""[0.9984332919120789, 0.0015666934195905924]"""
704,228963,Thread safety for file based configurations,"When autoSave is enabled on a FileConfiguration it's quite frequent to run into a ConcurrentModificationException, simply because setProperty is called from a thread while another thread is saving the configuration.

I suggest to leverage the ReadWriteLock in Java 5 to solve this issue. The read lock is acquired in getProperty(), and the write lock is acquired in setProperty().",smartshark_2_2,551,commons-configuration,"""[0.08679232746362686, 0.9132077097892761]"""
705,228964,Add support for constructor invocation in bean declarations,"Currently it is possible to declare simple beans in configuration files. These beans are created using a standard constructor; then properties can be set. This functionality is used internally (mainly by {{DefaultConfigurationBuilder}}), but is provided to client code as well.

When the library design goes more in the direction of immutable objects it will be necessary to support constructor invocation as well. I expect that this will be required for a new version of {{DefaultConfigurationBuilder}}, so it makes sense to implement this feature.",smartshark_2_2,606,commons-configuration,"""[0.9983610510826111, 0.0016389115480706096]"""
706,228965,Change package and maven coordinates,"Version 2.0 will break compatibility with the 1.x series. To avoid jar and maven hell, the package and the maven coordinates have to be adapted.

The new package name will be {{org.apache.commons.configuration2}}.

Maven coordinates will become:
groupId {{org.apache.commons}},
artifactId {{commons-configuration2}}",smartshark_2_2,485,commons-configuration,"""[0.9980011582374573, 0.0019988773856312037]"""
707,228966,INI removable delimiter spaces,org.apache.commons.configuration2.INIConfiguration.write creates INI files with spaces around the '=' delimiter. Older software which is not expecting extra spaces can break. Would be great if delimiter spaces were configurable. Thx!,smartshark_2_2,674,commons-configuration,"""[0.9972694516181946, 0.0027304785326123238]"""
708,228967,Make private methods in PropertiesConfiguration.PropertiesWriter protected,"I believe in version 1.7 the IOFactory was added to PropertiesConfiguration to make it easy to replace the reader and writer. The default reader, PropertiesReader, does not have any private methods. It has protected methods to set the property name, separator, and value. The same cannot be said for the writer, PropertiesWriter. It has private methods like escapeKey, escapeValue, handleBackslashs, and makeSingleLineValue. It makes extending this class difficult. Can those private methods be turned protected?",smartshark_2_2,562,commons-configuration,"""[0.9984633922576904, 0.001536675845272839]"""
709,228968,Provide support for using the XML Commons CatalogResolver or other EntityResolvers,"XMLConfiguration currently only supports a fairly primitive EntityResolver. Replacing this resolver requires creating a new DocumentBuilder, adding the EntityResolver to it and then updating XMLConfiguration to use it. This is fairly complicated and is difficult to do when using DefaultConfigurationBuilder.",smartshark_2_2,166,commons-configuration,"""[0.998431384563446, 0.001568537438288331]"""
710,228969,Persistence mechanism for all configurations,"All configuration should be persistence aware and not only the file based configuration. The proposal is to add two methods in the Configuration interface similar to the sync() and flush() methods of the Preferences API (this is roughly equivalent to reload() and save()).

",smartshark_2_2,523,commons-configuration,"""[0.9983649849891663, 0.0016349684447050095]"""
711,228970,Update Apache Commons Codec from 1.10 to 1.11,Update Apache Commons Codec from 1.10 to 1.11.,smartshark_2_2,682,commons-configuration,"""[0.9980571866035461, 0.0019427597289904952]"""
712,228971,Support Commons Configuration as PropertySource in Spring,"For Commons Configuration 1.0 a Spring modules factory bean (org.springmodules.commons.configuration.CommonsConfigurationFactoryBean) was available to directly use Commons Configuration with Spring's PropertyPlaceholderConfigurer. As this is no longer maintained, similar classes should become part of Commons Configuration 2.0 (with optional Spring dependency).

See mailinglist discussion:
http://mail-archives.apache.org/mod_mbox/commons-user/201604.mbox/%3CCACZkXPy-zQvxz98Z6GGcd8BBHKZ5cukFfgtihSDt97wnoktxyA%40mail.gmail.com%3E",smartshark_2_2,641,commons-configuration,"""[0.9984287619590759, 0.0015711960149928927]"""
713,228972,Treat hierarchy delimiter equivalent to hierarchy instead of escaping,"It would be nice if the . character could work as a hierarchy introducer, in such a way that those files

[learningtask]
trains.lp = 5

[learningtask.trains]
lp = 7

could be queried with config.getList(String.class, ""learningtask.trains.lp"");

(note, at the moment you need to use the escaped form and will only get one or the other: ""learningtask..trains.lp"" or ""learningtask.trains..lp""",smartshark_2_2,631,commons-configuration,"""[0.9984509944915771, 0.0015489726793020964]"""
714,228973,[configuration] AbstractConfiguration.addProperty doesn't split arrays,"The addProperty method in AbstractConfiguration is responsible for adding
multiple values separatly in the configuration, it splits Strings like ""a, b,
c"", it iterates through Collections, but it doesn't handle arrays (Object[],
int[], float[]...)",smartshark_2_2,38,commons-configuration,"""[0.3016580045223236, 0.698341965675354]"""
715,228974,Update user guide to prepare deprecation of ConfigurationFactory,"In the current user guide there is a large chapter about using {{ConfigurationFactory}}. However, there are plans to deprecate this class in favor of {{DefaultConfigurationBuilder}}. Therefore it makes sense to update the documentation that only the latter is described.",smartshark_2_2,393,commons-configuration,"""[0.9982243180274963, 0.0017756364541128278]"""
716,228975,Rework sychronization of configurations,"In the original approach configuration objects were not thread-safe. In later versions, more and more synchronization was added to support concurrent access to configuration properties, even when reloads occurred.

Unfortunately, the current implementation is not very clean and has a lot of problems. There a several bug reports related to this topic. Therefore, version 2.0 which will break backwards compatibility is a good opportunity to introduce a new concept for synchronization.

Of course, the new approach should be easier, cleaner, and customizable. I would also like to have an option to avoid synchronization completely if this is not required for a specific use case. Maybe a similar pattern as used by the Java collections framework can be applied: per default, configurations are not thread-safe, but there is an option to add synchronization so that they can be accessed concurrently.",smartshark_2_2,552,commons-configuration,"""[0.9983364939689636, 0.0016635243082419038]"""
717,228976,Add support for Commons Text 1.5 new string lookups as default lookups,"Since we depend on Apache Commons Text for string lookups, add support for Commons Text 1.5's new string lookup as default lookups.

Provides access to lookups defined in Apache Commons Text:
 * ""base64"" for the {{Base64StringLookup}} since Apache Commons Text 1.5.
 * ""const"" for the {{ConstantStringLookup}} since Apache Commons Text 1.5.
 * ""date"" for the {{DateStringLookup}}.
 * ""env"" for the {{EnvironmentVariableStringLookup}}.
 * ""file"" for the {{FileStringLookup}} since Apache Commons Text 1.5.
 * ""java"" for the {{JavaPlatformStringLookup}}.
 * ""localhost"" for the {{LocalHostStringLookup}}, see {{#localHostStringLookup()}} for key names; since Apache Commons Text 1.3.
 * ""properties"" for the {{PropertiesStringLookup}} since Apache Commons Text 1.5.
 * ""resourceBundle"" for the {{ResourceBundleStringLookup}} since Apache Commons Text 1.5.
 * ""script"" for the {{ScriptStringLookup}} since Apache Commons Text 1.5.
 * ""sys"" for the {{SystemPropertyStringLookup}}.
 * ""url"" for the {{UrlStringLookup}} since Apache Commons Text 1.5.
 * ""xml"" for the {{XmlStringLookup}} since Apache Commons Text 1.5.

Â ",smartshark_2_2,732,commons-configuration,"""[0.9984500408172607, 0.0015498814173042774]"""
718,228977,INIConfiguration does not support line continuation,"INIConfiguration does not support line continuation.  See http://en.wikipedia.org/wiki/INI_file#Escapes for a description.  Using INIConfiguration to read the following file:

[section1]
dotted.var1 = \
	foo
var2 = doodle

Produces the following contents:
    section1.dotted.var1= \
    section1.foo= 

I expected the following contents:
    section1.dotted.var1= foo
",smartshark_2_2,300,commons-configuration,"""[0.3976512551307678, 0.602348804473877]"""
719,228978,Update Java requirement from version 7 to 8.,"Update Java requirement from version 7 to 8.

Then the plan is to add support for some type from {{java.time}}.",smartshark_2_2,742,commons-configuration,"""[0.9982559084892273, 0.0017440974479541183]"""
720,228979,java.lang.IllegalArgumentException: Passed in key must select exactly one node:,"I am trying to pass a list of values as parameters(Key,Vaue) in a XML file. I am trying to pass the XML file to the XMLBeanDeclaration. It throws an error as noted below. Please note Key is the same but values are different in the list below. 

<?xml version=""1.0"" encoding=""ISO-8859-1"" ?>
<config>
  <gui>
    <windowManager config-class=""examples.windows.DefaultWindowManager""
      closable=""false"" resizable=""true"" defaultWidth=""400""
      defaultHeight=""250"">
      <styleDefinition config-class=""examples.windows.WindowStyleDefinition""
        backColor=""#ffffff"" foreColor=""0080ff"" iconName=""myicon"" />
     <styleDefinition config-class=""examples.windows.WindowStyleDefinition""
        backColor=""#00000"" foreColor=""8888ff"" iconName=""youricon"" />
    </windowManager>
  </gui>
</config>

I have a test configuration like this:
private TestConfiguration(String configFile) {
        try {
            URL configURL = getClass().getResource(""windowconfig.xml"");
            XMLConfiguration config = new XMLConfiguration(configURL);
            BeanDeclaration decl = new XMLBeanDeclaration(config, ""gui.windowManager"");
            WindowManager wm = (WindowManager) BeanHelper.createBean(decl);
        } catch (Exception exc) {
            throw new RuntimeException(""Initialization failed"", exc);
        }
    }


This is the exception I am getting from the above test:

java.lang.IllegalArgumentException: Passed in key must select exactly one node: styleDefinition 
                at org.apache.commons.configuration.HierarchicalConfiguration.configurationAt(HierarchicalConfiguration.java:581)
                at org.apache.commons.configuration.HierarchicalConfiguration.configurationAt(HierarchicalConfiguration.java:601)
                at org.apache.commons.configuration.beanutils.XMLBeanDeclaration.getNestedBeanDeclarations(XMLBeanDeclaration.java:313)

The HeirarchialConfiguration.configurationAt method only takes one value but not the list of values during XMLBeanDeclaration. 
Is there an alternative mechanism to pass a list of values? I would greatly appreciate any help with this.",smartshark_2_2,169,commons-configuration,"""[0.10874791443347931, 0.8912521004676819]"""
721,228980,[configuration] Tools for code generation of configuration properties,"I would like to just suggest a kind of code generation from a defined 
properties/xml file, so the user can avoid the use of string representing the 
properties on whole source code. For example if I have the following property:

ftpServer.login = anonymous

in order to get the value of such property, instead of:
getProperty(""ftpServer.login"");

to have an equivalente class representation, like this:
getFtpServer().getLogin();

or just:
ftpServer().getLogin();

with this solution, you avoid miss-spelling of the property name (for example: 
instead of ""ftpServer.loging"", ""ftpserver.login""), because you will get a 
compiler error.

in order to get this target we need a kind of code generation of the 
corresponding classes. Like any other generation tools it should be a mapping 
file for configuring the way the properties/subproperties/nodes could be 
mapped, for example tomorrows I would like to map: ftpServer.login to getServe
().getLogin(). With the default configuration the mapping will be the same.

It could be just and ant-taks for doing this, probably with velocity would be 
easy to implement.

Thanks in advance,

David Leal
achepati67@yahoo.es
(If you want I could colaborate)",smartshark_2_2,211,commons-configuration,"""[0.9983677268028259, 0.0016322884475812316]"""
722,228981,[configuration] Support the OpenStep property list format,"This is a RFE to implement a configuration implementation supporting the
OpenStep property list format. This format is used in Mac OSX.

Example file:
{
	""map"" = 
	{
		""key1"" = ""value1"";
		""key2"" = ""value2"";
		""key3"" = ""value3"";
	};
	
	""string"" = ""foo"";
	
	""list"" = 
	(
		""value1"",
		""value2"",
		""value3""
	);
	
	""nested"" = 
	{
		""map"" = 
		{
			""key1"" = ""value1"";
			""key2"" = ""value2"";
			""key3"" = ""value3"";
		};
		
		""string"" = ""bar"";
		
		""list"" = 
		(
			""value1"",
			""value2"",
			""value3""
		);
	};
}



References:
http://developer.apple.com/documentation/Cocoa/Conceptual/PropertyLists/Concepts/OldStylePListsConcept.html
http://www.gnustep.org/resources/documentation/Developer/Base/Reference/NSPropertyList.html

Java implementation:
http://zoe.nu/misc/SZProperties.tgz",smartshark_2_2,29,commons-configuration,"""[0.9983447790145874, 0.001655272557400167]"""
723,228982,YAMLConfiguration keys with double dots,"h2. Issue

Loading a YAML configuration with dotted keys causes dots to be duplicated in the keys of the YAMLConfiguration object.

h2. Repro
100% repro:

file.yml:
{code}some.key.with.dots: 123{code}

{code}
package foo;

import org.apache.commons.configuration2.YAMLConfiguration;
import org.apache.commons.configuration2.builder.FileBasedConfigurationBuilder;
import org.apache.commons.configuration2.builder.fluent.Parameters;
import org.apache.commons.configuration2.ex.ConfigurationException;

public class App {

    public static void main(String[] args) throws ConfigurationException {
        Parameters params = new Parameters();
        FileBasedConfigurationBuilder<YAMLConfiguration> builder1 =
                new FileBasedConfigurationBuilder<>(YAMLConfiguration.class)
                        .configure(params.fileBased().setFileName(""file.yml""));
        
        YAMLConfiguration conf = builder1.getConfiguration();
        conf.getKeys().forEachRemaining(System.out::println);

        System.out.println(""---"");
        YAMLConfiguration yaml = new YAMLConfiguration();
        yaml.read(new InputStreamReader(ClassLoader.getSystemResourceAsStream(""file.yml"")));
        yaml.getKeys().forEachRemaining(System.out::println);
    }
}
{code}
prints
{code}some..key..with..dots
---
some..key..with..dots
{code}


That is a serious bug for a configuration system. The issue may easily go untested for some keys and only reveal itself on production.",smartshark_2_2,689,commons-configuration,"""[0.43496620655059814, 0.5650337934494019]"""
724,228983,[configuration] [1.2RC1] ConfigurationDynaBean may implement java.util.Map,"ConfigurationDynaBean says 
""It also implements a Map interface so that it can be used in JSP 2.0 Expression
Language expressions.""

Map interface is not implemented (in 1.2RC1) and ConfigurationDynaBean cannot be
used from EL :
<c:out value=""${configuration['un.test']}""/> :
Unable to find a value for ""un.test"" in object of class
""org.apache.commons.configuration.beanutils.ConfigurationDynaBean"" using
operator ""[]"" (null)

Simply implementing Map.get(Object) may solve the problem.",smartshark_2_2,176,commons-configuration,"""[0.8933628797531128, 0.10663706809282303]"""
725,228984,DefaultConfigurationKey.setExpressionEngine - non-final method called by constructor,"If a constructor calls an overrideable method, sub-classes that override the method can cause failures",smartshark_2_2,501,commons-configuration,"""[0.9977400302886963, 0.0022599957883358]"""
726,228985,Accessing missing properties of immutable subset throws UndeclaredThrowableException instead of NoSuchElementException,"When I access a non-existing property of a configuration that has ThrowExceptionOnMissing set an java.lang.reflect.UndeclaredThrowableException is thrown. I think the desired behaviour would be to pass the underlying java.util.NoSuchElementException through from the ImmutableConfigurationInvocationHandler.

Example code:
{code}
import org.apache.commons.configuration2.ImmutableConfiguration;
import org.apache.commons.configuration2.PropertiesConfiguration;

public class ImmutableConfigurationStacktraceTest {

	public static void main(String[] args) {
		//create empty PropertiesConfiguration
		PropertiesConfiguration propertiesConfiguration = new PropertiesConfiguration();
		//throw error when non-existing property is requested
		propertiesConfiguration.setThrowExceptionOnMissing(true);
		
		//get immutable subset handled by ImmutableConfigurationInvocationHandler
		ImmutableConfiguration subset = propertiesConfiguration.immutableSubset("""");
		
		//this should throw java.util.NoSuchElementException
		subset.getString(""whoops!"");
	}

}
{code}

gives the stacktrace:

{code}
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at com.sun.proxy.$Proxy0.getString(Unknown Source)
	at test.main(test.java:10)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.commons.configuration2.ImmutableConfigurationInvocationHandler.invoke(ImmutableConfigurationInvocationHandler.java:79)
	... 2 more
Caused by: java.util.NoSuchElementException: Key 'whoops!' does not map to an existing object!
	at org.apache.commons.configuration2.AbstractConfiguration.throwMissingPropertyException(AbstractConfiguration.java:1873)
	at org.apache.commons.configuration2.AbstractConfiguration.convert(AbstractConfiguration.java:1784)
	at org.apache.commons.configuration2.AbstractConfiguration.getString(AbstractConfiguration.java:1343)
	... 7 more
{code}",smartshark_2_2,563,commons-configuration,"""[0.4649527370929718, 0.5350472927093506]"""
727,228986,Update Snakeyaml from 1.18 to 1.19,Update Snakeyaml from 1.18 to 1.19.,smartshark_2_2,686,commons-configuration,"""[0.9980913996696472, 0.001908609177917242]"""
728,228987,Update Apache Commons Text from 1.4 to 1.5,Update Apache Commons Text from 1.4 to 1.5.,smartshark_2_2,730,commons-configuration,"""[0.9980744123458862, 0.0019255171064287424]"""
729,228988,validation on testonborrow does not work,"Our use of dbcp (1.2.1 and 1.2.2) shows that connection validation on testonborrow does not work. The pool still return closed connections. The error is in the following code.  See the commented out line for the specific error.
----------------------
 ra = ref.get(""testOnBorrow"");
  if (ra != null && ra.getContent() != null) {
        	//comment out old code
        	//ikds.setTestOnBorrow(Boolean.getBoolean(ra.getContent().toString()));
        	ikds.setTestOnBorrow(Boolean.valueOf(ra.getContent().toString()).booleanValue());

        }

        ra = ref.get(""testOnReturn"");
        if (ra != null && ra.getContent() != null) {
            ikds.setTestOnReturn(Boolean.valueOf(
                ra.getContent().toString()).booleanValue());
        }",smartshark_2_2,254,commons-dbcp,"""[0.19071291387081146, 0.8092871308326721]"""
730,228989,All connections to MS SQL Server fail when SharedPoolDataSource has testOnBorrow set,"When testOnBorrow is set on a SharedPoolDataSource with connections on MS SQL Server, the first (and all subsequent) connections retrieved from the pool get the exception in this stack trace as a result of executing Connection.isValid() in the validation on borrow.

java -cp "".;$CLASSPATH"" Dbcp2TestOnBorrowFailure 'jdbc:sqlserver://myserver.example.com;databasename=mydb' myuser mypassword
Iteration: 0
PooledConnection was reused, withoutits previous Connection being closed.
java.sql.SQLException: PooledConnection was reused, withoutits previous Connection being closed.
        at org.apache.commons.dbcp2.cpdsadapter.PooledConnectionImpl.getConnection(PooledConnectionImpl.java:183)
        at org.apache.commons.dbcp2.datasources.InstanceKeyDataSource.getConnection(InstanceKeyDataSource.java:951)
        at Dbcp2TestOnBorrowFailure.getConnection(Dbcp2TestOnBorrowFailure.java:30)
        at Dbcp2TestOnBorrowFailure.main(Dbcp2TestOnBorrowFailure.java:42)

The code for this example is pasted in here (the preview shows that the code indentation and formatting is lost -- sorry):
----------------------------------------------------------------------
import java.sql.Connection;
import java.sql.Driver;
import java.sql.SQLException;
import java.util.Properties;

import org.apache.commons.dbcp2.cpdsadapter.DriverAdapterCPDS;
import org.apache.commons.dbcp2.datasources.SharedPoolDataSource;

public final class Dbcp2TestOnBorrowFailure {

    private static final int MAX_CONNECTIONS = 3;

    private final SharedPoolDataSource poolDataSource
        = new SharedPoolDataSource();

    public Dbcp2TestOnBorrowFailure(String jdbcUrl) {
        DriverAdapterCPDS cpds = new DriverAdapterCPDS();
        cpds.setUrl(jdbcUrl);

        poolDataSource.setConnectionPoolDataSource(cpds);
        poolDataSource.setMaxTotal(MAX_CONNECTIONS);
        poolDataSource.setMaxConnLifetimeMillis(300000);
        poolDataSource.setDefaultMaxWaitMillis(1000);
        poolDataSource.setDefaultAutoCommit(Boolean.TRUE);
        poolDataSource.setDefaultTestOnBorrow(true);
        poolDataSource.setValidationQueryTimeout(3000);
    }

    public Connection getConnection(String user, String pwd) throws Exception {
        return poolDataSource.getConnection(user, pwd);
    }

    public static void main(String[] args) {
        if (args.length != 3) {
            usage();
        }

        Dbcp2TestOnBorrowFailure poolFailure = new Dbcp2TestOnBorrowFailure(args[0]);
        for (int i = 0; i < 10; ++i) {
            System.err.println(""Iteration: "" + i);
            Connection conn = null;
            try {
                conn = poolFailure.getConnection(args[1], args[2]);
                conn.close();
                conn = null;
            }
            catch (Exception e) {
                System.err.println(e.getMessage());
                e.printStackTrace(System.err);
                System.exit(88);
            }
            finally {
                try {
                    if (conn != null) {
                        conn.close();
                    }
                } catch (SQLException e) {

                }
            }
        }
    }

    private static void usage() {
        System.err.println(""Usage: java -cp ... Dbcp2TestOnBorrowFailure jdbcUrl user password"");
        System.exit(77);
    }
}",smartshark_2_2,441,commons-dbcp,"""[0.09935204684734344, 0.9006479382514954]"""
731,228990,Wrong JMX base name derived in BasicDataSource#updateJmxName,"Prior to version 2.3, the registeredJmxName class member was of type ObjectName, which has its own toString implementation. From version 2.3 the registeredJmxName is of type ObjectNameWrapper which does not implement toString, causing the JMX base name derived in the updateJmxName method to be on the form
{noformat}
org.apache.commons.dbcp2.ObjectNameWrapper@<hash_code>,connectionpool={noformat}",smartshark_2_2,548,commons-dbcp,"""[0.5759680271148682, 0.4240320026874542]"""
732,228991,equals in DelegatingXXX is not reflexive and could be faster,"The equals() methods in the different DelegatingXXX classes do not start with a simple object equality check for this.
The equals methods do not implement the contract for equals because x.equals(x) where getInnermostDelegate() for x returns null will return false.
Performance-wise the check would help since you don't need to walk the delegate tree.

I will attach a patch.

Rob
",smartshark_2_2,390,commons-dbcp,"""[0.9855850338935852, 0.01441491860896349]"""
733,228992,infinite loop in PoolableConnection.isDisconnectionSqlException,"There is an infinite loop in PoolableConnection.isDisconnectionSqlException.  If e.getNextException() == e, which happens to be the case for me (I'm mybatis).  I think there should be a check around line 329 to see if e == e.getNextException() and if so, stop recursing.",smartshark_2_2,471,commons-dbcp,"""[0.07243704795837402, 0.9275630116462708]"""
734,228993,DelegatingConnection.equals fails if both delegates are null,"{code:java}
DelegatingConnection conn3 = new DelegatingConnection(null);
assertTrue(conn3.equals(new DelegatingConnection(null)));
{code}

Test fails.
",smartshark_2_2,392,commons-dbcp,"""[0.07174214720726013, 0.9282578825950623]"""
735,228994,DataSourceXAConnectionFactory does not store the XAConnection,"We have been diagnosing a leak with DBCP and XA mySQL and discovered that the mySQL driver expects close() to be invoked on the XAConnection obtained from xaDataSource.getXAConnection() instead of the java.sql.Connection returned by xaConnection.getConnection();

The following code snippet in DataSourceXAConnectionFactory illustrates how the XAConnection is lost:

    public Connection createConnection() throws SQLException {
        // create a new XAConection
        XAConnection xaConnection;
        if (username == null) {
            xaConnection = xaDataSource.getXAConnection();
        } else {
            xaConnection = xaDataSource.getXAConnection(username, password);
        }

        // get the real connection and XAResource from the connection
        Connection connection = xaConnection.getConnection();
        XAResource xaResource = xaConnection.getXAResource();

        // register the xa resource for the connection
        transactionRegistry.registerConnection(connection, xaResource);

        return connection;
    }

In the code snippet above, the XAConnection is basically discarded after using it to obtain the java.sql.Connection and XAResource. It would be ideal if it actually associates the XAConnection in the transactionRegistry as well so that when PooledManagedConnection handles reallyClose(), it can also invoke close() on the XAConnection by interrogating the TransactionRegistry for the actual XAConnection to close.

This may be something that's mySQL specific.",smartshark_2_2,352,commons-dbcp,"""[0.0958942398428917, 0.9041057825088501]"""
736,228995,org.apache.commons.dbcp2.cpdsadapter.DriverAdapterCPDS should use a char[] instead of a String to store passwords.,org.apache.commons.dbcp2.cpdsadapter.DriverAdapterCPDS should use a char[] instead of a String to store passwords.,smartshark_2_2,496,commons-dbcp,"""[0.9931994080543518, 0.006800544913858175]"""
737,228996,[dbcp] improved Exception nesting in ConnectionPool,"(reported on commons-dev by Meikel Bisping on 26/01/2005)
----------------------------------------------------------
I tried to establish a ConnectionPool with DBCP to an Informix
Database using your examples.
I always got the exeception ""pool exhausted"" when trying to open the
first connection.
I eventually found out during debugging that the problem was an older
Informix driver (that is generally still in use, though). 
It didn't support read-only mode and threw an SQLException even when
calling setReadOnly(false).
It would be helpful for users if in cases like that the actual
exception would be thrown, ""pool exhausted"" didn't help much.",smartshark_2_2,178,commons-dbcp,"""[0.9973568916320801, 0.0026431584265083075]"""
738,228997,org.apache.commons.dbcp2.managed.DataSourceXAConnectionFactory should use a char[] instead of a String to store passwords.,The classÂ {{org.apache.commons.dbcp2.managed.DataSourceXAConnectionFactory}} should use aÂ {{char[]}} instead of aÂ {{String}} to store passwords.,smartshark_2_2,499,commons-dbcp,"""[0.9951536655426025, 0.004846366588026285]"""
739,228998,BasicDataSource doesn't include PreparedStmt Pooling,"BasicDataSource was missing prepared statement pooling.   Here's the patch

*** BasicDataSource.java.orig	Sat Jul 20 18:38:36 2002
--- BasicDataSource.java	Fri Mar 14 11:06:10 2003
***************
*** 71,76 ****
--- 71,78 ----
  import org.apache.commons.dbcp.PoolableConnectionFactory;
  import org.apache.commons.dbcp.PoolingDataSource;
  import org.apache.commons.pool.impl.GenericObjectPool;
+ import org.apache.commons.pool.impl.GenericKeyedObjectPool;
+ import org.apache.commons.pool.impl.GenericKeyedObjectPoolFactory;
  
  
  /**
***************
*** 267,272 ****
--- 269,331 ----
      }
  
  
+     /**
+      * Prepared statement pooling for this pool.
+      */
+     protected boolean poolingStatements = true;
+     
+     /**
+      * Returns true if we are pooling statements.
+      * @return boolean
+      */
+     public boolean isPoolingStatements()
+     {
+         return poolingStatements;
+     }
+ 
+     /**
+      * Sets whether to pool statements or not.
+      * @param poolStatements pooling on or off
+      */
+     public void setPoolingStatements(boolean poolingStatements)
+     {
+          this.poolingStatements = poolingStatements;
+     }
+ 
+ 
+     /**
+      * The maximum number of open statements that can be allocated from
+      * the statement pool at the same time, or zero for no limit.  Since 
+      * a connection usually only uses one or two statements at a time, this is
+      * mostly used to help detect resource leaks.  A NoSuchElementException will
+      * be thrown when this limit is exceeded.
+      */
+     protected int maxOpenStatements = GenericKeyedObjectPool.DEFAULT_MAX_ACTIVE;
+ 
+     public int getMaxOpenStatements() {
+         return (maxOpenStatements);
+     }
+ 
+     public void setMaxOpenStatements(int maxOpenStatements) {
+         this.maxOpenStatements = maxOpenStatements;
+     }
+     
+     /**
+      * The maximum number of statements that can be in
+      * the statement pool at the same time, or zero for no limit
+      */
+     protected int maxStatements = GenericKeyedObjectPool.DEFAULT_MAX_IDLE;
+ 
+     public int getMaxStatements() {
+         return (maxStatements);
+     }
+ 
+     public void setMaxStatements(int maxStatements) {
+         this.maxStatements = maxStatements;
+     }
+ 
+  
+ 
      // ----------------------------------------------------- Instance Variables
  
  
***************
*** 550,555 ****
--- 609,626 ----
              connectionPool.setTestOnBorrow(true);
          }
  
+         // Set up statement pool, if desired
+         GenericKeyedObjectPoolFactory statementFactory = null;
+         if (poolingStatements) {
+         	statementFactory = new GenericKeyedObjectPoolFactory(null, 
+                         maxOpenStatements,
+                         // If statements are unlimited, then always grow
+                         (maxOpenStatements == 0 || maxStatements == 0) ?
+                             GenericKeyedObjectPool.WHEN_EXHAUSTED_GROW : 
+                             GenericKeyedObjectPool.WHEN_EXHAUSTED_FAIL, 
+                         0, maxStatements); 
+         }
+         
          // Set up the driver connection factory we will use
          if (username != null) {
              connectionProperties.put(""user"", username);
***************
*** 572,578 ****
              connectionFactory =
                  new PoolableConnectionFactory(driverConnectionFactory,
                                                connectionPool,
!                                               null, // FIXME - stmtPoolFactory?
                                                validationQuery,
                                                defaultReadOnly,
                                                defaultAutoCommit,
--- 643,649 ----
              connectionFactory =
                  new PoolableConnectionFactory(driverConnectionFactory,
                                                connectionPool,
!                                               statementFactory,
                                                validationQuery,
                                                defaultReadOnly,
                                                defaultAutoCommit,
*** BasicDataSourceFactory.java.orig	Fri Jun 21 15:56:14 2002
--- BasicDataSourceFactory.java	Fri Mar 14 10:16:13 2003
***************
*** 194,199 ****
--- 194,217 ----
                  (Boolean.valueOf(ra.getContent().toString()).booleanValue());
          }
  
+         ra = ref.get(""poolStatements"");
+         if (ra != null) {
+             dataSource.setPoolingStatements
+                 (Boolean.valueOf(ra.getContent().toString()).booleanValue());
+         }
+ 		
+         ra = ref.get(""maxStatements"");
+         if (ra != null) {
+             dataSource.setMaxStatements
+                 (Integer.parseInt(ra.getContent().toString()));
+         }
+ 
+         ra = ref.get(""maxOpenStatements"");
+         if (ra != null) {
+             dataSource.setMaxOpenStatements
+                 (Integer.parseInt(ra.getContent().toString()));
+         }
+ 
          // Return the configured data source instance
          return (dataSource);",smartshark_2_2,182,commons-dbcp,"""[0.7417608499526978, 0.25823915004730225]"""
740,228999,[dbcp] BasicDataSource : setter for connectionProperties,"Adding a javabean-style setter for connectionProperties would certainly ease the
configuration of a BasicDataSource within a Dependency Injection framework (eg
Spring).

see: http://article.gmane.org/gmane.comp.java.springframework.user/6501/

Thanks,
Maarten",smartshark_2_2,150,commons-dbcp,"""[0.9983769655227661, 0.001623002695851028]"""
741,229000,Hundreads of threads in Wait state with below stack trace,"Hello Team,

Our application suddenly stops responding, when we checked thread dump, most of the threads are in wait state with below stack trace, we had to restart server to make it active, can you pelase provide your inputs on the root cause & resolution?

Â 

""JSockConn Thread #4532"" #40906 prio=5 os_prio=0 tid=0x00007f84382ce800 nid=0xc692 waiting on condition [0x00007f83d38f8000]
 java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for <0x00000005c30a11b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
 at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:583)
 at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:442)
 at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)
 at org.apache.commons.dbcp2.PoolingDataSource.getConnection(PoolingDataSource.java:134)
 at org.apache.commons.dbcp2.BasicDataSource.getConnection(BasicDataSource.java:1533)",smartshark_2_2,542,commons-dbcp,"""[0.3134872615337372, 0.6865127682685852]"""
742,229001,"Allow connection, statement, and result set to be closed multiple times","This patch allows Connection, Statement, PreparedStatement, CallableStatement and ResultSet to be closed multiple times.  The first time close is called the resource is closed and any subsequent calls have no effect.  This behavior is required as per the JavaDocs for these classes.  The patch adds tests for closing all types multiple times and updates any tests that incorrectly assert that a resource can be closed more then once.


This patch fixes DBCP-134 and DBCP-3",smartshark_2_2,314,commons-dbcp,"""[0.9771630764007568, 0.02283695712685585]"""
743,229002,Automatic Restart of BasicDataSource After Changing Connection Properties Such as Url,"It would be nice if BasicDataSource could automatically ""restart"" after changing a connection property. For example, in our application, we sometimes have to change the connection url at runtime (i.e. we are connected to one database and then, under certain conditions, we switch to another database). Currently, we workaround this limitation by manually calling BasicDataSource.close() after calling BasicDataSource.setUrl().

Looking at the July 11, 2007 snapshot of the source code for BasicDataSource, it appears that the author was _starting_ to implement this feature. If you look at many of the setters such as setUrl(), setUsername() and setPassword(), you will see this line of code after the corresponding instance variable is set:

  this.restartNeeded = true;

Furthermore, there is this private restart() method (notice the comment ""not used currently""):

  /**
    * Not used currently
    */
  private void restart() {
      try {
          close();
      } catch (SQLException e) {
          log(""Could not restart DataSource, cause: "" + e.getMessage());
      }
  }

To finish implementing this, I think you would only need to add the following snippet at the very top of createDataSource():

  if (restartNeeded) {
    restart();
  }

Some users might not like this feature because it might possibly cause active connections to be killed abruptly (I'm not sure though because I haven't really looked at the implementation of close() very closely). To calm their fears, perhaps you could make this auto-restart feature optional by adding a boolean property called  ""restartable"". Then you could modify my snippet to this:

  if (restartable && restartNeeded) {
    restart();
  }

Anyway, just my two cents. I think the class is already pretty useful.",smartshark_2_2,411,commons-dbcp,"""[0.9984532594680786, 0.0015467163175344467]"""
744,229003,Add some toString() methods for debugging (never printing user names and passwords),"Add some toString() methods for debugging never printing user names and passwords:
 * org.apache.commons.dbcp2.cpdsadapter.DriverAdapterCPDS
 * org.apache.commons.dbcp2.cpdsadapter.PooledConnectionImpl
 * org.apache.commons.dbcp2.datasources.CPDSConnectionFactory
 * org.apache.commons.dbcp2.datasources.InstanceKeyDataSource
 * org.apache.commons.dbcp2.datasources.PerUserPoolDataSource
 * org.apache.commons.dbcp2.datasources.SharedPoolDataSource
 * org.apache.commons.dbcp2.datasources.UserPassKey (updated not to print passwords even though it was a char[] reference.)

I went YAGNI here and only added what I needed.",smartshark_2_2,526,commons-dbcp,"""[0.998369038105011, 0.0016309997299686074]"""
745,229004,Update Apache Commons Logging to 1.2 from 1.1.3,Update Apache Commons Logging to 1.2 from 1.1.3,smartshark_2_2,385,commons-dbcp,"""[0.997788667678833, 0.002211335813626647]"""
746,229005,Don't log and re-throw exception,In a couple of cases the stacktrace is written to the log and the exception re-thrown with a cause. Since DBCP 1.3 is now on JDK 1.4 and the cause will always be there this is IMO unnecessary,smartshark_2_2,232,commons-dbcp,"""[0.9975180625915527, 0.002481963951140642]"""
747,229006,Jitter max connection lifetime,DBCP-156Â - this Jira introduced max connection lifetime setting. But with the current implementation all connections created from a DataSource will be invalidated at the same time as we create all connections during startup. We need to be able to jitter the max connection lifetime so that connections get invalidated over a period of time and not together.,smartshark_2_2,538,commons-dbcp,"""[0.8421511054039001, 0.15784896910190582]"""
748,229007,Update Apache Commons Pool from 2.6.0 to 2.6.1,Update Apache Commons Pool from 2.6.0 to 2.6.1.,smartshark_2_2,546,commons-dbcp,"""[0.998029887676239, 0.0019701370038092136]"""
749,229008,getAutoCommit in PoolableConnectionFactory,"In passivateObject() method of PoolableConnectionFactory, getAutoCommit() is called twice. I am working with Sybase JDBC driver, in their implementation, getAutoCommit() makes a call to database so there are two roundtrips to database. I would like to understand if there is anything that prevent you to call getAutoCommit() once and store the result in a local variable to use it again?",smartshark_2_2,381,commons-dbcp,"""[0.7452448010444641, 0.2547551691532135]"""
750,229009,Support for connection validation interval,"It would be nice to have support for ""connection validation intervals"" like in the Tomcat JDBC connection pool that limit the frequency of validations.
",smartshark_2_2,455,commons-dbcp,"""[0.9984094500541687, 0.001590551808476448]"""
751,229010,Rollback on PoolableConnectionFactory.passivateObject() has to be configuration controlled.,"In certain databases like IBM DB2, the rollback calls are costly. Also, on any database, this avoids an unnecessary call to DB upon returning a connection back to pool, and thus enhances the application performance.

So it would be good, if there is an optional configuration parameter that controls the ""rollbackAsCleanUpAction"" behaviour. This can be made ""false"" (can be true by default, aiming current users) if the application can guarantee itself that, no connection is returned to the pool without an explicit call to commit() or rollback(), even in the case of a non-readonly and non-autocommit connection.

A few more bits: This caused a 1/11, i.e. approx 10% performance loss in our application. Hence making this a bug (a performance issue), and not a Feature Request / Improvement!",smartshark_2_2,328,commons-dbcp,"""[0.9972969889640808, 0.0027030357159674168]"""
752,229011,Allow subclasses of BasicDataSource to provide their own GenericObjectPool implementation,"I had the same need in an older version and I had to do a terrible hack which I would not like to do with a newer version.

The main idea is that I would like to be able to monitor borrow and return events (among other things) and for that I need to provide my own GenericObjectPool implementation.

It would be a small change to add a createObjectPool method and use it in createConnectionPool. Default implementation would be to 
{code}
if (abandonedConfig != null &&
                (abandonedConfig.getRemoveAbandonedOnBorrow() ||
                 abandonedConfig.getRemoveAbandonedOnMaintenance())) {
            gop = new GenericObjectPool<>(factory, config, abandonedConfig);
        }
        else {
            gop = new GenericObjectPool<>(factory, config);
        }
{code}

Subclasses would have the flexibility to provide their own pool implementation.",smartshark_2_2,473,commons-dbcp,"""[0.9984662532806396, 0.0015338128432631493]"""
753,229012,Track callers of active connections for debugging,"Lately we got the following exception
org.apache.commons.dbcp.SQLNestedException: Cannot get a connection, pool exhausted
        at
org.apache.commons.dbcp.PoolingDataSource.getConnection(PoolingDataSource.java:103)
        at
org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:540)

The reason for that was that some piece of code opened a connection, but never closed it. Tracking the active connections (and the callers of the getConnection method) would it make it easier to find such erroneous code.
One possible approach would be to add the connection returned by BasicDataSource.getConnection together with the stacktrace in a Map holding all active connections. And removing the connection from the map during PoolableDataSource.close().
",smartshark_2_2,361,commons-dbcp,"""[0.9962756633758545, 0.003724393667653203]"""
754,229013,[dbcp] Use commons-logging for debugging instead of System.out.println,"At the source code of commons-dbcp, some System.out.println() and
System.err.println() statements can be found, noticeably in the constructor of 
""java.org.apache.commons.dbcp.AbandonedObjectPool"".

These statements are annoying, because none of the developers want to see these
messages but they occuplied the precious space in the log files.

I think it is still appropriate to use this method to emit errors, but for
normal behavior, we should have an option never seeing them.",smartshark_2_2,175,commons-dbcp,"""[0.9984883069992065, 0.0015116926515474916]"""
755,229014,PoolingDataSource closes physical connections,"By executing the attached program and monitoring the process id of the physical connections at the database server, it is possible to demonstrate that the connections are being actually physically closed and reopened by the application at a very high rate.",smartshark_2_2,170,commons-dbcp,"""[0.16757281124591827, 0.8324271440505981]"""
756,229015,Connection validationQuery mechanism should be replaced by new method connection#isValid(),"Hello,
Current method of connection validation relies on validationQuery.
This method has a rather big performance impact on the DB (CPU, for example 3% with SELECT 1 FROM DUAL even with Oracle 10G FAST DUAL) and make an additional query for each borrow (when testOnBorrow is true).
Wouldn't it be better to use new JDBC 4 method isValid which relies on Driver check ? (Oracle would for example use its internal method ping)

Thank you
Philippe
http://www.ubik-ingenierie.com",smartshark_2_2,356,commons-dbcp,"""[0.9983469247817993, 0.0016530344728380442]"""
757,229016,how to kill a connection from the connection pool without shutting down the connection pool,"Hi:

This is really not a bug but probably a desired feature and something I am not aware of.

We use the Apache connection pool in a Servlet environment. We need the feature to kill or close a connection forcefully when it is being used by a servlet. The reason can be various: an evil servlet that holds a connection forever, db issue, network issue, etc. How can we do that? I notice that datasource.getConnection() always returns a new connection. How do we keep track of connections that are being used so we can close them if needed? Currently we can only kill one at the database side by a DBA.

Should we write a a util to keep track of connection objects that are borrowed by clients?",smartshark_2_2,360,commons-dbcp,"""[0.9737179279327393, 0.026282085105776787]"""
758,229017,ObjectCreateRule doesn't allow create objects wich type is specified in attributeName only,Current {{ObjectCreateRule}} implementation doesn't allow create objects wich type are specified in {{attributeName}} only.,smartshark_2_2,165,commons-digester,"""[0.2001490294933319, 0.7998509407043457]"""
759,229018,[digester] ExtendedBaseRules test fails,"When using the JDK1.2, ""ant test.matching.extended"" reports a failure in method
""testAncesterMatch"".

I have added logging to the ExtendedBaseRules class, and the problem appears not
be with jdk1.2, but rather with the order objects are returned from the HashMap
rule cache. It seems that iterating over the entryset of elements in the cache
returns the elements in different order in the two JVMs, and somehow this seems
to break ExtendedBaseRules. It shouldn't, of course.

I'll continue to look into it, but would appreciate some help from those who are
familiar with this class, as the matching rules are *very* complex.",smartshark_2_2,22,commons-digester,"""[0.6885825991630554, 0.31141743063926697]"""
760,229019,Link to core API documentation is broken on the front page of the Digester site,"The link to Core APIs on the front page of the
Digester project is broken. It points to:
http://commons.apache.org/digester/commons-digester-3.0/core.html

It probably should point to:
http://commons.apache.org/digester/guide/core.html
",smartshark_2_2,169,commons-digester,"""[0.9961446523666382, 0.0038553234189748764]"""
761,229020,Provide ability to capture namespace snapshots,"Attached patch adds the capability to snapshot current namespaces at any point during parsing. This is useful, for example, in Commons SCXML where the value of an attribute can contain an XPath expression which needs to be evaluated later. The testcase in the patch captures namespace snapshots for each object created by the digester. Feedback welcome.

As an aside, does anyone mind if I bring the Digester JIRA versions up to date? Thanks.

",smartshark_2_2,90,commons-digester,"""[0.9985023736953735, 0.0014975883532315493]"""
762,229021,Conversion from text to property value,"It may be that I don't understand the API well enough, but I'm missing a way 
to register a class that converts the string value of an attribute to an 
object of the appropriate type for a property. 
 
The Spring framework uses instances of java.beans.PropertyEditor for this 
purpose; it might be a good choice for Digester, too. 
 
Michael",smartshark_2_2,86,commons-digester,"""[0.9976866245269775, 0.0023133535869419575]"""
763,229022,"XML interface for Digester.addCallParam(java.lang.String pattern, int paramIndex, boolean fromStack)","Need an XML interface for Digester.addCallParam(java.lang.String pattern, int 
paramIndex, boolean fromStack). Currently objects on the stack cannot be 
accesses as parameters if we are using XML to specify digester rules.",smartshark_2_2,82,commons-digester,"""[0.9979780316352844, 0.002021975349634886]"""
764,229023,digester-rules.dtd is not in source or binary distributions,"The Javadoc for the package org.apache.commons.digester.xmlrules mentions the file 
digester-rules.dtd. This is not to be found in either the source or the binary distributions. If it is 
somewhere else on the Jakarta site then a link from the javadoc would be nice! 
 
Recommend either providing the file in the distributions or linking from the javadoc, or preferably 
both.",smartshark_2_2,77,commons-digester,"""[0.9968066215515137, 0.00319339195266366]"""
765,229024,[PATCH] commons-digester / Allow recursive match in ExtendedBaseRules.java (see patch),"Recursive tags in XML-rules-file only work on root node. Nested nodes like the one below to not work.
With the attached patch, it is possible to specify rules like
<...>
  <...>
    <pattern value=""*/properties"">
       <object-create-rule classname=""de.wsy.f4ja.alertbatches.configuration.alerting.alertingconfig.Properties"" />
       <set-properties-rule />
       <set-next-rule methodname=""addProperties""/>		         	 
   </pattern>
 
   <pattern value=""*/property"">
       <object-create-rule classname=""de.wsy.f4ja.alertbatches.configuration.alerting.alertingconfig.Property"" />
       <set-properties-rule />
       <set-next-rule methodname=""addProperty"" /> 
  </pattern>
  </...> 
</..>
",smartshark_2_2,133,commons-digester,"""[0.8998770713806152, 0.10012298077344894]"""
766,229025,MultiVariableExpander should work with variables defined in terms of another variables.,"Given properties defined like:
bar=bar
foobar=foo${bar}

I would expect ${foobar} being expanded as ""foobar"". Instead it's expanded as ""foo${bar}"".

A fix is trivial removing the last line in method expand:
index += varValue.length();
",smartshark_2_2,125,commons-digester,"""[0.9471405744552612, 0.05285942181944847]"""
767,229026,[digester] support embedded systems (null classloader),"Digester uses Class.getClassLoader() in numerous places, however this method is
documented as being able to return null, which it does under the custom Java in
Bentley MicroStation.  (In fact, *every* method that returns a classloader
actually returns null.)

It turns out that it's not necessary to use ClassLoader at all; Class offers
every method you need, and works every time.  So I made that patch -- using
Class.forName and Class.getResource instead of the ClassLoader versions.  It
works fine now.

If there's no reason to stick with ClassLoader, I'd suggest making this change
in the main code base.  I can't submit mine, as I have no good way to make a
patchfile.",smartshark_2_2,1,commons-digester,"""[0.9943960905075073, 0.005603936966508627]"""
768,229027,getPrefixLength return -1 if unix file contains colon,"A simple

{code:java}
int prefixLength = FilenameUtils.getPrefixLength(""/:foo/bar"");
{code}

return {{-1}}, but {{0}} was expected. The path is absolutely valid for unix systems.",smartshark_2_2,481,commons-io,"""[0.07532971352338791, 0.9246702790260315]"""
769,229028,ArrayIndexOutOfBoundsException in BOMInputStream when reading a file without BOM multiple times,"Resetting the BOMInputStream doesn't reset the _fbLength_ member variable. This causes _fbLength_ to grow bigger than the _firstBytes_ array (when the file doesn't contain a BOM), which leads to an ArrayIndexOutOfBoundsException in the _readFirstBytes_ method.

The attached test case reveals the problem.",smartshark_2_2,277,commons-io,"""[0.05980011075735092, 0.9401998519897461]"""
770,229029,TeeOutputStream does not call branch.close() when main.close() throws an exception,"TeeOutputStream.close() looks like this:
{code:title=TeeOutputStream.java|borderStyle=solid}
    /**
     * Closes both streams. 
     * @throws IOException if an I/O error occurs
     */
    @Override
    public void close() throws IOException {
        super.close();
        this.branch.close();
    }
{code} 

It is obvious that {{this.branch.close()}} is not executed when {{super.close()}} raises an exception. {{super.close()}} may in fact raise an IOException since {{ProxyOutputStream.handleIOException(IOException)}} is not overridden.",smartshark_2_2,248,commons-io,"""[0.09435760229825974, 0.9056423902511597]"""
771,229030,FileSystemUtils needs to call Process.destroy() on exec'd processes,"Calling the FileSystemUtils.getFreeSpace() method multiple times (~3000) will generate an IOException with the following text: ""Too many open files"". Documentation from Sun says this problem is due to not destroying the java.lang.Process object returned from the System.exec() call.

Some sample code I wrote confirms that calling destroy prevents this error from occurring.",smartshark_2_2,83,commons-io,"""[0.15179356932640076, 0.8482064008712769]"""
772,229031,EndianUtils.readSwappedUnsignedInteger() may return a negative number,"Methods about reading unsigned-integer in class EndianUtils may return a negative number, due to casting int to long.
Calculations with operator & etc. are under integer in these methods so its results are integer,
then implicit casting the results to long keeps its positive/negative sign.",smartshark_2_2,97,commons-io,"""[0.08929858356714249, 0.9107013940811157]"""
773,229032,CharSequenceInputStream - add tests for available(),"There are currently no tests for the available() method.

Tme method should return an estimate of the number of items remaining, so it should be possible to read - or skip - that many items without reaching EOF.

At present, the code measures the number of CharBuf entries remaining; I think that is wrong - it should be the number of bytes left in the byte stream.",smartshark_2_2,438,commons-io,"""[0.9315056800842285, 0.06849435716867447]"""
774,229033,FileCleaner thread never ends and cause memory leak in AS,"FileCleaner opens a thread and no solution is given to the user to end it. So when an application is undeployed
in an Application Server, a thread is still alive. The WebApp can't be undeployed and this results in a classloader
leak that will cause an OutOfMemoryError.

I think the API should be extended so that a user can end the thread. A better way would be to provide a class that
cleans everything for commons IO.",smartshark_2_2,81,commons-io,"""[0.15584759414196014, 0.8441524505615234]"""
775,229034,Potential unread bytes in readSwappedLong(InputStream),"In:

        byte[] bytes = new byte[8];
        input.read( bytes );
        return readSwappedLong( bytes, 0 );

There is no check on the int return value from input.read, so it's possible that 8 bytes won't be read.",smartshark_2_2,92,commons-io,"""[0.09750537574291229, 0.9024946093559265]"""
776,229035,StringBuilder Writer implementation," This implementation, as an alternative to java.io.StringWriter, provides an un-synchronized (i.e. for use in a single thread) implementation for better performance. StringBuilder is a JDK 1.5+ feature",smartshark_2_2,121,commons-io,"""[0.9981334805488586, 0.001866538543254137]"""
777,229036,[io] Deprecated FileUtils string methods,"My primary goal was deprecation of the string methods in FileUtils.  The obvious
ones are now deprecated.

In running the test cases for FileUtils, and recalling my previous experience,
it seems that the io project is in need of separate test data.  Currently, there
are a few (or maybe just 1) test file located in the src/test directory.  I
think that the data should be separate, so I created a data/test directory and
put the file there.  Small modifications were made to build.xml and project.xml
to allow this.

I'll post some more comments about this to the mailing list.",smartshark_2_2,64,commons-io,"""[0.998324453830719, 0.0016755596734583378]"""
778,229037,"make it more easy-to-use: org.apache.commons.io.FileUtils.listFiles(File, IOFileFilter, IOFileFilter)","when I only want to filter the directories, it should be better to allow me to set the second parameter to null or I have to implement the org.apache.commons.io.filefilter.IOFileFilter interface with nothing functionally task to do.",smartshark_2_2,492,commons-io,"""[0.9984056353569031, 0.0015943469479680061]"""
779,229038,Add read/readFully methods to IOUtils,"When reading from network etc, it's possible for a read to return before it has read as many bytes as would fit in the buffer.
Sometimes multiple reads are needed to fulfil the request.

It would be useful to have read() versions of the skip() methods.

{code}
int actual = read(Reader, byte buffer [,offset, length]) // returns normally on EOF
int actual = read(InputStream, char buffer [,offset, length])

int actual = readFully(Reader, byte buffer [,offset, length]) // throws EOFException
int actual = readFully(InputStream, char buffer [,offset, length])
{code}",smartshark_2_2,279,commons-io,"""[0.9984918832778931, 0.0015080493176355958]"""
780,229039,CharSequenceInputStream to efficiently stream content of a CharSequence,CharSequenceInputStream implementation can be used to efficiently stream content of a CharSequence. This can be useful when streaming out a large body of text to a network socket without having to create an intermediate byte array containing the entire content in binary form (using String#toByteArray() or similar).,smartshark_2_2,274,commons-io,"""[0.998173713684082, 0.0018263220554217696]"""
781,229040,NotFileFilter documentation is incorrect,"The documentation for NotFileFilter (http://commons.apache.org/io/api-release/index.html) incorrectly states that it, ""Checks to see if both filters are true.""  It looks to be the result of a hasty copy-and-paste from an old version of AndFileFilter (http://svn.apache.org/viewvc/commons/proper/io/trunk/src/java/org/apache/commons/io/filefilter/AndFileFilter.java?revision=140357&view=markup).  It should say something like, ""Returns the logical NOT of the underlying filter's return value for the same arguments.""

Patch is attached.",smartshark_2_2,167,commons-io,"""[0.9952968955039978, 0.004703117534518242]"""
782,229041,FileUtils.doCopyFile can potentially loop for ever,"FileUtils.doCopyFile caches the input file size and only exits the loop once it has read sufficient bytes.

If the input file is truncated after obtaining the file size, the loop might never exit.

One possible way round this might be to check whether anything has been transferred. However, I don't know if it's possible for FileChannel.transferFrom() to temporarily return 0 when there is data available; if so that could cause some existing applications to break.",smartshark_2_2,409,commons-io,"""[0.8461374044418335, 0.15386264026165009]"""
783,229042,.gitattributes not correctly applied,"the .gitattributes [commit 941a9a6] is ""corrupting"" test resources and several source resources did not yet have their line ending normalized [commit 9e2b2c0].

",smartshark_2_2,547,commons-io,"""[0.9849193096160889, 0.015080670826137066]"""
784,229043,Make fields final so classes are immutable/threadsafe,"There are quite a few fields which are only set up in constructors.

These could be made final so the classes can be immutable, and therefore thread-safe.

Patch to follow.

Note: some of the current filters cannot be made thread-safe this way.",smartshark_2_2,124,commons-io,"""[0.9984729886054993, 0.00152703991625458]"""
785,229044,Rounding issue with byteCountToDisplaySize(long size),"I do not understand the byteCountToDisplaySize(long size) method which is in  class FileUtils of the package org.apache.commons.io.
If  the parameter size is 2047 , the method will return 1 KB.Why it will lose precision.
I read the code. 
Maybe it is a bug?
",smartshark_2_2,590,commons-io,"""[0.6692677736282349, 0.33073216676712036]"""
786,229045,Provide a convenience mehod in FileFilterUtils to create a Size Range filter,Provide a convenience mehod in FileFilterUtils to create a Size Range filter - by combining two SizeFileFilter's (one >= minimum and the other <= maximum) with an AndFileFilter,smartshark_2_2,88,commons-io,"""[0.9983854293823242, 0.0016146139241755009]"""
787,229046,"Adding FileUtils.byteCountToDisplaySize(long size, boolean useSiUnits)","I have written a little Utility method that might benefit Commons IO:

{code}
public class FileUtils {

    /**
     * Returns a human-readable version of the file size (original is in bytes). The implementation has the following features:
     * <ul>
     * <li>Supports the SI or IEC units.</li>
     * <li>Supports I18n</li>
     * <li>Display a one digit remainder (rounded down if less than 5, rounded up otherwise)</li>
     * <li>Once the main unit is >= 100, drops the remainder which would be over precision.</li>
     * </ul>
     * 
     * @param size The number of bytes.
     * @param useSiUnits if false, uses the IEC (International Electrotechnical Commission) units (powers of 2), else uses SI (International System of Units)
     *            units (powers of 10).
     * @return A human-readable display value (includes units).
     */
    public static String byteCountToDisplaySize(long size, boolean useSiUnits) {
{code}",smartshark_2_2,359,commons-io,"""[0.998381495475769, 0.0016184595879167318]"""
788,229047,IO 2.0 - Move to JDK 1.5,"I just created IO-139 for a StringBuilder Writer implementation that requies JDK 1.5. So I thought I would look at the impact on IO of 1) Removing all deprecations and 2) Making appropriate JDK 1.5 changes (generics, using StringBuilder and new Appendable for Writers). Below is a summary, thought it could be a starting point for discussion about IO 2.0

1) DEPRECATIONS
    - CopyUtils
    - FileCleaner
    - WildcardFilter
    - FileSystemUtils freeSpace(String)
    - IOUtils toByteArray(String), toString(byte[]), toString(byte[], String) 

2) JDK 1.5
    - ConditionalFileFilter List (and also AndFileFilter and OrFileFilter implementations
        - getFileFilters() and setFileFilters() use generic List<IOFileFilter>
    - Constructor for NameFileFilter, PrefixFileFilter, SuffixFileFilter, WildcardFileFilter use generic List<String>
    - replace StringBuffer with StringBuilder where appropriate (FilenameUtils, FileSystemUtils, HexDump,IOUtils
    - FileUtils 
        - convertFileCollectionToFileArray() --> Collection<File>
        - listFiles() --> Collection<File>
        - listFiles() --> Collection<File>
        - writeStringToFile String-->CharSequence (JDK 1.4+)
    - ProxyReader - add read(CharBuffer)
    - IOUtils
        - readLines(Reader) return List<String>
        - toInputStream(String) --> toInputStream(CharSequence)  (JDK 1.4+)
        - write(String data, OutputStream) and write(StringBuffer data, OutputStream) --> write(CharSequence data, OutputStream) 
        - write(String, Writer) and write(StringBuffer, Writer) --> write(CharSequence data, Writer) 
    - LineIterator Iterator --> Iterator<String
    - NullWriter - add ""Appendable"" methods
    - ProxyWriter - add ""Appendable"" methods",smartshark_2_2,153,commons-io,"""[0.9984003901481628, 0.0015996356960386038]"""
789,229048,[io] Add a secureDelete method to FileUtils.java,"in org.apache.commons.io.FileUtils

Commons Fileupload uses at least the io's FileCleaner.track() method.
Unfortunately, they just use the plain java File.delete() method and not a more
sophisticated delete as offered in this package.

Especially, if servers running the FileUpload are sitting in DMZs and forward
all personal/private uploaded information in another (DB-)server behind another
firewall, one would not want that if the DMZ machine gets hacked, all previous
uploads that are supposedly deleted still can easily be found on the disk by a
not even that skilled attacker.

Therefore, it would be great to have a pgp-wipe alike secure delete method here!
it would overwrite the file multiple times and probably, it this should be
spawned as a separte thread since that may take longer than a state-of-the-art
GUI would want to wait for such an action to complete.",smartshark_2_2,9,commons-io,"""[0.9979255199432373, 0.002074472140520811]"""
790,229049,[IO][PATCH] - file and directory pollers,"Add classes that will monitor a file/directory and notify registered observers 
creation, deletion and content change events occur.  Source is in the 
attachement file_directory_poller.jar as they are all new files.",smartshark_2_2,107,commons-io,"""[0.9982907176017761, 0.0017092280322685838]"""
791,229050,New TrailerInputStream class,"A new input stream for reading data with fixed length trailers/suffixes. This was briefly discussed on the ML: http://markmail.org/thread/mh4elj7a53mdus2h.

Since I don't often write input streams and haven't contributed to this project before, I offer this up for review to see if it's a welcome addition.",smartshark_2_2,377,commons-io,"""[0.9982840418815613, 0.0017159284325316548]"""
792,229051,adds an endOfFileReached method to the TailerListener,My use case is tailing a log and publishing to a websocket.  I found that using the handleLine method resulted in too many messages being published.  Using a buffer in my listener and only publishing on endOfFileReached was a really simple solution.,smartshark_2_2,372,commons-io,"""[0.99399733543396, 0.006002690643072128]"""
793,229052,Document that IOCase assumes there are only two OSes: Windows and Unix,"Just noticed that IO assumes systems are either:

Windows-like:
* File.separatorChar = \
* Case-insensitive matching

or

Unix-like:
* File.separatorChar = /
* Case-sensitive matching

It may well be true that there are only two different file sepator characters, but it is not true that all non-Windows systems use case-sensitive matching.

For example, OpenVMS uses a file separator char of /, but originally only supported case-insensitive file names (always shown as uppercase).
Current versions of OpenVMS support both upper and lower-case; the default is to use case-insenstive matching (and upper case names).

Perhaps the IOCase.SYSTEM constant needs to have an ""Unknown"" or ""Variable"" setting for this; it would then be an error to use IOCase.SYSTEM to determine the case sensitivity.",smartshark_2_2,238,commons-io,"""[0.9508703947067261, 0.04912959039211273]"""
794,229053,Add Charset sister APIs to method that take a String charset name.,"Add Charset sister APIs to method that take a String charset name (aka encoding). 
For example: foo(..., String charsetName) -> foo(..., Charset charset).
Refactor such that we do not have code duplication of the algorithms.

Known issue: Source compatibility.

Now there are APIs that change only with the last type, String vs. Charset, you will get a compile error if you pass null to the old String API because the target method will be ambiguous: do you want to call the String or Charset version? You must type-cast to one type or the other.

Known issue: checked java.io.UnsupportedEncodingException vs. unchecked java.nio.charset.UnsupportedCharsetException

The JRE API Charset.forName throws the unchecked UnsupportedCharsetException. 
The Commons IO 2.2 String APIs throw the checked UnsupportedEncodingException, a subclass of IOException, when a charset is not available.
The refactored String APIs throw UnsupportedCharsetException from Charset.forName, an unchecked IllegalArgumentException. The String APIs throw IOException, so there is no source compatibility issue.

If you somehow relied on catching the checked UnsupportedEncodingException instead of IOException, its superclass, you should catch the unchecked java.nio.charset.UnsupportedCharsetException to act on the fact that the charset is not available.
",smartshark_2_2,457,commons-io,"""[0.998490571975708, 0.0015093644615262747]"""
795,229054,Add Methods for Buffering Streams/Writers To IOUtils,"I suggest adding utility methods for buffering streams and writers to the IOUtils class. The methods would have the following signatures:

BufferedInputStream buffer(InputStream inputStream)
BufferedOutputStream buffer(OutputStream outputStream)
BufferedReader buffer(Reader reader)
BufferedWriter buffer(Writer writer)",smartshark_2_2,261,commons-io,"""[0.9981812238693237, 0.0018187453970313072]"""
796,229055,[io] move Location and Locator to commons io,"The excellent platform-independence provided by the trio 
- org.apache.tools.ant.launch.Locator
- org.apache.tools.ant.Location.Location(String fileName)
- org.apache.tools.ant.util.FileUtils
is certainly useful for more than just ant.

RFE: move to commons io.

Thx to Matt who pointed out in Bug 35776 that there is already a
org.apache.commons.io.FileUtils, so for that one, I guess the suggestion is
rather ""merge"" the two since the ant-one is way more powerful, especially with
its ""normalize(final String path)"" method, etc.",smartshark_2_2,146,commons-io,"""[0.9984629154205322, 0.0015371382469311357]"""
797,229056,FileUtils#moveFileToDirectory() Overwrite Flag,"Create a version of moveFileToDirectory() that accepts a flag to enable overwrite.  If the destination file exists, copy over it.  Would require the code to perform a buffer-by-buffer file copy instead of file#rename().",smartshark_2_2,380,commons-io,"""[0.996256947517395, 0.003743070876225829]"""
798,229057,"Scalable Iterator for files, better than FileUtils.iterateFiles",Improve the way that iterateFiles generate an iterator. The current way it not scale. It's try to add all files in a list and then return the iterator of that list. A better way it would be create an customize Iterator<File> with a stack of arrays of File to go up and down in the directory tree.,smartshark_2_2,596,commons-io,"""[0.9983508586883545, 0.0016491753049194813]"""
799,229058,ValidatingObjectInputStream contribution - restrict which classes can be deserialized,"As discussed on the commons dev list I'd like to contribute my SLING-5288 code to commons-io. I'll attach a patch.

_Update: this is committed now, see [1] for an example_.

[1] https://svn.apache.org/repos/asf/commons/proper/io/trunk/src/test/java/org/apache/commons/io/serialization/MoreComplexObjectTest.java",smartshark_2_2,516,commons-io,"""[0.9985499978065491, 0.0014499241951853037]"""
800,229059,"Add support for moving a file to an existing path if the existing file is zero bytes in size -- org.apache.commons.io.FileUtils#moveFile(File, File)","Howdy, 

org.apache.commons.io.FileUtils#moveFile(File, File) currently throws an IOException if the destination File already exists. IMHO this is the expected behavior in cases where the destination file has is at least one byte in size. But in some cases the user might want to create a file before calling this method to be sure it was created successfully and use another filename if not. Since a generic IOException is thrown in cases where the destination file does already exist a user not able to implement counter-measures for such cases. So i suggest to add another method for this case.

",smartshark_2_2,211,commons-io,"""[0.998018741607666, 0.001981250010430813]"""
801,229060,[io] File checksum,This is a RFE to add a method to compute checksums in FileUtils.,smartshark_2_2,7,commons-io,"""[0.9981527924537659, 0.0018472125520929694]"""
802,229061,LineIterator could easily support Iterable,"It could be useful to make LineIterator support Iterable<String>, which is very easy to add.",smartshark_2_2,217,commons-io,"""[0.997601330280304, 0.002398685785010457]"""
803,229062,CSV component,"TableBuilder is 'Builder ' that maps the CSV to a matrix and provides interface that allows user to manipulate after it is build by parsing a csv file to it parse() method.(There is only one method implemented and it is for copying a column values to another position, as I could not  think of other operation that may be useful)

Within the TableBuilder, each column of the CSV is represented as byte[] and each becomes a target to be validated against Rule,represented by the interface that you find in the example code below. As TableBuilder ""buildTable"" ,when parse() method is invoked, a byte[]  representation of the value of each CSV cell  is passed to isValid() method of implementations of Rules, which you apply to the TableBuilder instance through the addRule() method. (you you can add as many Rule as you need.)

Rule gets executed until the validation fails or succeeds. If any of the Rule fails, then its replace() is called and the column value being processed gets replaced by the retun value of this method.
Another goodie is that it is possible to refer to the values of preceding cell values of the row within a Rule.
It is useful if you need to see the entries of the preceding cell when validating the value in a Rule. An example would be,

Given a csv,

A,B,C
1,2,3

in order for the value  3 of the column C is to be validated true, the Value of A needs to be less than the value of C.

TableBuilder is RFC 4180 compliant and therefore distinguishes NL exists by itself and NL found in double quotes.
So you can add Rule that practically removes all NL chars found in value enclosed within doublequotes. 
(useful when you need to remove CRLF in double quotes from  CSV exported from Excel)

Currently, TableBuilder implements a method called copyColumn with method signature of,
copyColumn(Rule rule,int from,int to, boolean override) which allows user to manipulate the parsed csv.

What it does is literarly copies column from that is specified  at 'from' and to 'to' position of the matrix.
If override is true, the copying colum is overriden else the column is right shifted and inserted at the specified position.

You can specify some kind of Rule here to rewrite the value being copied from the origin.
An example would be copy column value that all ends with .jpg or .gif and to the position specified prefixing the column value with ""http://some.server.com/imanges."" after checking the image exists, after checking that the named file exists at some location also by an implementation of  another Rule.

TableBuilder is just a ""rough skecth"" idea of CSV parsing.(The code below works fine though) it still needs alot of refactoring and so.
I appreciate any comment on this idea. What do you think? My code style sucks I know! 

Here is simple exampe to use TableBuilder.
{code:title=TableBuilder|borderStyle=solid}
    public static void main(String[] args)throws Exception{
        TableBuilder tableBuilder=new TableBuilder(""UTF-8"",
                new MessageHandler(){
                    public void handleMessage(String message) {
                        System.err.println(message);
                    }
                },0,true);
        tableBuilder.addRule(3,new RemoveNLChars()); //removing NL cahracters found in value.
        tableBuilder.parse(new FileInputStream(""test.txt""),TableBuilder.CSV);
        List<Record> list=tableBuilder.getRowAsListOf(Record.class);
        for(Record record:list)
            System.out.println(record.getA());//TODO not implemented yet!

        tableBuilder.writeTo(new FileOutputStream(""test_mod.txt""),TableBuilder.CSV);
    }

public class RemoveNLChars extends StringValueRuleAdapter {
    protected boolean isValid(String columnValue) {
        return !columnValue.contains(System.getProperty(""line.separator""));
    }

    protected String replace(String columnValue) {
        return columnValue.replaceAll(System.getProperty(""line.separator""),"""");
    }

    public String getMessage() {
        return """";
    }
}

public interface Rule {
    public void setRowReference(List<byte[]> rowReference);
    public void setCharsetName(String charsetName);
    boolean isValid(final byte[] columnValue);
    byte[] replace(final byte[] columnValue);
    String getMessage();
}

//StringValueruleAdapter is an adapter converts the byte[] representation of the cell value.

public  abstract class StringValueRuleAdapter implements Rule{
    private String charsetName;
    private List<byte[]> rowReference;
    
    public void setRowReference(List<byte[]> rowReference) {
        this.rowReference=rowReference;
    }

    public void setCharsetName(String charsetName) {
        this.charsetName=charsetName;
    }

    public final boolean isValid(final byte[] columnValue) {
        String strValue;
        try {
            if(columnValue.length>0)
                strValue=(charsetName!=null) ? new String(columnValue,charsetName) : new String(columnValue);
            else
                strValue="""";
        } catch (UnsupportedEncodingException e) {
            if(columnValue.length>0)
                strValue=new String(columnValue);
            else
                 strValue="""";
        }
        return isValid(strValue);
    }

    public final byte[] replace(final byte[] columnValue) {
        String strValue;
        try {
            if(columnValue.length>0)
                strValue=(charsetName!=null) ? new String(columnValue,charsetName):new String(columnValue);
            else
                strValue="""";
            return (charsetName!=null) ? replace(strValue).getBytes(charsetName):replace(strValue).getBytes();
        } catch (UnsupportedEncodingException e) {
            if(columnValue.length>0)
                strValue=new String(columnValue);
            else
                strValue="""";
            return replace(strValue).getBytes();
        }
    }

    protected String getRowValue(int column) {
        try {
            return (charsetName!=null) ? new String(rowReference.get(column),charsetName) :
                    new String(rowReference.get(column));
        } catch (UnsupportedEncodingException e) {
            return new String(rowReference.get(column));
        } catch(IndexOutOfBoundsException noListFound){
            throw new IllegalArgumentException(""no value exists at the requested column."");
        }
    }

    protected String getPrecedingRowValue(){
        return getRowValue(rowReference.size()-1);
    }

    protected abstract boolean isValid(String columnValue);
    protected abstract String replace(String columnValue);
}


public class TableBuilder {
    public static int CSV=0x2c;
    public static int TSV=0x09;
    private Map<Integer,Set<Rule>> columnRule=new  HashMap<Integer,Set<Rule>>();
    private Table currentTable;
    private byte[] newLineChars;
    private boolean endsWithNL;
    private String charsetName;
    private int rowOffset;
    private boolean useFirstColumnAsRowName;
    private MessageHandler msgHandler=new MessageHandler(){

        public void handleMessage(String message) {
            System.err.println(message);
        }
    };

    public TableBuilder(String charsetName,MessageHandler msgHandler,int rowOffset,boolean useFirstColumnAsRowName){
        this.charsetName=charsetName;
        this.rowOffset=rowOffset;
        this.msgHandler=msgHandler;
        this.useFirstColumnAsRowName=useFirstColumnAsRowName;
    }

    public TableBuilder(String charsetName){
        this.charsetName=charsetName;
    }

    public TableBuilder(){
        
    }

    public void addRule(int column, Rule rule){
        Set<Rule> ruleset;
        if((ruleset=columnRule.get(column))==null){
            ruleset=new LinkedHashSet<Rule>();
            columnRule.put(column,ruleset);
        }
        rule.setCharsetName(charsetName);
        ruleset.add(rule);
    }

    public void parse(InputStream in, int delimiter)throws Exception{
        int bytesRead;
        byte buf[]=new byte[1024];
        ByteArrayOutputStream outbuf=new ByteArrayOutputStream(buf.length);
        while((bytesRead=in.read(buf,0,buf.length))!=-1)
            outbuf.write(buf,0,bytesRead);
        in.close();
        ByteBuffer bytebuffer=ByteBuffer.allocateDirect(outbuf.size()).put(outbuf.toByteArray());
        bytebuffer.flip();
        currentTable=buildTable(bytebuffer,delimiter);
    }

    private class Table {
        private List<byte[]>[] columnMatrix;
        private List<List<byte[]>> rowMatrix;
        
        Table(List<byte[]>[] columnMatrix,List<List<byte[]>> rowMatrix){
            this.columnMatrix=columnMatrix;
            this.rowMatrix=rowMatrix;
        }

        public int getNumOfColumns() {
            return columnMatrix.length;
        }

        public int getNumOfRows(){
            return rowMatrix.size();
        }

        public byte[] getValueAt(int row, int column) {
            return columnMatrix[column].get(row);
        }

        public byte[] getColumnName(int column){
            return columnMatrix[column].get(0);
        }

        public List<byte[]> getColumn(int column){
            return columnMatrix[column];
        }

        public List<byte[]> getRow(int row){
            return rowMatrix.get(row);
        }
        
    }
  //TODO  extract csv row as JavaBean
    public <E> List<E> getRowAsListOf(final Class<E> clazz){
        List<E> list=null;
        Iterator<byte[]> header=currentTable.getRow(0).iterator();
        for(int i=1;i<currentTable.getNumOfRows();i++){
            try {
                E instance=clazz.newInstance();
                for(byte[] value:currentTable.getRow(i)){
                    String name=new String(header.next());
                    //BeanUtils.setProperty(instance,name,value);
                }
                if(list==null)
                    list=new ArrayList<E>();
                list.add(instance);
                header=currentTable.getRow(0).iterator();
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            } catch (InvocationTargetException e) {
                e.printStackTrace();
            } catch (InstantiationException e) {
                e.printStackTrace();
            }
        }
        return list;
    }

    public void writeTo(OutputStream out,int delimiter) throws IOException {

        for(int i=0,j=0;i<currentTable.getNumOfRows();i++,j=0){
            for(byte[] value:currentTable.getRow(i)){
                out.write(value);
                if(++j<currentTable.getNumOfColumns())
                    out.write(delimiter);
            }
            if(i<currentTable.getNumOfRows()-1)
                out.write(newLineChars);
            else{                                             
                if(endsWithNL)
                    out.write(newLineChars);
            }
        }
        out.close();
    }
     
     public void copyColumn(Rule rule,int from,int to, boolean override) {
            int numOfColumns=override ? currentTable.getNumOfColumns():currentTable.getNumOfColumns()+1;
            List<byte[]>[] columnMatrix=(List<byte[]>[])new List[numOfColumns];
            columnMatrix[to]=new ArrayList<byte[]>();
            for(int i=0,j=0;i<columnMatrix.length;i++){
                if(i==to){
                    for(int row=0;row<currentTable.getNumOfRows();row++){
                        byte[] value;
                        if(row>=rowOffset)
                            value=currentTable.getValueAt(row,from);
                        else
                            value=new byte[0];
                        if(rule!=null && row>rowOffset){
                            rule.setCharsetName(charsetName);
                            rule.setRowReference(currentTable.getRow(row));
                            if(!rule.isValid(value)){
                                String columnName;
                                byte[] columnNameByte=currentTable.getColumnName(from);
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                value=rule.replace(value);
                                String msg=rule.getMessage();
                                    if(msg.length()>0)
                                        try {
                                            handleMessage(msg
                                            .replace(""${column_from}"",""""+from)
                                            .replace(""${columnName}"",columnName)
                                            .replace(""${column_to}"",""""+(to+1))
                                            .replace(""${row}"",useFirstColumnAsRowName ? new String(currentTable.getRow(row).get(0),charsetName) : """"+(row+1)));
                                        } catch (UnsupportedEncodingException ignored) {
                                            
                                        }
                            }
                        }
                    columnMatrix[i].add(value);
                    if(override)
                        currentTable.rowMatrix.get(row).remove(i);
                    currentTable.rowMatrix.get(row).add(i,value);
                    }
                    if(override)
                        ++j;
                }else
                    columnMatrix[i]=currentTable.getColumn(j++);
            }
            currentTable=new Table(columnMatrix,currentTable.rowMatrix);
    }
    
    private Table buildTable(ByteBuffer buf,int delimiter) throws ParseException {
        List<byte[]>[] columnMatrix=null;
        List<List<byte[]>> rowMatrix=new ArrayList<List<byte[]>>();
        int i=0,j,currentRow=0,rowIndex=0,column_count=0,column=0;
        endsWithNL=true;
        newLineChars=null;
        int limit=buf.limit();
        int pos=0;

        while(i<limit && ((j=(buf.get(i)&0xff))==0x0d||(j=(buf.get(i)&0xff))==0x0a)){
            if(j==0x0a)
                ++currentRow;
            pos=++i;
        }
        
        int headRow=currentRow;
        while(i<limit){
            int tmp=buf.get(i) & 0xff;
                if(tmp==0x0a){
                    int k=i;
                    while(k>=0 &&((buf.get(k)&0xff)==0x0d||(buf.get(k)&0xff)==0x0a))
                        --k;
                    byte[] prev=new byte[++k-pos];
                    
                    buf.position(pos);
                    buf.get(prev,0,prev.length);
                    List<byte[]> row;
                    try{
                        row=rowMatrix.get(rowIndex);
                    }catch(IndexOutOfBoundsException noListFound){
                        rowMatrix.add(new ArrayList<byte[]>());
                        row=rowMatrix.get(rowIndex);
                    }
                    if(currentRow==headRow){
                        column_count=column;
                        row.add(prev);
                        columnMatrix=(List<byte[]>[])new ArrayList[column+1];
                        Iterator<byte[]> itr;
                        for(j=0,itr=row.iterator();j<columnMatrix.length;j++){
                            columnMatrix[j]=new ArrayList<byte[]>();
                            columnMatrix[j].add(itr.next());
                        }
                    }else if(column_count!=column){
                        throw new ParseException(""column count mismatch on row "",currentRow+1);

                    }else{
                        Set<Rule> ruleset=columnRule.get(column);
                        if(ruleset!=null && currentRow>rowOffset+headRow){
                            byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                            Rule rule=validate(ruleset,prev,row);
                            if(rule!=null){
                                String columnName;
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                prev=rule.replace(prev);
                                String msg=rule.getMessage();
                                if(msg.length()>0)
                                    try {
                                        handleMessage(msg
                                            .replace(""${column}"",""""+column)
                                            .replace(""${columnName}"",columnName.trim())
                                            .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                                    } catch (UnsupportedEncodingException ignored) {

                                    }
                            }
                        }
                        columnMatrix[column].add(prev);
                        row.add(prev);
                    }

                    if(newLineChars==null){
                        newLineChars=new byte[++i-k];
                        buf.position(k);
                        buf.get(newLineChars,0,newLineChars.length);
                    }else
                        ++i;
                while(i<limit && ((j=(buf.get(i)&0xff))==0x0d||(j=(buf.get(i)&0xff))==0x0a)){
                    if(j==0x0a)
                        ++currentRow;
                    ++i;
                }
                column=0;
                ++currentRow;

                ++rowIndex;
                pos=i;
            }else if(tmp==delimiter){
                List<byte[]> row;
                try{
                    row=rowMatrix.get(rowIndex);
                }catch(IndexOutOfBoundsException noListFound){
                    rowMatrix.add(new ArrayList<byte[]>());
                    row=rowMatrix.get(rowIndex);
                }
                byte[] prev=new byte[i-pos];
                buf.position(pos);
                buf.get(prev,0,prev.length);
                if(currentRow==headRow)
                    row.add(prev);
                else{
                    Set<Rule> ruleset=columnRule.get(column);
                    if(ruleset!=null && currentRow>rowOffset+headRow){
                        byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                            Rule rule=validate(ruleset,prev,row);
                            if(rule!=null){
                                String columnName;
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                prev=rule.replace(prev);
                        String msg=rule.getMessage();
                        if(msg.length()>0)
                            try {
                                handleMessage(msg
                                        .replace(""${column}"",""""+column)
                                        .replace(""${columnName}"",columnName.trim())
                                        .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                            } catch (UnsupportedEncodingException ignored) {

                            }

                            }
                    }
                    columnMatrix[column].add(prev);
                    row.add(prev);
                }
                ++column;
                pos=++i;
            }else
                if((i=_ESCAPED(buf,i))==i)
                    ++i;
        }

        if(pos!=limit){
            endsWithNL=false;
            byte[] remaining=new byte[limit-pos];
            buf.position(pos);
            buf.get(remaining,0,remaining.length);
            
            if(columnMatrix!=null){
                if(column_count!=column)
                    throw new ParseException(""column count mismatch on row "",+1+currentRow);
                List<byte[]> row=rowMatrix.get(rowIndex);
                row.add(remaining);
                Set<Rule> ruleset=columnRule.get(column);
                if(ruleset!=null && currentRow>rowOffset+headRow){
                    byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                    Rule rule=validate(ruleset,remaining,row);
                    if(rule!=null){
                        String columnName;
                        if(columnNameByte.length>0){
                            try {
                                if(charsetName!=null)
                                    columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                else
                                    columnName=""'""+new String(columnNameByte).trim()+""'"";
                            } catch (UnsupportedEncodingException e) {
                                columnName=""'""+new String(columnNameByte).trim()+""'"";
                            }
                        }else
                            columnName=""''"";
                        remaining=rule.replace(remaining);
                        String msg=rule.getMessage();
                        if(msg.length()>0)
                            try {
                                handleMessage(msg
                                .replace(""${column}"",""""+column)
                                .replace(""${columnName}"",columnName.trim())
                                .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                            } catch (UnsupportedEncodingException ignored) {

                            }

                    }
                }
                columnMatrix[column].add(remaining);
            }else{
                columnMatrix=(List<byte[]>[])new List[column+1];
                List<byte[]> row;
                try{
                    row=rowMatrix.get(rowIndex);
                }catch(IndexOutOfBoundsException noListFound){
                    rowMatrix.add(new ArrayList<byte[]>());
                    row=rowMatrix.get(rowIndex);
                }
                row.add(remaining);
                Iterator<byte[]> itr;
                for(j=0,itr=row.iterator();j<columnMatrix.length;j++){
                    columnMatrix[j]=new ArrayList<byte[]>(1);
                    columnMatrix[j].add(itr.next());
                }
            }
        }
        return new Table(columnMatrix,rowMatrix);
    }

    private int _ESCAPED(ByteBuffer src,int i){
        int org=i;
        if(i==src.limit())
            return i;
        int j;
        if((j=_DQUOTE(src,i))==i)
            return i;

        for(i=j;(j=_TEXTDATA(src,i))>i||(j=_COMMA(src,i))>i||(j=_CR(src,i))>i||(j=_LF(src,i))>i||(j=_2DQUOTE(src,i))>i;)
            i=j;

        if(i==_DQUOTE(src,i))
            return org;
        return i;
    }

    private int _TEXTDATA(ByteBuffer src,int i){
        if(i==src.limit())
            return i;
        if(_COMMA(src,i)==i && _CR(src,i)==i && _LF(src,i)==i && _DQUOTE(src,i)==i)
            return ++i;
        return i;
    }

    private int _2DQUOTE(ByteBuffer src,int i) {
        if(i==src.limit())
            return i;
        if(i==_DQUOTE(src,i))
            return i;
        if(i+1==_DQUOTE(src,i+1))
            return i;
        return i+2;
    }

    private int _DQUOTE(ByteBuffer src,int i) {
        return _CHAR(src,i,0x22);
    }

    public int _LF(ByteBuffer src,int i) {
        return _CHAR(src,i,0x0a);
    }

    private int _CR(ByteBuffer src,int i) {
        return _CHAR(src,i,0x0d);
    }

    private int _COMMA(ByteBuffer src,int i) {
        return _CHAR(src,i,0x2c);
    }

    private int _CHAR(ByteBuffer src,int i,int token){
        if(i==src.limit())
            return i;
        if((src.get(i) & 0xff)==token)
            ++i;
        return i;
    }
     
    private void handleMessage(String message) {
        msgHandler.handleMessage(message);
    }
    
    public Rule validate(Set<Rule> ruleset,byte[] value, List<byte[]> rowReference) {
        for(Rule rule:ruleset){
            if(rule!=null){
                rule.setRowReference(rowReference);
                if(!rule.isValid(value))
                    return rule;
            }
        }
        return null;
    }

}

{code} ",smartshark_2_2,503,commons-io,"""[0.9963770508766174, 0.003623007331043482]"""
804,229063,Reduce memory consumption when reading file into byte[],"Improvement for org.apache.commons.io.FileUtils method 
public static byte[] readFileToByteArray(File file):

The old implementations used twice the amount of memory than was actually needed to read a file content into a byte[].
E.g. to read a 1GB file 2GB of heap space where needed.",smartshark_2_2,466,commons-io,"""[0.9982682466506958, 0.0017317349556833506]"""
805,229064,[io][patch] add DeferredFileOutputStream.writeTo() + test,"avoid doubleing the memory requirement when all we want to do is transfer the
content onto another OutputStream

related to COM-1979",smartshark_2_2,66,commons-io,"""[0.9983592629432678, 0.0016407726798206568]"""
806,229065,WildcardFileFilter ctors should not use null to mean IOCase.SENSITIVE when delegating to other ctors,"WildcardFileFilter ctors should not use null to mean IOCase.SENSITIVE when delegating to other ctors.

Just because null happens to mean case-sensitive, does not mean that internal calls to ctors should use that feature. It makes the code harder to read.",smartshark_2_2,439,commons-io,"""[0.9945403933525085, 0.005459649953991175]"""
807,229066,Allow DirectoryWalker provide relative paths in handle*(),"{code}
handleFile( File file, int depth, Collection results )
{code}

and other methods provide a file object with full path.

As it's much easier to concat base path and additional path than ""substracting"" one path from other, I suggest:

The `File` object provided by `handleFile()` and other `handle*` methods should (optionally) contain path relative to the one passed to `walk()`.",smartshark_2_2,290,commons-io,"""[0.9984830021858215, 0.0015169616090133786]"""
808,229067,FileSystemUtils.freeSpace does not work on Sun Solaris,"The method FileSystemUtils.freeSpace does not work on Sun Solaris.

    [junit] Testcase: testGetFreeSpace_String took 0.216 sec
    [junit] 	Caused an ERROR
    [junit] Command line 'df' did not return info as expected for path '/'- response on first line was '/                  (/dev/dsk/c0t0d0s0 ):41676304 block  3133314 filer'
    [junit] java.io.IOException: Command line 'df' did not return info as expected for path '/'- response on first line was '/                  (/dev/dsk/c0t0d0s0 ):41676304 block  3133314 filer'
    [junit] 	at org.apache.commons.io.FileSystemUtils.freeSpaceUnix(FileSystemUtils.java:315)
    [junit] 	at org.apache.commons.io.FileSystemUtils.freeSpaceOS(FileSystemUtils.java:179)
    [junit] 	at org.apache.commons.io.FileSystemUtils.freeSpace(FileSystemUtils.java:126)
    [junit] 	at org.apache.commons.io.FileSystemUtilsTestCase.testGetFreeSpace_String(FileSystemUtilsTestCase.java:90)

This is because on Solaris the df command work different if the flag -k is used or not:

$ df /
/                  (/dev/dsk/c0t1d0s0 ): 1070402 blocks   294205 files
$ df -k /
Filesystem            kbytes    used   avail capacity  Mounted on
/dev/dsk/c0t1d0s0    1350955  815754  481163    63%    /

I haven't found any flag that makes it output GNU format even when -k is omitted.",smartshark_2_2,6,commons-io,"""[0.6095419526100159, 0.3904580771923065]"""
809,229068,Test takes a long time to run in Mustang,"    [junit] Running org.apache.commons.io.FileCleanerTestCase
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 116.485 sec

On 1.4.2 on the same machine/checkout, it takes only 1.x seconds.",smartshark_2_2,75,commons-io,"""[0.9772585034370422, 0.022741544991731644]"""
810,229069,BoundedInputStream.read() treats max differently from BoundedInputStream.read(byte[]...),"BoundedInputStream.read() treats max differently from BoundedInputStream.read(byte[]...)

read() checks for pos == max, whereas read(byte[]...) checks for pos >= max.

The latter check is safer (works even if pos overshoots max).",smartshark_2_2,273,commons-io,"""[0.0931374579668045, 0.9068624973297119]"""
811,229070,RemoteCacheFactory ignores host and port configuration on client,"I tested JCS client with remote cache and I had problem to connect to remote cache, my client was always trying to connect to port 1099 (default) and ignores configuration. I think that problem is in class RemoteCacheFactory. In attachment is my simple fix. It is now working, but I'm not sure if it is correct.

client.jcf:
jcs.auxiliary.RServer=org.apache.commons.jcs.auxiliary.remote.RemoteCacheFactory
jcs.auxiliary.RServer.attributes=org.apache.commons.jcs.auxiliary.remote.RemoteCacheAttributes
jcs.auxiliary.RServer.attributes.FailoverServers=localhost:1103

remoteServer.jcf
registry.host=localhost
registry.port=1103
remote.cache.service.port=1103",smartshark_2_2,129,commons-jcs,"""[0.13799022138118744, 0.8620097041130066]"""
812,229071,"incorrect equivalence judgement in BlockDisk.write(long, byte[])","In the trunk, BlockDisk.write(long, byte[]) is as follows:
{code}
private boolean write( long position, byte[] data )
        throws IOException
    {
        ByteBuffer buffer = ByteBuffer.allocate(HEADER_SIZE_BYTES + data.length);
        buffer.putInt(data.length);
        buffer.put(data);
        buffer.flip();
        int written = fc.write(buffer, position);
        fc.force(true);

        return written == data.length;
    }
{code}
where the return statement should be:
{code}
        return written == data.length + HEADER_SIZE_BYTES;
{code}",smartshark_2_2,107,commons-jcs,"""[0.07990486919879913, 0.9200950860977173]"""
813,229072,NullPointerException: defaultAuxValues in CompositeCacheManager,"When an attempt is made to use torque's JCS support, JCS crashes on initialisation with a NullPointerException as below.

The cause is that CompositeCacheConfiguration.parseRegion is passed a parameter called ""value"", and this value is not guarded against being null. The failure happens on this line:

        StringTokenizer st = new StringTokenizer( value, "","" );

If value is a required string, a proper error message explaining exactly what the problem is, and exactly what the end user must do to fix it should be thrown.

Looking further, the ""value"" comes from this field in CompositeCacheManager:

    /** The default auxiliary caches to be used if not preconfigured */
    protected String defaultAuxValues;

This field is given no default value, and no documentation exists explaining how an end user might populate this value, or what the significance of this value is.

The full stack trace looks like this:

Caused by: java.lang.NullPointerException
	at java.util.StringTokenizer.<init>(StringTokenizer.java:182)
	at java.util.StringTokenizer.<init>(StringTokenizer.java:204)
	at org.apache.jcs.engine.control.CompositeCacheConfigurator.parseRegion(CompositeCacheConfigurator.java:321)
	at org.apache.jcs.engine.control.CompositeCacheConfigurator.parseRegion(CompositeCacheConfigurator.java:283)
	at org.apache.jcs.engine.control.CompositeCacheManager.getCache(CompositeCacheManager.java:448)
	at org.apache.jcs.engine.control.CompositeCacheManager.getCache(CompositeCacheManager.java:387)
	at org.apache.jcs.engine.control.CompositeCacheManager.getCache(CompositeCacheManager.java:374)
	at org.apache.jcs.JCS.getInstance(JCS.java:66)
",smartshark_2_2,74,commons-jcs,"""[0.0664205551147461, 0.9335794448852539]"""
814,229073,org.apache.jcs.auxiliary.remote.server.RemoteCacheServerUnitTest#testAddListener_ToAll depends on other test,"When running maven test, this test fails.

Step to reproduce :
1/ Comment all test in RemoteCacheServerUnitTest except testAddListener_ToAll and save
2/ Run: mvn -Dtest=org.apache.jcs.auxiliary.remote.server.RemoteCacheServerUnitTest test
I get : Test failed.

Expected result: Test success.

When running ""mvn test"" under cygwin (maybe linux), the test testAddListener_ToAll is run before all ""testAddListenerToCache_****"", and ""mvn test"" fails.",smartshark_2_2,139,commons-jcs,"""[0.838879406452179, 0.16112060844898224]"""
815,229074,RemoteCacheServerFactory ignores configuration ,"RemoteCacheServerFactory ignores configuration from property file except remote atributtes and set default values. Properties jcs.default.*, jcs.region.* and jcs.auxiliary.* are ignored from file remote.cache.ccf



",smartshark_2_2,130,commons-jcs,"""[0.1531074345111847, 0.8468925356864929]"""
816,229075,Minor patches for tests,"Some tests fails when I try a mvn test.

Here some patches :

Class: BlockDiskCacheUnitTest
Method: testPutGetMatching_NoWait
Remove the <String, String> inside the disk path :
cattr.setDiskPath( ""target/test-sandbox/BlockDiskCache<String, String>UnitTest"" );
->
cattr.setDiskPath( ""target/test-sandbox/BlockDiskCacheUnitTest"" );

The property thread_pool.default.maximumPoolSize in files src/test-conf/cache.ccf and src/test-conf/thread_pool.properties should be set to 150 (which is the real default value) :
thread_pool.default.maximumPoolSize=151
",smartshark_2_2,81,commons-jcs,"""[0.9922303557395935, 0.0077696992084383965]"""
817,229076,"remote diskcache, out of memory and lost objects using remoting ","I try to put  and get  objects from a remote cache, that is running on Tomcat.  I use index_disk_cache on the server.  The communication between client and server works. In case of large objects or a large number of objects I get a java.lang.OutOfMemoryError 


Exception in thread ""CacheEventQueue.QProcessor-myRegion1"" java.lang.OutOfMemoryError: Java heap space
	at sun.reflect.ByteVectorImpl.trim(ByteVectorImpl.java:52)
	at sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:370)
	at sun.reflect.MethodAccessorGenerator.generateSerializationConstructor(MethodAccessorGenerator.java:95)
	at sun.reflect.ReflectionFactory.newConstructorForSerialization(ReflectionFactory.java:313)
	at java.io.ObjectStreamClass.getSerializableConstructor(ObjectStreamClass.java:1327)
	at java.io.ObjectStreamClass.access$1500(ObjectStreamClass.java:52)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:437)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:413)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1106)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1474)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)
	at sun.rmi.server.UnicastRef.marshalValue(UnicastRef.java:274)
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:133)
	at org.apache.jcs.auxiliary.remote.server.RemoteCacheServer_Stub.update(Unknown Source)
	at org.apache.jcs.auxiliary.remote.RemoteCache.update(RemoteCache.java:210)
	at org.apache.jcs.engine.CacheAdaptor.handlePut(CacheAdaptor.java:91)
	at org.apache.jcs.engine.CacheEventQueue$PutEvent.doRun(CacheEventQueue.java:688)
	at org.apache.jcs.engine.CacheEventQueue$AbstractCacheEvent.run(CacheEventQueue.java:607)
	at org.apache.jcs.engine.CacheEventQueue$QProcessor.run(CacheEventQueue.java:575)

My configurations are: 


1) client:

jcs.auxiliary.RFailover=org.apache.jcs.auxiliary.remote.RemoteCacheFactory
jcs.auxiliary.RFailover.attributes=org.apache.jcs.auxiliary.remote.RemoteCacheAttributes
jcs.auxiliary.RFailover.attributes.FailoverServers=pchl:1102
jcs.auxiliary.RC.attributes.RemoveUponRemotePut=true
jcs.auxiliary.RFailover.attributes.GetOnly=false
jcs.auxiliary.RFailover.attributes.MaxObjects=100
jcs.region.myRegion1=RFailover



#jcs.region.myRegion1=RFailover
#jcs.region.myRegion1.cacheattributes=org.apache.jcs.engine.CompositeCacheAttributes
#jcs.region.myRegion1.cacheattributes.MaxObjects=100
#jcs.region.myRegion1.cacheattributes.MemoryCacheName=org.apache.jcs.engine.memory.lru.LRUMemoryCache


Server:

# Remote RMI Cache set up to failover
jcs.auxiliary.RFailover=org.apache.jcs.auxiliary.remote.RemoteCacheFactory
jcs.auxiliary.RFailover.attributes=org.apache.jcs.auxiliary.remote.RemoteCacheAttributes
jcs.auxiliary.RC.attributes.RemoveUponRemotePut=true
jcs.auxiliary.RFailover.attributes.GetOnly=false

# this didn't help, the memory error occurs with and without
#jcs.region.testCache1=DC,RFailover
#jcs.region.testCache1.cacheattributes=org.apache.jcs.engine.CompositeCacheAttributes
#jcs.region.testCache1.cacheattributes.MaxObjects=1000
#jcs.region.testCache1.cacheattributes.MemoryCacheName=org.apache.jcs.engine.memory.lru.LRUMemoryCache

# Registry used to register and provide the
# IRemoteCacheService service.
registry.host=localhost
registry.port=1102
# call back port to local caches.
remote.cache.service.port=1102
# cluster setting
remote.cluster.LocalClusterConsistency=true
remote.cluster.AllowClusterGet=true





##############################################################
##### Default Region Configuration
jcs.default=DC
jcs.default.cacheattributes=org.apache.jcs.engine.CompositeCacheAttributes
jcs.default.cacheattributes.MaxObjects=100
jcs.default.cacheattributes.MemoryCacheName=org.apache.jcs.engine.memory.lru.LRUMemoryCache
jcs.default.cacheattributes.UseMemoryShrinker=false
jcs.default.cacheattributes.ShrinkerIntervalSeconds=60

##############################################################
##### CACHE REGIONS
jcs.region.myRegion1=DC
jcs.region.myRegion1.cacheattributes=org.apache.jcs.engine.CompositeCacheAttributes
jcs.region.myRegion1.cacheattributes.MaxObjects=-1
jcs.region.myRegion1.cacheattributes.MemoryCacheName=org.apache.jcs.engine.memory.lru.LRUMemoryCache
#jcs.region.myRegion1.cacheattributes.DiskUsagePattern=SWAP
##############################################################
##### AUXILIARY CACHES
# Indexed Disk Cache
jcs.auxiliary.DC=org.apache.jcs.auxiliary.disk.indexed.IndexedDiskCacheFactory
jcs.auxiliary.DC.attributes=org.apache.jcs.auxiliary.disk.indexed.IndexedDiskCacheAttributes
###jcs.auxiliary.DC.attributes.DiskPath=O:\\liantis/JCS/dump
jcs.auxiliary.DC.attributes.DiskPath=dump
jcs.auxiliary.DC.attributes.MaxPurgatorySize=100
jcs.auxiliary.DC.attributes.MaxKeySize=-1
jcs.auxiliary.DC.attributes.OptimizeAtRemoveCount=300000
jcs.auxiliary.DC.attributes.OptimizeOnShutdown=true
jcs.auxiliary.DC.attributes.MaxRecycleBinSize=7500
jcs.auxiliary.DC.attributes.EventQueueType=POOLED
jcs.auxiliary.DC.attributes.EventQueuePoolName=disk_cache_event_queue


# Disk Cache pool
thread_pool.disk_cache_event_queue.boundarySize=50
thread_pool.disk_cache_event_queue.useBoundary=true
thread_pool.disk_cache_event_queue.maximumPoolSize=15
thread_pool.disk_cache_event_queue.minimumPoolSize=1
thread_pool.disk_cache_event_queue.keepAliveTime=3500
thread_pool.disk_cache_event_queue.startUpSize=1

pchl is the server on that Tomcat is running. 

In case of smal objects I have the problem, that I will  lose  objects again. I've configured the queue and the threadpool at the server,  but it seams, that there is something too, that I've forgotten... 

I'll work with a absolute minimum on java heapsize,  that is the reson to use a cache in our case.


The problems will occur with and without the outcommented lines in the clientconfiguration 

The reason for the questions during the last days are , that we evaluate caching architectures and performance using jcs ...",smartshark_2_2,20,commons-jcs,"""[0.6072448492050171, 0.39275509119033813]"""
818,229077,Disk cache grows unnecessarily,"I'm using JCS cache in UPDATE mode and I found that disk cache may grow when adding the cache entries with the same key but larger value.

For example, 
cache.put(""key"", ""value"");
cache.put(""key"", ""value1"");
cache.put(""key"", ""value12"");

After that I see that *.data file contains all 3 entries. And the cache is not optimized at shutdown even when isShutdownOptimizationEnabled is set to true.

I found a workaround for this - delete the key before writing.",smartshark_2_2,21,commons-jcs,"""[0.699830174446106, 0.3001698851585388]"""
819,229078,Items read from disk auxilaries are spooled back to disk.,"Items that have been read from disk are spooling back to disk.  During a large volume of reads, this causes elements that have been read into the cache to be written back to disk, even though there have been no modifications.

This increase in writes doubles the over all average read time by a factor of 2.",smartshark_2_2,38,commons-jcs,"""[0.4903278946876526, 0.5096721053123474]"""
820,229079,Can we do a release of JCS 1.3 with the JCS Worker fix (and any other bug fixes available)?,"Can we do a release of JCS with some of the bug fixes in it?
It's been quite a while since a release has been done.

I don't know how much activity there is, but it would also be nice to migrate the project to the latest version of maven and possibly to JDK6.

I'm not sure what would be involved in all that, but I would be glad to help out.",smartshark_2_2,98,commons-jcs,"""[0.9983880519866943, 0.0016119458014145494]"""
821,229080,Is there any way to manage the cache file which is taking too much disk space,"As we know the JSC will write the object into the xxxx.cache file in the App server FileSystem when the memory/cache is used up.
Is there any way for JCS to manage the xxxx.cache file size to avoid application filesystem full issue?",smartshark_2_2,73,commons-jcs,"""[0.9972097277641296, 0.0027902787551283836]"""
822,229081,Class package in cache.ccf documentation on website,"Hi,
When we use the current cache.ccf as shown in the website (http://commons.apache.org/proper/commons-jcs/UsingJCSBasicWeb.html), the following properties result in a ClassNotFound exception.

jcs.default.cacheattributes=org.apache.commons.jcs.engine.CompositeCacheAttributes
jcs.default.cacheattributes.MemoryCacheName=org.apache.commons.jcs.engine.memory.lru.LRUMemoryCache

After inspecting the jcs1.3.jar, I saw that the packages exist in org/apache/jcs/engine/CompositeCacheAttributes and org/apache/jcs/engine/memory/lru/LRUMemoryCache respectively. (No ""commons"").

I think the document must be updated to reflect this packaging structure change.

Thanks,
Mathew",smartshark_2_2,90,commons-jcs,"""[0.9905126690864563, 0.009487316943705082]"""
823,229082,[JCACHE] How to configure caches used by interceptors (i.e. CacheResultInterceptor) related to JCache annotations,"I started using JCS cache as a JCache implementation provider a few days ago and everything wasÂ fine until I needed to set up expiration for cached data or even the number of elements stored in the cache. I realized that caches were created with EternalPolicy so I started to figure out how to set up both implementation specific properties and Standard JCache ones. After a couple of days, I don't have a pretty idea about how to configure the caches related to methods annotated with JCache annotations like CacheResult, CacheRemove and so on.

I mean that I already know that I can use the CacheDefaults annotation, both at class and method level, in order to set the name of the cache to be used. but another thing is to also set the CacheResolverFactory. Should I always set the CacheResolverFactory in the annotation in order to be able to create the cache using a specificÂ set of properties? Or maybe the idea is to create the cache programmaticallyÂ before is is needed? Anyway, It seems too much boilerplate to me. Maybe I am wrong but I think that we are not using the CDI power at all.

please, let me know if there is better/simplerÂ approach.

Another thing that I cannot understand is the readConfig method of theÂ JCSCachingManager class. It is loading the folder structure because it is loading the resources from the default uriÂ of the provider!Â 

where uriÂ is ""jcs://jcache.ccf"", so the path is ""/"". I think thatÂ this URI should notÂ be used for this, maybe the uri should be something different (URI of the default configuration?) or just use another implementation specific property.Â  Notice that adding an extra slash will look for /jcache.ccf file! maybe it is just a bug and there is a slash missing : ""jcs:///jcache.ccf""
{code:java}
Â final Enumeration<URL> resources = loader.getResources(uri.getPath());
{code}
Â 

What about this?

Add CachingProvider and CacheManager as beans if they are not already beans. Notice that this is what jcs-jcache-extras module already does!Â 

So as a developer I can configure my CachingProvider and even the cache manager already configured as beans.Â  They will be injected where needed instead of beingÂ createdÂ without configuration.

no configuration in CacheResolverFactoryÂ 
{code:java}
public CacheResolverFactoryImpl()
{
  provider = Caching.getCachingProvider();
  cacheManager = provider.getCacheManager(provider.getDefaultURI(), provider.getDefaultClassLoader());
}
{code}
but in JCacheFilterÂ class someÂ properties are set

Â 
{code:java}
manager = provider.getCacheManager(URI.create(uri), classLoader, properties);
{code}
One step further into CDI should be to also provide theÂ CacheResolverFactory as a bean and also theÂ 

CacheResolverFactoryImpl could be also injected with the cacheProvider and CacheManager and some CompleteConfigurationResolver of something like that in order to avoid creating caches with a hard-coded MutableConfiguration.Â 
{code:java}
public class CacheResolverFactoryImpl implements CacheResolverFactory
{
...
private Cache<?, ?> createCache(final String exceptionCacheName)
{
  cacheManager.createCache(exceptionCacheName, new   MutableConfiguration<Object, Object>().setStoreByValue(false));
  return cacheManager.getCache(exceptionCacheName);
}
{code}
Â 

Â with these changes, we reduce boilerplate and giveÂ more control to jcs-jcache library clients.

Â Can we move ExtraJCacheExtension toÂ jcs-jcacheÂ in order to be able to handle CachingProvider and CacheManager as beans?

best regards,

Ricard",smartshark_2_2,191,commons-jcs,"""[0.9937008619308472, 0.0062991245649755]"""
824,229083,divide zero error and others,"code snippetï¼
		String exp;
		Expression expression;
		JexlEngine engine = new JexlEngine();

		
		exp = ""1/0"";
		expression = engine.createExpression(exp);
		System.out.println(expression.evaluate(null));

this will print 0.0,shoud it indicate an error by throwing exception or return value?
----------------------------------------------


code snippetï¼
		exp = ""1/0.0"";
		expression = engine.createExpression(exp);
		System.out.println(expression.evaluate(null));
this will print 0.0 too,but in fact it should return Infinity.
----------------------------------------------


code snippet:
		exp = ""Math.abs(-1)"";
		expression = engine.createExpression(exp);
		System.out.println(expression.evaluate(null));

I don't know whether this is a bug.JEXL cannot invoke java static method directly,it will throw an exception
----------------------------------------------



code snippet:
		exp = ""a.abs(-1)"";
		class MyMath {
			public double abs(double d) {
				return Math.abs(d);
			} 
		}
		Object obj = new MyMath();
		expression = engine.createExpression(exp);
		MapContext c = new MapContext();
		c.set(""a"", obj);
		System.out.println(expression.evaluate(c));

er,this is very strange,if MyMath is not public(whether in a independent file or not),it will return null;but if MyMath is a public class,it will behave correctly
",smartshark_2_2,39,commons-jexl,"""[0.6301441788673401, 0.36985576152801514]"""
825,229084,dot-ed identifiers parsing failure,An expression like {code}foo.0.1{code} fails at parsing time; parser sees a double where it should not,smartshark_2_2,89,commons-jexl,"""[0.07141094654798508, 0.9285890460014343]"""
826,229085,InterruptedException is swallowed in function call in silent and non-strict mode,"The following test case fails with 

{code}
java.lang.AssertionError: Values should be different. Actual: 42
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failEquals(Assert.java:185)
	at org.junit.Assert.assertNotEquals(Assert.java:161)
{code}

{code}
    public static class TestContext extends MapContext implements JexlContext.NamespaceResolver {

        public int interrupt() throws InterruptedException {
            throw new InterruptedException();
        }
    }

    @Test
    public void testInterrupt() throws Exception {
        JexlEngine jexl = new JexlBuilder().arithmetic(new JexlArithmetic(false)).strict(false).silent(true).create();
        JexlScript e = jexl.createScript(""interrupt(); return 42"");
        Callable<Object> c = e.callable(new TestContext());

        Object t = c.call();
        Assert.assertNotEquals(42, t);
    }
{code}

Expected behaviour is to cancel script execution",smartshark_2_2,116,commons-jexl,"""[0.37792038917541504, 0.622079610824585]"""
827,229086,ASTAddNode does not add BigDecimal objects correctly,"The ASTAddNode only checks for Float or Double objects when adding floating point numbers. If the objects are not Float or Double, they are added as Long's. As a result, adding BigDecimal objects loses any existing decimal points.

Untested patch:

Index: ASTAddNode.java
===================================================================
--- ASTAddNode.java     (revision 476204)
+++ ASTAddNode.java     (working copy)
@@ -16,6 +16,8 @@
 
 package org.apache.commons.jexl.parser;
 
+import java.math.BigDecimal;
+
 import org.apache.commons.jexl.JexlContext;
 import org.apache.commons.jexl.util.Coercion;
 
@@ -71,8 +73,8 @@
          *  if anything is float, double or string with ( ""."" | ""E"" | ""e"")
          *  coerce all to doubles and do it
          */
-        if (left instanceof Float || left instanceof Double
-            || right instanceof Float || right instanceof Double
+        if (left instanceof Float || left instanceof Double || left instanceof BigDecimal
+            || right instanceof Float || right instanceof Double || right instanceof BigDecimal
             || (left instanceof String
                   && (((String) left).indexOf(""."") != -1 
                           || ((String) left).indexOf(""e"") != -1
",smartshark_2_2,207,commons-jexl,"""[0.06847362220287323, 0.9315263628959656]"""
828,229087,Interpreter.getAttribute() raises exception in non-strict mode when cached property resolver is used,"I'm chasing strange bug where regardless of the {{JexlContext}} operating in non-strict mode the {{ArrayIndexOutOfBoundsException}} is thrown in the script like this

{code}
entity = args[0]; @lenient {copy = args[1]; xwsp = args[2]}
{code}

here is the stack trace

{quote}
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at java.lang.reflect.Array.get(Native Method)
	at org.apache.commons.jexl3.internal.introspection.ListGetExecutor.tryInvoke(ListGetExecutor.java:88) 
	at org.apache.commons.jexl3.internal.Interpreter.getAttribute(Interpreter.java:1700) 
	at org.apache.commons.jexl3.internal.Interpreter.visit(Interpreter.java:945) 
	at org.apache.commons.jexl3.parser.ASTArrayAccess.jjtAccept(ASTArrayAccess.java:18) 
	at org.apache.commons.jexl3.internal.Interpreter.visit(Interpreter.java:1013) 
	at org.apache.commons.jexl3.parser.ASTReference.jjtAccept(ASTReference.java:18) 
	at org.apache.commons.jexl3.internal.Interpreter.executeAssign(Interpreter.java:1119) 
	at org.apache.commons.jexl3.internal.Interpreter.visit(Interpreter.java:1062) 
	at org.apache.commons.jexl3.parser.ASTAssignment.jjtAccept(ASTAssignment.java:18) 
	at org.apache.commons.jexl3.internal.Interpreter.visit(Interpreter.java:578) 
	at org.apache.commons.jexl3.parser.ASTBlock.jjtAccept(ASTBlock.java:18) 
	at org.apache.commons.jexl3.internal.Interpreter.processAnnotation(Interpreter.java:1848) 
	at org.apache.commons.jexl3.internal.Interpreter$1.call(Interpreter.java:1856) 
{quote}

Unfortunately I haven't managed to create and provide reproducible test case, but from looking into the code I think the problem fires when the Interpreter tries to call cached method {{ListGetExecutor.tryInvoke()}} but does not catch subsequent exception.  

I rewrote the code as follows and the problem seemed to go away
{code:title=Interpreter.java}
        ...
        Exception xcause = null;

        // attempt to reuse last executor cached in volatile JexlNode.value
        if (node != null && cache) {
            Object cached = node.jjtGetValue();
            if (cached instanceof JexlPropertyGet) {
                JexlPropertyGet vg = (JexlPropertyGet) cached;

                try {

                   Object value = vg.tryInvoke(object, attribute);
                   if (!vg.tryFailed(value)) {
                       return value;
                   }

                } catch (Exception xany) {
                    xcause = xany;
                }

            }
        }

        if (xcause == null) {

           // resolve that property

           List<PropertyResolver> resolvers = uberspect.getResolvers(operator, object);
           JexlPropertyGet vg = uberspect.getPropertyGet(resolvers, object, attribute);
           if (vg != null) {
               try {
                   Object value = vg.invoke(object);
                   // cache executor in volatile JexlNode.value
                   if (node != null && cache && vg.isCacheable()) {
                       node.jjtSetValue(vg);
                   }
                   return value;
               } catch (Exception xany) {
                   xcause = xany;
               }
           }
        }
        ...
{code}",smartshark_2_2,133,commons-jexl,"""[0.06169510260224342, 0.9383049011230469]"""
829,229088,Incorrect invoking methods with ObjectContext,"After apply fix in [JEXL-190|https://issues.apache.org/jira/browse/JEXL-190] folowing code doesn't work anymore:
{code}
public class Example {
    public static void main(String[] args) {
        JexlEngine jexl = new JexlBuilder().create();

        ObjectContext<Foo> context = new ObjectContext<>(jexl, new Foo());
        JexlExpression expression = jexl.createExpression(""bar()"");

        System.out.println(expression.evaluate(context));
    }

    public static class Foo extends HashMap<String, Object>{
        public String bar(){
            return ""bar"";
        }
    }
}
{code}


Also doesn't work this code:
{code}
public class Example {
    public static void main(String[] args) {
        HashMap<String, Object> functions = new HashMap<>();
        functions.put(null, new DefaultFunctions());

        JexlEngine jexl = new JexlBuilder()
                .namespaces(functions)
                .create();

        ObjectContext<Object> context = new ObjectContext<>(jexl, new Object());
        JexlExpression expression = jexl.createExpression(""bar()"");

        System.out.println(expression.evaluate(context));
    }

    public static class DefaultFunctions {
        public String bar(){
            return ""bar"";
        }
    }
}
{code}


Output:
{code}
Example.main@1:4 unsolvable function/method 'bar'
{code}

note: with MapContext all working correct",smartshark_2_2,145,commons-jexl,"""[0.07751182466745377, 0.9224881529808044]"""
830,229089,Permissions by super type in JexlSandbox,"At the moment, the permissions in {{JexlSandbox}} takes the object's class name only into the consideration. So, if someone adds {{java.util.Set}} into the white list, but if the real object is an empty set ({{Collections.emptySet()}}), then it cannot allow invocations on {{#contains(Object)}} operation, for instance.

I think it would be very convenient if it optionally allows to set whites or blacks based on super type (interfaces or base classes).

To minimize the effort, I'd suggest adding {{JexlSandbox#permissionsByType(Class<?> type, ...)}}, where the {{type}} means the object type or any super types.
So, if {{JexlSandbox#permissionsByType(java.util.Set.class, ...)}}, then any invocations on any concrete {{java.util.Set}} objects will be affected by that.

Related e-mail thread: ""[JEXL] white list classes, not by interfaces?"" (10/19/17).",smartshark_2_2,164,commons-jexl,"""[0.984499454498291, 0.015500525012612343]"""
831,229090,Redundant call of fillInStackTrace() in JexlEngine.createInfo() ?,"I wonder what is the call of {{fillInStackTrace()}} for ?

{code}
    public JexlInfo createInfo() {
        JexlInfo info = null;
        Throwable xinfo = new Throwable();
        xinfo.fillInStackTrace();
        ...
{code}

As far as I know the constructor of Throwable is implemented already calling this method. ",smartshark_2_2,126,commons-jexl,"""[0.9882521033287048, 0.011747949756681919]"""
832,229091,Extend application of operators startsWith and endsWith from String to CharSequence types,"It would be more useful to have operators {{=$}} and {{=^}} to work with objects that support {{CharSequence}} interface, like {{StringBuilder}}, which would have more broad usage then just with {{String}} ",smartshark_2_2,139,commons-jexl,"""[0.9982311129570007, 0.0017689552623778582]"""
833,229092,[jexl][patch] Rethrow exceptions in method execution,"Current ASTMethod just prints any methods during method execution to the console. This is 
undesirable in unattended server processes unless someone is always monitoring STDERR.

The attached patch causes the thrown exception to flow out of jexl, and the calling program can 
deal with it accordingly.

The patch also modifies UberspectImpl to remove a containing InvocationTargetException.",smartshark_2_2,272,commons-jexl,"""[0.9871091842651367, 0.012890811078250408]"""
834,229093,operator overloading / hooks on operator processing,"I'd like to use JEXL for a website framework. In order to make the scripting code easy readable it would be nice to let the users add hooks in the expression processing.
such a hook would allow me to add Date operators for example: Date  and Durations addition/substraction.

that would've been very easy for me to implement if I had access to the processing done in let's say ASTAddNode class. 

An easy way to do it is to add a method in the parser like
registerHook(Class astNodeClass, SimpleNode hook)

in ExpressionImpl.evaluate we call node.implValue instead of value. This method can check if  hooks where registered for the class and call for each hook.value before calling the value of the node itself. 
For perfomance reasons the list of these hooks can be stored by the each SimpleNode in the constructor SimpleNode(Parser p, int i) 

that's it, thanks
",smartshark_2_2,253,commons-jexl,"""[0.9983258843421936, 0.0016741228755563498]"""
835,229094,Decimal numbers literals should be 'double' by default (instead of 'float'),"Since Jexl 1.x, decimal number literals are 'float'; it would be more natural (pun unintended) to create 'double' instead since they are used more often (reduce 'surprise' effects).
Note that since 2.1, numbers literals notations allow typing; '123f' is a float literal.",smartshark_2_2,66,commons-jexl,"""[0.9985026121139526, 0.0014974644873291254]"""
836,229095,Release 3.1 to Maven Repository,"Any chance version 3.1 is scheduled for a release to the Maven Repository any time soon? Seems like there are plenty of improvements and bug fixes to warrant a release, and also -- last release was a year go at this point :)

https://mvnrepository.com/artifact/org.apache.commons/commons-jexl3

",smartshark_2_2,135,commons-jexl,"""[0.9981132745742798, 0.001886760233901441]"""
837,229096,Add callable method to JexlExpression interface,"It would be convenient to have the following method
{code}
   Callable<Object> callable(JexlContext context);
{code}
in both JexlExpression and JexlScript interfaces. ",smartshark_2_2,132,commons-jexl,"""[0.9983333945274353, 0.001666664844378829]"""
838,229097,BSF engine impl,"See thread titled ""[JEXL] Any plans for a BSF wrapper?"", started on dev list 09/11/07. On markmail, its at:

    http://commons.markmail.org/message/onjxo7rjmhxnc2as?q=order:date-backward&page=3

This is meant to provide a starting point, if there is interest. Attachment coming, the fix version is purely speculative.
",smartshark_2_2,7,commons-jexl,"""[0.9961464405059814, 0.0038535697385668755]"""
839,229098,Add public fields as targets of set/get property,The Uberspect should be able to wrap public field of classes in JexlPropertyGet / JexlPropertySet instances.,smartshark_2_2,252,commons-jexl,"""[0.998464822769165, 0.0015351439360529184]"""
840,229099,The way to cancel script execution with an error,"I don't see a way now to cancel script execution with some kind of error. Unfortunately it's not possible to just throw an exception from some method as this will rely on current settings of context/engine *strictness* and *verboseness*. Using InterruptedException for this purpose is not an option because I think it has special meaning of cancelling the current thread execution. Yet the task I beleive is quite common - to inform the executing environment that the script has encountered some unavoidable situation and can not continue. Just like *return* statement but returning not a value but an error. 

For this purpose we can simply introduce some new type of exception, for example 
{code}
    public static class Error extends JexlException {
    ...
{code}
and by throwing it from any method the JexlEngine will terminate the current script execution regardless of strictness/verboseness. May be this task even deserves to have a special operator, for example 

{code}
raise 'Something has happended';
{code}",smartshark_2_2,107,commons-jexl,"""[0.969754695892334, 0.030245283618569374]"""
841,229100,[jexl] Don't make null convertible into anything,"Currently JEXL is very tolerant with null (and unresolved variables)... 
I am unsure now but I think null will become false if the output of a test, null will become the empty 
string if concatenated to a string, null will become the number zero when added. Moreover, 
invoking a method on null gives you... null...

I would love a switch that just disables all this (I think this would impact quite a lot) allowing my jexl 
snippets to be quite more predictable!
paul",smartshark_2_2,202,commons-jexl,"""[0.9797338843345642, 0.02026609145104885]"""
842,229101,allow synchronization on iterableValue in foreach statement,"Since it is a requirement to synchronize on simple Collections and synchronized Collections while iterating over them and since jexl has no instrument to control synchronization in script, I think its reasonable to implement synchronization in jexl itself on iterableValue. In case of concurrent collections it will possibly block other threads only if they are synchronizing on those collections themselves, which will be complementary to required synchronization in jexl.",smartshark_2_2,112,commons-jexl,"""[0.9984641075134277, 0.0015359005192294717]"""
843,229102,Ability to continue interrupted scripts,"I'm trying to implement the {{@timeout}} annotation that should work like the following
{code:java}
@timeout(15000) { return longrunningcall(); }
Â {code}
The idea is to protect part of the script code from being executed indefinitely or more than allowed by business rules. The script should continue its evaluation after the {{@timeout}} annotation regardless of whether the timeout has taken place or not.

There is a straightforward implementation that starts guarding thread which should invoke {{Thread.interrupt()}} for the thread executing the script. The {{InterruptedException | JexlException.Cancel}} is then caught and swallowed inside the {{processAnnotation()}} method, and if the guard thread has fired, which means the timeout occured, the {{null}} value is returned.

I expected the script to continue its evaluation after the exception is processed inside {{processAnnotation()}} code, but the script nevertheless throwed {{JexlException.Cancel}} as a result. The suggestion is to allow script to continue its evaluation once {{InterruptedException}} or {{JexlException.Cancel}} is processed. ",smartshark_2_2,169,commons-jexl,"""[0.9983281493186951, 0.0016719079576432705]"""
844,229103,Allow to remove an element from iterator collection within for-loops,"The for-loop in JEXL provides a convenient way to iterate over different types of collections, however, its not possible for a script writer to utilize underlying
{code:java}
iterator.remove(){code}
method within such a loop. The proposal is to introduce new {{remove}} statement which should be used within for-loops and should internally call {{iterator.remove()}} method and skip the loop to the next element;

For example, the following code should remove items {{1,2,3}} from set and return value {{3}}.
{code:java}
var set = {1,2,3,4,5,6}; for (var item : set) if (item <= 3) remove; return size(set)
{code}
",smartshark_2_2,177,commons-jexl,"""[0.9985116124153137, 0.0014884263509884477]"""
845,229104,Extend Range literal to support Long values,"Add support for A .. B syntax where either A or B or both can be Long literals or objects. It can be useful for manipulation of timestamps which are in fact Long values. Implement operators =~ and !~ to support ranges, for example 123 =~ 10..1000.",smartshark_2_2,58,commons-jexl,"""[0.998190701007843, 0.0018092704704031348]"""
846,229105,conceptual improvement for the language,"the language dont distinguish when a object is null and when is undefined 
we could add a keyword ""undefined"" ....",smartshark_2_2,33,commons-jexl,"""[0.998242974281311, 0.0017570625059306622]"""
847,229106,Move code to org.apache.commons.jexl2,"Since there is substantial a risk that JEXL2 is not compatible enough with JEXL1, the whole set of JEXL classes need to move from o.a.c.jexl to o.a.c.jexl2.
See discussion here: http://n4.nabble.com/JEXL-2-0-o-a-c-jexl-or-o-a-c-jexl2-tp727081p727081.html

",smartshark_2_2,261,commons-jexl,"""[0.9984574317932129, 0.0015425266465172172]"""
848,229107,FastDateParser does not handle non-Gregorian calendars properly,"The following calendars are non-Gregorian [1]:

ja_JP_JP - Japanese Imperial)
th_TH (with any variant) - Thai Buddhist

This causes errors when testing round-trip conversions.

The simplest solution is to fall back to SimpleDateFormat for these calendars.

[1] http://docs.oracle.com/javase/6/docs/technotes/guides/intl/calendar.doc.html",smartshark_2_2,317,commons-lang,"""[0.08540469408035278, 0.9145953059196472]"""
849,229108,StrBuilder#replaceAll ArrayIndexOutOfBoundsException,"There is a bug in replace for StrBuilder, seems the use of nonupdated buffer and character count is off.

new StrBuilder(""Dear X, hello X."").replaceAll(StrMatcher.stringMatcher(""X""), ""012345678901234567"");

yields

{noformat}
java.lang.ArrayIndexOutOfBoundsException: 49
	at org.apache.commons.lang3.text.StrMatcher$StringMatcher.isMatch(StrMatcher.java:372)
	at org.apache.commons.lang3.text.StrBuilder.replaceImpl(StrBuilder.java:2115)
	at org.apache.commons.lang3.text.StrBuilder.replace(StrBuilder.java:2088)
	at org.apache.commons.lang3.text.StrBuilder.replaceAll(StrBuilder.java:2049)
{noformat}",smartshark_2_2,949,commons-lang,"""[0.0615118145942688, 0.9384881854057312]"""
850,229109,FastDateFormat doesn't respect summer daylight in localized strings,"FastDateFormat can't properly parse dates with daylight saving in the ""z"" pattern. It always returns date without daylight saving. Test case:

{code:java}
		SimpleDateFormat format = new SimpleDateFormat(""dd.MM.yyyy HH:mm:ss z"", Locale.GERMANY);
		Date d1 = format.parse(""26.10.2014 02:00:00 MESZ"");
		Date d2 = format.parse(""26.10.2014 02:00:00 MEZ"");
		System.out.println(d1);
		System.out.println(d2);
		FastDateFormat formatt = FastDateFormat.getInstance(""dd.MM.yyyy HH:mm:ss z"", Locale.GERMANY);
		Date d3 = formatt.parse(""26.10.2014 02:00:00 MESZ"");
		Date d4 = formatt.parse(""26.10.2014 02:00:00 MEZ"");
		System.out.println(d3);
		System.out.println(d4);	
{code}

returns:
SDF: Sun Oct 26 02:00:00 CEST 2014
SDF: Sun Oct 26 02:00:00 CET 2014
FDF: Sun Oct 26 02:00:00 CET 2014
FDF:  Sun Oct 26 02:00:00 CET 2014

FastDateFormat returns the same date, which is wrong.

Bug is in the FastDateParser.TimeZoneStrategy.setCalendar:
{code:java}
@Override
        void setCalendar(final FastDateParser parser, final Calendar cal, final String value) {
            TimeZone tz;
            if(value.charAt(0)=='+' || value.charAt(0)=='-') {
                tz= TimeZone.getTimeZone(""GMT""+value);
            }
            else if(value.startsWith(""GMT"")) {
                tz= TimeZone.getTimeZone(value);
            }
            else {
                tz= tzNames.get(value);
                if(tz==null) {
                    throw new IllegalArgumentException(value + "" is not a supported timezone name"");
                }
            }
            cal.setTimeZone(tz);
        }
{code}

It's not enough to just call: cal.setTimeZone.
If zone names in standard and daylight time are different, you have to check the name in DateFormatSymbols.getInstance(locale).getZoneStrings(); and if it's >= 3, you have to activate daylight mode.Just like SimpleDateFormat does it:
{code:java}
1491            // (abbreviation) for both standard and daylight time,
1492            // let the time zone in the Calendar decide which one.
1493            if (!useSameName) {
1494                calendar.set(Calendar.ZONE_OFFSET, tz.getRawOffset());
1495                calendar.set(Calendar.DST_OFFSET,
1496                             j >= 3 ? tz.getDSTSavings() : 0);
1497            }
{code}
",smartshark_2_2,755,commons-lang,"""[0.11909385025501251, 0.8809061646461487]"""
851,229110,EqualsBuilder#isRegistered: swappedPair construction bug,"See

https://stackoverflow.com/questions/45603317/org-apache-commons-lang3-builder-equalsbuilder

and

https://github.com/apache/commons-lang/pull/282

for details.",smartshark_2_2,987,commons-lang,"""[0.07722717523574829, 0.9227728843688965]"""
852,229111,NumberUtils.createNumber throws NumberFormatException for one digit long,"NumberUtils.createNumber throws a NumberFormatException when parsing ""1l"", ""2l"" .. etc...

It works fine if you try to parse ""01l"" or ""02l"".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for ""1l""",smartshark_2_2,1253,commons-lang,"""[0.09061428159475327, 0.9093857407569885]"""
853,229112,WordUtils.wrap throws StringIndexOutOfBoundsException when wrapLength is Integer.MAX_VALUE,"This is the sample code.
{code:java}
int max = Integer.MAX_VALUE;Â  // 2147483647
WordUtils.wrap(""foobar"", max, ""\n"", true);
{code}

and the error log.
{noformat}
Exception in thread ""main"" java.lang.StringIndexOutOfBoundsException: String index out of range: -2147483648
	at java.lang.String.substring(String.java:1967)
	at org.apache.commons.lang3.text.WordUtils.wrap(WordUtils.java:291)
	at org.apache.commons.lang3.text.WordUtils.wrap(WordUtils.java:179)
{noformat}

Seems WordUtils in commons-text has the same bug. I will file it on commons-text side later.",smartshark_2_2,4,commons-lang,"""[0.060226764529943466, 0.9397732615470886]"""
854,229113,ArrayUtils.contains returns false for instances of subtypes,"ArrayUtils.contains(Object[] array, Object objectToFind) wrongly returns false.

STEPS TO REPRODUCE
=========================================================
-Create a superclass ""Parent"" and override equals and hashcode based on some member id variable.
-Create a class ""Child"" extending ""Parent"". Do not override equals nor hashcode.

-Let ""childrens"" be an array of type Child[] containing several instances.
Create an instance of Parent ""p"" with the same id as childrens[0], such that childrens[0].equals(p) returns true and p.equals(childrens[0]) returns true as well.

Because they are equals, ArrayUtils.contains(childrens, p) should return true. However it returns false.


WHERE THE BUG IS LOCATED
=====================================================
-Go to ArrayUtils.class, line 1917. In the ""indexOf"" method implementation, before going into calling equals for each element of the input array, there is some sort of optimization check to make sure the instance to be found is an instance of the array type:

} else if (array.getClass().getComponentType().isInstance(objectToFind)) {

That line is wrong. In our case, the array contains elements of type ""Child"", whereas the object to be found is of type ""Parent"". They are equals according to the equals implementation of ""Parent"", but obviously Children.class.isInstance(p) is false.


EXPECTED BEHAVIOR
================================================
Since the method signature accepts an array of Object[] and an instance of Object, it should ignore the classes of the arguments. It should be possible to call ""ArrayUtils.contains(Child[] children, Parent p)"", in fact it should be possible to do this with any combination of classes, not only the ones assignable from the class hierarchy.",smartshark_2_2,798,commons-lang,"""[0.07642275094985962, 0.9235771894454956]"""
855,229114,"StopWatch: suspend() acts as split(), if followed by stop()","In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:

        StopWatch sw = new StopWatch();

        sw.start();
        Thread.sleep(1000);
        sw.suspend();
        // Time 1 (ok)
        System.out.println(sw.getTime());

        Thread.sleep(2000);
        // Time 1 (again, ok)
        System.out.println(sw.getTime());

        sw.resume();
        Thread.sleep(3000);
        sw.suspend();
        // Time 2 (ok)
        System.out.println(sw.getTime());

        Thread.sleep(4000);
        // Time 2 (again, ok)
        System.out.println(sw.getTime());

        Thread.sleep(5000);
        sw.stop();
        // Time 2 (should be, but is Time 3 => NOT ok)
        System.out.println(sw.getTime());


suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?
",smartshark_2_2,1125,commons-lang,"""[0.07750581949949265, 0.9224942326545715]"""
856,229115,Fix case-insensitive string handling,"{{String.to*Case()}} is locale-sensitive, this is usually not intended for case-insensitive comparisions. Please see [Common Bug #3|http://www.nabble.com/Re%3A-Common-Bugs-p14931921s177.html] for details.",smartshark_2_2,40,commons-lang,"""[0.17348363995552063, 0.8265163898468018]"""
857,229116,StringEscapeUtils.escapeXml(input) outputs wrong results when an input contains characters in Supplementary Planes.,"Hello.

I use StringEscapeUtils.escapeXml(input) to escape special characters for XML.
This method outputs wrong results when input contains characters in Supplementary Planes.

String str1 = ""\uD842\uDFB7"" + ""A"";
String str2 = StringEscapeUtils.escapeXml(str1);

// The value of str2 must be equal to the one of str1,
// because str1 does not contain characters to be escaped.
// However, str2 is diffrent from str1.

System.out.println(URLEncoder.encode(str1, ""UTF-16BE"")); //%D8%42%DF%B7A
System.out.println(URLEncoder.encode(str2, ""UTF-16BE"")); //%D8%42%DF%B7%FF%FD

The cause of this problem is that the loop to translate input character by character is wrong.
In CharSequenceTranslator.translate(CharSequence input, Writer out),
loop counter ""i"" moves from 0 to Character.codePointCount(input, 0, input.length()),
but it should move from 0 to input.length().
",smartshark_2_2,202,commons-lang,"""[0.0665457621216774, 0.9334542751312256]"""
858,229117,Use of ThreadLocals in ToStringStyle and HashCodeBuilder trigger memory leaks in container environments,"The thread local in org.apache.commons.lang3.builder.ToStringStyle is created but never removed and no API is provided to remove it. If a webapp's use of LANG triggers the loading of this class, a reference chain will be created that will cause a memory leak on web application reload.

See http://markmail.org/thread/uetw2fdrsqgbh2cv for more info.",smartshark_2_2,69,commons-lang,"""[0.16747140884399414, 0.8325285911560059]"""
859,229118,NumberUtils.createNumber(final String str)  Precision will be lost,"With commons-lang 3.2.2:
NumberUtils.createNumber(""-160952.54"");
The result is ""-160952.55"".

Should not be based on the length of the decimal point number to judge whether the floating point number.
Using the method (createFloat(str)) of dealing with the valid number greater than seven Numbers will cause accuracy loss.
The source code is as follows:

{code:java}
try {
            if(numDecimals <= 7){// If number has 7 or fewer digits past the decimal point then make it a float
                final Float f = createFloat(str);
                if (!(f.isInfinite() || (f.floatValue() == 0.0F && !allZeros))) {
                    return f;
                }
            }
        } catch (final NumberFormatException nfe) { // NOPMD
            // ignore the bad number
        }
        try {
            if(numDecimals <= 16){// If number has between 8 and 16 digits past the decimal point then make it a double
                final Double d = createDouble(str);
                if (!(d.isInfinite() || (d.doubleValue() == 0.0D && !allZeros))) {
                    return d;
                }
            }
        } catch (final NumberFormatException nfe) { // NOPMD
            // ignore the bad number
        }

        return createBigDecimal(str);
    }
{code}",smartshark_2_2,704,commons-lang,"""[0.2577705681324005, 0.7422294020652771]"""
860,229119,HashCodeBuilder throws StackOverflowError in bidirectional navigable association,"This is not the reflection methods, it is the regular HashCodeBuilder append methods. It causes EqualsBuilder, ToStringBuilder, CompareToBuilder to also throw the StackOverflowException, but those methods work when one of the HashCodeBuilder bidirectional association attributes .hashCode() is commented out. The problem is that all of the builders call registerObject() which creates a hashCode, but only the reflectionAppend method checks if an object is registered.

Bi-directional associations are a very common pattern in Jaxb and Hibernate. In this case, I generate code from a model in order to avoid the reflection penalty - I already know what the attributes are at compile time, so I use .append instead of .reflectionAppend.

See attached example + unit test. One side of the bidirectional association must be commented out in the hashCode method.",smartshark_2_2,445,commons-lang,"""[0.07590164989233017, 0.9240983128547668]"""
861,229120,LocaleUtils.toLocale does not parse strings starting with an underscore,"Hi,

Javadocs of Locale.toString() states that ""If the language is missing, the string will begin with an underbar."". This is not handled in the LocaleUtils.toLocale method if it is meant to be the inversion method of Locale.toString().

The fix for the ticket 328 does not handle well the case ""fr__P"", which I found out during fixing the first bug.

I am attaching the patch for both problems.",smartshark_2_2,367,commons-lang,"""[0.11555241793394089, 0.8844476342201233]"""
862,229121,StrBuilder appendFixedWidth does not handle nulls,Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.,smartshark_2_2,303,commons-lang,"""[0.061143819242715836, 0.9388562440872192]"""
863,229122,"DateUtils.getFragmentInDays(Date, Calendar.MONTH) returns wrong days","Commons lang3 screwed up my system after upgraded to 3.3 last night!

We use DateUtils.getFragmentInDays(Date, Calendar.MONTH) to extract days for later use. Basically (in 3.2), 'today' (Mar. 13) returns 13, but, it returns 12 in 3.3!

I compared the underlying method org.apache.commons.lang3.time.DateUtils.getFragment(Calendar, int, int) between 3.2 and 3.3:

3.2
{code:java}
        // Fragments bigger than a day require a breakdown to days
        switch (fragment) {
            case Calendar.YEAR:
                result += (calendar.get(Calendar.DAY_OF_YEAR) * MILLIS_PER_DAY) / millisPerUnit;
                break;
            case Calendar.MONTH:
                result += (calendar.get(Calendar.DAY_OF_MONTH) * MILLIS_PER_DAY) / millisPerUnit;
                break;
        }
{code}

3.3
{code:java}
        // Fragments bigger than a day require a breakdown to days
        switch (fragment) {
            case Calendar.YEAR:
                result += ((calendar.get(Calendar.DAY_OF_YEAR) -1) * MILLIS_PER_DAY) / millisPerUnit;
                break;
            case Calendar.MONTH:
                result += ((calendar.get(Calendar.DAY_OF_MONTH) -1) * MILLIS_PER_DAY) / millisPerUnit;
                break;
            default:
                break;
        }
{code}

Is there ANY ANY reason for adding '-1' in 3.3?! Plus, do you have any unit test for this method?",smartshark_2_2,544,commons-lang,"""[0.08731281757354736, 0.9126871228218079]"""
864,229123,FastDateParser could use \Q \E to quote regexes,"The escapeRegex method relies on knowing exactly what all the meta-characters are when excaping text. This is potentially fragile.

It would be a lot simpler to use the \Q \E quoting mechanism.

There would need to be special handling for unquote==true.",smartshark_2_2,345,commons-lang,"""[0.9983032941818237, 0.0016967261908575892]"""
865,229124,Add null-safe ObjectUtils.clone(),"If null, return null.
If not implementing Cloneable, throw exception.
Otherwise reflectively invoke a *public* clone().",smartshark_2_2,1342,commons-lang,"""[0.9983215928077698, 0.0016784238396212459]"""
866,229125,HashCodeBuilder.hashCode() should return the same value as HashCodeBuilder.toHashCode(),"HashCodeBuilder's toHashCode() method must currently be used to generate the calculated hash code value for an object.  However, this method name ('toHashCode') is very similar to the standard 'hashCode()' method, which will just return the (default) hash code of the HashCodeBuilder object itself.  Since these names are so similar (and otherwise have the same signature), they are easy to confuse.  If this happens, then the hashCode used for an object will be that of the HashCodeBuilder, which then will probably not be compatible with its 'equals' implementation.  This may result in serious bugs (e.g. memory leaks when combined with ToStringBuilder - see LANG-453).

I suggest either:

- Implement HashCodeBuilder.hashCode() to return the same value as .toHashCode() (maybe with a logged warning), in which case it will not matter if the wrong method is called.  This will still be compatible with HashCodeBuilder's equals/hashCode contract, since equals is not implemented, and should therefore be relatively backwards-compatible (though the hashCode will now change as calls are made to HashCodeBuilder).

- Throw an UnsupportedOperationException in hashCode().  This is more likely to break existing code though.",smartshark_2_2,47,commons-lang,"""[0.9862680435180664, 0.013731938786804676]"""
867,229126,Fix typo on appendIfMissing javadoc,Placeholder for https://github.com/apache/commons-lang/pull/129,smartshark_2_2,777,commons-lang,"""[0.995705783367157, 0.004294241312891245]"""
868,229127,NumberUtils: Rounding utilities for BigDecimal to primitive double avoiding NPEs.,"For the sake of formatting rounded {{BigDecimal}} values in JSON/XML I'm looking for the following methods:

1. 
{code:java}
public static double toDouble(BigDecimal value);
{code}
that defaults to {{0}} when {{value}} is null.

2.
{code:java}
public static double toDouble(BigDecimal value, double defaultValue);
{code}
that essentially does the same as 1. but accommodates for default values being specified.

3. 
{code:java}
public static BigDecimal toScaledBigDecimal(BigDecimal value, Integer scale, RoundingMode roudingMode);
that converts a {{BigDecimal}} to a {{BigDecimal}} whose scale is the specified value with input rounding mode applied
{code}

4. 
{code:java}
public static BigDecimal toScaledBigDecimal(BigDecimal value);
{code}
that converts a {{BigDecimal}} to a {{BigDecimal}} whose scale is 2 with {{RoundingMode.HALF_UP}} rounding mode applied.

5.
{code:java}
public static BigDecimal toScaledBigDecimal(Float value, Integer scale, RoundingMode roudingMode);
{code}
that converts a {{Float}} to a {{BigDecimal}} whose scale is the specified value with input rounding mode applied.

6.
{code:java}
public static BigDecimal toScaledBigDecimal(Double value, Integer scale, RoundingMode roudingMode);
{code}
that converts a {{Double}} to a {{BigDecimal}}  whose scale is the specified value with input rounding mode applied.

7.
{code:java}
public static BigDecimal toScaledBigDecimal(Double value);
{code}
that converts a {{Double}} to a {{BigDecimal}} whose scale is 2 with {{RoundingMode.HALF_UP}} rounding mode applied.

8.
{code:java}
public static BigDecimal toScaledBigDecimal(String value, Integer scale, RoundingMode roudingMode);
{code}
that converts a {{String}} to a {{BigDecimal}} whose scale is the specified value with input rounding mode applied

9.
{code:java}
public static BigDecimal toScaledBigDecimal(String value);
{code}
that converts a {{String}} to a {{BigDecimal}} whose scale is 2 with {{RoundingMode.HALF_UP}} rounding mode applied",smartshark_2_2,1,commons-lang,"""[0.6412875652313232, 0.35871249437332153]"""
869,229128,Code refactoring in NumberUtils,NumberUtils contains some redundant pieces of code which are doing the same validation on the array parameter. This validation should be extracted to a method which will be called by other methods in this class.,smartshark_2_2,336,commons-lang,"""[0.9982632994651794, 0.0017367673572152853]"""
870,229129,Missing method getRawMessage for ContextedException and ContextedRuntimeException,Method getMessage of ContextedException and ContextedRuntimeException will call getFormattedExceptionMessage internally. Therefore it is no longer possible for a derived class to access the original unformatted message. New method getRawMessage is required as counterpart to getFormattedExceptionMessage.,smartshark_2_2,232,commons-lang,"""[0.9591692090034485, 0.040830790996551514]"""
871,229130,"Charset may not be threadsafe, because the HashSet is not synch.","Charset may not be threadsafe, because the HashSet ""set"" is not synch.

The set is only updated by the protected add() method, which is currently only used by the protected ctors.

Perhaps add() should be private?
This would prevent any changes to the set after construction, and might be sufficient to ensure safe publication between threads.

Alternatively, make the set synch, as is done for COMMON.

",smartshark_2_2,103,commons-lang,"""[0.5056121349334717, 0.4943878650665283]"""
872,229131,Fix tests DateUtilsTest for Java 9 with en_GB locale,"On Java 9 I see DateUtilsTest fail. This is because ""MST7MDT"" zone is not formatted as ""MST"". I'm unclear as to why this is, but it is not relevant to what the test is trying to do. As such, I've changed the zone to ""America/Denver"" and changed the format letter from ""z"" to ""XXX"" to make it more reliable.

The PR also uses try-finally when setting the default time-zone to avoid pollution of global state if a test fails (the pollution here made it look like 8 test failures when there was only in fact 1).",smartshark_2_2,986,commons-lang,"""[0.9909685254096985, 0.009031432680785656]"""
873,229132,Levenshtein Distance Within a Given Threshold,"It'd be nice to have a function that calculates the Levenshtein distance only if it's within some integer threshold.  

Oftentimes you care less about the actual LD and more about it being within a certain range.  This common, limited computation can be performed much faster than the normal unbounded LD method; instead of O(nm), you can do it in O(km) (n and m are string lengths, k is the threshold).

Also, providing a function like this makes it easier for library users to rewrite the unbounded Levenshtein function to run in O(dm) time (d is the edit distance) if necessary.

I'm attaching a patch that implements this function and adds appropriate test cases.",smartshark_2_2,495,commons-lang,"""[0.9983285069465637, 0.0016714908415451646]"""
874,229133,Implement ParsePosition api for FastDateParser,FastDateParser does not update ParsePosition with appropriate index or errorIndex.  Update implementation to set ParsePosition.errorIndex to location of parse failure. Update implementation to set ParsePosition.index to location of last input character.,smartshark_2_2,742,commons-lang,"""[0.9880034923553467, 0.011996482498943806]"""
875,229134,Two new static methods in StringUtils: camelCaseToUnderscoreSeparated(String) and underscoreSeparatedToCamelCase(String),"Index: /CommonsLang/src/java/org/apache/commons/lang/StringUtils.java
===================================================================
--- /CommonsLang/src/java/org/apache/commons/lang/StringUtils.java	(revision 633306)
+++ /CommonsLang/src/java/org/apache/commons/lang/StringUtils.java	(working copy)
@@ -2782,6 +2782,60 @@
         list.add(new String(c, tokenStart, c.length - tokenStart));
         return (String[]) list.toArray(new String[list.size()]);
     }
+    /**
+     * Changes a camelCase string value to underscore separated
+     * @param input
+     * @param toLowerCase - if output string should be lower case
+     * @return underscore separated string
+     */
+    public static String camelCaseToUnderscoreSeparated(String input,
+			boolean toLowerCase) {
+		StringBuilder s = new StringBuilder();
+		if (input == null) {
+			return """";
+		}
+		int length = input.length();
+		for (int i = 0; i < length; i++) {
+			char ch = input.charAt(i);
+			if (Character.isUpperCase(ch) && i > 0) {
+				s.append(""_"");
+			}
+			if (ch == '.') {
+				s.append(""_"");
+			} else {
+				s.append(toLowerCase ? Character.toLowerCase(ch) : Character
+						.toUpperCase(ch));
+			}
+		}
+		return s.toString();
+	}
+	
+    /**
+     * Changes a underscore separated string value to camelCase
+     * @param input
+     * @return camelScape string
+     */
+	public static String underscoreSeparatedToCamelCase(String input) {
+		StringBuilder s = new StringBuilder();
+		if (input == null) {
+			return """";
+		}
+		int length = input.length();
+		boolean upperCase = false;
+		
+		for (int i = 0; i < length; i++) {
+			char ch = input.charAt(i);
+			if (ch == '_') {
+				upperCase = true;
+			} else if (upperCase) {
+				s.append(Character.toUpperCase(ch));
+				upperCase = false;
+			} else {
+				s.append(ch);
+			}
+		}
+		return s.toString();
+	}
 
     // Joining
     //-----------------------------------------------------------------------
",smartshark_2_2,1287,commons-lang,"""[0.9981263279914856, 0.0018736396450549364]"""
876,229135,Deprecate ClassUtils getShortClassName in favor of Class getSimpleName,"Added two null-safe ClassUtils.getSimpleName() APIs.

---------- Forwarded message ----------
From: Gary Gregory <garydgregory@gmail.com>
Date: Mon, Apr 11, 2011 at 10:18 AM
Subject: [Lang] ClassUtils getShortClassName != Class getSimpleName
To: Commons Developers List <dev@commons.apache.org>


Hi All:

Should we deprecate ClassUtils getShortClassName in favor of Class getSimpleName?

The behavior of getShortClassName is undocumented for arrays in the Javadoc and is different from getSimpleName.

When I replace the guts of getShortClassName to call getSimpleName, one test fails:

junit.framework.ComparisonFailure: null expected:<[ToStringStyleTest.]Person[name=John Q. ...> but was:<[]Person[name=John Q. ...>
    at junit.framework.Assert.assertEquals(Assert.java:81)
    at junit.framework.Assert.assertEquals(Assert.java:87)
    at org.apache.commons.lang3.builder.ShortPrefixToStringStyleTest.testPerson(ShortPrefixToStringStyleTest.java:86)

For now, I've made a note in the Javdoc to consider using getSimpleName.",smartshark_2_2,458,commons-lang,"""[0.9985092282295227, 0.0014907276490703225]"""
877,229136,Better advertizing of utils,"Recently I had a problem with Unicode characters and some backend system that does not support Unicode.
I searched the Web for existing source and found some which I reused for translating to/from Unicode escapes, so I could store the data in the backend system without garbling characters.

Now I just found out about StringEscapeUtils.escapeJava() and .unescapeJava() which does exactly this and works perfectly!

The Apache community never seizes to amaze me, well done - but please *do* advertize these handy utility functions more publicly! Why do only the ""front end"" projects get all the attention, when there are such jewels hiding away in the dark? :-)))
Thanks for another great library.
",smartshark_2_2,1258,commons-lang,"""[0.9984968900680542, 0.001503089675679803]"""
878,229137,Add AnnotationUtils,"Most importantly, provide a check of annotation equality consistent with that used by ""real"" annotations, but usable even by annotations generated by mechanisms that do not support overriding equals().",smartshark_2_2,114,commons-lang,"""[0.998461127281189, 0.0015389067120850086]"""
879,229138,Add decrementAndGet/incrementAndGet to MutableInt class,"Frequently when incrementing or decrementing a number, one needs to compare it to a limit. For example in loops.

At present that requires two calls:

MutableInt.decrement()
MutableInt.getValue()

It would be convenient to have a method that combined the two.

c.f. AtomicInteger.decrementAndGet()

There is less likelihood of needing these methods for other Mutable Number types as they are not often used in loops.",smartshark_2_2,729,commons-lang,"""[0.9984124898910522, 0.001587534905411303]"""
880,229139,[patch] Add includeantruntime=false to javac targets to quell warnings in ant 1.8.1 and better (and modest performance gain),"add 

<javac .... includeantruntime=""false"" .... >

so that ant doesn't pull in the ant libraries and quells a warning message on build.",smartshark_2_2,196,commons-lang,"""[0.9984146356582642, 0.0015853832010179758]"""
881,229140,ExceptionUtils cannot handle J2EE-Exception in a default way,"The ExceptionUtils are not usable in a default way (getCause(), getRootCause(),
...) if I have J2EE-exceptions (EJBException.getCausedByException,
ServletException.getRootCause). For getCause() I have the possibility of using
getCause() with methodnames as params - but not for getRootCause().
So either you should add the J2EE-exception names to the static final array or
make it possible to dynamically add recorgnized exception names.

Thanks.",smartshark_2_2,1102,commons-lang,"""[0.9218180775642395, 0.07818188518285751]"""
882,229141,Ant build file does not include ReflectTestSuite,"Ant build file does not include ReflectTestSuite.

Patch to follow.",smartshark_2_2,52,commons-lang,"""[0.9967104196548462, 0.003289602231234312]"""
883,229142,Deprecate o.a.c.lang.time.* package,"We have discussed [1] to deprecate the time package, because it will become obsolete in Java 8. 

[1] http://markmail.org/message/uw3lggwkt5ul5b7k",smartshark_2_2,890,commons-lang,"""[0.9982619881629944, 0.001738047692924738]"""
884,229143,StringUtils: startsWith / endsWith / startsWithIgnoreCase / endsWithIgnoreCase / removeStartIgnoreCase / removeEndIgnoreCase methods,"I'd like the following new start/end methods for StringUtils:

  startsWith - handles nulls
  endsWith - handles nulls
  startsWithIgnoreCase - handles nulls, case insensitive
  endsWithIgnoreCase - handles nulls, case insensitive

  removeStartIgnoreCase - handles nulls, case insensitive
  removeEndIgnoreCase - handles nulls, case insensitive",smartshark_2_2,1309,commons-lang,"""[0.9980425834655762, 0.0019574465695768595]"""
885,229144,Add a method in EnumUtils to 'translate' an enum value,"Add a method in EnumUtils to 'translate' an enum value.
Example :
Enum1
Enum2
Object.enum2

Enum1 getEnum(Enum1.class, Object.enum2);

If enum2 is null, returns 2, else returns the Enum1 with the same name as Object.enum2, throws exception if no such Enum1.
",smartshark_2_2,903,commons-lang,"""[0.9984627962112427, 0.0015372136840596795]"""
886,229145,[lang] ExceptionUtils: new getCause() methodname (for tomcat-exception),"What about a tomcat-exception??? getThrowable() sound very good.

org.apache.catalina.connector.ClientAbortException: getThrowable

Thanks",smartshark_2_2,1048,commons-lang,"""[0.9948541522026062, 0.005145853850990534]"""
887,229146,The field FastDateFormat.UnpaddedNumberField.INSTANCE_YEAR is never read locally,The field FastDateFormat.UnpaddedNumberField.INSTANCE_YEAR is never read locally and could be deleted,smartshark_2_2,39,commons-lang,"""[0.45869210362434387, 0.5413079261779785]"""
888,229147,Overhaul Validate class,Omnibus issue to collect all other issues affecting the development of {{org.apache.commons.lang.Validate}}.,smartshark_2_2,1362,commons-lang,"""[0.998424768447876, 0.0015752811450511217]"""
889,229148,StopWatch: increase precision by using System.nanoTime(),"In Java5, theres a new, more precise way to measure elapsed timespans:
http://java.sun.com/j2se/1.5.0/docs/api/java/lang/System.html#nanoTime()

It will probably break the class contract of the current StopWatch class, as the returned timestamps cannot be transformed to Dates etc., but it will add precision to the stopwatch aspect (as opposed to the timewatch aspect) of the class.
Perhaps the way to go is to introduce a new HighPrecisionStopWatch class.
",smartshark_2_2,1260,commons-lang,"""[0.998465895652771, 0.0015341546386480331]"""
890,229149,SystemUtils needs IS_JAVA_1_7 constant,This constant should exist since Lang 3.0 and JDK 1.7 are both in development.,smartshark_2_2,1415,commons-lang,"""[0.9983141422271729, 0.001685837167315185]"""
891,229150,Create fluent APIs where possible,"A lot of commons-lang is design in an old fashioned way using static util methods. The problem of this design is, that it encodes a language into method names leading to lots of variations of the same method in one class. For example StringUtils has over 180 methods, 9 of which a related to splitting.

Instead of overloading methods and creating method name variations, we should try to implement fluent APIs. Examples:

Instead of:
{code:java}
StringEscapeUtils.escapeHtml3(String)
StringEscapeUtils.escapeHtml4(String)
StringEscapeUtils.escapeXml(String)
{code}

the API could look like:

{code:java}
StringEscaping.escape(String).with(Escaping.HTML_3)
StringEscaping.escape(String).with(Escaping.HTML_4)
StringEscaping.escape(String).with(Escaping.XML)
{code}

So no additional methods are necessary when adding new escaping. There are more examples in commons-lang where the fluent design can be applied. ",smartshark_2_2,512,commons-lang,"""[0.9983246922492981, 0.0016752795781940222]"""
892,229151,Add containsAll methods to ArrayUtils,"The ArrayUtils class contains several ""contains"" methods.
It could be usefull to add the corresponding containsAll(Object[] array1, Object[] array2) methods (and corresponding for primitive types).
",smartshark_2_2,350,commons-lang,"""[0.9983479976654053, 0.0016520071076229215]"""
893,229152,DateUtils should test with the extremes,"Current testdates are a bit to generic. Especially for truncating, ceiling and rounding you shuold test for the extremes. 
So for rounding to hours you should test for 0:29 (rounding down); 0:30 (rounding up); 1:00 (no rounding required)
",smartshark_2_2,36,commons-lang,"""[0.9959807395935059, 0.004019218496978283]"""
894,229153,ClassUtils.getAllInterfaces(...) could be more efficient,"This could seem like a very minor thing but why not improve
the code once in a while...

Something like this could replace the current inefficient code:

{code}
    public static List<Class<?>> getAllInterfaces(Class<?> clazz)
    {
        if (clazz == null)
        {
            return null;
        }

        HashSet<Class<?>> interfacesSet = new HashSet<Class<?>>();
        LinkedList<Class<?>> interfacesList = new LinkedList<Class<?>>();

        getAllInterfaces(clazz, interfacesSet, interfacesList);

        return interfacesList;
    }

    private static void getAllInterfaces(
            Class<?> clazz,
            HashSet<Class<?>> interfacesSet, List<Class<?>> interfacesList)
    {
        while (clazz != null)
        {
            Class<?>[] interfaces = clazz.getInterfaces();

            for (Class<?> i : interfaces)
            {
                if (!interfacesSet.add(i))
                {
                    interfacesList.add(i);
                    getAllInterfaces(i, interfacesSet, interfacesList);
                }
            }

            clazz = clazz.getSuperclass();
        }
    }
{code}",smartshark_2_2,1426,commons-lang,"""[0.9981990456581116, 0.0018009609775617719]"""
895,229154,ISO 8601 misspelled throughout the Javadocs,"The Javadocs say: {{ISO8601}} but the correct format is {{ISO 8601}}. Note the space.

A patch can be provided.",smartshark_2_2,612,commons-lang,"""[0.9877752065658569, 0.012224726378917694]"""
896,229155,Duration Utils - Rounding Duration,"In Java 8 Duration is always rounded down - for instance if we have LocalDates instances with values 20:00 and 22:40, we will get 2 hours of duration between them. Purpose of this util class is to provide an easy wayÂ to round up a duration in a given time unit.Â ",smartshark_2_2,1022,commons-lang,"""[0.9983172416687012, 0.0016827317886054516]"""
897,229156,StringUtils#indexOfAny() methods with start position argument,"There is no StringUtils#indexOfAny() methods with start position argument, which would search for specified characters from the specified position.
Please add it.",smartshark_2_2,716,commons-lang,"""[0.9968348145484924, 0.003165267175063491]"""
898,229157,StringEscapeUtils should expose escape*() methods taking Writer argument,"StringEscapeUtils.escapeXML(String) is the only exposed variant, but the package private Entities.XML class it uses under the covers has a variant taking a Writer. The StringEscapeUtils.escapeXML(Writer, String) variant would be much more efficient when incrementally escaping values and adding them one-by-one and could seemingly be exposed with very little effort. Ditto for the unescapeXML() case, and presumably other styles of escape/unescape (like HTML) that use the Entities class under the covers.",smartshark_2_2,1175,commons-lang,"""[0.998439610004425, 0.0015604420332238078]"""
899,229158,Add new Validate methods,"Expand Validate to include more specific, human readable validation for commonly validated parameter values (so isTrue() isn't needed as often). Include new Validate methods:

matchesPattern(String string, String pattern)
throws an IllegalArgumentException stating (""The string "" + string + "" does not match the pattern ""  + pattern) when the provided string does not match the regular expression pattern.

matchesPattern(String string, String pattern, String message)
throws an IllegalArgumentException stating (message + string) when the provided string does not match the regular expression pattern.

matchesPattern(String string, Pattern pattern)
throws an IllegalArgumentException stating (""The string "" + string + "" does not match the pattern ""  + pattern) when the provided string does not match the regular expression pattern.

matchesPattern(String string, Pattern pattern, String message)
throws an IllegalArgumentException stating (message + string) when the provided string does not match the regular expression pattern.

inclusiveBetween(Comparable<T> start, Comparable<T> end, Comparable<T> value)
throws an IllegalArgumentException stating (""The value "" + value + "" is not in the specified inclusive range of "" + start + "" to "" + end) when the provided value does not fall between the two comparable values (inclusive).

inclusiveBetween(Comparable<T> start, Comparable<T> end, Comparable<T> value, String message)
throws an IllegalArgumentException stating (message + value) when the provided value does not fall between the two comparable values (inclusive).

exclusiveBetween(Comparable<T> start, Comparable<T> end, Comparable<T> value)
throws an IllegalArgumentException stating (""The value "" + value + "" is not in the specified exclusive range of "" + start + "" to "" + end) when the provided value does not fall between the two comparable values (exclusive).

exclusiveBetween(Comparable<T> start, Comparable<T> end, Comparable<T> value, String message)
throws an IllegalArgumentException stating (message + value) when the provided value does not fall between the two comparable values (exclusive).
",smartshark_2_2,1404,commons-lang,"""[0.9985576272010803, 0.001442400156520307]"""
900,229159,Add methods to Validate to check whether the index is valid for the array/list/string,"Validate should have methods to check whether the index to an array/collection/string is a valid index:

public void validIndex(Object[] array, int index)",smartshark_2_2,1352,commons-lang,"""[0.9984970092773438, 0.0015029716305434704]"""
901,229160,"-1, 0 or error for String/Array operations","what're design insights for below implementation:
#1: SHOULD throw exception for negative index?

{code:title= org.apache.commons.lang3.ArrayUtils.java|borderStyle=solid}
    public static int indexOf(final boolean[] array, final boolean valueToFind, int startIndex) {
        if (ArrayUtils.isEmpty(array)) {
            return INDEX_NOT_FOUND;
        }
        if (startIndex < 0) {
            startIndex = 0;
        }
        for (int i = startIndex; i < array.length; i++) {
            if (valueToFind == array[i]) {
                return i;
            }
        }
        return INDEX_NOT_FOUND;
    }
{code}

#2, Why does StringUtils.indexOf("""", """")  return 0, not -1?
first of all, """" is an empty String. '0' index doesn't exist in """". secondly, both null and """" strings are considered as none. I think it's inappropriate to say we found a none thing in position 0. so I suggest to return -1(not exists) to the index of any string in a null or """" string or null or """" empty string in any thing.

(i read the java doc and a few jira tickets. but didn't find the discussion related to this kind of issue. so i created a new ticket).
",smartshark_2_2,865,commons-lang,"""[0.9677679538726807, 0.03223203867673874]"""
902,229161,Provide method to generate random numbers without range restriction for all numeric types,"`RandomUtils` only allows to generate random numbers of different numeric types (`byte`, `short`, `double`, etc.) within the range 0 (inclusive) and the maximum value of the respective type. It'd be nice if there were methods to create random numbers for the range between the minimal and the maximal value for all numeric types.

I'm aware of workarounds like https://stackoverflow.com/questions/27976857/how-to-get-random-number-with-negative-number-in-range, but they're not tested and require quite some thinking which can be avoided (which is afaik one of the many motivations for the Apache commons libraries).

If such methods already exist they should be definitely linked in the Javadoc of `RandomUtils`.",smartshark_2_2,1025,commons-lang,"""[0.9984299540519714, 0.0015700394287705421]"""
903,229162,"Commons-lang StringUtils head, tail and indexOfNth (with patch)","Hello commons developers,
I would like to propose the addition of these four methods to StringUtils class:

public static String head(final String str, int lines)
public static String tail(String str, int lines)
public static int indexOfNth(String str, String searchStr, int occurrence)
public static int lastIndexOfNth(String str, String searchStr, int occurrence)",smartshark_2_2,1419,commons-lang,"""[0.9983042478561401, 0.0016956747276708484]"""
904,229163,ltrim() and rtrim() functions in StringUtils,"Originally posted at http://stackoverflow.com/questions/15567010

Would be great to have {{StringUtils.rtrim()}} and {{StringUtils.ltrim()}} functions, similar to JavaScript.",smartshark_2_2,270,commons-lang,"""[0.9983648657798767, 0.0016350920777767897]"""
905,229164,[patch] Increase test coverage of FieldUtils read methods and tweak javadoc,"A patch to increase the test coverage and contract testing of the exception paths of the FieldUtils read methods. Additionally, there is a small tweak of a few FieldUtils read methods to document an additional condition where IllegalArgumentException is thrown (when field can't be found).",smartshark_2_2,195,commons-lang,"""[0.9983622431755066, 0.0016377136344090104]"""
906,229165,StopWatch should provide getTime() which accepts TimeUnit,"I was migrating code which used Guava Stopwatch to using org.apache.commons.lang.time.StopWatch
{code}
   double throughputCells = (double)numCells / scanTimer.elapsedTime(TimeUnit.SECONDS);
{code}
I had to do conversion after calling getTime().

StopWatch should provide getTime() which accepts TimeUnit.",smartshark_2_2,804,commons-lang,"""[0.9980834722518921, 0.0019165768753737211]"""
907,229166,Update Java requirement from Java 6 to 7,Update Java requirement from Java 6 to 7.,smartshark_2_2,833,commons-lang,"""[0.9979129433631897, 0.0020870992448180914]"""
908,229167,RandomStringUtils add #randomGraph and #randomPrint which match corresponding regular expression class,As suggested in [https://github.com/apache/commons-lang/pull/101/commits/53ead9cd983a85c28f63b23fd8ca8c056621dfec],smartshark_2_2,805,commons-lang,"""[0.9976029992103577, 0.0023970501497387886]"""
909,229168,Better Javadoc for BitField class,"As mentioned in [LANG-730], the class is hard to understand. The current Javadocs are minimal.",smartshark_2_2,628,commons-lang,"""[0.9979789853096008, 0.002021046821027994]"""
910,229169,"Replace characters seems to not be able to replace Icelanding Thorn (Ã, Ã¾)","Failing test case :

{code}
    @Ignore
    public void convert_English_Thorn(){
    	beforeConversion = ""AAÃÃ¾aa"";
        expectedAfterConversion = ""aattaa"";
        Assert.assertEquals(""Test NOT correctly configured length not correct"",
                beforeConversion.length(),
                expectedAfterConversion.length());
        afterConversion = converter.replaceAccents(beforeConversion);
        Assert.assertEquals(""Cannot convert (Icelandic/old english thorn) '"" + beforeConversion + ""'."", expectedAfterConversion, afterConversion);
    }
{code}

{code}
 
    String replaceAccents(final String aField) {
        String result = StringUtils.lowerCase(aField),
               target = null,
               charsToRemove = null;
        char charToPut = Constants.UNDERSCORE_CHARACTER,
             charToRemove = Constants.UNDERSCORE_CHARACTER;
        for (Object element : conversionMappings.keySet()) {
            target = element.toString();
            if (target.equals(Constants.SPECIAL_CHARACTER_KEY)) {
                continue;
            }
            result = StringUtils.stripToNull(result);
            charsToRemove = conversionMappings.getProperty(target, Constants.EMPTY_STRING).toString();
            charToPut = target.charAt(Constants.ZERO);
            for (int i=0; i<charsToRemove.length(); i++) {
                charToRemove = charsToRemove.charAt(i);
                result = StringUtils.replaceChars(result, charToRemove, charToPut);
            }
        }

        return result;
    }
{code}



",smartshark_2_2,82,commons-lang,"""[0.1208123043179512, 0.8791877627372742]"""
911,229170,Manages Comparable in EqualsBuilder,"As explained by LANG-393 and LANG-467, {{EqualsBuilder}} fails to provide a proper way to handle {{BigDecimal}} since {{BigDecimal}} takes the scale into account when using {{equals()}} but not with {{compareTo()}}.

Would it be possible to add a method *{{appendComparable()}}* in {{EqualsBuilder}} to manages {{Comparable}} objects when we want to build equality on {{compareTo()}} and not {{equals()}}?

Making clear that {{EqualsBuilder}} is not compliant with {{HashCodeBuilder}} when using this method.

For example:
{code:borderStyle=solid}
    return new EqualsBuilder()
        .append(this.name, other.name)
        .appendComparable(this.amount, other.amount)
        .isEqual();
{code}

with:
{code:title=EqualsBuilder.java|borderStyle=solid}
	/**
	 * <p>Test if two <code>Comparable</code>s are equal using their
	 * <code>compareTo</code> method.</p>
	 * <p>This may break the <code>equals</code>/<code>hashcode</code> contract but
	 * it is useful in some situation, specially with {@link java.math.BigDecimal}.</p>
	 * 
	 * @param lhs  the left hand comparable
	 * @param rhs  the right hand comparable
	 * @return EqualsBuilder - used to chain calls.
	 */
	public <T extends Comparable<? super T>> EqualsBuilder appendComparable(T lhs, T rhs) {
		if (isEquals == false) {
			return this;
		}
		if (lhs == rhs) {
			return this;
		}
		if (lhs == null || rhs == null) {
			isEquals = false;
			return this;
		} else if (lhs.compareTo(rhs) != 0) {
			isEquals = false;
			return this;
		}
		return this;
	}
	
	/**
	 * <p>Performs a deep comparison of two <code>Comparable</code> arrays.</p>
	 * 
	 * @param lhs  the left hand <code>Comparable[]</code>
	 * @param rhs  the right hand <code>Comparable[]</code>
	 * @return EqualsBuilder - used to chain calls.
	 * @see #appendComparable(Comparable, Comparable)
	 */
	public <T extends Comparable<? super T>> EqualsBuilder appendComparable(T[] lhs, T[] rhs) {
		if (isEquals == false) {
			return this;
		}
		if (lhs == rhs) {
			return this;
		}
		if (lhs == null || rhs == null) {
			isEquals = false;
			return this;
		}
		if (lhs.length != rhs.length) {
			isEquals = false;
			return this;
		}
		for (int i = 0; i < lhs.length && isEquals; ++i) {
			appendComparable(lhs[i], rhs[i]);
		}
		return this;
	}
{code}",smartshark_2_2,640,commons-lang,"""[0.26950547099113464, 0.730494499206543]"""
912,229171,Add getCharArray()  to CharRange,"org.apache.commons.lang.CharRange would be a lot more useful if it had a means to obtain the characters in the range as a char array.

i.e.

new CharRange('a',''d').toCharArray() == new char[] { 'a', 'b', 'c', 'd' }",smartshark_2_2,308,commons-lang,"""[0.998451828956604, 0.001548200030811131]"""
913,229172,String prefixing/suffixing,"I had a use case that I wanted to search through a string and, when I found a match, prefix or suffix (or both) the matching term, i.e. to apply a highlighting class to a matching keyword in a search.
I noticed that this cannot be done by the StringUtils framework, so me and a colleague made this code and would like to donate it to the StringUtils project. In the attachment I included the class with the extensions plus a Unit test testing all the usecases. I hope you like it.

There are 3 methods in the attached class:

prefix: searches through a string and prefixes a match
suffix: searches through a string and suffixes a match
around: does both, prefix or suffix may be null. ",smartshark_2_2,80,commons-lang,"""[0.9873743653297424, 0.012625663541257381]"""
914,229173,"RandomStringUtils.random (count, letters=true, number=true) may not use numerics","Either there is a bug in an implementation or misunderstanding in docs.

RandomStringUtils.random (count, letters, numbers) is documented so that:

letters  if true, generated string *will* include alphabetic characters
numbers  if true, generated string *will* include numeric characters

But apparently the current implementation supports only that generated string *may* include either only letters, only numbers or both.

This is current implementation:

 if (letters && Character.isLetter(ch) || numbers && Character.isDigit(ch) || !letters && !numbers)

So there may be situation when generated string is not containing numbers at all which is in contrary with what the docs say. ",smartshark_2_2,376,commons-lang,"""[0.6677110195159912, 0.3322889506816864]"""
915,229174,Add hashCode-support to class ObjectUtils,"Class ObjectUtils can compare Objects in a null-safe way calling:
equals(Object o1, Object o2)
There should be a similar method for generating a null-safe hashCode:

public static int hashCode(Object o) {
    return ((o == null) ? 0 : o.hashCode());
}",smartshark_2_2,1051,commons-lang,"""[0.9984063506126404, 0.0015937217976897955]"""
916,229175,new method StringUtils.replaceIgnoreCase (with patch),"Method implementation:

	/**
	 * Searches for all appearances of <code>searchString</code> (ignoring case) in <code>text</code> and replaces them by <code>replacement</code>.
	 * The difference to {@link String#replace(CharSequence, CharSequence)} and {@link StringUtils#replace(String, String, String)} is that this implementation ignores case.
	 *  
	 * @param text The text in which to do replacements.
	 * @param searchString The string to remove from the text (ignoring case).
	 * @param replacement The string to put instead of the searchString.
	 * @return A new string with all searchString replaced by replacement.
	 * @author frickert
	 */
	public static String replaceIgnoreCase(String text, String searchString, String replacement)
	{
		String lowerCaseText = text.toLowerCase();
		String lowerCaseSearchString = searchString.toLowerCase();
		StringBuilder sb = new StringBuilder(text);

		int searchStart = 0;
		final int modifierPerReplacement = replacement.length() - searchString.length();
		int sbDrift = 0; // by doing replacements in sb, sb and the text drift off in length and index in case the searchString and the replacement are of different length

		int finding = lowerCaseText.indexOf(lowerCaseSearchString, searchStart);
		while (finding >= 0)
		{
			sb.replace(finding + sbDrift, finding + sbDrift + searchString.length(), replacement);
			sbDrift += modifierPerReplacement;
			searchStart = finding + searchString.length();
			finding = lowerCaseText.indexOf(lowerCaseSearchString, searchStart);
		}

		return sb.toString();
	}


test cases:

    public void testReplaceIgnoreCase() throws Throwable {
        String is;

        is = CommonHelpers.replaceIgnoreCase(""bobOBOBobOB"", ""Bob"", ""Flo"");
        assertEquals(""search really ignores case"", ""FloOFlooFlo"", is);

        is = CommonHelpers.replaceIgnoreCase(""bobOBOBobOB"", ""Bob"", ""Flo"");
        assertEquals(""replacement does care about case"", ""FloOFlooFlo"", is);

        is = CommonHelpers.replaceIgnoreCase(""bob bob bob"", ""Bob"", ""Florian"");
        assertEquals(""length difference of searchString and replacement > 0"", ""Florian Florian Florian"", is);

        is = CommonHelpers.replaceIgnoreCase(""bob bob bob"", ""Bob"", ""Ed"");
        assertEquals(""length difference of searchString and replacement < 0"", ""Ed Ed Ed"", is);

        is = CommonHelpers.replaceIgnoreCase(""GROSS und klein"", ""und"", ""&"");
        assertEquals(""originals case is preserved in not replace chars"", ""GROSS & klein"", is);
    }

",smartshark_2_2,263,commons-lang,"""[0.9977317452430725, 0.002268280368298292]"""
917,229176,Fix Java 8 related problem in site build,"After LANG-1024 has been fixed, the site build now fails with: 

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:site (default-site) on project commons-lang3: Execution default-site of goal org.apache.maven.plugins:maven-site-plugin:3.3:site failed: Invalid byte tag in constant pool: 18 -> [Help 1]
{code}",smartshark_2_2,586,commons-lang,"""[0.9773995876312256, 0.022600485011935234]"""
918,229177,Javadoc for EqualsBuilder.reflectionEquals() is unclear,"The Javadoc for {{EqualsBuilder.reflectionEquals()}} doesn't explain that {{.equals()}} is used to compare non-primitive fields in the class.

Consequently, readers may assume the method recursively builds {{.equals()}} for all fields. We should clarify this.

Related: LANG-1034",smartshark_2_2,626,commons-lang,"""[0.9956666231155396, 0.004333388060331345]"""
919,229178,Add an example with whitespace in StringUtils.defaultIfEmpty,"Hi there. 

Today I needed to use a method to return a default value if certain string was not given. 

I used the old ""Eclipse CTRL+Space"" trick and looked up in the StringUtils API javadoc for an example that suited my need :) 

I noticed that StringUtils.defaultIfEmpty missed an example with whitespace (like in StringUtils.defaultIfEmpty), and created a patch for it. The description in the Javadoc is very good, but my laziness asks for an example with whitespace ;)

Cheers, 
Bruno P. Kinoshita",smartshark_2_2,220,commons-lang,"""[0.9983487129211426, 0.001651286263950169]"""
920,229179,ArrayUtils should have method to convert null arrays to empty ones to help with Defensive coding,"There are APIs that I've come across that return <code>null</code> Arrays in the event where there are no results.  Often these APIs correctly throw exceptions when there is an ""exceptional event"", but no results isn't exceptional, and it often shouldn't be.   This causes the programmer to make extra tests for null throughout the code to deal with the null case, and sometimes these null cases are added after a customer searched for gobleygook and got a NullPointerException.  It's just far cleaner/safer to convert these null arrays to empty arrays.

Another benefit to this method is that if the array being passed in is actually already an empty array, it will swap the pointer for the <code>static final</code> in the ArrayUtils class to help decrease memory fragmentation.

e.g.

BEFORE:

try
{
  results = customer.getResults(query);
} catch ( IOException ioex ) {
  //  ...
}
if ( null == results )
{
   results = new int[0]{};
}
// do stuff

AFTER
try
{
  results = ArrayUtils.nullToEmpty(customer.getResults(query));
} catch ( IOException ioex ) {
  //  ...
}
// do stuff


",smartshark_2_2,1392,commons-lang,"""[0.9984706044197083, 0.0015293988399207592]"""
921,229180,[lang] CompositeFormat,"I wonder if this can be added.

/**
 * Formats using one formatter and parses using a different formatter.
 * An example of use for this would be a webapp where data is taken in one way
 * and stored in a database another way.
 * @author Archimedes Trajano
 */
public class CompositeFormat extends Format {
  private final Format parser;
  private final Format formatter;
  public CompositeFormat(final Format parser, final Format formatter) {
    this.parser = parser;
    this.formatter = formatter;
  }
  public StringBuffer format(final Object obj, final StringBuffer toAppendTo, 
final FieldPosition pos) {
    return formatter.format(obj,toAppendTo,pos);
  }
  public Object parseObject(final String source, final ParsePosition pos) {
    return parser.parseObject(source,pos);
  }
}",smartshark_2_2,1170,commons-lang,"""[0.9984473586082458, 0.0015526474453508854]"""
922,229181,"Utility class constructor javadocs should acknowledge that they may sometimes be used, e.g. with Velocity.","Utility class constructors currently have javadoc comments that say:

""StringUtils instances should NOT be constructed in standard programming.""

However, there are some cases where it is necessary to use them to create instances.  For example, using the utility methods in a Velocity context requires that an instance be created.

It is true that the current comment does not exclude this use, but the emphasis (""NOT"") implies that there is a possibility it will be deprecated, removed, or otherwise be made inaccessible in the future.

I'd like to suggest modifying the message to more explicitly acknowledge that the constructor's use is approved in some cases, so as to reassure developers that it will continue to be available in the future.

One possible wording would be to retain the existing comment, and add to it:

""However, in some cases (for example, for use with Velocity), it is necessary to create an instance of this class.  It is recommended that this constructor be used only in special cases such as this.""

(This issue really applies to all projects with utility classes with this javadoc, so feel free to copy it them as well.)",smartshark_2_2,1201,commons-lang,"""[0.9980416297912598, 0.0019583117682486773]"""
923,229182,CharSet.getInstance documentation does not clearly explain how to include negation character in set,"As discussed in [this Stack Overflow question|http://stackoverflow.com/questions/27070515/how-can-i-include-the-caret-character-in-an-apache-commons-charset], the documentation for {{CharSet.getInstance()}} don't explain clearly how to include the negation character ({{^}}) as a literal character.

The two solutions suggested in the SO question are:

{code:java}
// Add the '^' on its own
CharSet.getInstance(""^"", ""otherlettershere"");

// Add the '^' as the last character
CharSet.getInstance(""otherlettershere^"")
{code}

If those are the best options, we should add a line to the Javadoc to indicate this. If there is a better way, clearly that should be documented instead.",smartshark_2_2,807,commons-lang,"""[0.9436373710632324, 0.056362662464380264]"""
924,229183,Support use of mutable numbers in editable JTable models,"In order for mutable numbers to be used in editable JTable models, they need to have a constructor accepting a string. The following stack trace was the result of double-clicking on a table cell displaying a MutableDouble instance:

java.lang.NoSuchMethodException: org.apache.commons.lang.mutable.MutableDouble.<init>(java.lang.String)
 at java.lang.Class.getConstructor0(Class.java:2678)
 at java.lang.Class.getConstructor(Class.java:1629)
 at org.jdesktop.swingx.table.NumberEditorExt.getTableCellEditorComponent(NumberEditorExt.java:107)
Caused: java.lang.IllegalStateException: Number subclass must have a constructor which takes a string
 at org.jdesktop.swingx.table.NumberEditorExt.getTableCellEditorComponent(NumberEditorExt.java:110)
 at javax.swing.JTable.prepareEditor(JTable.java:3983)
 at org.jdesktop.swingx.JXTable.prepareEditor(JXTable.java:3458)
 at javax.swing.JTable.editCellAt(JTable.java:2688)
 at org.jdesktop.swingx.JXTable.editCellAt(JXTable.java:3852)
 at javax.swing.plaf.basic.BasicTableUI$Handler.adjustFocusAndSelection(BasicTableUI.java:955)
 at javax.swing.plaf.basic.BasicTableUI$Handler.mousePressed(BasicTableUI.java:922)
 at java.awt.AWTEventMulticaster.mousePressed(AWTEventMulticaster.java:222)
 at java.awt.AWTEventMulticaster.mousePressed(AWTEventMulticaster.java:221)
 at java.awt.Component.processMouseEvent(Component.java:5514)
 at javax.swing.JComponent.processMouseEvent(JComponent.java:3135)
 at java.awt.Component.processEvent(Component.java:5282)
 at java.awt.Container.processEvent(Container.java:1966)
 at java.awt.Component.dispatchEventImpl(Component.java:3984)
 at java.awt.Container.dispatchEventImpl(Container.java:2024)
 at java.awt.Component.dispatchEvent(Component.java:3819)
 at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4212)
 at java.awt.LightweightDispatcher.processMouseEvent(Container.java:3889)
 at java.awt.LightweightDispatcher.dispatchEvent(Container.java:3822)
 at java.awt.Container.dispatchEventImpl(Container.java:2010)
 at java.awt.Window.dispatchEventImpl(Window.java:1791)
 at java.awt.Component.dispatchEvent(Component.java:3819)
 at java.awt.EventQueue.dispatchEvent(EventQueue.java:463)

The editor had been installed in this way:

JTable table = ...;
table.setDefaultEditor(Number.class, table.getDefaultEditor(Double.class));",smartshark_2_2,1412,commons-lang,"""[0.995374858379364, 0.004625153727829456]"""
925,229184,Perhaps add containsAny() methods?,"Might be useful to have containsAny(String, char[]) and containsAny(String, String) methods.

They would be the inverse of containsNone() (so are not strictly necessary), but to my eyes

    containsAny(str,""abc"") 

looks nicer and is easier to read than the double negative

   !containsNone(str,""abc"")",smartshark_2_2,1220,commons-lang,"""[0.9983854293823242, 0.00161460239905864]"""
926,229185,Fix FindBugs warnings in DurationFormatUtils,"There are some FindBugs warnings in DurationFormatUtils: 

bq. Comparison of String objects using == or != in org.apache.commons.lang3.time.DurationFormatUtils.format(DurationFormatUtils$Token[], long, long, long, long, long, long, long, boolean)

We should either fix them or define a FindBugs exclude rule.",smartshark_2_2,810,commons-lang,"""[0.9982700347900391, 0.001729927258566022]"""
927,229186,[lang] Enhancement of ExceptionUtils.CAUSE_METHOD_NAMES,"Sun's Java Webservice Development Pack (jwsdp) uses the following 3 nested
exception names:
com.sun.xml.rpc.util.exception.JAXRPCExceptionBase: getLinkedException()
com.sun.xml.rpc.util.exception.LocalizableExceptionAdapter: getNestedException
javax.xml.rpc.JAXRPCException: getLinkedCause

Perhaps you can add them to the default ExceptionUtils.CAUSE_METHOD_NAMES.

Thanks",smartshark_2_2,1050,commons-lang,"""[0.99839848279953, 0.0016014764551073313]"""
928,229187,StringUtils.equals() / CharSequenceUtils.regionMatches() assumes that CharSequence.toString() implementation is effective,"In my case I have {{CharSequence}} that implements a ""lazy"" string which is stored on disk, and although {{toString()}} implementation is valid, it is very expensive plus can potentially cause OOM.

Thus {{CharSequenceUtils.regionMatches()}} should really do char-by-char comparison, leaving the optimization to underlying {{CharSequence}} implementation.

Maybe {{CharSequenceUtils.regionMatches()}} could check that passed {{CharSequence}} is standard implementation (like {{StringBuilder}}, {{StringBuffer}}) that has ""effective"" {{toString()}} implementation, but this implementation ends up with creating new {{String}} object and thus duplicating the character buffer. So we have classical speed/memory trade-off.

P.S. [Line 192 of CharSequenceUtils()|http://svn.apache.org/viewvc/commons/proper/lang/trunk/src/main/java/org/apache/commons/lang3/CharSequenceUtils.java?revision=1199894&view=markup#l192] reads

{{TODO: Implement rather than convert to String}}

",smartshark_2_2,272,commons-lang,"""[0.35441988706588745, 0.6455801129341125]"""
929,229188,Misleading Javadoc comment in StrBuilderReader class,"http://svn.apache.org/viewvc/commons/proper/lang/trunk/src/main/java/org/apache/commons/lang3/text/StrBuilder.java?view=markup
contains these lines of code:

2835    /**
2836     * Inner class to allow StrBuilder to operate as a writer.
2837     */
2838    class StrBuilderReader extends Reader {

That doesn't make much sense. The comment should probably read ""... as a reader"".",smartshark_2_2,502,commons-lang,"""[0.9975872039794922, 0.0024127729702740908]"""
930,229189,Encapsulate trival equal test into a method to avoid repeating over and over.....,"In class EqualsBuilder, the documentation gives sample code on how to use EqualsBuilder.  The proper usage of class EqualsBuilder is a bit long.  My suggestion is to encapsulate the following trivial test into a method inside the class EqualsBuilder.  You call it the new method trivalTest(Object obj1, Object obj2).

   // This is the code that should be put in a method to avoid repeating....
   if (obj == null) { return false; }
   if (obj == this) { return true; }
   if (obj.getClass() != getClass()) {
     return false;
   }
",smartshark_2_2,377,commons-lang,"""[0.9984272718429565, 0.0015727737918496132]"""
931,229190,BooleanUtils.toBoolean(Integer) method added,"Right now to convert an IntegerÂ to boolean there is either a null check needed:

{{final Integer value = 1;}}
 {{if (value != null) {}}
 {{Â  Â  if(BooleanUtils.toBoolean(value)) {}}

{{Â  Â  Â  Â  // some code}}

Â  Â  Â  }
 {{}}}

or two methods call:

Â 

{{final Integer value = 1;}}
 {{final Boolean b = BooleanUtils.toBooleanObject(value);}}
 {{if (BooleanUtils.toBoolean(b)) {}}
 {{Â  Â  // some code}}
 {{}}}

Some code-sugar is added with the help of method that accepts Integer and returns boolean:

{{final Integer value = 1;}}
 {{if (BooleanUtils.toBoolean(value)) {}}
Â  Â  Â  // some code
 {{}}}

Â ",smartshark_2_2,1019,commons-lang,"""[0.9800887703895569, 0.019911253824830055]"""
932,229191,DurationFormatUtils.formatDuration accepts formats with year/month but cannot use them,"The method DurationFormatUtils.formatDuration does not validate that the format is applicable. The method does not calculate months and years, so these values will be 0 if the format contains y or M.

Perhaps the method should throw IAE if one of the unused format chars is used?",smartshark_2_2,1006,commons-lang,"""[0.4539961814880371, 0.5460038185119629]"""
933,229192,[lang] javadoc example for StringUtils.splitByWholeSeparator incorrect,"Hi,

the javadoc examples for the StringUtils.splitByWholeSeparator(...) methods 
are not correct.

- The examples of splitByWholeSeperator(String,String) are illustrating the 
usage of the StringUtils.split(String,String) method.
- The 6th example of splitByWholeSeperator(String,String,int) is not correct 
and should be removed.

regards,
Maarten",smartshark_2_2,404,commons-lang,"""[0.9973973035812378, 0.0026027322746813297]"""
934,229193,[lang] New interpolation features,"This is the initial implementation of the new VariableFormat class as discussed
on the dev list.

VariableFormat provides several methods dealing with variable interpolation.",smartshark_2_2,1155,commons-lang,"""[0.9983086585998535, 0.001691297278739512]"""
935,229194,Add the Jaro-Winkler string distance algorithm to StringUtils ,"Add SimilarityMatch algorithm to evaluate a similarity matching ratio between two strings.

double matchscore = StringUtils.calculateSimilarityMatching(String s1, String s2)

I have a patch ready with implementation of similaritymatch.
This happens to be a usual need in science algorithm and directly using commons lang3 library for these string operation would be neat.

",smartshark_2_2,521,commons-lang,"""[0.9983757734298706, 0.0016242144629359245]"""
936,229195,Add bypass option for classes to recursive and reflective EqualsBuilder,"{quote}String are now compared with internal data (hash and char[]). But sometimes hash value is not initialized, this is probably a cache for the hashcode. So the comparison fails even if the char[] content is the same.
You should add the bypass for basic JRE classes (String, Datetime types, etc.){quote}

reported at https://github.com/apache/commons-lang/pull/202#issuecomment-333874925",smartshark_2_2,13,commons-lang,"""[0.9940961599349976, 0.00590382469817996]"""
937,229196,Added class hierachy support to CompareToBuilder.reflectionCompare(),"Added class hierachy support to CompareToBuilder.reflectionCompare() with the 
following patches.",smartshark_2_2,163,commons-lang,"""[0.9984920024871826, 0.001508026383817196]"""
938,229197,Visitors for Builders,"  While doing commons-lang stuff, I found the repetitive nature of
Builder stuff. Therefore, i decided to:

 a) Create a SingleBuilder (for ToString / HashCode) and a
DualBuilder (CompareTo / Equals) interface, and make their classes
implementing;
 b) Create a SingleBuilderVisitor and DualBuilderVisitor;
 c) Do the proper wiring, while creating new constructors using the visitors;
 d) Create new tests and ensuring javadoc plays nice along;

 There's a patch attached in this e-mail, done against the latest
svn STABLE url for 2.2
(http://svn.apache.org/repos/asf/jakarta/commons/proper/lang/branches/LANG_2_2_X/)

 There might be errors, but methinks it's a great idea to declare a
constant like this in your source code.

 private static final EQUALS_VISITOR = new DualBuilderVisitor() {
          public void visit(DualBuilder builder, Object lhs, Object rhs) {
             Type lhsObj = (Type) lhs;
             Type rhsObj = (Type) rhs;
              builder.append(lhsObj.getId(),
rhsObj.getId()).append(lhsObj.getName(), rhsObj.getName());
          }
 };

 And then, declare and instantiate builders like as, say:

 public boolean equals(Object anotherObj) {
  return new EqualsVisitor(EQUALS_VISITOR, this, anotherObj);
 }

 Please note this implementation, overrall, also lets you easily use
a single visitor for either equals and compareto, thus creating a
single point of management for all your
field-maintenance-in-value-objects-needs.",smartshark_2_2,62,commons-lang,"""[0.9984714388847351, 0.0015285697299987078]"""
939,229198,Clarify or improve behaviour of int-based methods in StringUtils,"The following methods use an {{int}} to represent a search character:

{code:java}
boolean contains(final CharSequence seq, final int searchChar)
int indexOf(final CharSequence seq, final int searchChar)
int indexOf(final CharSequence seq, final int searchChar, final int startPos)
int lastIndexOf(final CharSequence seq, final int searchChar)
int lastIndexOf(final CharSequence seq, final int searchChar, final int startPos)
{code}

When I see an {{int}} representing a character, I tend to assume the method can handle supplementary characters. However, the current behaviour of these methods depends upon whether the {{CharSequence}} is a {{String}} or not.

{code:java}
StringBuilder builder = new StringBuilder();
builder.appendCodePoint(0x2070E);

System.out.println(StringUtils.lastIndexOf(builder, 0x2070E)); // -1
System.out.println(StringUtils.lastIndexOf(builder.toString(), 0x2070E)); // 0
{code}

The Javadoc for these methods are ambiguous on this point, stating:

{quote}
This method uses {{String.lastIndexOf(int)}} if possible.
{quote}

I think we should consider updating the {{CharSequenceUtils}} methods used by this class to convert all {{CharSequence}} parameters to strings, enabling full code point support. The docs could be updated to make this crystal clear.

There is a question of whether this breaks backwards compatibility.",smartshark_2_2,943,commons-lang,"""[0.998035728931427, 0.001964257564395666]"""
940,229199,Add StringUtils.hash(String) and StringUtils.toHex(byte[]) methods.,"The Struts TokenProcessor class has a method toHex(byte[]) that turns the given 
byte[] into a String of hexadecimal digits.  This functionality is not Struts 
specific and I've found uses for it outside of Struts.  I've written a 
hash(String) method that takes a clear text String and runs it through an MD5 
hash and then the toHex() method to output a 32 character String of hexadecimal 
digits.  This has been useful in hashing passwords and other sensitive data.
If there's interest in adding this to commons-lang, I will create a patch file 
against StringUtils and StringUtilsTest.",smartshark_2_2,1281,commons-lang,"""[0.9985232949256897, 0.0014767084503546357]"""
941,229200,clone() method for ObjectUtils,"We have a simple clone() method in Commons Configuration that could be added to ObjectUtils. It calls the public clone() method of a cloneable object, or throws a CloneNotSupportedException.

Here is the code:

{code:java}
/**
 * An internally used helper method for cloning objects. This implementation
 * is not very sophisticated nor efficient. Maybe it can be replaced by an
 * implementation from Commons Lang later. The method checks whether the
 * passed in object implements the <code>Cloneable</code> interface. If
 * this is the case, the <code>clone()</code> method is invoked by
 * reflection. Errors that occur during the cloning process are re-thrown as
 * runtime exceptions.
 *
 * @param obj the object to be cloned
 * @return the cloned object
 * @throws CloneNotSupportedException if the object cannot be cloned
 */
public static Object clone(Object obj) throws CloneNotSupportedException
{
    if (obj instanceof Cloneable)
    {
        try
        {
            Method m = obj.getClass().getMethod(METHOD_CLONE);
            return m.invoke(obj);
        }
        catch (NoSuchMethodException nmex)
        {
            throw new CloneNotSupportedException(""No clone() method found for class"" + obj.getClass().getName());
        }
        catch (IllegalAccessException iaex)
        {
            throw new ConfigurationRuntimeException(iaex);
        }
        catch (InvocationTargetException itex)
        {
            throw new ConfigurationRuntimeException(itex);
        }
    }
    else
    {
        throw new CloneNotSupportedException(obj.getClass().getName() + "" does not implement Cloneable"");
    }
}
{code}",smartshark_2_2,1341,commons-lang,"""[0.9983893632888794, 0.0016106149414554238]"""
942,229201,StringUtils#startsWithAny has error in Javadoc,"startsWithAny says in its javaDoc: 
{code:java}
/** 
@return {@code true} if the CharSequence starts with any of the the prefixes, case insensitive, or both {@code null} 
**/
{code} 

but uses startsWith(final CharSequence str, final CharSequence prefix) which is *case sensitive*.

Either the JavaDoc of startsWithAny method should be changed or startsWithIgnoreCase() or startsWith(CharSequence,CharSequence,boolean) should be used.

",smartshark_2_2,756,commons-lang,"""[0.9643237590789795, 0.03567620366811752]"""
943,229202,Add XMLCharacter class,Add XMLCharacter class. Helps toward [LANG-1184].,smartshark_2_2,879,commons-lang,"""[0.9981570839881897, 0.0018428637413308024]"""
944,229203," ""\u2284"":""&nsub;"" mapping missing from EntityArrays#HTML40_EXTENDED_ESCAPE",see: https://github.com/apache/commons-lang/pull/159,smartshark_2_2,794,commons-lang,"""[0.43005242943763733, 0.5699475407600403]"""
945,229204,Move Documentation from user guide to package-info files,"As discussed on the ML [1], the user guide on the website currently contains documentation that would better suite in the corresponding package-info files. The Documentation should be split up and moved to those files.

[1] http://markmail.org/message/jgyacp3pv3t43s4h",smartshark_2_2,517,commons-lang,"""[0.9971471428871155, 0.0028528282418847084]"""
946,229205,Javadoc bug in org.apache.commons.lang.StringUtils.removeEndIgnoreCase(),"the org.apache.commons.lang.StringUtils online document 
http://commons.apache.org/lang/api-release/org/apache/commons/lang/StringUtils.html#removeEndIgnoreCase%28java.lang.String,%20java.lang.String%29

at removeEndIgnoreCase()  description, there is one line 
StringUtils.removeEnd(""www.domain.com"", "".com."")  = ""www.domain.com.""

it should be 
StringUtils.removeEnd(""www.domain.com"", "".com."")  = ""www.domain.com""


",smartshark_2_2,58,commons-lang,"""[0.9876745343208313, 0.012325507588684559]"""
947,229206,org.apache.commons.lang3.SystemUtils should not write to System.err,"The class {{org.apache.commons.lang3.SystemUtils}}Â should not write to {{System.err}}.

Initializing the following may cause to write to {{System.err}}Â if a {{SecurityException}} is caught:

AWT_TOOLKIT
FILE_ENCODING
FILE_SEPARATOR
JAVA_AWT_FONTS
JAVA_AWT_GRAPHICSENV
JAVA_AWT_HEADLESS
JAVA_AWT_PRINTERJOB
JAVA_CLASS_PATH
JAVA_CLASS_VERSION
JAVA_COMPILER
JAVA_ENDORSED_DIRS
JAVA_EXT_DIRS
JAVA_HOME
JAVA_IO_TMPDIR
JAVA_LIBRARY_PATH
JAVA_RUNTIME_NAME
JAVA_RUNTIME_VERSION
JAVA_SPECIFICATION_NAME
JAVA_SPECIFICATION_VENDOR
JAVA_SPECIFICATION_VERSION
JAVA_UTIL_PREFS_PREFERENCES_FACTORY
JAVA_VENDOR
JAVA_VENDOR_URL
JAVA_VERSION
JAVA_VM_INFO
JAVA_VM_NAME
JAVA_VM_SPECIFICATION_NAME
JAVA_VM_SPECIFICATION_VENDOR
JAVA_VM_SPECIFICATION_VERSION
JAVA_VM_VENDOR
JAVA_VM_VERSION
LINE_SEPARATOR
OS_ARCH
OS_NAME
OS_VERSION
PATH_SEPARATOR
USER_COUNTRY (3 matches)
USER_DIR
USER_HOME
USER_LANGUAGE
USER_NAME
USER_TIMEZONE

Â ",smartshark_2_2,1010,commons-lang,"""[0.9434574842453003, 0.05654248595237732]"""
948,229207,StringUtils#getLevenshteinDistance reduce memory consumption,see [https://github.com/apache/commons-lang/pull/189] for details,smartshark_2_2,929,commons-lang,"""[0.998206615447998, 0.0017933950293809175]"""
949,229208,Method returns number of inheritance hops between parent and subclass,"For example.

class A {
}
class B extends A {
}
class C extends B {
}

int d;
d = InheritanceUtils.distance(A.class, A.class);
Assert.assertEquals(0, d);
d = InheritanceUtils.distance(B.class, A.class);
Assert.assertEquals(1, d);
d = InheritanceUtils.distance(C.class, A.class);
Assert.assertEquals(2, d);",smartshark_2_2,497,commons-lang,"""[0.3814505934715271, 0.6185494661331177]"""
950,229209,A generic implementation of the Lazy initialization pattern,"This is a fully functional implementation of the double-check idiom for lazy initialization of an instance field as discussed in Joshua Bloch's ""Effective Java"".

If there is interest, this could be the first element of a set of helper classes related to concurrent programming.",smartshark_2_2,307,commons-lang,"""[0.998198926448822, 0.0018010904313996434]"""
951,229210,"New method replace(String text, Map replaceMap) in StringUtils","
Hi

I have a method that I would like to contribute to the StringUtils class. I unfortunately can't access a CVS server on the internet, so I included the method and its test in this mail. I'd appreciate it if someone can have a look and add the source if it is acceptable. 

The method basically replaces multiple strings in a String, but is very efficient. The description of the method is in the Javadoc.

Regards,
Gerhard
 
Here is method to add to org.apache.commons.lang.StringUtils

    /**
     * <p>Replace all occurrences of multiple strings in a string.</p>
     * 
     * <p>It is functionally equivalent to calling 
     *  replaceAll(searchString, replaceString) repeatedly on <code>text</code> 
     *  for all the strings you want to replace, but much faster </p>
     *  
     * <p><code>replaceMap</code> maps search strings to replacement strings.
     * Each key in the map will be replaced by its value in <code>text</code>.
     * </p>
     * 
     * @param text String to replace strings in. May be null
     * @param replaceMap Maps search strings to replacement strings.  
     * @return string with all values replaced. <code>null</code> if 
     *          text was null
     */
    public static String replace(String text, Map replaceMap) {
      if(isEmpty(text)) {
        return text;
      }
      StringBuffer buff = new StringBuffer(text.length());
      //map each replace string and it's next position in text
      TreeMap indexMap = new TreeMap();
      //populate indexMap with it's initial values
      for (Iterator iter = replaceMap.keySet().iterator(); iter.hasNext();) {
        String key = (String) iter.next();
        if(isBlank(key)) {
          continue;
        }
        int idx = text.indexOf(key);
        if(idx >= 0) {
          indexMap.put(new Integer(idx), key);
        }
      }
      
      //if there is nothing to replace
      if(indexMap.isEmpty()) return text;
      
      int prevIdx = 0;
      while(indexMap.size() > 0) {
        Integer idxI = (Integer)indexMap.firstKey();
        int idx = idxI.intValue();
        String keyS = (String)indexMap.remove(idxI);
        buff.append(text.substring(prevIdx, idx));
        buff.append((String)replaceMap.get(keyS));
        prevIdx = idx + keyS.length();
        idx = text.indexOf(keyS, prevIdx);
        if(idx > 0) {
          indexMap.put(new Integer(idx), keyS);
        }
      }
      buff.append(text.substring(prevIdx));
      return buff.toString();
    }


Here is the test method to add to StringUtilsTest.


    public void testReplace_StringMap() {
      Map replaceMapEmpty = new HashMap();
      Map replaceMap1 = new HashMap();
      replaceMap1.put(""foo"", ""bar"");
      replaceMap1.put(""boo"", ""far"");
      Map replaceMap2 = new HashMap();
      replaceMap2.put(""foo"", """");
      Map replaceMap3 = new HashMap();
      replaceMap3.put("""", ""foo"");
      
      assertEquals(null, StringUtils.replace(null,  replaceMapEmpty));
      assertEquals(null, StringUtils.replace(null, replaceMap1));

      assertEquals("""", StringUtils.replace("""", replaceMapEmpty));
      assertEquals("""", StringUtils.replace("""", replaceMap1));

      assertEquals(""foo"", StringUtils.replace(""foo"", replaceMapEmpty));
      assertEquals(""bar"", StringUtils.replace(""foo"", replaceMap1));
      assertEquals("""", StringUtils.replace(""foo"", replaceMap2));
      assertEquals(""foo"", StringUtils.replace(""foo"", replaceMap3));
      
      assertEquals(""foobar"", StringUtils.replace(""foobar"", replaceMapEmpty));
      assertEquals(""barbar"", StringUtils.replace(""foobar"", replaceMap1));
      assertEquals(""bar"", StringUtils.replace(""bar"", replaceMap2));

      assertEquals(""fobar"", StringUtils.replace(""fobar"", replaceMap1));
      assertEquals(""barobar"", StringUtils.replace(""fooobar"", replaceMap1));
      assertEquals(""barbar"", StringUtils.replace(""foofoo"", replaceMap1));
      assertEquals(""barfar"", StringUtils.replace(""fooboo"", replaceMap1));
      assertEquals(""barfarbarfar"", StringUtils.replace(""fooboofooboo"", replaceMap1));
      assertEquals(""barbarfarfar"", StringUtils.replace(""foofoobooboo"", replaceMap1));
     }

",smartshark_2_2,1279,commons-lang,"""[0.9983892440795898, 0.0016107828123494983]"""
952,229211,Add a Memoizer class,"I am currently using a class like the Memoizer class [1] from ""Java
Concurrency in Practice"" [2], a great book.

It would fit perfectly in org.apache.commons.lang3.concurrent.

[1] http://jcip.net/listings/Memoizer.java
[2] http://jcip.net/

There is no licensing issue because the code is in the public domain:

{noformat}
---------- Forwarded message ----------
From: Brian Goetz <brian@briangoetz.com>
Date: Tue, Aug 9, 2011 at 5:40 PM
Subject: Re: Apache Commons Lang and Memoizer
To: Gary Gregory <ggregory@apache.org>, Tim Peierls <tim@peierls.net>


No license issues -- the code is in the public domain:

   Written by Brian Goetz and Tim Peierls with assistance from members of
   JCP JSR-166 Expert Group and released to the public domain, as explained at
   http://creativecommons.org/licenses/publicdomain


Code for the samples can be downloaded from http://www.jcip.net/listings.html.

Cheers,
-Brian


On 8/9/2011 5:38 PM, Gary Gregory wrote:
>
> Hi Brian,
>
> I would like to include a Memoizer in the next release of Apache
> Commons Lang [1].
>
> Can we use the Memoizer pattern from ""Java Concurrency in Practice""? I
> think I would reuse the code from the class Memoizer and change names,
> things like that.
>
> We are talking about this on the Lang mailing list and are wondering
> if there are any licensing issues.
>
> [1] https://commons.apache.org/lang/
>



-- 
Thank you,
Gary

http://garygregory.wordpress.com/
http://garygregory.com/
http://people.apache.org/~ggregory/
http://twitter.com/GaryGregory
{noformat}",smartshark_2_2,938,commons-lang,"""[0.9983466863632202, 0.0016532723093405366]"""
953,229212,StringUtils#isAnyEmpty and #isAnyBlank should return false for an empty array,"An empty array does not contain any empty/blank CharSequences, so isAnyEmpty/isAnyBlank should return false for an empty array.",smartshark_2_2,881,commons-lang,"""[0.8601134419441223, 0.1398865431547165]"""
954,229213,Does StrSubstitutor really need setter methods for the fields that are settable via constructors?,"Does StrSubstitutor really need setter methods for the fields that are settable via constructors?

If these setters could be dropped, the instance variables could be made final.

I think this would then make the class thread-safe, provided that the Map used by StrLookup is thread-safe.",smartshark_2_2,1397,commons-lang,"""[0.9969936609268188, 0.0030062885489314795]"""
955,229214,Fix parsing edge cases in FastDateParser,"There are two commented out tests in FastDateParserSDFTest:
* testLowerCase
* testLowerCasePP

When activating these tests, the build fails with 6 failures. This indicates, that there are edge cases in date parsing, where FastDateFormat behave different vom SimpleDateFormat. This this be fixed so that the parsing result is the same for FastDateParser and SimpleDateParser.",smartshark_2_2,796,commons-lang,"""[0.9875756502151489, 0.01242432277649641]"""
956,229215,Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer,"The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:

If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.

So the if condition:
if (membershipMatrix[i][j] > maxMembership) {
                    maxMembership = membershipMatrix[i][j];
                    newCluster = j;
}
will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line:
clusters.get(newCluster)
                    .addPoint(point);

Adding the following condition can solve the problem:
double d;
if (sum == 0)
d = 1;
else
d = 1.0/sum;",smartshark_2_2,1023,commons-math,"""[0.20354682207107544, 0.7964531779289246]"""
957,229216,CMAESOptimizer fails sometimes when bounds are violated,"The CMAESOptimizer repairs points that are out of bounds by moving them into bounds, and adding a penalty based on how far they were moved.

The penalty added is scaled by the range of values in the current population.

The calculation of the valueRange, however, includes the penalty so at each iteration the amount of penalty can grow multiplicatively.  One solution, is to keep the value and penalties separate before calculating the scale factor for the penalties.  A patch that does this will be attached.
",smartshark_2_2,1197,commons-math,"""[0.06195012480020523, 0.9380499124526978]"""
958,229217,"Fraction(double, int) constructor strange behaviour","The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:

1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value

2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.

I have, as of yet, not found a solution. The constructor looks like this:

public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }

Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. 

The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.

This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.

* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that ""since fractions are always in lowest terms, numerators and can be compared directly for equality"", so it seems like this is the intention. ",smartshark_2_2,833,commons-math,"""[0.10874219983816147, 0.8912578225135803]"""
959,229218,Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception,"An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple
fraction.  For example:

double d = 0.5000000001;
Fraction f = new Fraction(d, 10);

Patch with unit test on way.",smartshark_2_2,827,commons-math,"""[0.2698383331298828, 0.730161726474762]"""
960,229219,"""getCovariances"" is broken (""o.a.c.m.optimization.general.LevenbergMarquardtOptimizer"")","Method ""getCovariances"" (defined in base class ""AbstractLeastSquaresOptimizer"") returns incorrect values when called from subclass ""LevenbergMarquardtOptimizer"".
",smartshark_2_2,227,commons-math,"""[0.07326270639896393, 0.9267372488975525]"""
961,229220,Jacobian rank determination in LevenbergMarquardtOptimizer is not numerically robust,"LevenbergMarquardtOptimizer is designed to handle singular jacobians,  i.e. situations when some of the fitted parameters depend on each other. The check for that condition is in LevenbergMarquardtOptimizer.qrDecomposition uses precise comparison to 0.

    if (ak2 == 0 ) {
                rank = k;
                return;
        }

A correct check would be comparison with a small epsilon. Hard coded 2.2204e-16 is used elsewhere in the same file for similar purpose.

final double QR_RANK_EPS = Math.ulp(1d); //2.220446049250313E-16
....
    if (ak2  < QR_RANK_EPS) {
                rank = k;
                return;
        }

Current exact equality check is not tolerant of the real world poorly conditioned situations. For example I am trying to fit a cylinder into sample 3d points. Although theoretically cylinder has only 5 independent variables, derivatives for optimizing function (signed distance) for such minimal parametrization are complicated and it  it much easier to work with a 7 variable parametrization (3 for axis direction, 3 for axis origin and 1 for radius). This naturally results in rank-deficient jacobian, but because of the numeric errors the actual ak2 values for the dependent rows ( I am seeing values of 1e-18 and less), rank handling code does not kick in.
Keeping these tiny values around then leads to huge corrections for the corresponding very slowly changing parameters, and consequently to numeric errors and instabilities. I have noticed the problem because tiny shift in the initial guess (on the order of 1e-12 in the axis component and origins) resulted in significantly different finally converged answers (origins and radii differing by as much as 0.02) which I tracked to loss of precision due to numeric error with root cause described above.
Providing a cutoff as suggested fixes the issue. After the fix, small perturbations in the initial guess had practically no effect to the converged result - as expected from a robust algorithm.
",smartshark_2_2,520,commons-math,"""[0.3363187313079834, 0.6636812686920166]"""
962,229221,Default sigma for CMAESOptimizer is wrong when using bounds,"The documentation suggests setting inputSigma to 1/3 the range you are fitting over.  However, in CMAESOptimizer.initializeCMA() if boundaries are specified the sigmaArray is by default assigned a value of 0.3 divided by the range.  If the user had specified the inputSigma to be 0.3 of the range (as suggested by the docs) then sigmaArray would have been assigned the value of 0.3.  Thus, it looks like the 0.3 should not be divided by the range, only a user-specified inputSigma should get divided by the range.",smartshark_2_2,1034,commons-math,"""[0.11649306863546371, 0.8835068941116333]"""
963,229222,Constructor of PolyhedronsSet throws NullPointerException,"The following statement throws a NullPointerException:
new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);

I found that other numbers also produce that effect. The stack trace:
java.lang.NullPointerException
        at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)
        at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)
        at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)
        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)
        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)",smartshark_2_2,1111,commons-math,"""[0.06003311648964882, 0.9399669170379639]"""
964,229223,ADJUSTED R SQUARED INCORRECT IN REGRESSION RESULTS,"I forgot to cast to double when dividing two integers:

            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    (  nobs / ( (nobs - rank)));
Should be
            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    ( (double) nobs / ( (double) (nobs - rank)));

Patch attached.",smartshark_2_2,768,commons-math,"""[0.13352207839488983, 0.8664778470993042]"""
965,229224,"Bugs in ""BrentOptimizer""","I apologize for having provided a buggy implementation of Brent's optimization algorithm (class ""BrentOptimizer"" in package ""optimization.univariate"").
The unit tests didn't show that there was something wrong, although (from the ""changes.xml"" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour.
Comparing with an implementation in Python, I could figure out the fixes. I'll modify ""BrentOptimizer"" and add a test. I also propose to change the name of the unit test class from ""BrentMinimizerTest"" to ""BrentOptimizerTest"".
",smartshark_2_2,389,commons-math,"""[0.09958034008741379, 0.9004197120666504]"""
966,229225,"CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer","CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer and the length of the initial guess array is 1.  Here is my example code:

{code:title=CurveFitter with LevenbergMarquardtOptimizer|borderStyle=solid}
  LevenbergMarquardtOptimizer optimizer = new LevenbergMarquardtOptimizer();
  CurveFitter fitter = new CurveFitter(optimizer);
  fitter.addObservedPoint(2.805d, 0.6934785852953367d);
  fitter.addObservedPoint(2.74333333333333d, 0.6306772025518496d);
  fitter.addObservedPoint(1.655d, 0.9474675497289684);
  fitter.addObservedPoint(1.725d, 0.9013594835804194d);
  SimpleInverseFunction sif = new SimpleInverseFunction(); // Class provided below
  double[] initialguess = new double[1];
  initialguess[0] = 1.0d;
  double[] bestCoefficients = fitter.fit(sif, initialguess); // <---- ALWAYS RETURNS A VALUE OF initialguess !

    /**
     * This is my implementation of ParametricRealFunction
     * Implements y = ax^-1 + b for use with an Apache CurveFitter implementation
      */
    private class SimpleInverseFunction implements ParametricRealFunction
    {
        public double value(double x, double[] doubles) throws FunctionEvaluationException
        {
            //y = ax^-1 + b
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            return a * Math.pow(x, -1d) + b;
        }
        public double[] gradient(double x, double[] doubles) throws FunctionEvaluationException
        {
            //derivative: -ax^-2
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            double derivative = -a * Math.pow(x, -2d);
            double[]gradientVector = new double[1];
            gradientVector[0] = derivative;
            return gradientVector; 
        }
    }
{code} 
",smartshark_2_2,669,commons-math,"""[0.10496418923139572, 0.8950358629226685]"""
967,229226,OLSMultipleLinearRegression STILL needs a way to specify non-zero singularity threshold when instantiating QRDecomposition,"A fix was made for this issue in MATH-1110 for the newSampleData method but not for the newXSampleData method.

It's a simple change to propagate the threshold to QRDecomposition:
237c237
<         qr = new QRDecomposition(getX());
---
>         qr = new QRDecomposition(getX(), threshold);",smartshark_2_2,1344,commons-math,"""[0.6343281865119934, 0.3656717836856842]"""
968,229227,CMAESOptimizer does not enforce bounds,"The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.",smartshark_2_2,807,commons-math,"""[0.060998041182756424, 0.9390019774436951]"""
969,229228,BitsStreamGenerator#nextBytes(byte[]) is wrong,"Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.

When nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.

I made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.",smartshark_2_2,1229,commons-math,"""[0.14479415118694305, 0.8552058339118958]"""
970,229229,Fraction percentageValue rare overflow,"The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.

The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.",smartshark_2_2,221,commons-math,"""[0.06178293749690056, 0.938217043876648]"""
971,229230,"MathUtils.gcd(u, v) fails when u and v both contain a high power of 2","The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.

        assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15)));

Fix: Replace the test at the start of MathUtils.gcd()

        if (u * v == 0) {

by

        if (u == 0 || v == 0) {
",smartshark_2_2,503,commons-math,"""[0.08428286015987396, 0.9157171249389648]"""
972,229231,Inconsistent result from Levenberg-Marquardt,"Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost",smartshark_2_2,109,commons-math,"""[0.703892171382904, 0.29610779881477356]"""
973,229232,numerical problems in rotation creation,"building a rotation from the following vector pairs leads to NaN:
u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377
u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10
v1 = 1, 0, 0
v2 = 0, 0, 1

The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:
 <v1'|v1'> == <u1|u1>
 <v2'|v2'> == <u2|u2>
 <u1 |u2>  == <v1'|v2'>

Once the (v1', v2') pair has been computed, we compute the cross product:
  k = (v1' - u1)^(v2' - u2)

and the scalar product:
  c = <k | (u1^u2)>

By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)].
c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.

However, there are numerical problems with the vector above with the way these computations are done, as shown
by the following comparisons, showing the result we get from our Java code and the result we get from manual
computation with the same formulas but with enhanced precision:

commons math:   k = 38514476.5,            -84.,                           -1168590144
high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...

and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get:
commons math    c = -1.2397173627587605E20
high precision: c =  558382746168463196.7079627...

We have lost ALL significant digits in cancellations, and even the sign is wrong!
",smartshark_2_2,301,commons-math,"""[0.06502117216587067, 0.9349788427352905]"""
974,229233,ResizableDoubleArray does not work with double array of size 1,"When attempting to create a ResizableDoubleArray with an array of a single value (e.g. {4.0}), the constructor creates an internal array with 16 entries that are all 0.0

Bug looks like it might be on line 414 of ResizableDoubleArray.java:

        if (data != null && data.length > 1) {
",smartshark_2_2,1277,commons-math,"""[0.10733948647975922, 0.8926605582237244]"""
975,229234,Constructor parameter not used,"the constructor public ArrayFieldVector(Field<T> field, T[] v1, T[] v2)
sets this
""this.field = data[0].getField();""
in the fast line...

""this.field = field;""

would be right - field was explicitly provided.
",smartshark_2_2,431,commons-math,"""[0.3544245660305023, 0.6455754041671753]"""
976,229235,exception in LevenbergMarquardtEstimator,"I get this exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.qrDecomposition(LevenbergMarquardtEstimator.java:772)
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.estimate(LevenbergMarquardtEstimator.java:232)
       at quadraticFitterProblem.QuadraticFitterProblem.<init>(QuadraticFitterProblem.java:27)
       at quadraticFitterProblem.QuadraticFitterProblem.main(QuadraticFitterProblem.java:40)
on the code below.

The exception does not occur all the weights in the quadraticFitter are 0.0;


---------------------------------------------------------------------------------------------

package quadraticFitterProblem;

import org.apache.commons.math.estimation.EstimationException;
import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
//import org.apache.commons.math.estimation.WeightedMeasurement;

import com.strategicanalytics.dtd.data.smoothers.QuadraticFitter;

public class QuadraticFitterProblem {

       private QuadraticFitter quadraticFitter;

       public QuadraticFitterProblem() {
         // create the uninitialized fitting problem
         quadraticFitter = new QuadraticFitter();

         quadraticFitter.addPoint (0,  -3.182591015485607, 0.0);
         quadraticFitter.addPoint (1,  -2.5581184967730577, 4.4E-323);
         quadraticFitter.addPoint (2,  -2.1488478161387325, 1.0);
         quadraticFitter.addPoint (3,  -1.9122489313410047, 4.4E-323);
         quadraticFitter.addPoint (4,  1.7785661310051026, 0.0);

         try {
           // solve the problem, using a Levenberg-Marquardt algorithm with
default settings
           LevenbergMarquardtEstimator estimator = new LevenbergMarquardtEstimator();
           //WeightedMeasurement[] wm = quadraticFitter.getMeasurements();
           estimator.estimate(quadraticFitter);

         } catch (EstimationException ee) {
               System.err.println(ee.getMessage());
         }
       }

       /**
        * @param args
        *
        */
       public static void main(String[] args) {

                       new QuadraticFitterProblem();
                       System.out.println (""Done."");
       }

}

----------------------------------------------------------------------------------------------
import org.apache.commons.math.estimation.EstimatedParameter;
//import org.apache.commons.math.estimation.EstimationException;
//import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
import org.apache.commons.math.estimation.SimpleEstimationProblem;
import org.apache.commons.math.estimation.WeightedMeasurement;

public class QuadraticFitter extends SimpleEstimationProblem {

       // y = a x<sup>2</sup> + b x + c
   private EstimatedParameter a;
   private EstimatedParameter b;
   private EstimatedParameter c;

   /**
    * constructor
    *
    *Fitter for a quadratic model to a sample of 2D points.
    * <p>The model is y(x) = a x<sup>2</sup> + b x + c
    * its three parameters of the model are a, b and c.</p>
    */
   public QuadraticFitter() {

       // three parameters of the model
       a = new EstimatedParameter(""a"", 0.0);
       b = new EstimatedParameter(""b"", 0.0);
       c = new EstimatedParameter(""c"", 0.0);

       // provide the parameters to the base class which
       // implements the getAllParameters and getUnboundParameters methods
       addParameter(a);
       addParameter(b);
       addParameter(c);
   }

   /**
    * Add a sample point
    *
    * @param x abscissa
    * @param y ordinate
    * @param w weight
    */
   public void addPoint(double x, double y, double w) {
       addMeasurement(new LocalMeasurement(x, y, w));
   }

   /**
    * Get the value of the quadratic coefficient.
    *
    * @return the value of a for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getA() {
       return a.getEstimate();
   }

   /**
    * Get the value of the linear coefficient.
    *
    * @return the value of b for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getB() {
       return b.getEstimate();
   }

   /**
    * Get the value of the constant coefficient.
    *
    * @return the value of ac for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getC() {
       return c.getEstimate();
   }

   /**
    * Get the theoretical value of the model for some x.
    * <p>The theoretical value is the value computed using
    * the current state of the problem parameters.</p>
    *
    * Note the use of HÃ¶rner's method (synthetic division) for
evaluating polynomials,
    * (more efficient)
    *
    * @param x explanatory variable
    * @return the theoretical value y = a x<sup>2</sup> + b x + c
    */
   public double theoreticalValue(double x) {
       //System.out.println (""x = "" + x + ""  a.getEstimate() = "" +
a.getEstimate() + ""  b.getEstimate() = "" + b.getEstimate() + ""
c.getEstimate() = "" + c.getEstimate());
       return ( (a.getEstimate() * x + b.getEstimate() ) * x +
c.getEstimate());
   }

   /**
    * Get the partial derivative of the theoretical value
    * of the model for some x.
    * <p>The derivative is computed using
    * the current state of the problem parameters.</p>
    *
    * @param x explanatory variable
    * @param parameter estimated parameter (either a, b, or c)
    * @return the partial derivative dy/dp
    */
   private double partial(double x, EstimatedParameter parameter) {
       // since we know the only parameters are a, b and c in this
       // class we simply use ""=="" for efficiency
       if (parameter == a) {
           return x * x;
       } else if (parameter == b) {
           return x;
       } else {
           return 1.0;
       }

   }


   /** Internal measurements class.
    * <p>The measurement is the y value for a fixed specified x.</p>
    */
   private class LocalMeasurement extends WeightedMeasurement {

       static final long serialVersionUID = 1;

       private final double x;

       // constructor
       public LocalMeasurement(double x, double y, double w) {
           super(w, y);
           this.x = x;
       }

       public double getTheoreticalValue() {
           // the value is provided by the model for the local x
           return theoreticalValue(x);
       }

       public double getPartial(EstimatedParameter parameter) {
           // the value is provided by the model for the local x
           return partial(x, parameter);
       }

   }

 }",smartshark_2_2,499,commons-math,"""[0.36192479729652405, 0.6380752325057983]"""
977,229236,"during ODE integration, the last event in a pair of very close event may not be detected","When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step.

If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.

This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.",smartshark_2_2,148,commons-math,"""[0.08555224537849426, 0.9144477844238281]"""
978,229237,Kolmogorov-Smirnov exactP gives incorrect p-values for some D-statistics,"The exactP method in KolmogorovSmirnovTest, which is used by default for small samples in 2-sample tests, can give slightly incorrect p-values in some cases.  The reason for this is that p(D > d) is computed by examining all m-n partitions and counting the number of partitions that give D values larger than the observed value.  D values are not rounded, so some values that are mathematically identical to the observed value compare less than or greater than it.  This results in small errors in the reported p-values.",smartshark_2_2,1139,commons-math,"""[0.11878617852926254, 0.881213903427124]"""
979,229238,PolynomialFitter.fit() stalls,"Hi, in certain cases I ran into the problem that the PolynomialFitter.fit() method stalls, meaning that it does not return, nor throw an Exception (even if it runs for 90 min). Is there a way to tell the PolynomialFitter to iterate only N-times to ensure that my program does not stall?",smartshark_2_2,817,commons-math,"""[0.09972614049911499, 0.900273859500885]"""
980,229239,add support to constrained parameter estimation,The current estimation package supports only unconstrained problems. It should at least support simple bounds constrains on parameters.,smartshark_2_2,778,commons-math,"""[0.9983972907066345, 0.0016027428209781647]"""
981,229240,add a double[][] solve(double[][]) method to Decompositionsolver,"The decomposition solver provides both RealVector and double[] types for vector solving, but it provides only RealMatrix type for matriw solving.
This breaks symmetry and is inconvenient in some algorithms which perform lots of iterations. Lots of copying occurs while converting back and forth between RealMatrix and arrays, with no value added when the matrix aspect of the data is only used in the solve operation.
Adding a double[][] counterpart to RealMatrix in decomposition is also trivial as internally the various implementations already use that.",smartshark_2_2,705,commons-math,"""[0.9983957409858704, 0.0016042519127950072]"""
982,229241,Functions could be more object-oriented without losing any power.,"UnivariateRealFunction, for example, is a map from R to R.  The set of such functions has tons and tons of structure: in addition to being an algebra, equipped with +,-,*, and scaling by constants, it maps the same space into itself, so it is composable, both pre and post.

I'd propose we add:

{code}
  UnivariateRealFunction plus(UnivariateRealFunction other);
  UnivariateRealFunction minus(UnivariateRealFunction other);
  UnivariateRealFunction times(UnivariateRealFunction other);
  UnivariateRealFunction times(double scale);
  UnivariateRealFunction preCompose(UnivariateRealFunction other);
  UnivariateRealFunction postCompose(UnivariateRealFunction other);
{code}

to the interface, and then implement them in an AbstractUnivariateRealFunction base class.  No implementer would need to notice, other than switching to extend this class rather than implement UnivariateRealFunction.

Many people don't need or use this, but... it makes for some powerfully easy code:

{code}UnivariateRealFunction gaussian = Exp.preCompose(Negate.preCompose(Pow2));{code}

which is even nicer when done anonymously passing into a map/collect method (a la MATH-312).
",smartshark_2_2,634,commons-math,"""[0.9984159469604492, 0.0015839880798012018]"""
983,229242,make FieldVector generic,"make FieldVector generic, so one can extend i.e. ArrayVieldVector<Complex> to ArrayComplexVector an introduce new methoids (getReal())...

if one has an equation complexvector.copy the original type ArrayComplexVector is lost thus access to getReal() is not possible.

solution:

public class InheritationTest {

    public static interface FieldVector<T extends FieldElement<T>, R extends FieldVector> {

        R copy();
    }

    public abstract static class ArrayFieldVectorExtendable<T extends FieldElement<T>, R extends FieldVector> implements FieldVector<T, R>, Serializable {

        protected T[] data;

        @Override
        public R copy() {
            return createVector(data);
        }

        abstract protected R createVector(T[] data);
    }

    public static class ArrayFieldVector<T extends FieldElement<T>> extends ArrayFieldVectorExtendable<T, ArrayFieldVector> {

        @Override
        protected ArrayFieldVector<T> createVector(T[] data) {
            ArrayFieldVector<T> result = new ArrayFieldVector<T>();
            result.data = data;
            return result;
        }
    }

    public static class ArrayComplexVector extends ArrayFieldVectorExtendable<Complex, ArrayComplexVector> {

        @Override
        protected ArrayComplexVector createVector(Complex[] data) {
            ArrayComplexVector result = new ArrayComplexVector();
            result.data = data;
            return result;
        }

        public double[] getReal() {
            return null;
        }

        public double[] getImaginary() {
            return null;
        }
    }

    public void test() {
        ArrayComplexVector v = new ArrayComplexVector();
        ArrayComplexVector v1 = v.copy();  // FiledVector type survives ...
    }
}",smartshark_2_2,94,commons-math,"""[0.9986031651496887, 0.0013968617422506213]"""
984,229243,Simplex method intermediate solutions,"I suggest allowing access to intermediate solutions of simplex algorithm (not final vertexes at polyhedron). It can be useful for progress illustrating or early interrupting of process. 

It can be like this:
{code}
SimplexSolver simplexSolver = new SimplexSolver();

// ... prepare constraintSet and goalFunction

simplexSolver.setSimplexSolverInterceptor(new SimplexSolverInterceptor() {
            public boolean intercept(PointValuePair pointValuePair) {
                // pointValuePair is intermediate solution
                // return if the optimization must be stopped here
                return false;
            }
        });

PointValuePair pair = simplexSolver.optimize(goalFunction, constraintSet, GoalType.MINIMIZE);
{code}

I needed it, so I've done it. Commit is here: https://github.com/kosbr/commons-math/commit/20f6c3d7428b7e5d5c695af7c4f51df979245297  

Here is the description of what I've done in my blog. https://kosbr.github.io/2016/11/07/simplex-interceptor.html

I'm able to do the same enhancement to the commons-math library respecting rules like commit name.",smartshark_2_2,1302,commons-math,"""[0.9985130429267883, 0.0014869968872517347]"""
985,229244,Add methods negate() / reciprocal() to the FieldElement interface,"As discussed on the mailing list, it is proposed to add two methods to the {{FieldElement}} interface
* {{negate()}} : returns the additive inverse of {{this}} element.
* {{reciprocal()}} : returns the multiplicative inverse of {{this}} element.

Several name couples have been proposed by Phil
# {{negate}}, {{invert}}
# {{opposite}}, {{reciprocal}}
# {{additiveInverse}}, {{multiplicativeInverse}}

Looking at the classes implementing this interface in the core CM library, we find that
* {{Complex}}, {{Dfp}}, {{BigFraction}} and {{Fraction}} already have a {{negate()}} method.
* Besides, {{BigFraction}} and {{Fraction}} already have a {{reciprocal()}} method.

So the best naming option would seem to be (for the time being) a mixture of what Phil proposed. I realize it's not completely satisfactory because one is a noun and one a verb. Do we want to have good grammar, or preserve what's already implemented? I tend to favour the first option (consistently change the name of existing methods).",smartshark_2_2,401,commons-math,"""[0.9984116554260254, 0.0015883605228736997]"""
986,229245,"Include a VIF and TOLERANCE check for a 2 dimensional double array, to determine variables that cause multi-colinearity issues and should be excluded from the models","Multicollinearity is a statistical phenomenon in which two or more predictor variables in any multiple regression model are highly correlated. Tolerance and VIF are checks that allows to avoid optimization failes due to ""inability to converge"". Most of the times, the major packages (SAS, SPSS etc), have a check prior to running the model and they exclude variables that might cause these kind of problems. It is quite a useful tool to be in common maths.",smartshark_2_2,991,commons-math,"""[0.9984453320503235, 0.0015546474605798721]"""
987,229246,NonLinear Optimizers seem to have a hard time hitting NIST standards,"As per a discussion on the mailing list, I am opening this ticket. In applying the nonlinear optimizers in commons, I noticed what I believe to be instability in the techniques. Further investigation investigation (both of my tests) and the code in prod is warranted. 

I will be pushing a first set of tests which should illustrate what I am seeing. 

 ",smartshark_2_2,1348,commons-math,"""[0.5517444014549255, 0.4482555687427521]"""
988,229247,ContinuousOutputModel for ordinary differential equations should provide derivatives,"The ContinuousOutputModel class allows to store the evolution of the state of an ODE system throughout integration. It aims to provides at a global level all the information that is available at local level in step handlers. It does so by storing the step interpolators as integration is performed, and delegate data retrieval to the interpolator matching the time step.

However, there is a missing part: step interpolators do provide access to the derivatives of the state, whereas ContinuousOutputModel does not. A getInterpolatedDerivatives method should be added, matching the method with the same name in StepInterpolator.",smartshark_2_2,1019,commons-math,"""[0.9974302649497986, 0.0025697790551930666]"""
989,229248,"Remove occurences of ""FunctionEvaluationException""","As per the issue MATH-440, {{FunctionEvaluationException}} is now redundant and is going to removed.
We should remove the ""throws"" clauses that refer to it from all the methods' signature.
",smartshark_2_2,187,commons-math,"""[0.9983804225921631, 0.0016196048818528652]"""
990,229249,"Accessor in ""NeuronSquareMesh2D""","In class {{NeuronSquareMesh2D}} (package {{o.a.c.m.ml.neuralnet}}):
A new method to select a neighbour in a certain ""direction"" (to be defined
with an ""enum"") would be useful (for some visualizations that need the exact location of each neighbour):
{code}
public class NeuronSquareMesh2D {
   // ...

   public enum HorizontalDirection {
       RIGHT, CENTER, LEFT,
   }
   public enum VerticalDirection {
       UP, CENTER, DOWN,
   }

   public Neuron getNeuronNeighbour(int i,
                                    int j,
                                    HorizontalDirection iDir,
                                    VerticalDirection jDir) {
       // ...
   }
}
{code}
",smartshark_2_2,1370,commons-math,"""[0.9984101057052612, 0.0015898804413154721]"""
991,229250,Add scalar multiply to Complex,Adds a scalar multiply method to Complex class,smartshark_2_2,142,commons-math,"""[0.9981472492218018, 0.001852754270657897]"""
992,229251,"""optimization"": Make the maximum number of evaluations a parameter of the ""optimize"" methods","Currently, the number of allowed evaluations is set by the {{setMaxEvaluations}} method.
That parameter should be passed to the {{optimize}} method. The setter will be removed.
",smartshark_2_2,555,commons-math,"""[0.9985272884368896, 0.0014726780354976654]"""
993,229252,"Support writing a linear problem to a commonly used format, e.g. MPS",This would allow users to more easily create test reports by saving their problem and attaching it to an issue.,smartshark_2_2,1184,commons-math,"""[0.9980132579803467, 0.0019867054652422667]"""
994,229253,"Remove ""assert"" from ""MathUtils.equals""","The ""assert"" in methods ""equals(double,double,int)"" and ""equals(float,float,int)"" is not necessary.
",smartshark_2_2,550,commons-math,"""[0.9970990419387817, 0.0029009650461375713]"""
995,229254,Merge interface and implementation for statistical test classes in stat.inference package,"Generally, there is only one implementation of the respective interfaces in this package, so I am proposing that we eliminate the interface and rename the corresponding xxxTestImpl to xxxTest.",smartshark_2_2,439,commons-math,"""[0.9974735379219055, 0.002526500029489398]"""
996,229255,SOFM: Implement various visualization algorithms,"There are various ways for visualizing the properties of a SOFM:
* U-matrix
* Topographic error
* Hit histogram
* Smoothed data histograms
* ...

Some are currently implemented in {{MapUtils}}, but they should be defined in their own class, implementing interfaces proposed in MATH-1268.",smartshark_2_2,975,commons-math,"""[0.9983692765235901, 0.0016307249898090959]"""
997,229256,RandomDataImpl.nextPoisson() is extreme slow for large lambdas,"The RandomDataImpl.nextPoisson() is extreme slow for large lambdas:

E.g. drawing 100 random numbers with lambda = 1000 takes around 10s on my dual core with 2.2GHz.
With lambda smaller than 500 everything is fine. Any ideas?

    RandomDataImpl r = new RandomDataImpl();
    r.reSeed(101);

    int d = 100;
    long poissonLambda = 1000;

    long st = System.currentTimeMillis();
    for (int row = 0; row < d; row++) {
      long nxtRnd = r.nextPoisson(poissonLambda);
    }
    System.out.println(""delta "" + (System.currentTimeMillis() - st));
",smartshark_2_2,628,commons-math,"""[0.950694739818573, 0.049305230379104614]"""
998,229257,"For 4.0, deprecate method name getArgument() and replace with arg()","Currently magnitude of complex is returned by calling abs while phase angle is returned by calling getArgument(). I propose deprecating getArgument and using arg() because:

-- it's not really an accessor method since the argument is computed
-- arg() is shorter, more elegant and similar to abs() with which its usage will sometimes be paired",smartshark_2_2,1073,commons-math,"""[0.9984862208366394, 0.0015137525042518973]"""
999,229258,Implement cKMeans as a clustering algorithm,"cKMeans implementation has been described here
https://cran.r-project.org/web/packages/Ckmeans.1d.dp/index.html and https://journal.r-project.org/archive/2011-2/RJournal_2011-2_Wang+Song.pdf

The algorithm described here is O(kn^2) where k: number of clusters and n: number of 1D points. But, there exists an efficient implementation in later versions of cKMeans which is O(knlogn)

cKMeans is faster than kMeans and also deterministic in nature. It is supposed to be one of the best clustering algorithms for clustering 1D points",smartshark_2_2,1437,commons-math,"""[0.9983324408531189, 0.0016675997758284211]"""
1000,229259,[math] beta test cases,"I changed Beta.java to gracefully handle NaN and invalid domain values per 
Tim's comments.  I also create some test cases to verify the new handling.

I also made some non-code changes to eliminate some checkstyle warnings.",smartshark_2_2,215,commons-math,"""[0.9984219074249268, 0.001578095369040966]"""
1001,229260,Gamma incomplete,"Hello.

In the class ""special.Gamma"", there seem to be no explicit way to obtain the incomplete gamma function.[1]
Could you provide it?

Thanks,
Gilles

[1] http://mathworld.wolfram.com/IncompleteGammaFunction.html
",smartshark_2_2,642,commons-math,"""[0.9938161373138428, 0.006183904595673084]"""
1002,229261,Missing license information,"Several unit tests (Java source files) and test data files lack a license header (cf. ""RAT"" report).
",smartshark_2_2,1390,commons-math,"""[0.997796893119812, 0.0022030898835510015]"""
1003,229262,"Various distance methods in ""Vector3D""","Hi.

As was discussed in the ML (cf. '[math] ""equals"" in ""Vector3D""' thread), it would be useful to add the various distance methods: L1 norm, L2 norm, L-infinity norm.

One suggestion has been to use an enum:

     enum Norm { L_0, L_1, L_2, L_infinity };
     static double distance(Vector3D a, Vector3D b, Norm n) { ... }

Another, to define several methods:

  a.distance1(b) { ... }
  a.distance(b) { ... }
  a.distanceInf(b) { ... }

At the cost of longer typing (but maybe there are other advantages) the above could also be static.

Best,
Gilles
",smartshark_2_2,505,commons-math,"""[0.9983598589897156, 0.0016401071334257722]"""
1004,229263,Add the Jacobi polynomials in the class PolynomialsUtils,"Jacobi polynomials Pk(vw) are a generalization of the legendre polynomial. Legendre Polynomials are Jacobi polynomials for v = w = 0.
They are orthogonal polynomials and are defined from a recursion formula, giving the n+1 element of the polynomial from the nth element and the (n-1)'s one
Therefore, they could be implemented in the PolynomialsUtils and use the generic frame already used in this class to create and store them.
",smartshark_2_2,776,commons-math,"""[0.9984874725341797, 0.0015125335194170475]"""
1005,229264,Add a method in Interval that verifies if a double is inside the Interval,"It would be convenient if class org.apache.commons.math3.geometry.euclidean.oned.Interval had a method such as the one below.

{code}
Here is the method code that may be added to the class.
    /** Verifies if x is inside this Interval.
     * @return true if x is a value between this Interval lower and upper
     * bounds, false otherwise
     */
    public boolean contains(double x) {
        return upper >= x && lower <= x;
    }
{code}

Notice that I'm assuming that an Interval includes its endpoints which may not be true in certain applications. This may imply the need to improve the Interval class (by adding methods or internal state) that describe the inclusion or exclusion of endpoints.",smartshark_2_2,596,commons-math,"""[0.9984044432640076, 0.0015955725684762]"""
1006,229265,Multiple Regression API should allow specification of whether or not to estimate intercept term,The OLS and GLS regression APIs should support estimating models including intercepts using design matrices including only variable data.,smartshark_2_2,52,commons-math,"""[0.9983285069465637, 0.0016714398516342044]"""
1007,229266,"Never propagate a ""NullPointerException"" resulting from bad usage of the API","Package ""exception"" contains a class named ""NullArgumentException"" meant to signal that an argument was ""null"" where it shouldn't have been.
For consistency, every method of the API should be checked for ""null"" arguments and throw ""NullArgumentException"" appropriately.

This means hunting down all unchecked uses of references. Is there a tool that will report those?
",smartshark_2_2,90,commons-math,"""[0.8885633945465088, 0.11143656820058823]"""
1008,229267,Unnecessary Parameter in EigenDecomposition?,"I am still new to the Commons Math code base, so I am sorry if this is a dumb question.  I am browsing the code for EigenDecomposition, and both of its constructors have an unused parameter called splitTolerance.  The comments indicate that this is for backward compatibility.

Would it be acceptable for me to simply include a one-parameter constructor that calls the two-parameter one, so backward compatibility is preserved and client code no longer needs to provide a pointless double?",smartshark_2_2,591,commons-math,"""[0.9980356097221375, 0.001964421244338155]"""
1009,229268,Weighted Mean evaluation may not have optimal numerics,"I recently got this in a test run
{code}
testWeightedConsistency(org.apache.commons.math.stat.descriptive.moment.MeanTest)  Time elapsed: 0 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0.002282165958997601> but was:<0.002282165958997157>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:441)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:178)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:153)
	at org.apache.commons.math.stat.descriptive.UnivariateStatisticAbstractTest.testWeightedConsistency(UnivariateStatisticAbstractTest.java:170)
{code}

The correction formula used to compute the unweighted mean may not be appropriate or optimal in the presence of weights:

{code}
// Compute initial estimate using definitional formula
double sumw = sum.evaluate(weights,begin,length);
double xbarw = sum.evaluate(values, weights, begin, length) / sumw;

// Compute correction factor in second pass
double correction = 0;
for (int i = begin; i < begin + length; i++) {
  correction += weights[i] * (values[i] - xbarw);
}
return xbarw + (correction/sumw);
{code}",smartshark_2_2,758,commons-math,"""[0.8915587067604065, 0.10844124853610992]"""
1010,229269,Multidimensional interpolation,"A robust interpolation algorithm is explained here:
  http://www.dudziak.com/microsphere.php

Although it is probably not very efficient, it seems interesting in that it provides interpolation in N dimensions.
",smartshark_2_2,151,commons-math,"""[0.9981356859207153, 0.001864328165538609]"""
1011,229270,"""AbstractLeastSquares""  cleanup","Class ""AbstractLeastSquares"" (in package ""o.a.c.m.optimization.general"") has a lot of ""protected"" fields that are modified by its subclasses (e.g. ""LevenbergMarquardt""). This decreases encapsulation which in turn makes the code more difficult to understand and modify.
I propose to deprecate (in 3.1) and remove or make ""private"" (in 4.0) all fields that could be hidden or should be modified through setter methods.

Part of the code could also be more readable (and self-documenting) by making use of higher-level constructs such as ""RealMatrix"" rather than using explicit loops.",smartshark_2_2,928,commons-math,"""[0.9983217120170593, 0.001678228029049933]"""
1012,229271,"Changes in ""HarmonicCoefficientsGuesser""","(1) The ""guess"" method throws ""OptimizationException"" when the algorithm fails to determine valid values for amplitude and angular frequency.
There are no test showing how such a situation can occur.
Moreover, since this procedure is used to provide an initial guess to an optimizer, it is better to pick any values for those parameters (e.g. zero) and let the optimizer proceed from that initial point.

(2) The class javadoc seems very thorough in explaining the algorithm, but is quite unreadable in the source code, making it fairly useless for checking how the code complies with the comments. I think that this explanation should go in the user guide (and leave a mostly ""plain text"" outline of the algorithm, referring to the guide for details). [Does the format of the user guide allow such tricky (ASCII ""art"") constructs?]
",smartshark_2_2,707,commons-math,"""[0.8964397311210632, 0.10356026887893677]"""
1013,229272,KMeansPlusPlusClusterer Exception when clusters>variables,"It would be nice when KMeansPlusPlusClusterer generates an exception when the number of cluster is larger than the number of variables. 

KMeansPlusPlusClustererTest:
 /**
     * 2 variables cannot be clustered into 3 clusters.
     */
    @Test
    public void testPerformClusterAnalysisToManyClusters() {
        KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
                new Random(1746432956321l));
        EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
                new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
                new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
        try {
            transformer.cluster(Arrays.asList(points), 3, 1);
            Assert.fail();
        } catch (Exception e) {
            // 2 variables cannot be clustered into 3 clusters.
        }
    }
",smartshark_2_2,537,commons-math,"""[0.7543100118637085, 0.24568995833396912]"""
1014,229273,LevenbergMarquardtOptimizer: able to customize convergence condition,"Currently it is not possible to customize the condition for exiting the iterations in the doOptimize() of LevenbergMarquardtOptimizer. I ran into the scenario where the following condition was already met after 2 iterations, however in this scenario a better result could be achieved after 118 iterations:

Current condition:
                // tests for convergence.
                if (((Math.abs(actRed) <= costRelativeTolerance) &&
                        (preRed <= costRelativeTolerance) &&
                        (ratio <= 2.0)) ||
                        (delta <= parRelativeTolerance * xNorm)) {
                    return new VectorialPointValuePair(point, objective);
                }

Preferred condition:
delta <= 2.2204e-16 * xNorm

Eg. code should probably invoke VectorialConvergenceChecker in AbstractLeastSquaresOptimizer",smartshark_2_2,684,commons-math,"""[0.9924638271331787, 0.007536202669143677]"""
1015,229274,"add a ""rectangular"" Cholesky-like decomposition","The CorrelatedRandomVectorGenerator class uses a kind of rectangular Cholesky-like transform M = B.Bt where B is a rectangular matrix. The difference with respect to a regular Cholesky decomposition is that rows/columns may be permuted (hence the rectangular shape instead of the traditional triangular shape) and there is a threshold to ignore small diagonal elements. This is used for example to generate correlated random n-dimensions vectors in a p-dimension subspace (p < n). In other words, it allows generating random vectors from a covariance matrix that is only positive semidefinite, and not positive definite.

It would be nice to have this decomposition available as a stand-alone class outside of the CorrelatedRandomVectorGenerator.",smartshark_2_2,539,commons-math,"""[0.9984601736068726, 0.0015398337272927165]"""
1016,229275,"""FastMath.atan"" is slow","This issue is related to
  MATH-740
  MATH-901

Micro-benchmarks show that ""FastMath.atan2"" is faster than ""Math.atan2"" but that ""FastMath.atan"" is slower than ""Math.atan"". However, both ""FastMath.atan2"" and ""Math.atan"" call the internal method ""FastMath.atan(double,double,boolean)"".

It seems that some performance improvement could be achieved, through understanding why the results seem contradictory.
",smartshark_2_2,932,commons-math,"""[0.9964158535003662, 0.0035840754862874746]"""
1017,229276,Fields which could be private and/or final,"BaseAbstractUnivariateIntegrator has several fields that are not currently changed after construction and could be final:
protected double absoluteAccuracy;
protected double relativeAccuracy;
protected int minimalIterationCount;
protected Incrementor iterations;
protected Incrementor evaluations;

These all have getters as well, so could also be made private.",smartshark_2_2,1163,commons-math,"""[0.9983956217765808, 0.0016043120995163918]"""
1018,229277,[math] Additional method for Fraction to bring several fractions to common denominator,"I have extended the class org.apache.commons.math.fraction by a method to bring
an  array of fractions to a common denominator. I attach the methods to be added
to org.apache.commons.math.fraction.Fraction and FractionTest directly, since I
do not have SVN, only CVS in use.",smartshark_2_2,98,commons-math,"""[0.9983370304107666, 0.001662924187257886]"""
1019,229278,superfluously null check of SparseIterator.next(),Looking at the implementation of SparseIterator in OpenMapRealVector.OpenMapSparseIterator there is no chance that the entry return by next() is ever null - so there is no need to chek this in nearly every loop?,smartshark_2_2,831,commons-math,"""[0.9974494576454163, 0.0025504622608423233]"""
1020,229279,RegressionResults,"In org.apache.commons.math3.stat.regression.RegressionResults

Two useful additions:
1. Have the ability to estimate y given the model and the X values.
2. It would be nice to be able to print out the ANNOVA table.",smartshark_2_2,1202,commons-math,"""[0.9984560012817383, 0.0015439415583387017]"""
1021,229280,Make RealLinearOperator an interface,"The conjugate gradient package requires only a very limited set of methods to function. In many instances it might not be as convenient to extend an abstract class, rather than just to add an interface to already existing classes. 

From a style point of view, there is no code that all possible implementations of RealLinearOperator would share or even benefit from, so there is no need for an abstract class.",smartshark_2_2,1056,commons-math,"""[0.9981385469436646, 0.0018614260479807854]"""
1022,229281,"New superclass for ""AbstractScalarDifferentiableOptimizer""","In package {{optimization.general}}, I propose to create a new class: ""AbstractScalarOptimizer"".  It would contain all code currently in ""AbstractScalarDifferentiableOptimizer"" that is not related to derivatives. ""AbstractScalarDifferentiableOptimizer"" would then extend that class to add the derivative-related code.

This new layer in the hierarchy will be the base class for algorithms that do not use derivatives.",smartshark_2_2,770,commons-math,"""[0.9983546137809753, 0.0016453467542305589]"""
1023,229282,create solvers for Dfp based function,"When validating some regular computations, we often end up doing some comparison with higher accuracy reference data.
The Dfp package is a wonderful tool for that. This package can already be used in some of our regular algorithms as it already implements the Field interface. However, Dfp instances cannot be used in solvers yet.

Setting up solvers that would take Field based function is not possible, as solvers do need an absolute value method to check convergence, and an ordering relation for bracketing. However, no such restriction exist for Dfp.

It is very easy to convert some existing solvers to have a Dfp version. As the main purpose is high accuracy, and as implementing function for Dfp is cumbersome, a high order method using function values only and no need for implementing derivatives such as depicted in MATH-635 is a good choice.",smartshark_2_2,200,commons-math,"""[0.9985082745552063, 0.001491754548624158]"""
1024,229283,[math] exponential distribution, ,smartshark_2_2,329,commons-math,"""[0.9903939366340637, 0.009606067091226578]"""
1025,229284,Add HAC clustering algorithm,"Add at least one hierarchical clustering algorithm.
With the refactoring of the clustering package, this should now be feasible as the Cluster class can be extended.",smartshark_2_2,243,commons-math,"""[0.9981063604354858, 0.0018936330452561378]"""
1026,229285,Easy conversion from BigDecimal to BigFraction,"Users of BigFraction might also be working with BigDecimal.  Here's a simple way to convert BigDecimal to BigFraction:

{code}
    public static BigFraction bigDecimalToBigFraction(BigDecimal bd) {
        int scale = bd.scale();

        // If scale >= 0 then the value is bd.unscaledValue() / 10^scale
        if(scale >= 0)
            return new BigFraction(bd.unscaledValue(), BigInteger.TEN.pow(scale));
        // If scale < 0 then the value is bd.unscaledValue() * 10^-scale
        return new BigFraction(bd.unscaledValue().multiply(BigInteger.TEN.pow(-scale)));
    }
{code}

It might be nice to have this incorporated into the BigFraction class as a constructor.",smartshark_2_2,1238,commons-math,"""[0.9984568357467651, 0.001543158432468772]"""
1027,229286,"Replace ""package.html"" by ""package-info.java""","The newest (and recommended, I think) way of documenting packages is through ""package-info.java"" files.
CM still uses ""package.html"" files.
",smartshark_2_2,775,commons-math,"""[0.9983110427856445, 0.0016889512771740556]"""
1028,229287,Add documentation to math.ml,"I have written a little piece of documentation for the clustering features in the ML package. I hope its fine - I'm not a native speaker ;) - but I guess it could definitely help new users to know that clustering exists and give them a quick idea how to use it.

",smartshark_2_2,1187,commons-math,"""[0.9981445074081421, 0.0018555066781118512]"""
1029,229288,EnumeratedDistributions should provide constructors taking input datasets,"It should be possible to create an enumerated real or integer distribution from an input array of values, with the frequency of occurrence of a value determining its probability.  For example, {code}EnumeratedIntegerDistribution([0,1,1,2]){code} would be the same as {code}EnumeratedIntegerDistribution([0,1,2], [.25, .5, .25]){code}.",smartshark_2_2,1233,commons-math,"""[0.9984850287437439, 0.0015150136314332485]"""
1030,229289,Random Generator from Stable Distribution,"Stable random generator based on Chambers-Mallows-Stuck method as it is described in ""Handbook of computational statistics: concepts and methods"" by James E. Gentle, Wolfgang HÃ¤rdle, Yuichi Mori",smartshark_2_2,194,commons-math,"""[0.9978005290031433, 0.00219940859824419]"""
1031,229290,Enhance Complex.java,"Add some double shorthand methods to Complex fix different NaN checks in add and subtract ! Testcase  testAddNaN will fail (what should be the result ?)

What is missing JavaDoc and testcases.",smartshark_2_2,547,commons-math,"""[0.9977805018424988, 0.0022195514757186174]"""
1032,229291,HarmonicCoefficientsGuesser.sortObservations() potentlal NPE warning,"HarmonicCoefficientsGuesser.sortObservations()

generates an NPE warning from Eclipse which thinks that mI can be null in the while condition.

The code looks like:
{code}
WeightedObservedPoint mI = observations[i];
while ((i >= 0) && (curr.getX() < mI.getX())) {
    observations[i + 1] = mI;
    if (i-- != 0) {
        mI = observations[i];
    } else {
        mI = null;
    }
}
// mI is not used further
{code}

It looks to me as though the ""mI = null"" statement is either redundant or wrong - why would one want to replace one of the observations with null during a sort?",smartshark_2_2,685,commons-math,"""[0.7172658443450928, 0.2827341556549072]"""
1033,229292,Add quartiles to SummaryStatistics,"Using PSquarePercentile, we can add quartile computation to SummaryStatistics.  Since maintaining quartiles will add some overhead, implementation should allow the feature to be turned off via some kind of constructor flag.  This does open the can of worms regarding turning on / off other stats, which is probably a good thing to think about as the implementation of this feature is developed.",smartshark_2_2,1074,commons-math,"""[0.9983443021774292, 0.0016557200578972697]"""
1034,229293,nonworking code example in User Guide > Statistics > Multiple linear regression,"OLSMultipleLinearRegression example needs to be modified as

- double[][] x (not double[] x)
- regression.newSampleData(y,x); (not regression.newSample(y,x);)",smartshark_2_2,1039,commons-math,"""[0.9930134415626526, 0.00698655191808939]"""
1035,229294,"DecompositionSolver: merging unique ""...Impl"" classes with their interface","From the ML
{quote}
Hi.

The ""...Decomposition"" interfaces in package ""linear"" have a unique
implementation. Should the ""...Impl"" classes be renamed (removing the
interfaces)?


Regards,
Gilles
{quote}",smartshark_2_2,293,commons-math,"""[0.9976198077201843, 0.002380192745476961]"""
1036,229295,"Move ""PerfTestUtils"" to ""src/main"" repository","The class ""org.apache.commons.math3.PerfTestUtils"" is in the ""src/test"" part of the code repository.
It was intended to perform micro-benchmarks and compare alternative implementations. However its location makes it difficult to use by users that rely on automatic dependency resolution.

Having it more visible will hopefully lead to useful suggestions for improving the benchmarking methodology.
",smartshark_2_2,481,commons-math,"""[0.9982129335403442, 0.0017870995216071606]"""
1037,229296,"""RandomDataGenerator"" is brittle","Class {{RandomDataGenerator}} can easily be misused as it advertizes a method to access its internal RNG (which is _not_ thread-safe).

The class is also a mixed bag of ""data generators"" that are either ""secure"" or not.
Moreover it uses the ""lazy initialization"" pattern (for the RNG instance) solely because of this duality; otherwise users that need one or the other form of data generation will obviously always use the RNG since all data generation methods need it.
This entails also a performance hit (albeit tiny) as each call checks whether the RNG has been initialized already.
The clean solution would be to separate the two types of data generation (secure vs not) into different classes.
",smartshark_2_2,1295,commons-math,"""[0.9702755212783813, 0.029724538326263428]"""
1038,229297,Fix some javadoc errors; add @Deprecated annotations to @deprecated methods,"And here is another patch touching up some javadoc problems -- missing or outdated references, and along the way, adding @Deprecated annotations to methods javadoc'ed as @deprecated.",smartshark_2_2,938,commons-math,"""[0.9982547163963318, 0.0017452567117288709]"""
1039,229298,Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign,"Javadoc for ""public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" claims that ""if the values of the function at the three points have the same sign"" an IllegalArgumentException is thrown. This case isn't even checked.",smartshark_2_2,631,commons-math,"""[0.07693393528461456, 0.9230660200119019]"""
1040,229299,Constrained version of the Nelder-Mead simplex method and bi-cubic interpolation,"The library http://www.ee.ucl.ac.uk/~mflanaga/java/ offers a constrained version of the Nelder-Mead simplex method through the addition of a penalty function.  Such a possibility seems to be missing from commons-math.

The same library also offers some bi-cubic interpolation which is also absent in commons-math.",smartshark_2_2,409,commons-math,"""[0.9973481893539429, 0.002651780378073454]"""
1041,229300,Support for Clustering Algorithms,It'd be nice if Commons Math could run K-means or some other clustering algorithms.,smartshark_2_2,32,commons-math,"""[0.99750155210495, 0.0024984024930745363]"""
1042,229301,ODE integrator: different size needed for state vector and tolerance error vector dimension,"The user should be allowed to chose a tolerance vector dimension different from the state vector dimension.
For example, using the FirstOrderIntegratorWithJacibians, we don't want to set some tolerance error for the jacobian part in order to not interfere with the main state vector integration.
This is really a problem with the dimension of the tolerance vector, there is no work-around assigning some particular value to the tolerance vector.",smartshark_2_2,523,commons-math,"""[0.997523844242096, 0.0024761375971138477]"""
1043,229302,"Create a test class for ""JDKRandomGenerator""","{{JDKRandomGenerator}} has no unit test.

Creating a test class that inherits from {{RandomGeneratorAbstractTest}} will require to handle the  exceptions thrown from two of its test methods:
* testNextIntNeg
* testNextIntIAE2

Those expect {{MathIllegalArgumentException}} whereas {{JDKRandomGenerator}} would throw {{IllegalArgumentException}}.",smartshark_2_2,1136,commons-math,"""[0.9982925057411194, 0.0017075251089408994]"""
1044,229303,[Math] Clirr report and 'org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression',"Clirr report states that method 'protected double calculateErrorVariance()' has been added, but in the javadoc no ""@since 2.2"" tag found...",smartshark_2_2,59,commons-math,"""[0.9840392470359802, 0.015960732474923134]"""
1045,229304,General framework for iterative algorithms,"Following the thread [Monitoring iterative algorithms|http://mail-archives.apache.org/mod_mbox/commons-dev/201108.mbox/%3CCAGRH7HrgcgoBA=jcoKovjiQU=TjpQHnspBkOGNCu7oDdKk=K4w@mail.gmail.com%3E], here is a first attempt at defining a general enough framework for iterative algorithms at large. At the moment, the classes provide support for
* maximum number of iterations
* events handling
** initialization event (prior to entering the main loop),
** iteration event (after completion of one iteration),
** termination event (after termination of the main loop).

These classes do not yet provide support for a stopping criterion.
Some points worth to note
* For the time being, the classes are part of the o.a.c.m.linear package.
* For the time being, {{IterativeAlgorithm.incrementIterationCount()}} throws a {{TooManyEvaluationsException}}. If the proposed new feature is integrated into CM, then a proper {{TooManyIterationsException}} should be created, from which the former could derive.",smartshark_2_2,1384,commons-math,"""[0.9983307719230652, 0.0016692627687007189]"""
1046,229305,"""ComplexFormat"" when the imaginary part is equal to ""1"" (as a formatted string)","Currently, for the instance 
{code}
Complex(1, 1)
{code}
the ""format"" method in ""ComplexFormat"" outputs
{noformat}
1 + 1i
{noformat} 
Shall I modify the code so that it will output a nicer
{noformat}
1 + i
{noformat}
instead?
",smartshark_2_2,177,commons-math,"""[0.36718448996543884, 0.6328155398368835]"""
1047,229306,Improve performance of MathArrays.sortInPlace,Performance suffers from lots of copying.,smartshark_2_2,606,commons-math,"""[0.9980260133743286, 0.0019740485586225986]"""
1048,229307,patch for Mantissa,"Here is the patch for Mantissa, it includes the following items:

 - fix a problem when switching functions triggered derivatives discontinuities
 - remove methods and classes that were deprecated in Mantissa
   and do not need to be preserved in commons-math as upward compatibility
   is not a problem for this newly integrated code
 - changed Vector3D and Rotation to immutable classes for ease of use
 - improved some javadoc in class Rotation",smartshark_2_2,22,commons-math,"""[0.9933244585990906, 0.006675612181425095]"""
1049,229308,Add convolution,"I created a function performing (one-dimensional) convolution. Currently, the function is in a class called Filter, but I am not sure whether this is the appropriate place for it(?).",smartshark_2_2,899,commons-math,"""[0.998358428478241, 0.001641507027670741]"""
1050,229309,"Lazy evaluation needed in ""o.a.c.m.fitting.leastsquares""","In ""LocalLeastSquaresProblem"" (private inner class defined in ""o.a.c.m.fitting.leastsquares.LeastSquaresFactory""), the ""evaluate"" method computes the values of both the model and the Jacobian at creation of the ""Evaluation"" instance.
Optimizers (""LevenbergMarquardtOptimizer"" in particular) may not need both for all of the evaluated points. And this can lead to too many evaluations of the model which in some applications is the costliest part.

In my use-case, the current code in ""o.a.c.m.fitting.leastquares"" leads to a performance degradation of about 20% w.r.t. the implementation in ""o.a.c.m.optim.nonlinear.vector.jacobian"".
",smartshark_2_2,1018,commons-math,"""[0.91656094789505, 0.08343898504972458]"""
1051,229310,Current Multiple Regression Object does calculations with all data incore. There are non incore techniques which would be useful with large datasets.,"The current multiple regression class does a QR decomposition on the complete data set. This necessitates the loading incore of the complete dataset. For large datasets, or large datasets and a requirement to do datamining or stepwise regression this is not practical. There are techniques which form the normal equations on the fly, as well as ones which form the QR decomposition on an update basis. I am proposing, first, the specification of an ""UpdatingLinearRegression"" interface which defines basic functionality all such techniques must fulfill. 

Related to this 'updating' regression, the results of running a regression on some subset of the data should be encapsulated in an immutable object. This is to ensure that subsequent additions of observations do not corrupt or render inconsistent parameter estimates. I am calling this interface ""RegressionResults"".  

Once the community has reached a consensus on the interface, work on the concrete implementation of these techniques will take place.

Thanks,

-Greg",smartshark_2_2,348,commons-math,"""[0.9966719150543213, 0.0033280535135418177]"""
1052,229311,Fix and then deprecate isSupportXxxInclusive in RealDistribution interface,"The conclusion from [1] was never implemented. We should deprecate these
properties from the RealDistribution interface, but since removal
will have to wait until 4.0, we should agree on a precise
definition and fix the code to match it in the mean time.

The definition that I propose is that isSupportXxxInclusive means
that when the density function is applied to the upper or lower
bound of support returned by getSupportXxxBound, a finite (i.e. not
infinite), not NaN value is returned.

[1] http://markmail.org/message/dxuxh7eybl7xejde
",smartshark_2_2,1213,commons-math,"""[0.9915387034416199, 0.008461262099444866]"""
1053,229312,Bicubic interpolation,"Second feature requested in [https://issues.apache.org/jira/browse/MATH-353]
",smartshark_2_2,630,commons-math,"""[0.9979088306427002, 0.002091186586767435]"""
1054,229313,Indirect access to instance variables,"In all the methods (except the setters/getters) of

*  BinomialDistributionImpl
*  CauchyDistributionImpl
* ExponentialDistributionImpl
* FDistributionImpl
*  GammaDistributionImpl
*  HypergeometricDistributionImpl
*  TDistributionImpl
*  NormalDistributionImpl
*  WeibullDistributionImpl
*  ZipfDistributionImpl

the instance variables are accessed through their respective getter.
This is confusing (and possibly inefficient).
What would be the expected behaviour of the getter if it were overriden?
",smartshark_2_2,36,commons-math,"""[0.9631118774414062, 0.036888137459754944]"""
1055,229314,Missing state events due to events between t0 and t0+e being ignored,"The Commons Math page on ODEs (http://commons.apache.org/math/userguide/ode.html) states in section 13.3 (Discrete Events Handling), that: ""Note that g function signs changes at the very beginning of the integration (from t0  to t0 + Îµ where Îµ is the events detection convergence threshold) are explicitely ignored. This prevents having the integration stuck at its initial point when a new integration is restarted just at the same point a previous one had been stopped by an event.""

However, due the following issues:
 - MATH-586: Allow using a custom root-finding algorithm to detect state events
 - MATH-599: Re-implementation of Secant-based root finding algorithms

we can now use for instance the PegasusSolver to detect state events. Using the AllowedSolutions.RIGHT_SIDE we can guarantee that we have passed the event. As such, skipping (future) events between t0 and t0+e is not desired.

I attached a Java class to show this issue. It has 2 continuous variables, each starts at 0.0. The first has derivative 1.0, the second 2.0. Whenever they become larger than 1.0, they are reset. We thus expect resets for event 1 at 1.0, 2.0, 3.0, etc. We expect resets for event 2 at 0.5, 1.0, 1.5, etc. The events overlap (at 1.0, 2.0, etc). Due to numerical differences, the events however are not detected at the exact same times. After we processed the first, the 'skip everything between t0 and t0+e' may result in skipping events, as can be observed from the failing unit test. The second test has a hack to get around this problem: it is manually checked whether the guard changes, by evaluating t0 and t0+e. If an event is detected, a step of e is done, and integration is restarted from t0+e. This solves the issue, and the unit tests succeeds (we get the events at the expected times, and we don't miss any events).

From what I understand, event detection is complicated, as discussed in MATH-484. I propose to make the skipping of events betweeen t0 and t0+e optional, as that is no longer needed in the cases I described above, and in fact causes severe problems that can only be solved by hacks. For other (non-bracketed solution) algorithms, it may still be necessary to skip such roots. Maybe an option could be introduced to control this behavior?

So, if an event is detected at time t, integration may continue from t0=t, and if there is a sign change for t0 and t0+e, then the step handler should be called for t0+e, and the step handler should be called for t0+e as well, with isLast=true. I'm not sure what the value of e should be. It could be the absolute accuracy of the root-finding algorithm. if there are multiple ones, maybe the maximum of all of them. Maybe even the minimal integration step should be taken into account, taking the maximum of that an dall the absolute accuracies of the root-finding algorithms?",smartshark_2_2,720,commons-math,"""[0.23972828686237335, 0.7602716684341431]"""
1056,229315,Vector is-not-a Point,"The class hierarchy for geometry claims that Vector is-a Point: https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/geometry/Point.html

This is mathematically incorrect, see e.g. http://math.stackexchange.com/a/645827

Just because they share the same numerical representation, Point and Vector shouldn't be crammed into a common class hierarchy.",smartshark_2_2,1359,commons-math,"""[0.9118630290031433, 0.08813698589801788]"""
1057,229316,NoBracketingException after event was found,"The BracketingNthOrderBrentSolver used by the EmbeddedRungeKuttaIntegrator fails, if an event is detected twice with a NoBracketingException.

The problem lies in line EventState.java line 262 (version 3.1.1). Here the event detection function f is applied to an arbitrary choosen value of time. If the event detector crosses zero before this time, the solver throws the mentioned exception.",smartshark_2_2,883,commons-math,"""[0.07362810522317886, 0.9263719320297241]"""
1058,229317,"Simplify ""DirectSearchOptimizer""","This issue refers to classes in package {{optimization.direct}}.

Currently the class {{NelderMead}} inherits from {{DirectSearchOptimizer}}. However the method {{doOptimize}} is implemented in {{DirectSearchOptimizer}}. This is backwards from the intended design (where an ""optimizer"" is defined as a class that implements a specific algorithm within {{doOptimize}}). According to this ""terminology"", {{DirectSearchOptimizer}} is the optimizer whereas {{NelderMead}} could be considered as a specific way to construct a simplex. Indeed, that's what seems intended since it overrides the abstract method {{iterateSimplex}}.

I suggest to create 2 classes that will make it clear the separation between the ""optimizer"" and the ""simplex manager"".
",smartshark_2_2,179,commons-math,"""[0.9982343912124634, 0.0017656132113188505]"""
1059,229318,Fast Hadamard Transform,"To date, the mathematical transforms package of Commons Maths, org.apache.commons.math.transform, only contains implementations for the Fourier, Sine, and Cosine transforms.

This issue serves to propose and track the creation of an implementation for the Hadamard transform.



Definition of the hadamard transform:
http://en.wikipedia.org/wiki/Hadamard_transform#Definition

Unfortunately, Mathworld does not provide a very detailed definition.
http://mathworld.wolfram.com/HadamardTransform.html

An elegant algorithm for the fast hadamard transform can be found here:
http://www.archive.chipcenter.com/dsp/DSP000517F1.html
",smartshark_2_2,258,commons-math,"""[0.9982789754867554, 0.0017210140358656645]"""
1060,229319,Duplicate code,"In package optimization:

{code:title=SimpleRealPointChecker.java|borderStyle=solid}
public boolean converged(final int iteration, final RealPointValuePair previous, final RealPointValuePair current) {
    final double[] p        = previous.getPoint();
    final double[] c        = current.getPoint();
    for (int i = 0; i < p.length; ++i) {
        final double difference = Math.abs(p[i] - c[i]);
        final double size       = Math.max(Math.abs(p[i]), Math.abs(c[i]));
        if ((difference > (size * relativeThreshold)) && (difference > absoluteThreshold)) {
            return false;
        }
    }
    return true;
}
{code}

{code:title=SimpleVectorialPointChecker.java|borderStyle=solid}
public boolean converged(final int iteration, final VectorialPointValuePair previous, final VectorialPointValuePair current) {
    final double[] p = previous.getPointRef();
    final double[] c = current.getPointRef();
    for (int i = 0; i < p.length; ++i) {
        final double pi         = p[i];
        final double ci         = c[i];
        final double difference = Math.abs(pi - ci);
        final double size       = Math.max(Math.abs(pi), Math.abs(ci));
        if ((difference > (size * relativeThreshold)) &&
            (difference > absoluteThreshold)) {
            return false;
        }
    }
    return true;
}
{code}

Do they do the same thing or am I missing something?

Also in
{code:title=SimpleScalarValueChecker.java|borderStyle=solid}
public boolean converged(final int iteration, final RealPointValuePair previous, final RealPointValuePair current) {
    final double p          = previous.getValue();
    final double c          = current.getValue();
    final double difference = Math.abs(p - c);
    final double size       = Math.max(Math.abs(p), Math.abs(c));
    return (difference <= (size * relativeThreshold)) || (difference <= absoluteThreshold);
}
{code}

it seems overkill that one must create two {{RealPointValuePair}} objects when one just wants to compare two {{double}}. Shouldn't this class contain a method like

{code}
public boolean converged(int iteration, double previous, double current) {
    final double difference = Math.abs(previous - current);
    final double size       = Math.max(Math.abs(previous), Math.abs(current));
    return (difference <= (size * relativeThreshold)) || (difference <= absoluteThreshold);
{code}
?

Also none of these methods seem to need an {{iteration}} parameter.
",smartshark_2_2,680,commons-math,"""[0.9610438942909241, 0.03895612806081772]"""
1061,229320,Let the VectorXD classes in the geometry package implement Clusterable,"It would be quite convenient if the Vector1D, Vector2D and Vector3D classes could be used directly by the clustering algorithms. The updated Clusterable interface now only requires one method:

 * double[] getPoint()

which is already supported by these classes: toArray()

We could let them implement the Clusterable interface by simply delegating the call to getPoint() to toArray().",smartshark_2_2,844,commons-math,"""[0.9982813596725464, 0.001718688989058137]"""
1062,229321,Improve javadoc for iterative linear solvers with preconditioners,"Preconditioning is the replacement of the linear system {{A * x = b}} with {{A * M^(-1) * y = b}}, followed by {{x = M^(-1) * y}}, where {{M}} approximates in some sense {{A}}. There is no consensus in the literature as to whether {{M}} of {{M^(-1)}} should be called the preconditioner.

In {{o.a.c.m3.linear}}, the Javadoc currently states that {{M}} is the preconditioner. However, following MATH-735, the solver must be passed {{M^(-1)}} (not {{M}}!) as a {{RealLinearOperator}}. This makes the whole Javadoc a bit obscure. It would be logical to call preconditioning the replacement of the initial system with {{A * M * y = b}}, where {{M}} approximates in some sense {{A^(-1)}} and will be called the preconditioner.

Such a change will make the javadoc more readable. However, it requires careful review of the existing Javadoc for the following classes
* {{PreconditionedIterativeLinearSolver}},
* {{ConjugateGradient}},
* {{SymmLQ}},
* {{JacobiPreconditioner}},

Also, in {{PreconditionedIterativeLinearSolver}} (and its concrete implementations), the parameter {{minv}} in {{solve()}} should be renamed {{m}}.",smartshark_2_2,785,commons-math,"""[0.9983250498771667, 0.001674957456998527]"""
1063,229322,Making the first and second moments reducable. So should Storeless stats in general.,"The storeless stats in general should be ""Reducable"" to be used in the Map/Reduce framework.",smartshark_2_2,1382,commons-math,"""[0.9985139966011047, 0.0014859598595649004]"""
1064,229323,SimplexOptimizer: short-circuit unnecessary computations,"In {{SimplexOptimizer}}, at line 165, we could break out of the {{for}} loop as soon as variable {{converged}} is {{false}}.",smartshark_2_2,999,commons-math,"""[0.9971617460250854, 0.002838229527696967]"""
1065,229324,Add Semivariance calculation,"I've added semivariance calculations to my local build of commons-math and I would like to contribute them.

Semivariance is described a little bit on http://en.wikipedia.org/wiki/Semivariance , but a real reason you would use them is in finance in order to compute the Sortino ratio rather than the Sharpe ratio.

http://en.wikipedia.org/wiki/Sortino_ratio gives an explanation of the Sortino ratio and why you would choose to use that rather than the Sharpe ratio.  (There are other ways to measure the performance of your portfolio, but I wont bore everybody with that stuff)

I've already got the coding completed along with the test cases and building using mvn site.

The only two files I've modified is src/main/java/org/apache/commons/stat/StatUtils.java and src/test/java/org/apache/commons/math/stat/StatUtilsTest.java
",smartshark_2_2,633,commons-math,"""[0.9983792304992676, 0.0016208004672080278]"""
1066,229325,"Patch: Faster implementation of double org.apache.commons.math3.util.FastMath.copySign(double, double)","Between 40-50% faster than both current implementation and java.lang.Math.copySign by my (sloppy) benchmark. Patch attached.
",smartshark_2_2,411,commons-math,"""[0.9981427192687988, 0.0018573038978502154]"""
1067,229326,"Depend on ""Commons Statistics""","New component focused on statistics:
https://git1-us-west.apache.org/repos/asf?p=commons-statistics.git;a=tree
",smartshark_2_2,1443,commons-math,"""[0.9985895752906799, 0.0014104093424975872]"""
1068,229327,Refactor fixed size chromosomes,"Currently, only chromosomes derived from AbstractListChromosome can be used for most of the crossover policies. This is quite inconvenient to use, especially when working with primitive types.

Need to think about ways to refactor this for better support of other datatypes, e.g. CharSequence or primitive arrays.",smartshark_2_2,931,commons-math,"""[0.9983123540878296, 0.0016877120360732079]"""
1069,229328,Utility method to aggregate SummaryStatistics (was MATH-224),"As per closed issue MATH-224, my project also has a requirement to aggregate SummaryStatistics.  That is, I would like to keep track of the mean \bar{X_i}, and variance of several different random variables { X_1, X_2, .. X_n } as separate distributions with separate means \bar{X}_i, and then later combine these distributions to estimate the mean and variance of the larger combined distribution; eg, the mean and variance of the random variable Y = (X_1 + X_2 .. + X_n) / n.  I am currently using the patch attached to  MATH-224 which works for me, however I would prefer to link with a release version of the library.  Is there any way this issue can be re-opened?",smartshark_2_2,352,commons-math,"""[0.9984322190284729, 0.0015677392948418856]"""
1070,229329,org.apache.commons.math3.exception.TooManyEvaluationsException should return best found result,"The org.apache.commons.math3.exception.TooManyEvaluationsException should contain within it the best found solution so far. There are numerous examples when the optimization might not have converged to the stopping condition but the minimum point is better than the starting point that was provided. The user should have the ability to at least decide if it is good enough, or use it as a starting point into a different optimization run.",smartshark_2_2,832,commons-math,"""[0.998263418674469, 0.00173656374681741]"""
1071,229330,A varied class of Array2DRowRealMatrix is needed to contain float type instead of double.,"The current implementation of Array2DRowRealMatrix takes only double type as its base element value in the matrix.
However, the memory size of double is bigger than float, the downside of which makes the matrix dimension quite limited, compared to float type as its base element type. For small sized problem, this does not make such a big difference, but for large problems, this limits the usability of this library quite severely. In my case, I easily hit an error even after I increase the memory option to 1G. This could have been much more enhanced just by using 'float[][]' instead of the current Array2DRowRealMatrix.

Therefore, the solution I may suggest is to add another class similar to Array2DRowRealMatrix containing float type for its matrix variable instead of double. Of course, a better way is welcome as long as the needs can be fulfilled.",smartshark_2_2,1085,commons-math,"""[0.9872310757637024, 0.012768884189426899]"""
1072,229331,High order Brent-like bracketing solver,"The new bracketing solvers greatly improve usage. Unfortunately, for now there are only secant-based bracketing solvers available, and a wrapper which basically ends by adding a few secant steps to regular non-bracketing solvers.

Changing the Brent solver to provide bracket selection on the final result would depart from the standard algorithm, which is a wrong move (see MATH-599 for a similar case recently resolved).
It would be nice to set up another solver in the same spirit as Brent (i.e. using inverse polynomial interpolation when possible and falling back to dichotomy) while retaining bracketing. A nice and simple improvement is also to use higher order inverse polynomial interpolation by retaining several previous points. This allows to build a solver that have an higher order than Newton for example but still only needs function values and no derivatives at all.",smartshark_2_2,83,commons-math,"""[0.9983550906181335, 0.001644911477342248]"""
1073,229332,FTPClient.setPassiveNatWorkaround assumes host is outside site local range,"We have a NAT firewall between two ""site local"" 10.x networks. The effect is that the FTP library tries to make data connections to the wrong host because the passive NAT workaround doesn't operate if the FTP connection is made to a ""site local"" private address and the host returned in the PASV reply is also ""site local"".

I see that Damon Dan references pretty much the exact issue within bug NET-363 when the workaround was originally introduced.

Users with ""site local"" networks would be quite at liberty to subnet within the network, I guess, to suit their administrative needs, so this seems like a valid issue.

Options I can see:
1) Include a way of forcing the workaround in place
2) Remove the selectivity around rewriting the host only if the PASV reply is ""site local"" and original host isn't... Issue here is around a server that has multiple endpoints for data connections?
3) Allow the user to specify their own data host via API
4) Check for whether the PASV reply address is in a different subnet to the original host we connected to and apply the workaround if so

I haven't yet identified a workaround within the current code!",smartshark_2_2,604,commons-net,"""[0.09162041544914246, 0.9083795547485352]"""
1074,229333,NullPointerException when disconnecting TelnetClient twice with JDK 7,"When using the TelnetClient class, a {{NullPointerException}} may occur when calling the {{disconnect}} method twice, in the {{_closeOutputStream}} method called under the hood, if the Telnet connection is lost (for instance, server is hardly shut down).

1. The first call to {{disconnect}} resets completely the TelnetClient instance.
2. The second call to {{disconnect}} leads to the NPE exception, because the {{\_output\_}} property is {{null}}, in the {{_closeOutputStream}} method.

*NOTE: the NPE does not occur with JDK 8, because, the first call to {{disconnect}} throws an I/O exception (socket is closed), leaving the TelnetClient instance with a non-null {{\_output\_}} property. Then a second call to disconnect does not throw a NPE. It seems the JDK 8 behaves differently when a client socket loses a connection. So there is also a bug with JDK 8, as disconnection shall close quietly resources without an I/O exception, and without leaving non-null resources, and then disconnect the client socket. The {{SocketClient.disconnect}} is a good implementation to start with.*

The problem is that the TelnetOutputStream class closes the Socket output stream under the hood, but doesn't check if it is null and doesn't reset it to null once done. _The implementation of the TelnetOutputStream is quite strange, as there is a cycling dependency between this class and the TelnetClient class. The {{TelnetClient}} class shall handle itself the close of its internal resources, and disconnect the client socket. But this responsibility is delegates to the TelnetOutputStream._

Here's the stack trace of the NPE exception:
{quote}
|java.lang.NullPointerException
     at org.apache.commons.net.telnet.TelnetClient._closeOutputStream(TelnetClient.java:83)
      at org.apache.commons.net.telnet.TelnetOutputStream.close(TelnetOutputStream.java:163)
      at org.apache.commons.net.telnet.TelnetClient.disconnect(TelnetClient.java:124)
{quote}

A way to workaround this bug, is to always check if the {{TelnetClient}} instance is connected, before calling the {{disconnect}} method.
",smartshark_2_2,599,commons-net,"""[0.07420859485864639, 0.9257913827896118]"""
1075,229334,"Failure to parse times from SYST_L8 systems that report as ""WINDOWS Type: L8""","When getting file entries from a Type: L8 system running on a Windows server the code cannot extract the timestamp of the file correctly (it returns null).

The entry format returned by the server is as follows:
-rwxrwxrwx    1 user     group             2490 Sep    7 2016 file.txt

The reason for this appears to be a bug in the way the DefaultFTPFileEntryParserFactory constructs the CompositeFileEntryParser (createNTFTPEntryParser method), as when it passes the config object through to the NTFTPEntryParser first, the config passed through is updated with the Default timestamp format for Windows FTP servers ""MM-dd-yy hh:mma"", and then when the same config object is passed to the UnixFTPEntryParser it picks up this default and tries to use it instead of what it should be which is ""MMM d yyyy"".

Not sure when this bug was introduced but it is at least present in 3.3 and 3.5.

The problem may also be present for the createOS400FTPEntryParser, though I have not confirmed this.

Potential Solution:
When passing the config through to the parsers for each part of the CompositeFileEntryParser they should be using a clone of the original to avoid this kind of cross-contamination between different parser types.",smartshark_2_2,597,commons-net,"""[0.08210299164056778, 0.9178970456123352]"""
1076,229335,Telnet does not convert LF to CRLF in ASCII mode,"Hi,

We are using your library to connect/login to some Windows and Foundary devices via Telnet.
The source(client) is a Windows 7 professional windows machine and all the servers we are trying to connect are Windows machines running Microsoft(R) Windows(R) Server 2003, Standard Edition.
In version 2.2 we were able to connect and then login successfully to these devices with no issues.
However, on upgrade to 3.3 version, we are unable to login to these devices successfully.
In v 3.3, after entering the username and hitting enter, there is no response at all. However in v 2.2, as soon as you enter the username and hit enter, password prompt comes back for us to enter the password and we are logged in successfully. 

I compraed the packet captures between the two versions for the same target server machine. The only difference I can see is when the user name is set, in v2.2 a '\r' is appended to the end of the username where is v 3.3, this '\r' is missing. Is this due to any change in library?

I can provide the packet capture/logs if required. Please let us know if this is a known issue or if there are any workarounds. 



Thanks
-Pradeep",smartshark_2_2,532,commons-net,"""[0.14226123690605164, 0.8577387928962708]"""
1077,229336,Incorrect error handling in method initiateListParsing of FTPClient,"In method initiateListParsing of FTPClient, errors when opening the data connection are not correctly handled.
This leads to ignore message delivered by the server and further operation with the FTPClient instance are not valid.

As an example, considere the attached unit test. When such an error is met, using the _sendNoOp_ command returns false.
Here is an output of the test:
{noformat}
F:\dev\jakarta\>run org.apache.commons.vfs.provider.ftp.FTPClientUnitTest 10.0.0.x 21 /GILLES/source usr pass 5500
Ignoring error when creating client at iteration: 1152
Error at iteration: 1155
java.net.SocketTimeoutException: Accept timed out
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:384)
        at java.net.ServerSocket.implAccept(ServerSocket.java:450)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at org.apache.commons.net.ftp.FTPClient._openDataConnection_(FTPClient.java:489)
        at org.apache.commons.net.ftp.FTPClient.initiateListParsing(FTPClient.java:2296)
        at org.apache.commons.net.ftp.FTPClient.initiateListParsing(FTPClient.java:2269)
        at org.apache.commons.net.ftp.FTPClient.listFiles(FTPClient.java:2046)
        at org.apache.commons.vfs.provider.ftp.FTPClientUnitTest.testConnectTimeoutInList(FTPClientUnitTest.java:70)
        at org.apache.commons.vfs.provider.ftp.FTPClientUnitTest.main(FTPClientUnitTest.java:110)
Exception in thread ""main"" junit.framework.AssertionFailedError: NOOP failed at iteration 1155
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at org.apache.commons.vfs.provider.ftp.FTPClientUnitTest.testConnectTimeoutInList(FTPClientUnitTest.java:75)
        at org.apache.commons.vfs.provider.ftp.FTPClientUnitTest.main(FTPClientUnitTest.java:110)
{noformat}
",smartshark_2_2,289,commons-net,"""[0.09784851968288422, 0.9021515250205994]"""
1078,229337,StackOverflowError in Threader,"The loop detection code in Threader.buildContainer() is currently:
                // Link references together in the order they appear in the References: header,
                // IF they dont have a have a parent already &&
                // IF it will not cause a circular reference
                if ((parentRef != null)
                    && (ref.parent == null)
                    && (parentRef != ref)
                    && !(parentRef.findChild(ref))) {
                    // Link ref into the parent's child list
Deep circular references where 'parentRef' is already a child of 'ref' and ref is the root container are possible.
The test should be:
               if ((parentRef != null)
                    && (ref.parent == null)
                    && (parentRef != ref)
                    && !(ref.findChild(parentRef))) {
                    // Link ref into the parent's child list
",smartshark_2_2,352,commons-net,"""[0.06720732897520065, 0.9327926635742188]"""
1079,229338,SubnetUtils does not handle /31 and /32 CIDRs well,"SubnetUtils does not handle /31 and /32 CIDRs well.

This should be documented and/or fixed.

192.168.0.1/32 causes an IllegalArgumentException to be thrown (by the way, that should show the upper and lower bounds)

192.168.0.3/31 gives 
addressCount=0, 
low =192.168.0.3 
high=192.168.0.2
and 192.168.0.3 is rejected as not in range",smartshark_2_2,271,commons-net,"""[0.15073345601558685, 0.849266529083252]"""
1080,229339,KeyManager and TrustManager not used for data socket,"When setting the KeyManager and TrustManager for FtpsClient, it will not be used for the data connection. In addition, the current code eats an exception and just dumps it to stderr. ",smartshark_2_2,170,commons-net,"""[0.13334736227989197, 0.8666526079177856]"""
1081,229340,FTPClient.CSL.cleanUp() fails to restore timeout value on exception,"The CSL class temporarily resets the socket timeout.

The cleanUp() method restores the original setting, however this may be skipped if an exception occurs.

The code should ensure that the original setting is always restored.",smartshark_2_2,485,commons-net,"""[0.1008150652050972, 0.8991849422454834]"""
1082,229341,Regression: TelnetInputStream#available() blocks,"When testing with Commons Net 3.1 for Eclipse https://bugs.eclipse.org/bugs/show_bug.cgi?id=194473 , I found that our telnet client blocks when the ReaderThread is running and waiting for new data. Investigation shows that our code blocks on TelnetInputStream#available().

This regression is due to the code introduced for NET-437 ""TelnetInputStream doesn't support non-blocking IO when reader thread is not enabled"":

TelnetInputStream#available() now calls super.available() which is declared as ""synchronized"" on BufferedInputStream in JDK 1.6.0_21 at least. But at the same time, the telnet ReaderThread has already the Monitor on BufferedInputStream and doesn't give it up while sitting in read0().

This seems to be exactly the situation that the comment before TelnetInputStream#close() warns about:
    // Cannot be synchronized.  Will cause deadlock if run() is blocked
    // in read because BufferedInputStream read() is synchronized.

This is a severe issue since it violates the specification and use of available().",smartshark_2_2,452,commons-net,"""[0.10360627621412277, 0.8963937759399414]"""
1083,229342,"Exception for new SubnetUtils(""0.0.0.0/0"")","The following example in SubnetUtils throws an exception:

{code:java}
SubnetUtils utils = new SubnetUtils(""0.0.0.0/0"");
{code}

As '0.0.0.0/0' is within the IPv4 specification, it should be supported.",smartshark_2_2,531,commons-net,"""[0.41881126165390015, 0.5811887979507446]"""
1084,229343,FTPSClient not properly supporting CCC and PROT P,"FTPSClient does not behave properly after issuing CCC (Clear Command Channel). Proper behaviour is to close SSLSocket, but keep underlying connection without SSL open.
To achieve this, the SSLSocket should be created with ""false"", like this on line 255 (of FTPSClient v2.2)

SSLSocket socket =
(SSLSocket) ssf.createSocket(_socket_, ip, port, false);

Furthermore, on sendCommand CCC, sslSocket must be closed before setting _socket = _plainsocket on line 493:
   _socket.close();
   _socket = _plainsocket;
   ...


And finally, it is wrong to set socket factory to null on line 500 of the same method; this is set properly in exexPROT and should not be reset on CCC.







",smartshark_2_2,327,commons-net,"""[0.1156417727470398, 0.8843582272529602]"""
1085,229344,SMTP.getReplyStrings returns array of nulls,"The SMTP.getReplyStrings method returns an array of nulls, as it doesn't copy the internal list to the result String[] correctly.",smartshark_2_2,237,commons-net,"""[0.08319568634033203, 0.916804313659668]"""
1086,229345,IMAP fails to quote/encode mailbox names,"Mailbox names need to be quoted if they contain spaces or double-quote.

If they contain non-ASCII characters then they also need to be encoded [1]

There may be other parameters that are not being encoded/quoted correctly.

[1] https://tools.ietf.org/html/rfc3501#section-5.1.3",smartshark_2_2,659,commons-net,"""[0.07686059921979904, 0.9231393337249756]"""
1087,229346,"Add support for FTP commands MLSD, MLST","MLSD and MSLT are intended for machine parsing of listings, so if the server supports the features it would be beneficial to use them.",smartshark_2_2,314,commons-net,"""[0.9983453750610352, 0.0016546753467991948]"""
1088,229347,FTPClient - support for processing arbitrary commands that only use the control channel,"For FTP commands that don't require a separate data channel, it would be useful to have methods that issued the command and returned the response.

This would allow simpler access to commands which are not yet supported.",smartshark_2_2,319,commons-net,"""[0.998384952545166, 0.001615022076293826]"""
1089,229348,"Problem with  private OutputStream __storeFileStream(int command, String remote) in org.apache.commons.net.ftp.FTPClient","private OutputStream __storeFileStream(int command, String remote)
        throws IOException
    {
        Socket socket;
        if((socket = _openDataConnection_(command, remote)) == null)
            return null;
        OutputStream output = socket.getOutputStream();
        if(__fileType == 0)
        {
            output = new BufferedOutputStream(output, 1024);
            output = new ToNetASCIIOutputStream(output);
        }
        return new SocketOutputStream(socket, output);
    }

This method in FTPClient.java is called by the method storeFileStream(String str). At line number 6 in the above mentioned method, it returns null when unable to open DataConnection which could be because of concurrent file access issues. In such cases instead of returning null, this method should throw an IOException like:

Socket socket;
        if((socket = _openDataConnection_(command, remote)) == null) throw new IOException;

As of now in my code i have used a null check to avoid this. However i could not understand why an outputstream creation method will return a null reference and not throw an IOException. It was hard to believe that I was getting null pointer exception while getting an output stream reference.",smartshark_2_2,374,commons-net,"""[0.12428183853626251, 0.8757181763648987]"""
1090,229349,API changes report for Commons Net,"The review of API changes for the Commons Net library since 1.0.0 version: https://abi-laboratory.pro/java/tracker/timeline/commons-net/

Hope it will be helpful for users and maintainers of the library. The report is generated by the https://github.com/lvc/japi-tracker tool for jars found at http://central.maven.org/maven2/commons-net/commons-net/ according to https://wiki.eclipse.org/Evolving_Java-based_APIs_2.

Thank you.

!commons-net-2.png|API symbols timeline!
!commons-net-1.png|API changes review!",smartshark_2_2,635,commons-net,"""[0.9983835220336914, 0.0016165146371349692]"""
1091,229350,"mlistDir doc should be ""MLSD"" not ""MSLD""","In FTPClient.class ""Generate a directory listing for the current directory using the MSLD command."" should use ""MLSD"" not ""MSLD"".",smartshark_2_2,400,commons-net,"""[0.9941310286521912, 0.005868936888873577]"""
1092,229351,mlist hangs - but FileZilla and winscp succeed,"It might be that the www.fonoteca.ch:990 is not properly set up for passive mode (winscp says: ""Using host address 62.2.199.158 instead of the one suggested by the server: 192.168.5.17"")

See fileZilla log in https://sourceforge.net/p/jsch/bugs/71/ 

Thread [main] (Suspended)	
	PlainSocketImpl.socketAccept(SocketImpl) line: not available [native method]	
	SocksSocketImpl(AbstractPlainSocketImpl).accept(SocketImpl) line: 398	
	SSLServerSocketImpl(ServerSocket).implAccept(Socket) line: 530	
	SSLServerSocketImpl.accept() line: 317	
	FTPSClient(FTPClient)._openDataConnection_(String, String) line: 832	
	FTPSClient._openDataConnection_(String, String) line: 600	
	FTPSClient(FTPClient)._openDataConnection_(FTPCmd, String) line: 759	
	FTPSClient(FTPClient).initiateListParsing(FTPFileEntryParser, String) line: 3293	
	FTPSClient(FTPClient).initiateListParsing(String, String) line: 3271	
	FTPSClient(FTPClient).listFiles(String) line: 2930	
	FTPSClient(FTPClient).listFiles() line: 2977	

",smartshark_2_2,499,commons-net,"""[0.14028172194957733, 0.8597182631492615]"""
1093,229352,522 Data connections must be encrypted.,"1. I tried FTPSExample.java
2. My parameters will try to download a file securely from an ubuntu vsftpd server with the following secure configuration 

ocal_enable=YES
rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
ssl_enable=YES
force_local_logins_ssl=YES
force_local_data_ssl=YES
ssl_tlsv1=YES
ssl_sslv2=YES
ssl_sslv3=YES

3. When I run the test, i got the following message from the console
522 Data connections must be encrypted.

4. I checked the file downloaded and it's size is zero.

5. I tried reconfiguring the ftp server and set the parameter force_local_data_ssl=NO, then tried the test again. this time the download is successful.

is this a bug, that when we force data connection to use SSL will always fail?

",smartshark_2_2,187,commons-net,"""[0.5265825986862183, 0.4734174609184265]"""
1094,229353,FTPHTTPClient should use socket factory to create sockets,"FTPHTTPClient can't set socket timeout when connecting to remote http proxy. This may cause hangouts for calls to remote http ftp proxies.

Expected: FTPHTTPClient  supports creation sockets with Factory methods. We had same approach for regular FTPClient calling .setSocketFactory(...) method and have control over socket creation.
Actual: FTPHTTPClient  creates Socket inside public void connect(...) directly, and it is impossible to set timouts since this object will be used immediately. It is even impossible to inherit from FTPHTTPClient   to override connect - since proxyHost, proxyPort and tunnelHandshake is private.

so we may change 
      _socket_ = new Socket(proxyHost, proxyPort);
to 
     _socket_ = _socketFactory_.createSocket();

and add support for FTPHTTPClient  to handle socket factories for http proxies calls.

Is it possiible?

",smartshark_2_2,549,commons-net,"""[0.9642747640609741, 0.035725221037864685]"""
1095,229354,Javaoc for new classes in 2.0 should have @since 2.0,It would be helpful to identify the classes that are new to 2.0,smartshark_2_2,173,commons-net,"""[0.9958369731903076, 0.004162984900176525]"""
1096,229355,System Information Leak in ftp parser,"Exception is printed to console in src/main/java/org/apache/commons/net/ftp/parser/MVSFTPEntryParser.java which can leak system information:
{code}
    private boolean parseMemberList(FTPFile file, String entry) {
        if (matches(entry)) {
            file.setRawListing(entry);
            String name = group(1);
            String datestr = group(2) + "" "" + group(3);
            file.setName(name);
            file.setType(FTPFile.FILE_TYPE);
            try {
                file.setTimestamp(super.parseTimestamp(datestr));
            } catch (ParseException e) {
                e.printStackTrace();
                // just ignore parsing errors.
                // TODO check this is ok
                return false; // this is a parsing failure too.
            }
            return true;
        }

        return false;
    }
{code}

",smartshark_2_2,606,commons-net,"""[0.1338208168745041, 0.8661791086196899]"""
1097,229356,SocketClient should ensure input and output streams are closed,"From the Java 6 SDK, the socket.close() method is responsible for also closing its input and output streams. But if socket.close() throws an IOException before the streams could be closed, the streams will remain open and they will also not be set to null. We need a way to ensure that the input and output streams are also closed. There are various ways to achieve this:

Proposal 1:
Implement a public SocketClient.paranoidDisconnect() method that makes sure the socket, input and output are closed:
public void paranoidDisconnect() {
try { this.disconnect(); }
catch (IOException ioe) {
// the first thing that SocketClient.disconnect() does is to close the socket
// but if that fails, we have to manually close the input and output streams
if (this.input != null) {
try { this._input_.close(); }
catch (IOException ioe2) {
}
}

if (this.output != null) {
try { this._output_.close(); }
catch (IOException ioe3) {
}
}
}
finally { this._socket_ = null; this._input_ = null; this._output_ = null; }
}

Proposal 2:
Expose the socket, input and output stream objects with getter methods at the SocketClient level, handing the responsibility over to the calling application. TelnetClient already exposes the input and output streams with getInputStream() and getOutputStream() methods respectively.",smartshark_2_2,195,commons-net,"""[0.6977456212043762, 0.302254319190979]"""
1098,229357,IMAP FETCH example,It would be useful to have an IMAP FETCH example that can demonstrate the chunked listener.,smartshark_2_2,558,commons-net,"""[0.9979875087738037, 0.00201251613907516]"""
1099,229358,"How to get time values (hour, min, sec) using FileFTP.getTimestamp() method","Hello,

I am using commons-net-2.0 in my application to access FTP server.

My aim is to get value of ""last modified date/time"" value for any file present on FTP Server.

The FTP Server resides at different location and I don't have direct access to it.

Beloew are some lines I can see when I connect to FTP Server using ftp client software.
220 <server-name> X2 WS_FTP Server 5.0.4 (2790816660)  
USER <userid>  
331 Password required  
PASS **********  
230 user logged in  
SYST  
215 UNIX

Though it was listed ""UNIX"", I confirmed with my team that actual operating system on FTP Server is Windows 2000.

To achieve my aim, I used getTimestamp() method of FileFTP class.

I have used default settings. Even I have not configured FTP server using FTPClientConfig.

The problem is - the Calendar object returned by invoking getTimestamp() method only contains correct date (day, month & year) value. It does not contain time parts (hour, min, sec, etc.). When I tried to print date in yyyy-MM-dd HH:mm:ss format, it printed zeros for HH, mm & ss parts (e.g. 2007-03-26 00:00:00). Any idea on how to get date value value along with time parts? Am I missing some other configurations?

Note: If above option fails, I may plan to use getModificationTime() method of FTPClient class.

Thank you,
Jignesh",smartshark_2_2,188,commons-net,"""[0.9133656024932861, 0.08663446456193924]"""
1100,229359,[POP3] [Solution] The POP3 client does not support SSL/TLS connections,The POP3Client class does not support automatic SSL negotiation when connecting to the POP3S port and it does not support TLS negotiation when connecting to the standard POP3 port.,smartshark_2_2,332,commons-net,"""[0.7163214087486267, 0.2836785316467285]"""
1101,229360,Option to override SSL negotiation,"The following use case. Decide upon the server features if the connection should be TLS secured or not. Therefore A FTPSClient is instantiated and the SSL negotiation methods overridden to decide wether to pass through or not depending if AUTH TLS is available in the features list for example.

Patch attached making methods execAUTH, sslNegotiation protected.",smartshark_2_2,357,commons-net,"""[0.9983645081520081, 0.0016354868421331048]"""
1102,229361,[contribution] DNS utils,"Hi all,
I developed DNS methods for resolving IP address by host name and vice verse; receiving mail exchange and authoritative name servers for domain; and receiving other information about domain from DNS server.

JDK provides above functionality in JNDI DNS service provider [1]. But JNDI technology is too abstract and obfuscates communication with DNS servers, which leads to users don't use JNDI DNS SP.

Attached classes provide evident API for receiving information from DNS server as wrapper for JNDI DNS SP. Also these classes provide necessary additional parsing of answers.

Examples how to use created API can be found in DNSClientTest unit test. This library requires only JDK and JUnit to run the test.

[1] http://java.sun.com/javase/6/docs/technotes/guides/jndi/jndi-dns.html",smartshark_2_2,191,commons-net,"""[0.9984402060508728, 0.0015598470345139503]"""
1103,229362,[SMTP] [Solution] The SMTP client does not support SSL/TLS connections,The SMTPClient class does not support automatic SSL negotiation when connecting to the SMTPS port and it does not support TLS negotiation when connecting to the standard SMTP port.,smartshark_2_2,283,commons-net,"""[0.716060221195221, 0.28393983840942383]"""
1104,229363,OS400FTPEntryParser,Attached you will find the FTPEntryParser usable for OS/400.,smartshark_2_2,117,commons-net,"""[0.9837733507156372, 0.016226666048169136]"""
1105,229364,Telnet client: Support Client-initiated Subnegotiation Messages,"I am in the process of trying to implement a Java client for [RFC 2217|http://tools.ietf.org/html/rfc2217], which is the protocol for accessing serial ports over TCP. Unfortunately, the commons-net telnet client is insufficient for this relatively simple task.

There are two missing features in the commons-net telnet client code, one of which is a show stopper and the other of which would be a ""real nice to have"". This issue documents the first problem:

RFC 2217 specifies that serial port events (such as server notifiying about a change in carrier detect, or the client instructing the server to change the baud rate) are delivered to the peer by subnegotiations. For example, to notify the client about carrier detect, the server sends {{IAC SB COM-PORT-OPTION NOTIFY-MODEMSTATE <value> IAC SE}} to the client; to set the serial port baud rate, the client sends {{IAC SB COM-PORT-OPTION SET-BAUD <value(4)> IAC SE}} to the server. These messages can happen at any time and are not associated with any WILL/WONT/DO/DONT negotiation (according to my understanding).

The problem is that while one can _receive_ such messages via {{TelnetOptionHandler.answerSubnegotiation()}}, the {{TelnetClient}} class doesn't provide any way to _send_ (i.e., intiate) these messages. 

What's needed here is simply to expose {{Telnet._sendSubnegotiation()}} (properly renamed, etc.) as a public method.

I'm going to attempt to come up with a patch and will attach it here if successful.
",smartshark_2_2,350,commons-net,"""[0.9978095889091492, 0.002190440194681287]"""
1106,229365,Add shorthand FTPClientConfig constructor,"Classes that instantiate FTPClientConfig mostly only need to configure the date format strings, and don't need to set the server language etc, so these are passed as null.

It would simplify the code to add a shorter form constructor omitting the rarely used parameters",smartshark_2_2,603,commons-net,"""[0.9983718991279602, 0.0016280758427456021]"""
1107,229366,"Move class ""ThreadContainer"" from Threader.java into its own source file","In the commons-net-2.0.zip binary release, find commons-net-2.0.jar

This JAR contains ""org.apache.commons.net.nntp.ThreadContainer.class"" which has no counterpart in any of the released source archives (I checked commons-net-2.0-sources.jar which is included in the binary dist as well as the original source dist). Looking in SVN, the source is also not there: http://svn.apache.org/viewvc/commons/proper/net/trunk/src/java/org/apache/commons/net/nntp/

I find this issue problematic, because if I am unable to check the provenience of that binary file, I might be introducing license problems. Just assume that the problematic ""ThreadContainer.class"" contains GPL'd code, I might end up running into severe issues.
",smartshark_2_2,234,commons-net,"""[0.998342752456665, 0.0016572183230891824]"""
1108,229367,FTPClient in PASSIVE_LOCAL_DATA_CONNECTION_MODE cannot work when host have several different IP,"When host have several different IP and client setting to PASSIVE_LOCAL_DATA_CONNECTION_MODEï¼_openDataConnection_ creating socket may use the unexpected ipã
For example, The host have two Ips, one for intra and the another for extern, only the extern one can be connect with the outsideãBut when in PASSIVE_LOCAL_DATA_CONNECTION_MODE, FTPClient may be select the intra Ip, So it didn't workã
So I change code like this, it can work normally by the specific ip.
Code:
In FTPClient._openDataConnection_(String command, String arg)ï¼line 761 change as bellow:
socket = _socketFactory_.createSocket();
            
// add begin
// local as client, transfer data must use the appointed local host
socket.bind(new InetSocketAddress(getHostAddress(), 0));
// add end

After log in success,User can set the ip which he want to use by call the method in FTPClient:
setActiveExternalIPAddress(String ip); 
",smartshark_2_2,457,commons-net,"""[0.373810350894928, 0.626189649105072]"""
1109,229368,Check Pure-FTPd FTPES (explicit TLS),"Hi,

With Pure-FTPd, commons-net3.5, and FTPES (explicit TLS) I get ""Connection closed without indication.""

I have found some old bug with similar behaviour:

https://issues.apache.org/jira/browse/NET-354

I have run  the CCCTester.java:

https://issues.apache.org/jira/secure/attachment/12476247/CCCTester.java

with some minor changes:

ftps = new FTPSClient(""TLS"", false);
....
ftps.enterLocalPassiveMode();     
if (!FTPReply.isPositiveCompletion(ftps.getReplyCode()))
    throw new FTPException(FTPException.kPassiveModeRefused);    
.....
int port = 21;

This is the output:

REPLY: 220---------- Welcome to Pure-FTPd [privsep] [TLS] ----------
220-You are user number 1 of 50 allowed.
220-Local time is now 02:28. Server port: 21.
220-This is a private system - No anonymous login
220-IPv6 connections are also welcome on this server.
220 You will be disconnected after 15 minutes of inactivity.
COMMAND: AUTH TLS
REPLY: 234 AUTH TLS OK.
COMMAND: USER awrtesting@webweaving.com
REPLY: 331 User awrtesting@webweaving.com OK. Password required
COMMAND: PASS **********
REPLY: 230 OK. Current restricted directory is /
COMMAND: PBSZ 0
REPLY: 200 PBSZ=0
COMMAND: PROT P
REPLY: 200 Data protection level set to ""private""
COMMAND: CCC
REPLY: 200 Control connection unencrypted
COMMAND: SYST
REPLY: 215 UNIX Type: L8
COMMAND: PASV
REPLY: 227 Entering Passive Mode (209,188,80,204,117,54)
COMMAND: LIST
REPLY: 150 Accepted data connection
org.apache.commons.net.ftp.FTPConnectionClosedException: Connection closed without indication.
	at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:316)
	at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:292)
	at org.apache.commons.net.ftp.FTP.getReply(FTP.java:712)
	at org.apache.commons.net.ftp.FTPClient.completePendingCommand(FTPClient.java:1857)
	at org.apache.commons.net.ftp.FTPClient.initiateListParsing(FTPClient.java:3420)
	at org.apache.commons.net.ftp.FTPClient.initiateListParsing(FTPClient.java:3335)
	at org.apache.commons.net.ftp.FTPClient.listFiles(FTPClient.java:3012)
	at org.apache.commons.net.ftp.FTPClient.listFiles(FTPClient.java:3065)
	at com.caphyon.utils.CCCTester.test(CCCTester.java:47)
	at com.caphyon.utils.CCCTester.main(CCCTester.java:106)

Can you please check this too?

Best Regards,
Stefan Matei",smartshark_2_2,573,commons-net,"""[0.7531992197036743, 0.2468007504940033]"""
1110,229369,Should Telnet class Exception blocks write to System.err?,"[Extracted from NET-194]

Should Telnet class Exception blocks write to System.err?",smartshark_2_2,252,commons-net,"""[0.9971938133239746, 0.0028061899356544018]"""
1111,229370,Payload of events sent to current scxml session using <send> tag not injected into engine,Events which are sent to the current scxml session using the <send> tag (i.e. target = empty) may contain payload specified by the namelist attribute. This payload is currently not fed back into engine when the event is created.,smartshark_2_2,241,commons-scxml,"""[0.12338076531887054, 0.8766192197799683]"""
1112,229371,Bug in parallel state with onentry element,"Hi Rahul,

in <parallel> section, a <onentry> element have to work also with context variables
(http://www.w3.org/TR/scxml/#N102F2) - but I got an exception like shown below.

If I remove the log-statement in parallel.onentry, the fsm works fine ...

It must be a bug - isn't it ?

Regards
Danny


Rahul said:
> Looks like it. Originally, the spec didn't require (and therefore,
> Commons SCXML didn't implement) <onentry> / <onexit> children of
> <parallel>.
> 
> Please submit a smallest possible test case (and a patch, if you want)
> to the Commons SCXML project in JIRA. Thanks!
> 
> -Rahul
",smartshark_2_2,271,commons-scxml,"""[0.08833128213882446, 0.9116686582565308]"""
1113,229372,Cast to State in TransitionTargetComparator,"The line 90 of the /commons/proper/scxml/trunk/src/main/java/org/apache/commons/scxml/semantics/TransitionTargetComparator (compare(...) method)
looks like this:
 
State s = (State) iter.next();
 
The cast to State causes a cast exception in case if the state machine has nested parallels (without intermediate states). The line should be replaced with
TransitionTarget s = (TransitionTarget) iter.next();
",smartshark_2_2,28,commons-scxml,"""[0.06739823520183563, 0.9326017498970032]"""
1114,229373,SCXMLSerializer does not serialize custom namespace declarations,"The SCXMLSerializer writes the scxml namespace as the default namespace to the <scxml> root tag and adds a ""cs"" (""commons-scxml"") namespace by default.

But it does not take into account that the original scxml document might contain other namespace declarations, e.g. on child tags:

<?xml version=""1.0"" encoding=""UTF-8""?>
<scxml xmlns=""http://www.w3.org/2005/07/scxml"" xmlns:cs=""http://commons.apache.org/scxml"" version=""1.0"" initial=""S1"">
 <state id=""S1"">
  <!-- some E4X like condition using XML namespaces -->
  <!-- redefine default namespace to ""foo"" and use explicit namespace prefix for scxml:transition tag -->
  <scxml:transition event=""foo"" cond=""a.b.bar::c.* == 3"" target=""S2"" xmlns=""foo"" xmlns:scxml=""http://www.w3.org/2005/07/scxml"" xmlns:bar=""bar"">
  </transition>
 </state>
 <state id=""S2"" final=""true"">
 </state>
</scxml>

The above SCXML document is parsed properly and namespaces are forwarded to the executor (i.e. it works correct).  But if it comes to serialization the namespaces are not serialized properly.",smartshark_2_2,276,commons-scxml,"""[0.11900323629379272, 0.8809967637062073]"""
1115,229374,transition leaving a child state of parallel ,"The following SCXML is not correctly executed

<?xml version=""1.0"" encoding=""UTF-8""?>
<scxml xmlns=""http://www.w3.org/2005/07/scxml"" version=""1.0"" exmode=""lax"" initialstate=""initialstate"">
        <state id=""initialstate"">
                <transition event=""start"" target=""onetwo"" />
        </state>
        <parallel id=""onetwo"">
                <transition event=""onetwo_three"" target=""three"" />
                <state id=""one"">
                </state>
                <state id=""two"">
                        <transition event=""two_four"" target=""four"" />
                </state>
        </parallel>
        <state id=""three"">
                <transition event=""three_one"" target=""one"" />
                <transition event=""three_four"" target=""four"" />
        </state>
        <state id=""four"">
                <transition event=""four_onetwo"" target=""onetwo"" />
                <transition event=""four_three"" target=""three"" />
        </state>
</scxml>

Issuing event ""onetwo_three"" while in the parallel correctly brings the state machine in ""three"".

Issuing event ""two_four"" while in the parallel raises an ILLEGAL_CONFIG error.

Here is a detailed log transcription showing first working transitions and lastly the failing one:

3-giu-2011 3.30.35 org.apache.commons.scxml.model.Log execute
INFO: null: entering initialstate

event: start
3-giu-2011 3.30.46 org.apache.commons.scxml.model.Log execute
INFO: null: leaving initialstate
3-giu-2011 3.30.46 org.apache.commons.scxml.model.Log execute
INFO: null: entering onetwo
3-giu-2011 3.30.46 org.apache.commons.scxml.model.Log execute
INFO: null: entering one
3-giu-2011 3.30.46 org.apache.commons.scxml.model.Log execute
INFO: null: entering two

event: onetwo_three
3-giu-2011 3.30.53 org.apache.commons.scxml.model.Log execute
INFO: null: leaving two
3-giu-2011 3.30.53 org.apache.commons.scxml.model.Log execute
INFO: null: leaving one
3-giu-2011 3.30.53 org.apache.commons.scxml.model.Log execute
INFO: null: leaving onetwo
3-giu-2011 3.30.53 org.apache.commons.scxml.model.Log execute
INFO: null: entering three

event: three_four
3-giu-2011 3.31.04 org.apache.commons.scxml.model.Log execute
INFO: null: leaving three
3-giu-2011 3.31.04 org.apache.commons.scxml.model.Log execute
INFO: null: entering four

event: four_onetwo
3-giu-2011 3.31.08 org.apache.commons.scxml.model.Log execute
INFO: null: leaving four
3-giu-2011 3.31.08 org.apache.commons.scxml.model.Log execute
INFO: null: entering onetwo
3-giu-2011 3.31.08 org.apache.commons.scxml.model.Log execute
INFO: null: entering one
3-giu-2011 3.31.08 org.apache.commons.scxml.model.Log execute
INFO: null: entering two

event: two_four
3-giu-2011 3.31.15 org.apache.commons.scxml.env.SimpleErrorReporter onError
AVVERTENZA: ILLEGAL_CONFIG (Not all AND states active for parallel onetwo): /onetwo : [/onetwo/one]
Illegal state machine configuration!
3-giu-2011 3.31.15 org.apache.commons.scxml.env.SimpleErrorReporter onError
AVVERTENZA: ILLEGAL_CONFIG (Multiple top-level OR states active!): <SCXML> : [/four, /onetwo]
",smartshark_2_2,6,commons-scxml,"""[0.08305047452449799, 0.9169495105743408]"""
1116,229375,SCXMLParser: Should the parser throw an exception on a parsing error?,"I noticed that the SCXMLParser::DigestSrcAttributeRule() function only logs a message (and returns to continue parsing at next element) in case there is trouble in resolving the src-Attribute (i.e. to source in content from other SCXML documents). The particular use case is a wrong fraction of the src-Attribute of the <state> tag, i.e. if the state which was refered to using the url fraction does not exist in the target document.

Thus when using the parser it is not possible to know whether the parsing was actually successful. In the above given use case the parser returns a valid - but incomplete - statemachine. And this is a severe problem. 

Hence, shouldn't the parsing error bubble up, e.g. using an exception?",smartshark_2_2,270,commons-scxml,"""[0.7533487677574158, 0.24665124714374542]"""
1117,229376,Improve SCXML state configuration handling and optional validation ,"The (new) SCXML state configuration handling isn't properly handling intermediate (in-progess) transitions yet as it currently only updates/refreshes the state configuration (through the Status object) after processing a step.
This causes errors like reported on SCXML-208 but also [SCXML IRP|http://www.w3.org/Voice/2013/scxml-irp/] test failures (tests 409, 411 and 417). 

As the Status object is directly mutable *and* dynamically derives the active states, this needs a new/immutable implementation without (need for) caching the active states.
To keep supporting setting the current state configuration a new API will be added to the SCXMLExecutor to (only) set the state configuration through atomic stateIds, which also first will check the resulting configuration is a valid/legal configuration.

Finally, as checking the legal configuration in itself can be relatively time-consuming and this not always might be needed (if the state machine is properly defined and controlled), checking the configuration will be made optional (default enabled). 

These changes will have some API changes as result but should be trivial to be resolved at compile time.",smartshark_2_2,134,commons-scxml,"""[0.9936767220497131, 0.006323329173028469]"""
1118,229377,User of SCXMLExecutor does not have access to the current context data,"Currently, SCXMLExecutor does not provide method for client to get local variables associated with the Context of the current state. 
All created Context are maintained in the SCInstance object within the SCXMLExecutor class, and currently the getSCInstance() of SCXMLExecutor does not have any protected or public qualifier. Hence getSCInstance() cannot only be accessed by SCXMLExecutor or classes within the org.apache.commons.scxml package. Can we change getSCInstance() method to be protected or public ? This allows the client to call getSCInstance().getContext() to return the Context associated with the current state or create a child class of SCXMLExecutor that return variables associated with the current context.",smartshark_2_2,228,commons-scxml,"""[0.683652400970459, 0.31634756922721863]"""
1119,229378,Tune performance of compiled code,"* Perform data acquisiton and anlysis to determine which SC compilation strategy offers fastest execution speed, least memory usage, etc.
* Take optimal strategies, run through a profiler to look for other performance bottlenecks. 
",smartshark_2_2,65,commons-scxml,"""[0.9979666471481323, 0.002033425262197852]"""
1120,229379,An Eclipse based visual editor and debugger for SCXML which can also generate SCXML statechart to Java code,"I have been using Apache Commons SCXML Engine for more than two years,Now i have an idea to improve it, and I wanted to post it and see if there would be any interest in it in the Apache SCXML community. If possible, i want to hold it as a GSoC(url:http://socghop.appspot.com/) open source project.

The basic idea of the project is to create a Eclipse based visual editor and debugger for SCXML which can also generate SCXML statechart to Java code.
 
State Chart XML (SCXML) is currently a Working Draft published by the World Wide Web Consortium (W3C). SCXML provides a generic state-machine based execution environment based on Harel State Tables. SCXML is a candidate for the control language within multiple markup languages coming out of the W3C.It is very useful to handler complex status transfer logic,but if a SCXML file is really so huge and complex,it will become too difficult to maintain and refactor or to test its logic validity.
 
This project is to develop a Eclipse plunge-in visual editor which is capable of showing a graphical representation of the process model and allows for adding breakpoints to activities, variable modifications and managing the debugging process for SCXML.I have already done some work on both SCXML and Eclipse GMF,even i tried to implement a SCXML editor using GMF and got a little achievement,so i want to develop this editor with GMF and have enough confidence to finish it.
Usually,State Chart XML file will be running on a SCXML engine(for example, our Apache Commons SCXML),if some one only want to achieve the SCXML file's business logic,but do not want to or can not use some SCXML engine due to licence problem etc,this tool can export SCXML file's logic to executeable Java code which has the same state logic translation with the SCXML file.
 
This is my mainly idea and Rahul Akolkar told me this is a good idea,but he has another GSoC project to mentor already, so he can not mentor me, and he suggested me come here to find some proper mentor.

So,if anyone has any questions or comments, or would be interested in mentoring this project, please let me know,you can send me an Email, my mail address is ustbcoder@gmail.com.Thanks.",smartshark_2_2,4,commons-scxml,"""[0.9982988238334656, 0.0017011529998853803]"""
1121,229380,Support for the pnuts scripting language,Add support for the pnuts scripting language.,smartshark_2_2,46,commons-scxml,"""[0.9982513785362244, 0.001748611219227314]"""
1122,229381,SCXMLserializer ignores log label ,"When serializing SCXML to xml  the label attribute of the log element does not gets serialized. 

see https://mail.google.com/mail/?hl=en&shva=1#inbox/12d8e2481b4caf97",smartshark_2_2,42,commons-scxml,"""[0.14741414785385132, 0.8525859117507935]"""
1123,229382,Provide a state machine support class to allow for delegation.,"This is not completely thought out yet, but if folks like the idea I might persue it further.

I would like to use AbstractStateMachine but cannot extend from it:

class B extends A /*, AbstractStateMachine */ {
  // copy source from AbstractStateMachine here
}


I propose here a StateMachineSupport class that provides for use by subclassing and for use by delegation.  The constructors are a little messy, but in the end I have

public class StateMachineSupport {
  // use by subclassing
  protected StateMachineSupport(...) {
  }

  // use by delegation
  public StateMachineSupport(Object delegator, ...) {
  }

  // ... methods from AbstractStateMachine
}

public abstract class AbstractStateMachine extends StateMachineSupport {
  protected AbstractStateMachine(...) {
    super(...);
  }
}


// use by subclassing
class ConcreteStateMachine extends AbstractStateMachine {
  ConcreteStateMachine() {
    super(...""stopwatch.xml"");
  }

  public void reset() { ... }
  public void running() { ... }
  public void paused() { ... }
  public void stopped() { ... }
}

// use by delegation
class DelegatingStateMachine extends SomethingElse {
  StateMachineSupport delegate;

  DelegatingStateMachine() {
    delegate = new StateMachineSupport(this, ...""stopwatch.xml"");
  }

  public void reset() { ... }
  public void running() { ... }
  public void paused() { ... }
  public void stopped() { ... }
}

StateMachineSupport.java, AbstractStateMachine2.java, StateMachineSupportTest.java, and AbstractStateMachine2Test.java attached.",smartshark_2_2,211,commons-scxml,"""[0.9983540773391724, 0.001645925221964717]"""
1124,229383,Project Page Contains Broken Link to Eclipse Plugin,"The project page http://commons.apache.org/proper/commons-scxml/ contains a link, ""Commons SCXML - Eclipse"" with a reference to http://commons.apache.org/sandbox/gsoc/2010/scxml-eclipse/.  The link is bad.",smartshark_2_2,150,commons-scxml,"""[0.9971771240234375, 0.002822929760441184]"""
1125,229384,Incorrect log level for entry/exit trace etc,"Many of the classes use INFO level for what appear to be DEBUG level information.

For example, SCXMLListener#onEntry() uses INFO level to log the state parameter. Such logs should be reduced to DEBUG level.",smartshark_2_2,282,commons-scxml,"""[0.9917168021202087, 0.008283203467726707]"""
1126,229385,Map the <scxml> initial attribute to an initial Transition instead of a (single) TransitionTarget,"The <scxml> initial attribute defines the default transition target.
This target may define multiple states (assuming the parallel ancestor(s) here), but in the current SCXML model object the initial is assumed to be a single state (TransitionTarget) only.

To be conforming to the specification, I'll replace the SCXML TransitionTarget #initialTarget with a Transition #initialTransition instead so fix this, and adjust the usages accordingly.",smartshark_2_2,99,commons-scxml,"""[0.8237307071685791, 0.1762693226337433]"""
1127,229386,<state> can't have <parallel> ,"According to the latest SCXML W/D the <state> can have <parallel> as its child.
But, Commons SCXML 0.7 doesn't  seems to support that. 

The  org.apache.commons.scxml.io.SCXMLParser can't parse this scxml doc. 

<?xml version=""1.0""?>
<scxml xmlns=""http://www.w3.org/2005/07/scxml"" version=""1.0""
  initialstate=""microwave""> 
  <state id=""microwave"">
   <initial>
          <transition  target=""parts""/>	 
    </initial>
    <parallel id=""parts"">
      <state id=""oven"">
          <initial>
               <transition  target=""oven2""/>	 
          </initial>
             <state id= ""oven2"">
               <transition event=""exit""  target=""dest""/>
             </state>
      </state>
      <state id=""door"">
             <initial>
                   <transition  target=""door2""/>	 
             </initial>
              <state id= ""door2"">
	 <transition event=""e"" target=""dest""/>
             </state>
      </state>
    </parallel>
  </state>
  <state id= ""dest""  final =""true""/>
</scxml>

The SCXML  engine generates the following exception during parsing the scxml doc.
Exception in thread ""main"" java.lang.ClassCastException: org.apache.commons.scxml.model.Parallel
	at org.apache.commons.scxml.io.ModelUpdater.updateState(ModelUpdater.java:213)
	at org.apache.commons.scxml.io.ModelUpdater.updateSCXML(ModelUpdater.java:78)
	at org.apache.commons.scxml.io.SCXMLParser.parse(SCXMLParser.java:268)
	at org.apache.commons.scxml.io.SCXMLParser.parse(SCXMLParser.java:152)
	at org.apache.commons.scxml.test.StandaloneUtils.execute(StandaloneUtils.java:80)
	at org.apache.commons.scxml.test.StandaloneJexlExpressions.main(StandaloneJexlExpressions.java:60)

So, I fixed  the 'updateState()' in org.apache.commons.scxml.io.ModelUpdater class like this 
(and also org.apache.commons.scxml.io.SCXMLSerializer.serializeState() )
        ..........
        } else {
            Iterator j = c.keySet().iterator();
            /* ---- your code  ----
            while (j.hasNext()) {
                updateState((State) c.get(j.next()), targets);
            }
             */
            /* -------   my fixed code ---------*/
            while (j.hasNext()) {
               TransitionTarget tt = (TransitionTarget) c.get(j.next());
                if (tt instanceof State) {
                   updateState((State) tt, targets);
                } else {
                   updateParallel((Parallel) tt, targets);
               }
           }
          /*  ------------------------------------*/
        }
    } /* end of  'updateState()' */

So, the engine works well. 

I think this is a critical bug. ",smartshark_2_2,259,commons-scxml,"""[0.45207661390304565, 0.5479234457015991]"""
1128,229387,Javascript expression evaluator,A Javascript expression evaluator for SCXML expression offers some functionality (e.g. functions) not supported by the sample JEXL expression evaluator included in the SCXML project - and is easily implemented as a wrapper around the Javascript engine included in the Java 6 JDK.,smartshark_2_2,258,commons-scxml,"""[0.9978433847427368, 0.0021565535571426153]"""
1129,229388,Add <script> and EcmaScript + E4X support,I propose to add <script> support and an EcmaScript evaluator which supports E4X. I will provide a patch.,smartshark_2_2,95,commons-scxml,"""[0.9982276558876038, 0.001772350980900228]"""
1130,229389,Add parameter to Evaluator.newContext,"The Evaluator.newContext method currently takes one argument, the parent context. However, to make it easier for the context to link to relevant application data, it would be nice if it also took a TransitionTarget argument, i.e. have the signature     public Context newContext(TransitionTarget target, Context parent); By including the target in the context, it becomes easier for expressions to refer to contextual data. In my case, SCXML is used together with data bindings, and expressions are used to refer to data in the context of states and transitions. I had to modify SCXML as suggested above, to provide the link between state and data bindings.
",smartshark_2_2,135,commons-scxml,"""[0.9984307885169983, 0.0015692405868321657]"""
1131,229390,Add custom handling for injecting data in the Javascript engine (Nashorn),"To be able to use Java Arrays and Lists as a native Javascript array, Nashorn provides the Java.from extension to wrap the Java object. ",smartshark_2_2,172,commons-scxml,"""[0.998393714427948, 0.0016062885988503695]"""
1132,229391,"<assign location=""..."" expr=""...""> problem for context with different internal XML representation","We are currently working on a prototype implementation of a ECMAScript context and evaluator based on Rhino.

It appears that Rhino has its internal context and uses an internal XML representation different from org.w3c.dom.node. This means we have to convert every XML Node/Document when it is stored into the context into Rhino's internal XML representation (e.g. when the datamodel is stored into the context). This works fine.

The problem occurs with the current implementation of <assign location=""..."" expr=""..."">. Assign uses evalLocation() twice to get two org.w3c.dom.node objects (one for location and another for expr). Then the implementation of Assign iterates through the two node structures and stores the expr node under the location node.

Doing this, Assign manipulates a variable within the context from outside, i.e. it manipulates the XML data tree without using a set() at the context. To do this it makes use of object references.

For our context this appears to be a problem. As mentioned earlier we have a different internal XML representation (i.e. other Java classes than org.w3c.dom). Therefore our evalLocation() returns a org.w3c.dom.node object (as required by the context interface) which is a converted copy of the internal XML object. Assign therefore works on our copies and as it does not store the result explicitly back into the context we loose it.

I see the current implementation of Assign therefore as a violation of the context abstraction (because Assign manipulates data within the context from outside) and propose to change it. I propose to think about a solution which avoids data convertion (as described above) whereever possible for evaluators/contexts which use different internal (XML) data representation.",smartshark_2_2,128,commons-scxml,"""[0.1936521977186203, 0.8063477873802185]"""
1133,229392,add a new message for listeners to capture when an event is not handled by the current set of active states,"i would like to be able to know if an event is sent to a finite state machine but no active state can handle it. There are several possible definitions for ""not handled/used event""; currently i would define an unused event as an event that didn't contribute to the firing of any transition (for the execution step in which the event was processed).",smartshark_2_2,179,commons-scxml,"""[0.9984586238861084, 0.0015413387445732951]"""
1134,229393,Drop ant build support,"The ant build support requires extra manual maintenance, while there seems to be no real interest for it anymore.
To simplify the current maintenance, the ant support will therefore be removed.",smartshark_2_2,25,commons-scxml,"""[0.9974797368049622, 0.0025202888064086437]"""
1135,229394,CUSIPCheckDigit thinks invalid CUSIP is valid,"When testing if a specific CUSIP is valid using org.apache.commons.validator.routines.checkdigit.CUSIPCheckDigit.CUSIP_CHECK_DIGIT.isValid, a call to this returns true when it should return false. A specific example is with the following invalid CUSIP: DUS0421CW.

What seems to be happening is when toInt is called on W it turns it to an int value and then sends it to weightedValue. This is fine for the first 8 characters of a CUSIP, but not the check digit. The expected result should be to return false because the check digit is a letter (on a CUSIP a check digit must be 0-9). 

With the current implementation, I believe each CUSIP can have up to 4 valid check digits.

A test is attached.",smartshark_2_2,257,commons-validator,"""[0.10914084315299988, 0.8908591866493225]"""
1136,229395,Validating array integers fails,"I posted this issue on Struts and it got determined to be a validator bug (or appears to be).

There seems to be some issue with validating integer arrays: 

html: 

<html:select property=""types"" multiple=""true"" size=""3"" > 
<html:option value=""1"">should work</html:option> 
<html:option value=""2"">should work as well</html:option> 
<html:option value=""blah"">this will fail validation</html:option> 
</html:select> 

actionform: 

public class PropertyFilter implements Serializable 
  { 
  private String[] types; 
  
    public String[] getTypes() { 
    return types; 
  } 
    public void setTypes(String[] types) { 
    this.types = types; 
  } 
  } 

validation.xml: 
    
<formset> 
<form name=""PropertyFilterForm""> 
   
<field property=""types"" depends=""integer""> 
<arg0 key=""text.validation.type""/> 
</field> 
    
</form> 
</formset> 

Even if all values are integer, it will fail validation regardless. According to http://www.strutskickstart.com/IndexedPropertiesandValidation.ppt#18 I should 
do something like: 
    
<field property=""types"" indexedListProperty=""types"" depends=""integer""> 
<arg0 key=""text.validation.type""/> 
</field> 
    
This time, it does indeed check all the values are of type integer, however, 
if you do not select anything (the field is not required) the result is: 
    
java.lang.NullPointerException 
org.apache.commons.validator.Field.getIndexedProperty(Field.java:796) 
org.apache.commons.validator.Field.validate(Field.java:891) 
  .. 

Also having scoured all documentation in current and future builds and there is no example of validating an integer array. ",smartshark_2_2,125,commons-validator,"""[0.11524562537670135, 0.8847543597221375]"""
1137,229396,IBANValidator fails for Seychelles and Ukraine,"The IBANValidator does not validate the IBANs from the Seychelles or Ukraine, even though they are part of IBAN registry.

SC18SSCB11010000000000001497USD, fails but should pass
UA213996220000026007233566001, fails but should pass
",smartshark_2_2,373,commons-validator,"""[0.1375458538532257, 0.8624541759490967]"""
1138,229397,"EmailValidator#isValid(String) returns true for ""joe@foo/bar.com""","This assertion fails for me:

assertFalse(EmailValidator.getInstance().isValidEmail(""joe@foo/bar.com""));
",smartshark_2_2,203,commons-validator,"""[0.09769681096076965, 0.902303159236908]"""
1139,229398,isValid return false for a valid URL,"IsValid returns false for the following valid Url:

    http://tech.yahoo.com/rc/desktops/102;_ylt=Ao8yevQHlZ4On0O3ZJGXLEQFLZA5

Stepping thru the code isValidPath return false for the above URL.

Test Code:

  String[] schemes = {""http"",""https""}. 
  UrlValidator urlValidator = new UrlValidator(schemes) ; 
  urlValidator.isValid(""http://tech.yahoo.com/rc/desktops/102;_ylt=Ao8yevQHlZ4On0O3ZJGXLEQFLZA5"") ;",smartshark_2_2,137,commons-validator,"""[0.0965966135263443, 0.9034033417701721]"""
1140,229399,IbanCheckDigit.isValid() returns True for some invalid IBANs,"For example, isValid() returns True for both of these IBANs; ""IE01AIBK93118702569045"" and ""IE98AIBK93118702569045"".  The ""IE98"" version is the correct one (confirmed with online checkers, which also fail the ""IE01"" version).

calculate() correctly returns ""98"".  As a workaround I'm calling calculate() and comparing the result with the checksum in the original IBAN.
",smartshark_2_2,306,commons-validator,"""[0.0823691338300705, 0.9176308512687683]"""
1141,229400,Some TLDs are missing from DomainValidator,"As reported on list <http://www.mail-archive.com/dev@commons.apache.org/msg29436.html>, DomainValidator is missing some of the new TLDs

The full list is available from <http://data.iana.org/TLD/tlds-alpha-by-domain.txt>",smartshark_2_2,219,commons-validator,"""[0.993486225605011, 0.006513841450214386]"""
1142,229401,Add ISINValidator,"There is an ISINCheckDigit class, but as yet no ISINValidator class.",smartshark_2_2,384,commons-validator,"""[0.9983938336372375, 0.001606095815077424]"""
1143,229402,[validator] validator-rules.xml JavaScript fails when field not present in jsp,"Sometimes, esp. when writing a wizard-like interface, not all the form fields 
will be present in a single jsp - each jsp asks for a subset of the required 
data from the user. It's useful to validate each piece of data. If you use the 
validator.xml as supplied, it will fail for rules where the particular field 
doesn't exist in the jsp.

I propose adding a check in each of the validators to check if the form field 
is undefined (i.e. doesn't exist) before attempting to use its properties.

Each validator must have the following line or similar (in the case 
of ""required"") under the for (x in oRequired) { line:

for (x in oRequired) {
    if (form[oRequired[x][0]] != undefined) {


Hope this is of use!

Andrew",smartshark_2_2,101,commons-validator,"""[0.33055463433265686, 0.6694453358650208]"""
1144,229403,URLValidator should check for illegal Hex characters,"URLs with illegal hex characters (because of invalid escape sequences) are not detected as invalid.

Example: http://finance.yahoo.com/news/Owners-54B-NY-housing-apf-2493139299.html?x=0&ap=%fr

UrlValidator.isValid() returns true, when it should return false.",smartshark_2_2,172,commons-validator,"""[0.7236266732215881, 0.27637335658073425]"""
1145,229404,Abstract Validation framework to Actions framework.,"Apache Commons validator can be ABSTRACTED to a common framework called ACTIONS framework.
 
The Actions framework will act in similar manner as validation framework but will be independent of the way in which actions are execute on the value.
 
Abstraction points.
1.Validation can be treated as any action
2.The value to be validated is treated as source value
3.The return value true/false can be treated as a target value
4.Source Value ---pass to--- > list of actions to be executed ----results in---> Target Value
 
We have already done this in our enterprise product, we use apache validation framework even for transforming values in our custom ETL framework.
 
Once the apache validator is abstracted as actions framework one can use the same to not only validate the source value but to transform the value or even in describing and modifying styles of controls in UI and much more. The advantage here is we use the same digester , loading of elements, extending forms and stuff, this code remains common.",smartshark_2_2,371,commons-validator,"""[0.9982837438583374, 0.0017162344884127378]"""
1146,229405,Generated JavaScript Causes HTML Page to Contain Illegal HTML,"Various JavaScript functions contain JavaDoc comments at the start such as:

     /*$RCSfile: validateMaxLength.js,v $ $Rev: 376673 $ $Date: 2006-02-10 13:42:31 +0000 (Fri, 10 Feb 2006) $ */
     /**
     * A field is considered valid if less than the specified maximum.
     * Fields are not checked if they are disabled.
     * <p>
     * <strong>Caution:</strong> Using <code>validateMaxLength</code> on a password field in a
     *  login page gives unnecessary information away to hackers. While it only slightly
     *  weakens security, we suggest using it only when modifying a password.</p>
     * @param form The form validation is taking place on.
     */
     function validateMaxLength(form) {

This causes the W3C tidy application to report the following:

 ""<"" + ""/"" + letter not allowed here
 Cause:

 The 2 characters ""</"" have been detected in a wrong place.
 Solution:

 In most case, this is due to wrong javascript:

 BAD   <document.write(""</h1>"");
 GOOD  <document.write(""<\/h1>"");

 References:

 W3C faq: http://validator.w3.org/docs/help.html#faq-javascript
 HtmlHelp: http://www.htmlhelp.com/tools/validator/problems.html#script

And thus produces illegal HTML.

The proposed solution is to remove the offending tags.",smartshark_2_2,142,commons-validator,"""[0.9647073745727539, 0.03529259189963341]"""
1147,229406,Copy remaining Validation Routines to the new routines package,"Copy remaining Validation Routines (i.e. Credit Card, Date, Email, ISBN and URL) into routines package and deprecate original versions.

Need to decide what to do with GenericValidator and GenericTypeValdiator - just deprecate and remove later or copy to new routines package. At this moment I'm in favour of deprecating and removing. Validators in the new routines package have a static getInstance() method to provide access to the validation methods.",smartshark_2_2,343,commons-validator,"""[0.9984550476074219, 0.001544940285384655]"""
1148,229407,ISBN style numbers outside ISBN range returning true for isValid,"All 13 digit EANs with the 979 prefix are validating as ISBNs (ISBNValidator). There are 2 issues with this.
 # 979-0 prefixed numbers are allocated to the ISMN agency and represent music works. [https://www.ismn-international.org/whatis.html]
 # 979-[2-9] are unassigned at present, that is they have not yet been allocated to a country agency (or another authority).
see attached xml (from [www.isbn-international.org/export_rangemessage.xml|http://www.isbn-international.org/export_rangemessage.xml] [^RangeMessage.xml] ) for the current valid ranges for ISBN 
and [https://en.wikipedia.org/wiki/List_of_ISBN_identifier_groups#Identifiers_of_the_979-_prefix]

An example unassigned 'ISBN' is 9792222222223",smartshark_2_2,424,commons-validator,"""[0.20846731960773468, 0.7915326952934265]"""
1149,229408,Change groupId to org.apache.commons,"Some existing Commons projects have made the POM move to org.apache.commons with their new releases. 
http://repo.mergere.com/maven2/org/apache/commons/

I think a 1.4 release is a good time to change the groupId as well.",smartshark_2_2,230,commons-validator,"""[0.9983128309249878, 0.0016871754778549075]"""
1150,229409,DomainValidator - allow access to internal arrays,"Seems to me it might be useful to allow users access to the current lists of TLD entries. This could be done with a simple clone of the array. 

It might also be useful to have a public constant containing the version of the IANA file from which the entries were last updated.",smartshark_2_2,326,commons-validator,"""[0.9983962178230286, 0.0016038129106163979]"""
1151,229410,isValid checks if the given address is only IPV4 address and not IPV6,function isValid(String inetAddress) checks if the given address is only IPV4 and not IPV6. So the function returns false for a valid IPV6 address. Would it be possible to add IPV6 validation as well?,smartshark_2_2,225,commons-validator,"""[0.36191463470458984, 0.6380853056907654]"""
1152,229411,Validate 19 digit VPay (VISA),"I am using the org.apache.commons.validator.CreditCardValidator class in the 1.3.1 release to validate our credit card numbers.  We are looking to change our service to accept 19 digit Visa credit card numbers, but your validator does only validates 13-16 digit credit card numbers.  This is the same for the routines validator in 1.4.1.

Can you change the validator in 1.3.1 and/or 1.4.1 to accept Visa credit cards from 13 to 19 digits?",smartshark_2_2,313,commons-validator,"""[0.9969773292541504, 0.0030226646922528744]"""
1153,229412,Accept Discover cards of 17 digits long that start with 6011,"The latest version of the apache-validator 1.5.1 has this regex for the Discover validator:
{code}
/** Discover Card regular expressions */
private static final RegexValidator DISCOVER_REGEX = new RegexValidator(new String[] {""^(6011\\d{12})$"", ""^(64[4-9]\\d{13})$"", ""^(65\\d{14})$""});
{code}

It does not accept the Discover cards that start with 6011 and 17 digits long. Would be nice to support it.

Suggested regex:
{code}
^(6011\\d{12})$"", ""^(6011\\d{13})$"", ""^(64[4-9]\\d{13})$"", ""^(65\\d{14})$""
{code}

Example of the valid card:
{quote}
60115564485789458
{quote}",smartshark_2_2,352,commons-validator,"""[0.8852697610855103, 0.11473022401332855]"""
1154,229413,TimeValidator/CalendarValidator common validate code could be pushed up,"The TimeValidator and CalendarValidator classes have identical validate methods.

These could be pushed up into a common superclass.",smartshark_2_2,353,commons-validator,"""[0.9980146884918213, 0.0019852500408887863]"""
1155,229414,Expose JsFunction property in ValidatorAction,"Exposing the JsFunction property in ValidatorAction would make it possible for other tools to include Javascript routines as external scripts, inside of inline Javascript in an HTML page. This would reduce the size of pages containing client-side validation and the overall size of content transferred from server to browser.

Shale commons validator integration uses commons validator to provide both client side and server side mapping. When the JSF renderer writes the validation Javascript code, it looks up the ValidatorAction for each validation routine. It then inserts the Javascript inline using ValidatorAction.getJavascript(). Since this code never changes and is likely to be used multiple times, I would like to instead insert something like this:

<script src=""/weblets/validator/validateEmail.js"" />

I can almost do this with Shale commons validator and Weblets, but I need to get the fully qualified resource name from the ValidatorAction, i.e. JsFunction. I can then map the resource name (org.apache.commons.validator.javascript.validateEmail) to a URL (""/weblets/validator/validateEmail.js"") using Weblets. 

The entire change required is adding:

/**
     * Gets the fully qualified class path of the Javascript function.
     * This is optional and can be used instead of the Javascript property. 
     * 
     * @return The Javascript function's fully qualified class path, or
     * null if this property has not been set.
     */    
    public String getJsFunction() {
        return jsFunction;            
    }

to ValidatorAction.java.

After the change above, I am able output <script src=""""> tags in Shale commons validator.

The change could also be used in other web frameworks to do the same thing. ",smartshark_2_2,249,commons-validator,"""[0.9983698725700378, 0.0016301419818773866]"""
1156,229415,Enhance the IndexedListProperty to handle nested lists.,"Allowing lists to be validated helps a great deal but the code that is supplied 
with the commons-validator 1.0 could be simplified and enhanced.  There are two
features that it would be nice to have:

1) When the IndexedListProperty is used it would be nice if all the fields in 
the list were validated, i.e. the validation did not stop at the first error.  
This would need a different loop mechanism and the key to be set to the fully 
qualified path, e.g list[0].value and not list[].value.

2) It would be very nice if the IndexedListProperty could be nested, e.g. the 
size for all the doors on all the rooms must be greater than n.  I am not 
sure of the syntax in the configuration file that makes sense but offer 
list1.list2.list3. as a suggestion.  The PropertyUtils could then be called 
on the bean using the ""list1"" property to get the first list and then called 
on each of the returned objects using the ""list2"" property, etc.  So, if you 
had two rooms, the first with one door, the second with two and the 
validation.xml Field looked like:

<field 
	property=""size""
	indexedListProperty=""rooms.doors""
	depends=""min"">
	<arg0 key=""error.door.size""/>
	<var>
		<var-name>min</var-name>
		<var-value>${n}</var-value>
	</var>		   
</field>             

The validator waould in effect be called for:

rooms[0].doors[0].size
rooms[1].doors[0].size
rooms[1].doors[1].size

The code below provides an example of how this could be done.  It uses the 
IndexedProperty to pass information down a reentrant stack because I did not 
want to change the parameters to the Validator.validate method but that would 
obviously be safer.  Also, by putting this code snipit in place, all the other 
isIndexed code could be removed so you don't have to loop twice (as it does in 
version 1.0).


Thanks...Peter

//////////////////////////////////////////////////////////////////////////////
// These snippits are out of the Validator class.  There is a one line
// change to validate, one new method and one tidied up method.
// The code has been tested and works but you may not like the way it 
// changes the BEAN_KEY and uses the IndexProperty.
//////////////////////////////////////////////////////////////////////////////

    /**
     * Performs validations based on the configured resources.
     * Needed as we cannot access (override) the private methods.
     *
     * @return	The <code>Map</code> returned uses the property
     *		of the <code>Field</code> for the key and the value
     *		is the number of error the field had.
     */
    public ValidatorResults validate() throws ValidatorException
    {
        ValidatorResults results = new ValidatorResults();
        Locale locale = null;

        if (hResources.containsKey(LOCALE_KEY))
        {
            locale = (Locale)hResources.get(LOCALE_KEY);
        }
        hResources.put(VALIDATOR_KEY, this);

        if (locale == null)
        {
            locale = Locale.getDefault();
        }

        Form form = null;
        if (resources == null)
        {
            throw new ValidatorException(""Resources not defined for Validator"");
        }
        if ((form = resources.get(locale, formName)) != null)
        {
            for (Iterator i = form.getFields().iterator(); i.hasNext(); )
            {
                Field field = (Field)i.next();
                if (field.getPage() <= page)
                {
		    // *************************************************
                    // This is the only line that changed in this method.
		    // *************************************************
                    validateFieldNested(field, results);
		    // *************************************************
                }
            }
        }
        return results;
    }

    /**
     * Validate field nested.  This method handles nested list validation
     * of the form IndexedListProperty = list1.list2.list3.  It gets all the
     * instances in list1 off the BEAN_KEY (root bean) using the PropertyUtils
     * then gets all the list2 entries of all the list1 objects, then all the
     * list3 objects of the list2 objects, etc.  The result is that the
     * validateField method is called on all the list3 objects with the property
     * as it was but the BEAN_KEY set to the current list3 object and the
     * Field key set to the fully qualified key (e.g. list1[0].list2[0].list3
[0]).
     *
     * @param field         See Validator.validateField
     * @param allResults    See Validator.validateField
     */
    private void validateFieldNested (Field field, ValidatorResults allResults)
        throws ValidatorException
    {
        // Does it have an IndexedList property?
        String indexedList = field.getIndexedListProperty();
        if (null != indexedList && 0 < indexedList.length())
        {
            // Is it nested?
            String nestName = indexedList;
            String restName = null;
            int nestOffset = indexedList.indexOf(""."");
            if (-1 != nestOffset)
            {
                nestName = indexedList.substring(0, nestOffset);
                restName = indexedList.substring(nestOffset + 1);
            }

            // Build the field object based on the nesting.
            Field indexedField = (Field)field.clone();
            indexedField.setIndexedListProperty(restName);

            // The keyBase is a local copy of the IndexedProperty so that
            // the appropriate instance ([n]) can be added inside the
            // loop.  The IndexedProperty is not currently used by the
            // validator framework and so we use it to pass the revised
            // nesting level through the reenterant code.
            String keyBase = field.getIndexedProperty();
            if (null == keyBase)
            {
                keyBase = nestName;
            }
            else
            {
                keyBase += ""."" + nestName;
            }

            // Save the current BEAN_KEY, the object that the validator
            // is working on, so we can reset it as we pop out of the
            // reenterant loop.
            Object oldBean = hResources.get(BEAN_KEY);

            // Call for each of the objects at this level
            Object[] list = getIndexedList(oldBean, nestName);
            for (int i = 0; i < list.length; i++)
            {
                // Set the BEAN_KEY to the current object
                hResources.put(BEAN_KEY, list[i]);

                // Use the indexedProperty to pass the current level into
                // the next level so that it does not have to do a load of
                // parsing and replacement when it builds the key.  The key
                // is used to make the field error (if any) unique.
                indexedField.setIndexedProperty(keyBase + ""["" + i + ""]"");
                indexedField.setKey(indexedField.getIndexedProperty() + "".""
                + indexedField.getProperty());

                // Call ourselves to handle the next (if any) level of
                // nesting.
                validateFieldNested(indexedField, allResults);
            }
            // Put the old BEAN_KEY back as we pop the reentrancy stack.
            hResources.put(BEAN_KEY, oldBean);
            return;
        }
        // If it is not nested (actually IndexedList) use the normal method.
        validateField(field, allResults);
    }

    /**
     * Get an array of instances for the supplied property, whether it is a
     * collection or an array.  This is just a tidy up of an existing method
     * in the Validator class.
     *
     * @param bean          The bean that contains the list or array.
     * @param property      The name of the property that will return a list
     *                      or array on the supplied bean.
     * @return Object[]     An array of objects that were retrieved from the
     *                      supplied property.
     */
    private Object[] getIndexedList(Object bean, String property)
    {
        Object oIndexed;
        try
        {
            oIndexed = PropertyUtils.getProperty(bean, property);
        }
        catch (Exception e)
        {
            log.error(""in validateFieldNested"", e);
            return null;
        }

        Object indexedList[] = new Object[0];

        if (oIndexed instanceof Collection)
        {
            indexedList = ((Collection)oIndexed).toArray();
        }
        else if (oIndexed.getClass().isArray())
        {
            indexedList = (Object[]) oIndexed;
        }
        return indexedList;

    }",smartshark_2_2,368,commons-validator,"""[0.9984093308448792, 0.0015907486667856574]"""
1157,229416,URLValidator returns false for http://example.rocks,It seems the (valid) subdomain is the problem because it returns true for http://example.com,smartshark_2_2,278,commons-validator,"""[0.3012252748012543, 0.6987746953964233]"""
1158,229417,[validator] Added support for enforcing min or max numeric values,"Field checks doesn't contain support for enforcing min/max values when a range 
is not applicable.  This is a very common user input validation step that can 
be added fairly easily.",smartshark_2_2,184,commons-validator,"""[0.9982660412788391, 0.0017340430058538914]"""
1159,229418,Add script attribute to control script generation,"Add a script=""true|false"" attribute to <field> to control whether JavaScript should be generated.

Also see: https://issues.apache.org/struts/browse/STR-1888





",smartshark_2_2,153,commons-validator,"""[0.9983154535293579, 0.0016845969948917627]"""
1160,229419,ISIN validator,"I wrote an ISIN validator. Might be a good contribute (or not). I did not really ask the dev list first.

https://github.com/apache/commons-validator/pull/7",smartshark_2_2,336,commons-validator,"""[0.9984858632087708, 0.0015141229378059506]"""
1161,229420,validator & inheritance,"Possibility of inheriting validation rules from other forms.
Possibility of making a default form-validation for the default form-set and 
extend them from any other form-set.
Possibility of overriding inherited validation rules so a default validation 
can be given.",smartshark_2_2,69,commons-validator,"""[0.9983515739440918, 0.0016484986990690231]"""
1162,229421,"NamingTests - assertSameName invokes relName.replace('\\', '/'); without using return value","NamingTests - assertSameName invokes relName.replace('\\', '/'); without using return value.

So the test does not change the string.
",smartshark_2_2,268,commons-vfs,"""[0.8527102470397949, 0.14728973805904388]"""
1163,229422,HDFS Provider is not removing cached files,The HDFS file provider is not removing files from the local cache when the file is deleted from HDFS.,smartshark_2_2,485,commons-vfs,"""[0.14665095508098602, 0.8533490300178528]"""
1164,229423,Closed OutputStream from FileContent does not throw IOException if further written to,"I've a question regarding the behaviour of OutputStreams received from
org.apache.commons.vfs.FileContent.getOutputStream(): If I call close()
on them and further write some bytes to the stream, these writes take
place silently without throwing an IOException (as it is done by ""usual""
Java streams like java.io.FileOutputStream); the data written after close()
has been called does not appear in the target file, neither.

As an example:

byte[] SOME_BYTES = new byte[]{70, 71, 72};
FileObject tmpFile = VFS.getManager().resolveFile(""tmp:test.txt"");
OutputStream os = tmpFile.getContent().getOutputStream();
os.write(SOME_BYTES);
os.close();
os.write(SOME_BYTES); // Neither IOE is thrown nor bytes are written",smartshark_2_2,531,commons-vfs,"""[0.09089802205562592, 0.9091020226478577]"""
1165,229424,[VFS] SoftRefFilesCache Doesn't Ever Clear Cache,"The SoftRefFilesCache doesn't ever clear its cache.  I found this out when I notice my JVM was sucking up hundreds of megs as it load, processed, and unloaded, thousands of files.

I have attached an example program that demonstrates the problem with SoftRefFilesCache.java#startThread().  The thread is never actually launched.  I have only tried it on Java 1.7.   Basically, I am trying to demonstrate that calling AtomicReference#compareAndSet never returns true when ""expected"" is a null value.  So what happens is that the call returns false, the loop is re-entered, newThread gets blown away, the thread is found, and the loop exits, so the thread is never launched.  I'm using VFS in a singleton fashion.  Well, I was using it before this issue.",smartshark_2_2,363,commons-vfs,"""[0.08588497340679169, 0.9141150712966919]"""
1166,229425,HTTP only allows reading from one file at a time,"VFS 164 modified HttpClientFactory to use a single connection per thread. The consequence of this is that only a single file can be accessed at a time.  Several applications, such as Commons Configuration and XML includes will read a second file while processing the first. In the case of Commons Configuration an IOException is being thrown when the first file is closed because it was already closed by ThreadLocalHttpConnectionManager.",smartshark_2_2,116,commons-vfs,"""[0.193735733628273, 0.8062642812728882]"""
1167,229426,Bad integer comparison in CBZip2InputStream,There are three places where 'thech' (twice) and 'ch' (once) are compared to -1; when a char value should never be -1. ,smartshark_2_2,190,commons-vfs,"""[0.09701076149940491, 0.9029892683029175]"""
1168,229427,FTPFileObject monitor doesn't retrieve lastModifiedTime,"Monitoring of files on a ftp server using DefaultFileMonitor doesn't seem to register changes in the last modified time.

From the email correspondance with Mario:

1.
This is how I've set up my DefaultFileMonitor:

  DefaultFileMonitor fm = new DefaultFileMonitor(new FileListener() {
    public void fileCreated(FileChangeEvent arg0) throws Exception {
	System.out.println(""File created. "" + arg0.getFile().getName());
    }
    public void fileDeleted(FileChangeEvent arg0) throws Exception {
	System.out.println(""File deleted. "" + arg0.getFile().getName());
    }
    public void fileChanged(FileChangeEvent arg0) throws Exception {
	System.out.println(""File changed. "" + arg0.getFile().getName());
  }
  });

  fm.setDelay(60000);
  fm.addFile(file);
  fm.start();


2.
Thanks for the code snipped. I tried it with the default
VFS.getManager() (without setting the CacheStrategy) and it works here with my ftp server as expected.
I am pretty sure for your tests you do not use a delay of 60000, do you?
I tried it with 5000 (5 seconds) and get the events promptly.
But ...... I pointed the monitor to a directory ....

> And this is how I run my checks:
>
>   try {
>     for (int i = 0; 1 < 100; i++) {
>       Thread.sleep(60000); //  1 minute
>       System.out.println(file.getContent().getLastModifiedTime() + ""  ""
> + file.getName());
>     }
>   } catch (InterruptedException ie) {
>     ie.printStackTrace();
>   }
>   
.... ok - got it. Its a bug in the FTPFileObject, unhappily I have no workaround yet.

Thank you for your help and sorry again for the extra work.",smartshark_2_2,6,commons-vfs,"""[0.10960618406534195, 0.8903937339782715]"""
1169,229428,NullPointerException during getting InputStream from SftpFileObject,"Hi, 

I experienced unregular NullPointerExceptions while getting an InputStream from an SftpFileObject. It only occures in a multithreading environment.

I made a patch. By now it seems to work!

Regards,

Tim",smartshark_2_2,542,commons-vfs,"""[0.06975654512643814, 0.9302434325218201]"""
1170,229429,DefaultFilesCache leaks closed filesystems,"The org.apache.commons.vfs2.cache.DefaultFilesCache does not remove the filesystem specific cache if a filesystem gets cleared. This leads to the problem that instances if the FileSystem kept alive after the FileProvider has closed the filesystem. This is not so much a problem for a smaller number of filesystems configured for a prefix, but it is a problem for layered or virtual filesystems which get created and destroyed. (See also VFS-544). The more advanced filesystem caches support clearing the keys (but have other races I think).

The behaviour is somewhat documented ""lifetime of the FileSystemManager"", but I don't think it is expected or required.",smartshark_2_2,416,commons-vfs,"""[0.06847123056650162, 0.9315287470817566]"""
1171,229430,Enhance FileSystemOptions so that the ConfigBuilder classes are no longer necessary.,"Many of the file providers accept options. The options are stored in the FileSystemOptions class. In order to store and retrieve options a FileSystemOptions instance must be created and than manipulated by a FileSystemConfigBuilder class with the appropriate get and set methods. This can be clumsy and cumbersome. A better approach is to allow each file system extend the FileSystemOptions class to provide the appropriate get and set methods. However, various file systems must be able to store their options in the same underlying data structure. ",smartshark_2_2,194,commons-vfs,"""[0.9983713030815125, 0.0016287753824144602]"""
1172,229431,Attributes are case-senstive,"The set and list methods don't lowercase attribute names; why should get?
",smartshark_2_2,588,commons-vfs,"""[0.9805387258529663, 0.01946129836142063]"""
1173,229432,Add RandomAccessMode.getModeString(),"As $summary, that returns ""r"", ""w"", ""rw"", or """", as appropriate.",smartshark_2_2,591,commons-vfs,"""[0.9955581426620483, 0.004441829398274422]"""
1174,229433,"fix ""// TODO - this isn't really true"" in VirtualFileSystem class refering to real capabilities","As once FileObject has been added as a junction into VirtualFileSystem, we loose it's FS capabilities info.

Solution could be like adding method for checking capability for particular FileObject based on FileSystem obtained from getJunctionForFile.. but not compatible with FileSystem interface. Still this can be useful for those who assume VirtualFileSystem class instance as FileSystem of a FileObject.

I can send you patch proposal if you accept that solution.",smartshark_2_2,126,commons-vfs,"""[0.9126772284507751, 0.08732280135154724]"""
1175,229434,Make it possible to generate VFS sandbox jar with Ant,"I use the VFS-sandbox.jar in my project.
Unfortunately, gump cannot build my project at the moment because the VFS-sandbox.jar isn't available in gump since gump doesn't support M2.
If this sandbox jar could be built via Ant, it could be made available in gump.

Maarten",smartshark_2_2,537,commons-vfs,"""[0.9983377456665039, 0.0016622754046693444]"""
1176,229435,Add a FileExtensionSelector class,"Add a {{FileExtensionSelector}} class, a {{FileSelector}} that selects based on file extensions.",smartshark_2_2,473,commons-vfs,"""[0.9981687068939209, 0.0018312417669221759]"""
1177,229436,Update test dependencies,"Update test dependencies. I'd like to keep this issue open until we release vfs 2.1, because I don't see any value in individual entries in the change log for this internal task.",smartshark_2_2,383,commons-vfs,"""[0.998437225818634, 0.0015627205139026046]"""
1178,229437,Update Apache Commons Logging from 1.1.3 to 1.2.,Update Apache Commons Loggong from 1.1.3 to 1.2.,smartshark_2_2,504,commons-vfs,"""[0.9980649352073669, 0.0019350770162418485]"""
1179,229438,AbstractFileName and AbstractFileObject serialization,"I'm implementing a VFS plug-in that implements a distributed, writeable file system for Google App Engine (GAE) Java (see gaevfs.appspot.com for more info). In order for this to work properly, I need to create a distributed FilesCache implementation using the memcache API provided by GAE. However, memcache requires that both keys and values must be serializable. Therefore, to complete my implementation, I need to make AbstractFileName and AbstractFileObject serializable (because my implementation classes inherit from these).",smartshark_2_2,188,commons-vfs,"""[0.991179883480072, 0.008820101618766785]"""
1180,229439,Changes in interface RandomAccessContent.,"I think the interface RandomAccessContent should not extend DataOutput nor DataInput;
Instead, it could declare the following new methods:

1)   int read(byte b[], int off, int len) throws IOException;
2)   void write(byte b[], int off, int len) throws IOException;
    
3)   int pread(long pos, byte b[], int off, int len) throws IOException;
4)   void pwrite(long pos, byte b[], int off, int len) throws IOException;

Instead of changing this interface, a new one could be created (RandomAccessStream, maybe).

Benefits:
Currently, if a class implements this interface, it has to implement method ""skipBytes"", which is redundant since it already implements method ""seek"".

It also has to implement 14 methods related to reading, which is also redundant since they can be expressed in terms of specific calls to more primitive methods such as those proposed above.

In other words, I don't think different classes implementing RandomAccessContent (or RandomAccessStream) should all implement methods ""readShort"", ""readInt"""", ""readLong"", etc, since all implementations of them will be the same, if they are expressed in terms of calls to read(byte b[], int off, int len).

",smartshark_2_2,7,commons-vfs,"""[0.996269702911377, 0.003730293596163392]"""
1181,229440,Microsoft FTP Virtual Folder support,"FTP Object should return child by name, even if it's name of unlisted virtual directory.

IIS support creation FTP virtual folders. Such foldres is not listed in LS command, but can be accessed by name (CWD/LS) as well as files in it.",smartshark_2_2,316,commons-vfs,"""[0.8396313190460205, 0.1603686511516571]"""
1182,229441,Make DefaultFileSelectInfo public,"See $summary, and patch.",smartshark_2_2,557,commons-vfs,"""[0.9983125925064087, 0.0016874180873855948]"""
1183,229442,Allow file operations to return a result,"Currently the file operations can not return any result after being executed. A simple interface should allow marking the operations that return something without being too strict about what they return.

I'm working on a command line shell based on VFS that also allows executing operations (http://vfs-utils.sourceforge.net/shell). For the moment I found nothing better than to check for a method getResult() but it I would prefer to just check for the interface.

The interface could be something like this:

package org.apache.commons.vfs.operations;

public interface FileOperationResult {

	public abstract Object getResult();
}

For me returning a String would be good enough,  but I think returning an Object the most appropriate. In the vfs shell I just do a toString() on it. 

",smartshark_2_2,283,commons-vfs,"""[0.9984961748123169, 0.0015037811826914549]"""
1184,229443,add RandomAccessContent.setLength() method,"I'm developing a Commons VFS plug-in for Google App Engine (http://code.google.com/p/gaevfs/), and also using this as a basis for a pluggable file system for the H2 relational database (http://www.h2database.com/html/main.html) that will allow H2 to run on Google App Engine. In order to support H2, I need to support a setLength() method for my RandomAccessContent implementation. Note that setLength() is supported by java.io.RandomAccessFile (http://java.sun.com/javase/6/docs/api/index.html), so adding this method will bring RandomAccessContent a little closer to RandomAccessFile.",smartshark_2_2,265,commons-vfs,"""[0.9984487295150757, 0.0015512678073719144]"""
1185,229444,VfsClassLoaderTests fails on Java 9,"Running the build with Oracle Java 9 on Windows 10 fails. The same failures happen with version 2.1.

{noformat}
Tests run: 84, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 4.01 sec <<< FAILURE! - in org.apache.commons.vfs2.provider.jar.test.NestedJarTestCase
testSealing(org.apache.commons.vfs2.impl.test.VfsClassLoaderTests)  Time elapsed: 0 sec  <<< ERROR!
java.lang.ClassNotFoundException: code.sealed.AnotherClass
        at org.apache.commons.vfs2.impl.VFSClassLoader.findClass(VFSClassLoader.java:152)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:563)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
        at org.apache.commons.vfs2.impl.test.VfsClassLoaderTests.testSealing(VfsClassLoaderTests.java:88)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:564)
        at org.apache.commons.vfs2.test.AbstractProviderTestCase.runTest(AbstractProviderTestCase.java:190)
        at junit.framework.TestCase.runBare(TestCase.java:141)
        at junit.framework.TestResult$1.protect(TestResult.java:122)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at junit.framework.TestResult.run(TestResult.java:125)
        at junit.framework.TestCase.run(TestCase.java:129)
        at junit.framework.TestSuite.runTest(TestSuite.java:252)
        at junit.framework.TestSuite.run(TestSuite.java:247)
        at junit.extensions.TestDecorator.basicRun(TestDecorator.java:23)
        at org.apache.commons.vfs2.test.AbstractTestSuite$1.protect(AbstractTestSuite.java:132)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at org.apache.commons.vfs2.test.AbstractTestSuite.run(AbstractTestSuite.java:137)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:86)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: org.apache.commons.vfs2.FileSystemException: Could not retrieve the certificates of ""jar:jar:file:///C:/vcs/svn/apache/commons/trunks-proper/vfs/commons-vfs2/target/test-classes/test-data/nested.jar!/test.jar!/code/sealed/AnotherClass.class"".
        at org.apache.commons.vfs2.provider.DefaultFileContent.getCertificates(DefaultFileContent.java:331)
        at org.apache.commons.vfs2.impl.VFSClassLoader.defineClass(VFSClassLoader.java:180)
        at org.apache.commons.vfs2.impl.VFSClassLoader.findClass(VFSClassLoader.java:150)
        ... 27 more
Caused by: java.lang.IllegalStateException: zip file closed
        at java.base/java.util.zip.ZipFile.ensureOpen(ZipFile.java:664)
        at java.base/java.util.zip.ZipFile.getInputStream(ZipFile.java:334)
        at java.base/java.util.jar.JarFile.getBytes(JarFile.java:761)
        at java.base/java.util.jar.JarFile.checkForSpecialAttributes(JarFile.java:970)
        at java.base/java.util.jar.JarFile.isMultiRelease(JarFile.java:366)
        at java.base/java.util.jar.JarFile$JarFileEntry.realEntry(JarFile.java:642)
        at java.base/java.util.jar.JarFile$JarFileEntry.getCertificates(JarFile.java:626)
        at org.apache.commons.vfs2.provider.jar.JarFileObject.doGetCertificates(JarFileObject.java:120)
        at org.apache.commons.vfs2.provider.DefaultFileContent.getCertificates(DefaultFileContent.java:325)
        ... 29 more

testLoadClass(org.apache.commons.vfs2.impl.test.VfsClassLoaderTests)  Time elapsed: 0 sec  <<< ERROR!
java.lang.ClassNotFoundException: code.ClassToLoad
        at org.apache.commons.vfs2.impl.VFSClassLoader.findClass(VFSClassLoader.java:152)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:563)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
        at org.apache.commons.vfs2.impl.test.VfsClassLoaderTests.testLoadClass(VfsClassLoaderTests.java:61)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:564)
        at org.apache.commons.vfs2.test.AbstractProviderTestCase.runTest(AbstractProviderTestCase.java:190)
        at junit.framework.TestCase.runBare(TestCase.java:141)
        at junit.framework.TestResult$1.protect(TestResult.java:122)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at junit.framework.TestResult.run(TestResult.java:125)
        at junit.framework.TestCase.run(TestCase.java:129)
        at junit.framework.TestSuite.runTest(TestSuite.java:252)
        at junit.framework.TestSuite.run(TestSuite.java:247)
        at junit.extensions.TestDecorator.basicRun(TestDecorator.java:23)
        at org.apache.commons.vfs2.test.AbstractTestSuite$1.protect(AbstractTestSuite.java:132)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at org.apache.commons.vfs2.test.AbstractTestSuite.run(AbstractTestSuite.java:137)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:86)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: org.apache.commons.vfs2.FileSystemException: Could not retrieve the certificates of ""jar:jar:file:///C:/vcs/svn/apache/commons/trunks-proper/vfs/commons-vfs2/target/test-classes/test-data/nested.jar!/test.jar!/code/ClassToLoad.class"".
        at org.apache.commons.vfs2.provider.DefaultFileContent.getCertificates(DefaultFileContent.java:331)
        at org.apache.commons.vfs2.impl.VFSClassLoader.defineClass(VFSClassLoader.java:180)
        at org.apache.commons.vfs2.impl.VFSClassLoader.findClass(VFSClassLoader.java:150)
        ... 27 more
Caused by: java.lang.IllegalStateException: zip file closed
        at java.base/java.util.zip.ZipFile.ensureOpen(ZipFile.java:664)
        at java.base/java.util.zip.ZipFile.getInputStream(ZipFile.java:334)
        at java.base/java.util.jar.JarFile.getBytes(JarFile.java:761)
        at java.base/java.util.jar.JarFile.checkForSpecialAttributes(JarFile.java:970)
        at java.base/java.util.jar.JarFile.isMultiRelease(JarFile.java:366)
        at java.base/java.util.jar.JarFile$JarFileEntry.realEntry(JarFile.java:642)
        at java.base/java.util.jar.JarFile$JarFileEntry.getCertificates(JarFile.java:626)
        at org.apache.commons.vfs2.provider.jar.JarFileObject.doGetCertificates(JarFileObject.java:120)
        at org.apache.commons.vfs2.provider.DefaultFileContent.getCertificates(DefaultFileContent.java:325)
        ... 29 more
{noformat}

Also, we get these warnings:

{noformat}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.commons.vfs2.test.AbstractTestSuite (file:/C:/vcs/svn/apache/commons/trunks-proper/vfs/commons-vfs2/target/test-classes/) to field java.lang.Thread.target
WARNING: Please consider reporting this to the maintainers of org.apache.commons.vfs2.test.AbstractTestSuite
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
{noformat}
",smartshark_2_2,626,commons-vfs,"""[0.9723642468452454, 0.027635743841528893]"""
1186,229445,StaticUserAuthenticator prevents reuse of FileSystem,"If a StaticUserAuthenticator is used in the FileSystemOptions it causes that the FileSystem cannot be reused, since the options are part of the key used in the map that tracks the FileSystems. This UserAuthenticator should have proper equals and hashCode methods as well as implement Comparable, since the map is actually a TreeMap.",smartshark_2_2,129,commons-vfs,"""[0.15667538344860077, 0.843324601650238]"""
1187,229446,Update to Apache Commons Compress 1.5,Update to Apache Commons Compress 1.5 from 1.4.1.,smartshark_2_2,447,commons-vfs,"""[0.998099148273468, 0.0019008195959031582]"""
1188,229447,FUSE integration,"Integrating FUSE (http://fuse.sourceforge.net/) and Commons-VFS would ease the creation of new file system drivers for the GNU/linux and FreeBSD platforms.

Besides, it would be really cool to mount a partition whose handling is done in Java ;-)",smartshark_2_2,88,commons-vfs,"""[0.9982454776763916, 0.001754540833644569]"""
1189,229448,[FTP] Configuration does not include option for setting socket timeout.,"The FTP Configuration includes an option to set a timeout for the data connection, but not for the socket timeout. This is a problem, as idle sockets can cause your download to hang forever and never timeout.",smartshark_2_2,515,commons-vfs,"""[0.7896239757537842, 0.21037603914737701]"""
1190,229449,Add FileSystemManager.resolveFile(URI) and resolveFile(URL),Add FileSystemManager.resolveFile(URI) and resolveFile(URL).,smartshark_2_2,374,commons-vfs,"""[0.9984257221221924, 0.001574285444803536]"""
1191,229450,Port file filters from Apache Commons IO to VFS,"Apache Commons IO has a similar file filter concept as VFS and provides ~20 filter implementations. VFS misses such implementations.

Therefore I ported the IO filters, created & updated unit tests and now I want to contribute that code to VFS.

The code is currently available on GitHub:
https://github.com/fuinorg/commons-vfs2-filters
",smartshark_2_2,291,commons-vfs,"""[0.9983589053153992, 0.0016410528915002942]"""
1192,229451,Add FileObject[] getFiles() from VFSClassLoader,"In walking through ClassLoaders in a system, it would be very useful to get the FileObjects that define a VFSClassLoader similar to how URLClassLoader allows retrieval of the URLs that make up the classloader.",smartshark_2_2,534,commons-vfs,"""[0.9984303116798401, 0.0015696976333856583]"""
1193,229452,Allow pluggable HttpClient implementation for providers using http,We have a need to supply our own implementation of HttpClient in order to track the network traffic.   To do this was a very small change; I have a patch and I intended to attach it to this bug report.  But right now I can't see anywhere to do so.  Perhaps that happens after I submit it initially.,smartshark_2_2,544,commons-vfs,"""[0.9983469247817993, 0.0016530218999832869]"""
1194,229453,Support FEAT and MLSx commands for FTP performance improvements.,"While using VFS to synchronize/push files between a local file system and an FTP server I noticed severe performance issues whenever the FTP directory had a large number of files. Each refresh of an FtpFileObject asks for a list of the files in the parent directory, and then attempts to pick out the information for a single file.  Performance will degrade quickly as the number of files to be listed increases.

I'd suggest using MLST and MLSD as an alternative to LIST, where possible.  MLST can retrieve the necessary file information without listing a whole directory.  Both commands also provide unambiguous time-stamps, which was also a problem that I ran into while trying to compare modification times between the local and FTP file systems.

Attached is a patch that probably does the job.  In cases where MLSx features are not supported, the original LIST command will be used.  Tests are included.  Since I'm new to VFS I'm not sure if this covers all cases, so be warned.",smartshark_2_2,324,commons-vfs,"""[0.9982953667640686, 0.0017046737484633923]"""
1195,229454,HttpFileObject read/write attributes should reflect underlying FileSystem capabilities,"When you do a:
FileObject fo = fsManager.resolveFile(""http://host:port/somefile.xml"");
VFS will create a HTTP FileSystem that has only read capabilities. However, fo.isWritable() returns true where
fo.getFileSystem().hasCapability(Capability.WRITE_CONTENT)); return false as expected.

This is inconsistent in my opinion and should be fixed or is it intentionally implemented that way?
  ",smartshark_2_2,413,commons-vfs,"""[0.8127144575119019, 0.18728554248809814]"""
1196,229455,"[VFS][PATCH] Rename File, Push down FileSystem configuration, ....","I will attach a path with the following enhancements:

*) RenameFile
If the file is in the same filesystem, it will be renamed by the protocol
command, else the copyFrom/delete is issued

*) PushDown FileSystem configuration
On resolveFile you could pass a properties collection which will be used to
configure the underlaying FileSystem. e.g. with FTP you could set the
FTPFileEntryParserFactory
Internally a new FileSystem will be created if you call resolveFile with a
different set of properties.

*) WebDav
Fixed the compile-error with WebDav, now it is compilable with the latest webdav
again.

*) commons-net, jcifs, jsch
Moved to latest version",smartshark_2_2,57,commons-vfs,"""[0.9948240518569946, 0.00517595000565052]"""
1197,229456,Allow files to return a custom FileContentInfo,See attached patch.,smartshark_2_2,552,commons-vfs,"""[0.9983508586883545, 0.0016491557471454144]"""
1198,229457,[FTP] Allow configuring remoteVerificationEnabled on FTPClient instances,"VFS provides no easy/robust way to have setRemoteVerificationEnabled(..) called on the org.apache.commons.net.ftp.FTPClient instances that it creates.

This is a problem for me because I need to call setRemoteVerificationEnabled(false) for my application to work correctly.

In order to call this method I had to extend FtpFileProvider and FtpFileSystem, and I had to completely copy and paste FtpClientWrapper to create my own variant that calls setRemoteVerificationEnabled(..) on its wrapped FTPClient.

This situation could be improved by augmenting FtpFileSystemConfigBuilder to add a remote verification option. I found an old patch to do this: http://apache-commons.680414.n4.nabble.com/VFS-PATCH-Allow-configuration-of-FTP-remote-verification-td716436.html",smartshark_2_2,356,commons-vfs,"""[0.9966118931770325, 0.0033881310373544693]"""
1199,229458,Add FileContent write APIs,"Add APIs to write FileContent objects to an OutputStream, FileContent, and FileObject.

In 1.0, 2.0, all writing must be done manually via FileContent#getOutputStream() or FileUtil. 

Make writing a first class citizen in the API. 

Make FileUtil reuse the new FileContent API.

{code:java}
write(FileContent)
write(FileObject)
write(OutputStream)
write(OutputStream, int)
{code}

In more detail:

{code:java}
    /**
     * Writes this content to another FileContent.
     * 
     * @param output
     *            The target OutputStream.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(FileContent output) throws IOException;
    
    /**
     * Writes this content to another FileObject.
     * 
     * @param file
     *            The target FileObject.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(FileObject file) throws IOException;
    
    /**
     * Writes this content to an OutputStream.
     * 
     * @param output
     *            The target OutputStream.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(OutputStream output) throws IOException;
    
    /**
     * Writes this content to an OutputStream.
     * 
     * @param output
     *            The target OutputStream.
     * @param bufferSize
     *            The buffer size to write data chunks.
     * @throws IOException
     *             if an error occurs writing the file.
     * @since 2.1             
     */
    long write(OutputStream output, int bufferSize) throws IOException;
{code}",smartshark_2_2,372,commons-vfs,"""[0.9983969330787659, 0.0016030953265726566]"""
1200,229459,[VFS] Synchronized copy,"There exists a ant-task called ""org.apache.commons.vfs.tasks.SyncTask""
It would be relevant to use this feature standalone (without Ant)

See :
http://mail-archives.apache.org/mod_mbox/jakarta-commons-user/200505.mbox/%3c4281C805.4030207@sophia.inria.fr%3e",smartshark_2_2,42,commons-vfs,"""[0.9983868598937988, 0.001613211934454739]"""
1201,229460,[tests] ProviderWriteTests#testListener does not fail cleanly,"When running ProviderTests against my new implementation the #testListener() test failed (expected). However the exception cause the listener to not be unregistered, which in turn leads to a (random) next test failing with unexpected events. A try/finally solves the problem.

    public void testListener() throws Exception
    {
        final FileObject baseFile = createScratchFolder();

        FileObject child = baseFile.resolveFile(""newfile.txt"");
        assertTrue(!child.exists());

        FileSystem fs = baseFile.getFileSystem();
        TestListener listener = new TestListener(child);
        fs.addListener(child, listener);
+        try
+        {
        // Create as a folder
...
+        } finally {
            fs.removeListener(child, listener);
+        }
       }

The problem happens for me in 2.0, but trunk seems to have the same problem: http://svn.apache.org/viewvc/commons/proper/vfs/trunk/core/src/test/java/org/apache/commons/vfs2/test/ProviderWriteTests.java?revision=1437556&view=markup (Line 629)",smartshark_2_2,360,commons-vfs,"""[0.8910856246948242, 0.108914315700531]"""
1202,229461,add ProviderTestConfig.getDefaultFileSystemManager() method,I'm developing a plug-in for Google App Engine (http://code.google.com/p/gaevfs/) and in order to run junit tests on this plug-in I need to be able to create an instance of my own subclass of DefaultFileSystemManager.,smartshark_2_2,142,commons-vfs,"""[0.9985573887825012, 0.0014426647685468197]"""
1203,229462,WeldContainerControl throws a NullPointer before a null check.,"If not called the right way, the calls for BeanManager are invoked after a null check.  We should do the null check first, then continue.  Throw the log message still.",smartshark_2_2,364,deltaspike,"""[0.133257657289505, 0.8667423129081726]"""
1204,229463,WindowHandler: JS error on IE8,because .bind doesn't exist in IE8,smartshark_2_2,1001,deltaspike,"""[0.2157628834247589, 0.7842370867729187]"""
1205,229464,java.util.Properties Provider does not close InputStream after usage,"{{org.apache.deltaspike.core.api.resourceloader.AbstractResourceProvider}} does not close it's manually retrieved {{java.io.InputStream}} after it has read in the stream into the Properties, causing a potential memory leak.",smartshark_2_2,843,deltaspike,"""[0.09807552397251129, 0.9019244313240051]"""
1206,229465,f:viewAction is executed twice with LAZY window handling mode,"If you define h:head within your site and use ds:windowId (ClientWindowRenderMode.LAZY), then f:viewAction is called twice (2 window ids are generated)

{code mypage.xhtml}
<?xml version=""1.0"" encoding=""UTF-8"" ?>
<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
<html xmlns=""http://www.w3.org/1999/xhtml""
	xmlns:f=""http://xmlns.jcp.org/jsf/core""
	xmlns:h=""http://xmlns.jcp.org/jsf/html""
	xmlns:ui=""http://xmlns.jcp.org/jsf/facelets""
	xmlns:p=""http://xmlns.jcp.org/jsf/passthrough""
	xmlns:jsf=""http://xmlns.jcp.org/jsf""
	xmlns:ds=""http://deltaspike.apache.org/jsf"">

	<ui:remove>without h:head everything works correct</ui:remove>
	<h:head></h:head>

	<f:metadata>
	   	<f:viewParam id=""myid"" name=""myid"" value=""#{viewParamBean.myId}""/>
	   	<f:viewAction action=""#{initCtr.initApplication()}""/>
	</f:metadata>
	
	<h:body>
		<ds:windowId/>
		<h:form>
			#{viewParamBean.myId}
		</h:form>
	</h:body>
</html>
{code}

A testcase is attached. 
Start application and open http://localhost:8080/jsftest22/mypage.jsf
You can see in the logs, that the viewAction is called two times.
If you remove h:head from site, viewAction is called only once, which is the correct behaviour.
If you don't use ds:windowId and have h:head defined, then viewAction is also called only once, which is the correct behaviour.

I don't know if this is a deltaspike bug or a bug within myfaces.",smartshark_2_2,666,deltaspike,"""[0.11654427647590637, 0.8834556937217712]"""
1207,229466,NPE when invoking proxy for custom jsf converter,"I am not sure whether it's during invoking or creating proxy class but it manifests with following exception. The problem is that org.apache.deltaspike.core.util.proxy.invocation.DelegateManualInvocationHandler#proceedOriginal is called before any delegateInvocationHandler is actually set (see org.apache.deltaspike.core.util.proxy.DeltaSpikeProxyContextualLifecycle#create):
{noformat}
13:40:53,059 SEVERE [org.jboss.examples.deltaspike.expensetracker.app.exception.ExceptionHandlers] (http-/127.0.0.1:8080-2) Handled exception: java.lang.NullPointerException
	at org.apache.deltaspike.core.util.proxy.invocation.DelegateManualInvocationHandler.proceedOriginal(DelegateManualInvocationHandler.java:41) [deltaspike-core-api-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.apache.deltaspike.core.util.invocation.AbstractManualInvocationHandler.invoke(AbstractManualInvocationHandler.java:62) [deltaspike-core-api-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.apache.deltaspike.core.util.proxy.invocation.DelegateManualInvocationHandler.staticInvoke(DelegateManualInvocationHandler.java:34) [deltaspike-core-api-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.jboss.examples.deltaspike.expensetracker.app.converter.TestCurrencyConverter$$DSJsfProxy.clearInitialState(Unknown Source)
	at javax.faces.convert.NumberConverter.setLocale(NumberConverter.java:474) [jboss-jsf-api_2.1_spec-2.1.28.Final-redhat-1.jar:2.1.28.Final-redhat-1]
	at org.jboss.examples.deltaspike.expensetracker.app.converter.TestCurrencyConverter.<init>(TestCurrencyConverter.java:12) [classes:]
	at org.jboss.examples.deltaspike.expensetracker.app.converter.TestCurrencyConverter$$DSJsfProxy.<init>(Unknown Source)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [rt.jar:1.8.0_31]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [rt.jar:1.8.0_31]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [rt.jar:1.8.0_31]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408) [rt.jar:1.8.0_31]
	at java.lang.Class.newInstance(Class.java:438) [rt.jar:1.8.0_31]
	at org.apache.deltaspike.core.util.proxy.DeltaSpikeProxyContextualLifecycle.create(DeltaSpikeProxyContextualLifecycle.java:63) [deltaspike-core-api-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.apache.deltaspike.core.util.bean.ImmutableBean.create(ImmutableBean.java:72) [deltaspike-core-api-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.jboss.weld.context.AbstractContext.get(AbstractContext.java:103) [weld-core-1.1.28.Final-redhat-1.jar:1.1.28.Final-redhat-1]
	at org.jboss.weld.bean.proxy.ContextBeanInstance.getInstance(ContextBeanInstance.java:90) [weld-core-1.1.28.Final-redhat-1.jar:1.1.28.Final-redhat-1]
	at org.jboss.weld.bean.proxy.ProxyMethodHandler.invoke(ProxyMethodHandler.java:79) [weld-core-1.1.28.Final-redhat-1.jar:1.1.28.Final-redhat-1]
	at org.jboss.examples.deltaspike.expensetracker.app.converter.TestCurrencyConverter$Proxy$_$$_WeldClientProxy.markInitialState(TestCurrencyConverter$Proxy$_$$_WeldClientProxy.java) [classes:]
	at javax.faces.component.UIOutput.markInitialState(UIOutput.java:197) [jboss-jsf-api_2.1_spec-2.1.28.Final-redhat-1.jar:2.1.28.Final-redhat-1]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1295) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1299) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.markInitialState(FaceletViewHandlingStrategy.java:1285) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.application.view.FaceletViewHandlingStrategy.buildView(FaceletViewHandlingStrategy.java:940) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.lifecycle.RenderResponsePhase.execute(RenderResponsePhase.java:99) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.lifecycle.Phase.doPhase(Phase.java:101) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at com.sun.faces.lifecycle.LifecycleImpl.render(LifecycleImpl.java:139) [jsf-impl-2.1.28.redhat-7.jar:2.1.28.redhat-7]
	at org.apache.deltaspike.jsf.impl.listener.request.DeltaSpikeLifecycleWrapper.render(DeltaSpikeLifecycleWrapper.java:111) [deltaspike-jsf-module-impl-ee6-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at javax.faces.webapp.FacesServlet.service(FacesServlet.java:594) [jboss-jsf-api_2.1_spec-2.1.28.Final-redhat-1.jar:2.1.28.Final-redhat-1]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:295) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.deltaspike.servlet.impl.event.EventBridgeFilter.doFilter(EventBridgeFilter.java:59) [deltaspike-servlet-module-impl-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:246) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.deltaspike.servlet.impl.produce.RequestResponseHolderFilter.doFilter(RequestResponseHolderFilter.java:63) [deltaspike-servlet-module-impl-1.3.1-SNAPSHOT.jar:1.3.1-SNAPSHOT]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:246) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:231) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:149) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.jboss.as.jpa.interceptor.WebNonTxEmCloserValve.invoke(WebNonTxEmCloserValve.java:50) [jboss-as-jpa-7.5.0.Final-redhat-17.jar:7.5.0.Final-redhat-17]
	at org.jboss.as.jpa.interceptor.WebNonTxEmCloserValve.invoke(WebNonTxEmCloserValve.java:50) [jboss-as-jpa-7.5.0.Final-redhat-17.jar:7.5.0.Final-redhat-17]
	at org.jboss.as.web.security.SecurityContextAssociationValve.invoke(SecurityContextAssociationValve.java:169) [jboss-as-web-7.5.0.Final-redhat-17.jar:7.5.0.Final-redhat-17]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:150) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:97) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:102) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:344) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:854) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:653) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:926) [jbossweb-7.5.4.Final-redhat-1.jar:7.5.4.Final-redhat-1]
	at java.lang.Thread.run(Thread.java:745) [rt.jar:1.8.0_31]
{noformat}",smartshark_2_2,842,deltaspike,"""[0.08877887576818466, 0.9112211465835571]"""
1208,229467,activateGlobalAlternatives is broken,"for OWB:

a.) We shall not need to do anything special for OWB at all. OWB by default comes without BDA enabled. If people have BDA enabled in their OWB installation, then they can _easily_ switch it off again via a simple property. There is really no need to do something special for OWB.

b.) replacing the AnnotatedType with a completely different return Type screws up the CDI container. It's really not expected and I'm not sure if this is allowed at all.

c.) by replacing the target type, you end up not scanning one of this class at all, effectively disabling Extension processing for it.


for Weld:
instead of introducing our own properties we shall scan the beans.xml and act accordingly.


in general: mixing this complicated topic with the @Exclude extension is highly confusing.
",smartshark_2_2,275,deltaspike,"""[0.10084998607635498, 0.8991499543190002]"""
1209,229468,Secured Stereotypes are not applied to inherited methods,"I have a @Secured @Stereotype annotation

{code:java}
@Retention( RUNTIME )
@Stereotype
@Inherited
@Secured( CustomAccessDecisionVoter.class ) 
@Target( { ElementType.TYPE, ElementType.METHOD } ) 
public @interface Permission {

}
{code}

And my decision voter:

{code:java}
@ApplicationScoped
public class CustomAccessDecisionVoter extends AbstractAccessDecisionVoter {
    @Override
    protected void checkPermission( AccessDecisionVoterContext voterContext, Set<SecurityViolation> violations )
    {
        System.out.println( ""Checking permission for "" + voterContext.<InvocationContext> getSource().getMethod().getName() );
    }

}
{code}

And now a bean that inherits from another class

{code:java}
public class Animal
{
    public String getParentName()
    {
        return ""parent"";
    }
}
{code}

{code:java}
@Named
@Permission
public class Dog extends Animal
{
    public String getChildName()
    {
        return ""dog"";
    }
}
{code}

In JSF dogName: 
{code}#{dog.childName}{code} will invoke the checkPermission whereas   {code}#{dog.parentName}{code} will not

This is in contrast to the @SecurityBindingType 

{code:java}
@Retention( value = RetentionPolicy.RUNTIME ) 
@Target( { ElementType.TYPE, ElementType.METHOD } ) 
@Documented 
@SecurityBindingType
public @interface UserLoggedIn {

}
{code}

{code:java}
@ApplicationScoped
public class LoginAuthorizer
{
    @Secures
    @UserLoggedIn
    public boolean doSecuredCheck( InvocationContext invocationContext ) throws Exception
    {
        System.out.println( ""doSecuredCheck called for: "" + invocationContext.getMethod().getName() );

        return true;
    }
}
{code}

Now applying @UserLoggedIn to  the Dog class will cause the doSecuredCheck to fire for both getChildName and getParentName

",smartshark_2_2,1267,deltaspike,"""[0.1998462677001953, 0.8001536726951599]"""
1210,229469,DeltaSpikeFacesContextFactory construction issue in non EE ,"In non-EE environments you may be using Weld Servlet to initialize CDI. 
However because of JSF zeroconfig you cannot guarantee that weld servlet will be started before JSF. 
During JSF startup DeltaSpikeFacesContextFactory is created and clientWindow is populated, but it throws an exception because the following will only work once the CDI container is started: 
this.clientWindow = BeanProvider.getContextualReference(ClientWindow.class, true);

Recommend not caching the clientWindow.

I can confirm that JSF seems to load OK after this is fixed.",smartshark_2_2,476,deltaspike,"""[0.26255863904953003, 0.73744136095047]"""
1211,229470,EntityManagerLookup isn't thread-safe,"EntityManagerLookup has a private variable (dependentResolverProvider) which can be modified by multiple threads, if the repository is called by multiple threads.
",smartshark_2_2,1041,deltaspike,"""[0.0645427256822586, 0.9354572296142578]"""
1212,229471,NPE when job startScopes is set to empty,"When setting a job to start empty scope, it will trigger a NPE when trying to destroy contextControl.
In org.apache.deltaspike.scheduler.impl.QuartzScheduler.JobListenerContext it doesn't seem to initialize contextControl when start scope is emtpy. Therefore when invoking stopStartedScopes(), it will cause a NPE.",smartshark_2_2,983,deltaspike,"""[0.06833604723215103, 0.9316639304161072]"""
1213,229472,BeanBuilder#readFromType should respect @Typed,"{{BeanBuilder#readFromType}} currently includes all superclasses of a type in the set of bean types and it does not respect the {{@Typed}} annotation.

This currently also causes a problem with PartialBeans which use {{@Typed}}.

Example:
{code}
public interface Rules {}
{code}
{code}
public class BaseRules implements Rules {}
{code}
{code}
@SomePartialBeanBinding
@Typed(AdminRules.class)
public abstract class AdminRules implements Rules {}
{code}

{code}
public class Controller {
    @Inject private Rules rules;
}
{code}


This causes a deployment exception - ambiguous dependencies. The expected behaviour would be, that {{AdminRules}} is not resolvable as {{Rules}}.",smartshark_2_2,697,deltaspike,"""[0.6724715828895569, 0.3275284469127655]"""
1214,229473,ds:disableClientWindow prevents child rendering with CLIENTWINDOW mode,"        <ds:disableClientWindow>
            <h:link value=""Link"" outcome=""test.xhtml"" />
        </ds:disableClientWindow>

Link is not rendered with CLIENTWINDOW mode.",smartshark_2_2,536,deltaspike,"""[0.0936790183186531, 0.9063209891319275]"""
1215,229474,DefaultMessageInterpolator/LocaleResolver doesn't get picked up properly,"DefaultMessageInterpolator doesn't get picked up properly. It only gets set on a MessageContext#reset().

We shall also change it to a @Dependent bean which can be replaced with an own @Alternative.
Same is true for LocaleResolver.",smartshark_2_2,269,deltaspike,"""[0.12905117869377136, 0.8709487915039062]"""
1216,229475,improved testability of @WindowScoped beans,"currently it can be done easily if you just have deltaspike-core on the classpath. once the jsf module is on the classpath as well, myfaces-test is needed in addition even though several constellations could be tested without it.",smartshark_2_2,733,deltaspike,"""[0.9983886480331421, 0.0016114171594381332]"""
1217,229476,Create a new module for proxy,"We are now using asm to create our own proxies - with 1.3.0 that was added to the partial bean module - now We replace the optional proxies created in the JSF module for converters/validators with the same approach -> the proxy code was moved to ds-core. that works but isn't nice and not that flexible -> the goal is an own module ""side by side"" with core and all modules which need proxy stuff have core and that new proxy module as dep. (currently the partial-bean and JSF module).It allows to support different versions of asm once it's needed... (e.g. for diff. versions of java if the latest version drops e.g. backward compat. to and old version...) It's already discussed and agreed on the dev list so you can just create a ticket and just move the code + update the release poms so that the new module is part of the release as well...

The new module should be called proxy-utils. GAV: org.apache.deltaspike.modules:deltaspike-proxy-module-api

*don't* move org.apache.deltaspike.core.util.ProxyUtils - because it doesn't depend on asm and is valid for any proxy (weld-proxy, owb-proxy, javassist-proxy,...) We support at all + it's needed a lot in ds-core. If you would move it ds-core would depend on the proxy-module (which shouldn't be the case).

The goal is really that the proxy-module is just needed once you need the partial-bean module and/or the optional proxies in the JSF module so the proxy-module needs to be an optional dep. of the JSF module... but in case of the partial-bean module the proxy-api module is a compile dep. since it's required anyway

That's currently the release blocker, because now ds-core has a dependency to a fixed version of asm and so far Core didn't introduce a 3rd party dep. So We need to move to the new module before the next release...


",smartshark_2_2,870,deltaspike,"""[0.9985378980636597, 0.0014620785368606448]"""
1218,229477,Provide add() methods instead of save() methods in EntityRepository,"The semantics of Repository is to model a Collection of persistent objects.  There is no need to add() an object to a Collection if it is already in that Collection.

By providing save() methods you lead developers to invoke save() on objects which they got from the Repository.  This is (usually) inappropriate.

Please provide add() methods to parallel the save() method signatures.  Ideally the save() methods would be deprecated.  

Even if the save() methods remain, IDE inspections can then be configured to forbid their invocation.",smartshark_2_2,929,deltaspike,"""[0.9983931183815002, 0.0016068123513832688]"""
1219,229478,documentation for ContainerControl API,the wiki is outdated,smartshark_2_2,538,deltaspike,"""[0.9966593980789185, 0.003340567462146282]"""
1220,229479,Refactor RepositoryComponent/s,"Currently a RepositoryComponent is initialized during PAT. Therefore RepositoryComponent requires a lazyInit.
If we move the actual creation of the RepositoryComponent's to AfterBeanDiscovery, we don't need such lazy init ""hacks"".

IMO we should also split the RepositoryComponent into a data object and a initializer as it currently feels very unstructured.
Same applies for the RepositoryMethod and RepositoryEntity.

We could also allign the naming a little bit:
RepositoryComponents -> something MetadataStore or MetadataManager
RepositoryComponent -> RepositoryMetadata
RepositoryMethod -> RepositoryMethodMetadata
RepositoryEntity -> EntityMetadata



",smartshark_2_2,1229,deltaspike,"""[0.9984706044197083, 0.0015293769538402557]"""
1221,229480,AnnotatedTypeBuilder,we agreed on it - see http://s.apache.org/VuD,smartshark_2_2,2,deltaspike,"""[0.9979661703109741, 0.0020338469184935093]"""
1222,229481,@Exclude,"we agreed on merging: DELTASPIKE-6, DELTASPIKE-7, DELTASPIKE-8
see http://s.apache.org/KB5

first draft of the result:
@Exclude
@Exclude(inProjectStage=Production.class)
@Exclude(notInProjectStage=UnitTest.class)
@Exclude(onExpression=""myProperty==myValue"")
@Exclude(onExpression=""[my custom expression syntax]"", interpretedBy=CustomExpressionInterpreter.class)",smartshark_2_2,21,deltaspike,"""[0.9301442503929138, 0.06985577940940857]"""
1223,229482,Allow the use of @Alternative to implement a custom TimestampsProvider,"I'm using the JPA {{@EntityListeners(AuditEntityListener.class)}} and the audit functionality of Deltaspike Data ({{@CreatedOn}}, {{@ModifiedOn}} and {{@ModifiedBy}} annotations) on an entity bean with the difference that I have a custom implementation of {{java.time.ChronoLocalDateTime}} which converts any {{LocalDateTime}} + {{ZoneOffset}} OR a {{ZonedDateTime}} to be the UTC date & time.

Is there a way to implement my own {{org.apache.deltaspike.data.impl.audit.PrePersistAuditListener}} and {{org.apache.deltaspike.data.impl.audit.PreUpdateAuditListener}} and use them to create the instance of {{UTCDateTime}} required by he fields marked with {{@InsertedOn}} and {{@ModiefiedOn}}?

See this [SO Question|http://stackoverflow.com/questions/41057116/deltaspike-data-cdi-jpa-custom-prepersistauditlistener-and-preupdateauditlis] and the discussion on this [mail thread|https://lists.apache.org/thread.html/c47a6187d03fc8f665c6b3c7ab760e80084db3b8d4f83fe9d34015d8@%3Cusers.deltaspike.apache.org%3E]",smartshark_2_2,1356,deltaspike,"""[0.9985608458518982, 0.0014392107259482145]"""
1224,229483,Support WLS Profile in Data Module,Add a WLS-specific test-persistence.xml included by the WLS profiles.,smartshark_2_2,601,deltaspike,"""[0.9984869956970215, 0.0015129880048334599]"""
1225,229484,discuss manual interceptor implementation,"due to a major restriction concerning interceptors for custom Bean<T>, InjectionTarget<T> and Producer<T>, we should think about a manual interceptor implementation for them. with that we could support interceptors e.g. for Bean<T> created via BeanBuilder, partial-beans as well as mocked beans. (as we have seen) esp. several use-cases for partial-beans and mocked beans need interceptors.",smartshark_2_2,1228,deltaspike,"""[0.9984377026557922, 0.0015622838400304317]"""
1226,229485,deltaspike-cdictrl-owb has a transient runtime dependency on Shrinkwrap and Arquillian,"deltaspike-cdictrl-impl-owb contains a compile dependency on deltaspike test-utils, which in turn has dependencies on pretty old versions of Shrinkwrap, Arquillian and maven-artifact:

[https://github.com/apache/deltaspike/blob/3ed354b373c574442d0e75c4b576010bce42efb8/deltaspike/cdictrl/impl-owb/pom.xml#L41-L45]

Is this necessary?

It can be annoying to find out why dependencies are suddenly wrong if you're not aware of it.Â ",smartshark_2_2,1334,deltaspike,"""[0.9965658783912659, 0.003434160491451621]"""
1227,229486,Support for TransactionAttributeType REQUIRE_NEW ,"@Transactional currently open a transaction when method is invoked.
if a sub method is invoked also annoted @Transactional the second method uses the first method transaction and not a new one.

ex:
@Transactional
void createEntity(){
  ...
  audit()
   throw Exception(""a problem occurs here"")
}

@transactional
void audit(){
   //persist some audit stuff
   ...
}

Actually everything is rolled back because of the exception in the top method (createEntity)
@transactional should have a parameter like (REQUIRE_NEW, NEVER...) to handle with details transaction's lifecycle
",smartshark_2_2,281,deltaspike,"""[0.9243298768997192, 0.07567013055086136]"""
1228,229487,deltaspike website,"see:
http://www.staging.apache.org/dev/project-site.html#svnpubsub
http://www.apache.org/dev/cms.html
http://www.apache.org/dev/cmsref.html",smartshark_2_2,194,deltaspike,"""[0.9972182512283325, 0.002781742252409458]"""
1229,229488,"add dynamic annotations feature, configurable via config","A long time ago I created a CDI Extension which can dynamically add annotations to classes identified via a regular expression.

The Extension is called InterDyn (Interceptor Dynamics) and is hosted at github.

[https://github.com/struberg/InterDyn]

Â 

I'm the sole originary author, so I have all the IP.

The project actually consists of two parts: InterDyn for dynamic interceptors and InvoMon, an invocation monitor interceptor. Kind of small man profiler.

The InterDyn part is actually not restricted to add interceptor annotations, but really could be anything. But adding interceptors obviously makes the most sense. This should go into ds-core-impl. It doesn't even need any api changes.

Â 

The 2nd part (the performance InvocationMonitor. Might become either part of core, or a separate module.

Â 

Â ",smartshark_2_2,1302,deltaspike,"""[0.9984649419784546, 0.0015351262409240007]"""
1230,229489,re-visit wls profile,"wls 12.1.3 was just released -> we can test again, if all issues are resolved already.",smartshark_2_2,600,deltaspike,"""[0.9902181029319763, 0.009781889617443085]"""
1231,229490,review ConfigProperty and Converter logic,"Our current @ConfigProperty implementation is based on complex and expensive ProcessInjectionTarget handling. It also uses the @Qualifier annotation, but in fact it isn't a Qualifier!

We can implement all the original requirements with simple producer methods. 

There is also no need for all the Converter stuff as CDI allows a user to simply provide his own Qualifier and producer methods.",smartshark_2_2,169,deltaspike,"""[0.9984460473060608, 0.0015539985615760088]"""
1232,229491,Port functionality from Seam Mail module,seam-mail is a nice module that eases the mail sending and contains a fluent api for that. Such API will be very useful for many users.,smartshark_2_2,742,deltaspike,"""[0.9983067512512207, 0.0016932470025494695]"""
1233,229492,optional adapter for MockedJsfTestContainer,myfaces-test v1.0.6+ will provide MockedJsfTestContainer -> with an adapter we can delegate to it,smartshark_2_2,434,deltaspike,"""[0.9979522228240967, 0.002047708723694086]"""
1234,229493,Add #isScopeActive to ContextControl,"to allwow e.g.
ctr.isScopeActive(SessionScoped.class);",smartshark_2_2,655,deltaspike,"""[0.9892541170120239, 0.01074590440839529]"""
1235,229494,upgrade version of the tomee-arquillian adapter,to test against tomee 1.7.1,smartshark_2_2,1088,deltaspike,"""[0.998329222202301, 0.0016707851318642497]"""
1236,229495,optional ClassFilter spi,"in several cases it's needed to get rid of beans and/or pre-defined beans.
 that's possible with @Exclude, @Vetoed, @Typed(), scan-tag and ProcessAnnotatedType#veto.
 only #veto supports really dynamic cases - esp. in cases you don't have the class/es in question under your control (or diff. teams have diff. requirements).

a simple (but optional) ClassFilter spi allows:
 * full control (like withÂ #veto - just simpler)
 * the concept of overrulingÂ based on config-ordinals
 * to get rid of special behaviors defined forÂ alternative-beans (see e.g. observers inÂ alternative-beans)
 * to use the concept itself for different features/use-cases
(e.g. a class-filter per test-class via @TestControl to support more isolated tests)",smartshark_2_2,1324,deltaspike,"""[0.9982677698135376, 0.001732165110297501]"""
1237,229496,add @Priority dyn. to deltaspike interceptors to avoid the manual config,"owb doesn't restrict interceptors to the bda which hosts the config -> interceptors provided by deltaspike are active per default for the whole application. weld users have to add all of our interceptors to the beans.xml to use them. since cdi 1.1 global enablement is also supported by weld once @Priority gets used.
we can add it dyn. in case @Priority is on the classpath -> the usage of ds gets simpler for weld users and doesn't look that fragmented.

currently weld has a bug which adds an interceptor twice if it is enabled via beans.xml as well as @Priority - therefore we could e.g. detect multiple invocations (for the same method) in project-stage development and unit-test (-> additional benefit for users of our test-control module) or check the content of beans.xml in case of the same project-stages (during bootstrapping). that would also mean that we can't use our own interceptors internally (since they are enabled for owb 1.1.x/1.2.x users via beans.xml). however, currently we aren't doing that anyway, but we have to check it during bootstrapping to avoid side-effects with features added later on.",smartshark_2_2,727,deltaspike,"""[0.9986633062362671, 0.0013366412604227662]"""
1238,229497,data-documentation missing Optional return value,"http://deltaspike.apache.org/documentation/data.html lists the format for methods:
(Entity|List<Entity>) (prefix)(Property[Comparator]){Operator Property [Comparator]}

Here java.util.Optional is missing:

""(Entity|Optional<Entity>|List<Entity>) (prefix)(Property[Comparator]){Operator Property [Comparator]}
 
Or in more concrete words:
â¢	The query method must either return an entity or an java.util.Optional or a list of entities.
â¢	It must start with the findBy prefix ...""",smartshark_2_2,1235,deltaspike,"""[0.9912034273147583, 0.008796541020274162]"""
1239,229498,Relese Javadoc for 1.4.1 and 1.4.2-SNAPSHOST,It is giving 404 at http://deltaspike.apache.org/javadoc.html,smartshark_2_2,893,deltaspike,"""[0.9977237582206726, 0.002276235492900014]"""
1240,229499,Ability to start transaction scope manually,"Currently, the CDI Context controller class defines methods to manually start the CDI 1.0 provided scopes - Request, Session, Application and ConversationScoped.

There's presently no way in the DS codebase to manually start a TransactionScope using the controllers

See here

https://github.com/apache/deltaspike/blob/master/deltaspike/cdictrl/impl-weld/src/main/java/org/apache/deltaspike/cdise/weld/WeldContextControl.java#L76
https://github.com/apache/deltaspike/blob/master/deltaspike/cdictrl/impl-owb/src/main/java/org/apache/deltaspike/cdise/owb/OpenWebBeansContextControl.java#L72",smartshark_2_2,937,deltaspike,"""[0.9986164569854736, 0.0013835231075063348]"""
1241,229500,AnnotationInstanceProvider,allows to create annotation instances with different values dynamically,smartshark_2_2,159,deltaspike,"""[0.9963654279708862, 0.0036344872787594795]"""
1242,229501,missing cleanup in EmbeddedServletContainer,"there is a shutdown of the servlet-container, but no shutdown of the cdi-container which causes weld to fail with ""java.lang.IllegalStateException: javax.enterprise.context.RequestScoped started already"" in other tests.",smartshark_2_2,712,deltaspike,"""[0.8738806843757629, 0.12611928582191467]"""
1243,229502,Improve JavaDoc,Where we bundle all kind of javadoc related work if we find 'problems',smartshark_2_2,327,deltaspike,"""[0.9974835515022278, 0.002516454318538308]"""
1244,229503,Use newer release version of Apache RAT / release audit tool,"Currently v0.8 is used, while the latest release is 0.12.

{code}
diff --git a/deltaspike/pom.xml b/deltaspike/pom.xml
index 1460bb0..dd6835f 100644
--- a/deltaspike/pom.xml
+++ b/deltaspike/pom.xml
@@ -83,7 +83,7 @@
                 <plugin>
                     <groupId>org.apache.rat</groupId>
                     <artifactId>apache-rat-plugin</artifactId>
-                    <version>0.8</version>
+                    <version>0.12</version>
                     <configuration>
                         <excludes>
                             <exclude>.idea/**/*</exclude>
{code}",smartshark_2_2,1206,deltaspike,"""[0.9982438087463379, 0.0017562172142788768]"""
1245,229504,UserTransaction not available in WAS 8.0 and 8.5 during EJB CMT call,"It seems that due to different interpretations of the EE Spec WAS behaves differently when calling a CDI bean from a CMT EJB. While TomEE for example gives access to the UserTransaction, WAS will not (@Resource inject fails as well as lookup via JNDI ""java:comp/UserTransaction"". This behavior currently (0.6-SNAPSHOT) results in a NameNotFoundException in BeanManagedUserTransactionStrategy#resolveUserTransaction()

To provide a workaround for WAS the idea would be to resort back to TransactionSynchronisationRegistry which also exposes the current Transaction Status (even on WAS) and should therefore be sufficient to implicitely ""join"" the currently ongoing transaction. A Status != ACTIVE should probably still result in an error indicating probably a misconfigured RESOURCE_LOCAL Datasource",smartshark_2_2,361,deltaspike,"""[0.27719709277153015, 0.7228029370307922]"""
1246,229505,Document ClientWindow configuration,like isClientWindowStoreWindowTreeEnabled / isClientWindowTokenizedRedirectEnabled,smartshark_2_2,949,deltaspike,"""[0.9971119165420532, 0.002888015005737543]"""
1247,229506,PersistenceUnits#instance singleton does not work in EAR  scenarios,"Currently the info regarding the various persistence.xml files on the classpath only gets scanned once and stored in the static PersistenceUnits#instance.
This does not work in EAR scenarios if one of the persistence.xml is in a webapp or if the deltaspike-data-impl is on any other shared ClassLoader.

We should at least document this restriction it until it's properly fixed.

A possible fix would be to have a (weak) Map<ClassLoader, PersistenceInfo> which also looks up the parent ClassLodaer chain.
",smartshark_2_2,1122,deltaspike,"""[0.34171804785728455, 0.6582819223403931]"""
1248,229507,MockAwareInjectionTargetWrapper breaks interceptors in unit tests,"The automatic usage of MockAwareInjectionTargetWrapper breaks method-level interceptors under OWB:

org.apache.webbeans.config.BeansDeployer#validate creates the interceptor stack of all beans while validating the deployment (Line 474). This code depends on owbBean.getProducer() returning an AbstractProducer (Line 462).

TestControl replaces that AbstractProducer in some circumstances with an instance of MockAwareInjectionTargetWrapper, completely deactivating the if-branch which would active the interceptors.

It seems that, depending where the interceptor binding is defined on the intercepted bean, interceptors work or don't work: using the annotation on the class level results in getProducer returning a AbstractProducer -> interceptors work. Defining interceptors only on methods shows the broken behaviour described here.",smartshark_2_2,711,deltaspike,"""[0.8487567901611328, 0.1512431502342224]"""
1249,229508,Missing cdi-api dependency in jbossas-remote-7 profile,The mentioned profile is missing provided dependency to cdi-api therefore tests can't run due to compilation errors. ,smartshark_2_2,290,deltaspike,"""[0.9945738911628723, 0.005426127463579178]"""
1250,229509,Create Identity Model,Provide identity model for security module. ,smartshark_2_2,184,deltaspike,"""[0.9980796575546265, 0.0019203686388209462]"""
1251,229510,Add more tests around quartz in Scheduler,"While implementing DELTASPIKE-651 I found that the scheduler module does not actually test the quartz implementation listed.  It creates a custom implementation and tests that.

These tests should be moved to the api module, and instead we need to add tests to the impl that test quartz.",smartshark_2_2,746,deltaspike,"""[0.998489260673523, 0.00151066726539284]"""
1252,229511,Create a better default TransactionStrategy,Create a better default TransactionStrategy that handles more use cases out of the box.,smartshark_2_2,1348,deltaspike,"""[0.9983800649642944, 0.00161994737572968]"""
1253,229512,Doc: JSF/ExceptionControl integration redirect example,"http://deltaspike.apache.org/documentation/jsf.html#_intergration_with_exception_control_since_0_6 , Redirect example:

* {{FacesContext.getCurrentInstance().setResponseComplete();}} is incorrect, the method on FacesContext is called just {{responseComplete()}}
* the example should maybe show DS's own {{ViewNavigationHandler}} since it performs the same operation as is demonstrated in the example",smartshark_2_2,805,deltaspike,"""[0.9957959651947021, 0.004203998018056154]"""
1254,229513,Missing OSGi headers in Proxy modules,"{{deltaspike-partial-bean-module-impl}} imports package {{org.apache.deltaspike.proxy.api}}, but the new proxy modules are missing OSGi manifest headers.

This breaks Partial Beans and Data modules for OSGi users.",smartshark_2_2,1169,deltaspike,"""[0.991422712802887, 0.008577303029596806]"""
1255,229514,implement WindowContext,"Introduce a way to manage different beans depending on a 'window' or browser tab notion.

A WindowContext stores all the information for a single 'window' or browser tab. The WindowContextManager keeps all open WindowContexts (for a session) and allows to set or determine the one which got activated for the current Thread.",smartshark_2_2,243,deltaspike,"""[0.9984826445579529, 0.0015173191204667091]"""
1256,229515,unit and integration tests for BeanManagerProvider,"we need tests for:
 - after a container is bootstrapped, BeanManagerProvider.getInstance().getBeanManager() has to return a bm
 - usage in shared lib scenarios
 - shutdown of a container -> cleanup has to be successful",smartshark_2_2,43,deltaspike,"""[0.9984671473503113, 0.0015327883884310722]"""
1257,229516,Provide a way to obtain all property names or map of properties,"ConfigResolver API provides a way to resolve singular property or multivalued property, however there is lack of method which allows to read all properties discovered from all ConfigSources.",smartshark_2_2,294,deltaspike,"""[0.998374342918396, 0.0016256393864750862]"""
1258,229517,HttpServletRequest (and others) injection not working in servlet filters from web.xml,"DeltaSpike uses a filter to record the request and response objects for injection in that thread. This filter is configured in web-fragment.xml, which is loaded before other web-fragments, but not before web.xml. Filters from web.xml will be first in the chain, breaking request and response injection into these filters with ""Attempt to access the request/response without an active HTTP request"" (RequestResponseHolder:85).

We are moving from Solder, where the request and response objects were recorded in a ServletRequestListener, which is always fired first, regardless of it's position in the web.xml/web-fragment.xml. I think DeltaSpike should use the same approach.

For now, a workaround is to copy the configuration of RequestResponseHolderFilter to your web.xml and put it before all other filters.",smartshark_2_2,473,deltaspike,"""[0.2524818480014801, 0.7475181221961975]"""
1259,229518,@Exclude on abstract classes,"currently @Exclude doesn't get inherited.
we have to think about using @Inherited here.
however, a common use-case is to @Exclude jpa entities.
in most cases there is an abstract entity-class which could allow to @Exclude all entities (which inherit from it) at a central point.
-> we can support it manually while we think about the possible impacts of @Inherited for @Exclude.",smartshark_2_2,572,deltaspike,"""[0.9960952401161194, 0.003904791781678796]"""
1260,229519,DS 1.7.0 introduced a transitive dependency to arquillian,TestControl doesn't need any Arquillian dependency at all. No idea why this got introduced...,smartshark_2_2,1278,deltaspike,"""[0.9963733553886414, 0.003626585938036442]"""
1261,229520,Ability to test CDI implementation specific issues,"Such things would include deployment exceptions. There may be others, but this is what comes to mind first.",smartshark_2_2,231,deltaspike,"""[0.9983513355255127, 0.001648683100938797]"""
1262,229521,JobRunnableAdapter should be pluggable,that allows to provide a custom class (which implements org.quartz.Job) to execute a task e.g. via ManagedExecutorService,smartshark_2_2,1043,deltaspike,"""[0.9984244108200073, 0.0015756170032545924]"""
1263,229522,LockedTest fails on OWB 1.1.x,Run {{mvn clean install -POWB -Dowb.version=1.1.8 -Dopenejb.owb.version=1.2.6 -Dtest=LockedTest}} from the core/impl directory.  See that the JVM just hangs.,smartshark_2_2,1197,deltaspike,"""[0.9230509996414185, 0.07694895565509796]"""
1264,229523,RequestResponseHolderListener fails if requestInitialized() is called more than once,"Tomcat seems to call RequestResponseHolderListener.requestInitialized() more than once in some situations. I'm not sure if this is spec compliant.

Currently RequestResponseHolderListener fails with ""There is already an instance bound to this thread"" in this situation.

The implementation should be more robust and handle this case correctly.

See:
http://mail-archives.apache.org/mod_mbox/deltaspike-users/201507.mbox/%3C559E1F24.40408%40senat.fr%3E",smartshark_2_2,920,deltaspike,"""[0.12590789794921875, 0.874092161655426]"""
1265,229524,OSGi support for Servlet Module,"Goal:

Users can work with DeltaSpike Servlet in OSGi applications, using Pax CDI.",smartshark_2_2,931,deltaspike,"""[0.9980831146240234, 0.0019168970175087452]"""
1266,229525,actively release ConfigSources and ConfigFilters if they implement Autocloseable,When releasing the Config we should invoke close() on all the ConfigSources and ConfigFilters if they implement java.lang.Autocloseable,smartshark_2_2,1317,deltaspike,"""[0.9524779915809631, 0.04752204567193985]"""
1267,229526,Add convention for @MessageResource annotated types.,"Add the possibility to use convention for @MessageResource annotated types.
If there is no @MessageTemplate given for a defined method then the method itself shall provide the information for the property key.

Example:

@MessageResource
class MyMessages() {

      String getErrorMessage();

      String getGetError();

      @MessageTemplate(""MESSGAE_WARN"")
      String getWarnMessage();
}

The convention would be:
1. If method name starts with 'get' remove it.
2. Split the method name at the upper case characters and seperate the parts with underscore.

getErrorMessage() -> {Error, Message} -> ERROR_MESSAGE.
getGetError() -> {Get, Error} -> GET_ERROR. (not pretty i know, just an example)
getWarnMessage() is already defined

Maybe the missing annotation could be added at the method of the type at deployment time.
So there would be no need to provide property key name for every method, because if someone like me has a lot of messages and kept to the convention i described, it would be easier to switch to delta-spike i18n and also less work to do.

So the existing implementation could be used as it is now. And during runtime, the interface would be as i18n needs it.
",smartshark_2_2,128,deltaspike,"""[0.9985199570655823, 0.0014800189528614283]"""
1268,229527,add converter option to @ConfigProperty,"This will configure a nomals-scoped bean class used to lookup a CDI bean used as converter for the configuration:

{code}
    @Inject
    @ConfigProperty(name = ""urlListFromProperties"", converter = UrlList.class)
    private List<URL> urlListFromProperties;
{code}

assuming we have:

{code}
    public class UrlList implements ConfigResolver.Converter<List<URL>>
    {
        @Override
        public List<URL> convert(final String value)
        {
            final List<URL> urls = new ArrayList<URL>();
            if (value != null)
            {
                for (final String segment : value.split("",""))
                {
                    try
                    {
                        urls.add(new URL(segment));
                    }
                    catch (final MalformedURLException e)
                    {
                        throw new IllegalArgumentException(e);
                    }
                }
            }
            return urls;
        }
    }
{code}",smartshark_2_2,1287,deltaspike,"""[0.9985740184783936, 0.0014259936287999153]"""
1269,229528,the de-/activation logic for extensions should be unified,"we should switch to the new approach which uses @Observes BeforeBeanDiscovery.
(+ the default value should be true because the extension is enabled by default)",smartshark_2_2,183,deltaspike,"""[0.9984862208366394, 0.0015137639129534364]"""
1270,229529,Multiple test fixes,"- TomEE container needs to declare dep on CDI enricher from Arq to do CDI enrichment.
- Update arquillian.xml globally to handle cdicontainer.version in managed containers.",smartshark_2_2,409,deltaspike,"""[0.9975554943084717, 0.002444468205794692]"""
1271,229530,GlobalAlternatives must not get handled for OpenWebBeans,We currently also rewrite the AnnotatedType for OWB. But this is not needed for OWB because we handle the BDA flat anyway. Thus standard <alternatives> in beans.xml is plenty enough for OWB. OWB users of containers which come with BDA enabled (e.g. IBM WebSphere) should instead just disable this feature via openwebbeans.properties,smartshark_2_2,306,deltaspike,"""[0.3651275932788849, 0.6348724365234375]"""
1272,229531,AnnotationUtils should support literal instances,"literal instances can ""violate"" rules for annotations e.g. annotation-methods can return null. AnnotationUtils needs to be aware of that.",smartshark_2_2,834,deltaspike,"""[0.9976165294647217, 0.002383548766374588]"""
1273,229532,remove hard dependency to javassist,"some users don't like to have it as a required dependency, but they would like to use interfaces for partial beans (and don't need abstract classes as partial beans).
",smartshark_2_2,259,deltaspike,"""[0.9984176158905029, 0.0015824083238840103]"""
1274,229533,Remove != JavaEE8 workarounds,"* EntityGraphHelper
 * JSF module (e.g. remove all the reflection hacks for ClientWindow)
 * remove OWB 1.1.x workarounds in proxy module",smartshark_2_2,1328,deltaspike,"""[0.9983275532722473, 0.0016724523156881332]"""
1275,229534,PropertyFileConfig shouldn't implement DeltaSpikeConfig,"{{DeltaSpikeConfig}} is ""Marker interface for all classes used for configuration of DeltaSpike itself."" while {{PropertyFileConfig}} is an interfaces for registration of property files as config sources. ",smartshark_2_2,831,deltaspike,"""[0.994312047958374, 0.005687934812158346]"""
1276,229535,Add ability to return QueryResult without predicate,"A following method name in repository would be handy for dynamic sorting and paging of all entities:

    QueryResult<Person> findAllBy();

Currently the library don't know how to handle that and it requires one to add @Query(""from Person"") annotation. It would be handy if the query annotation could be left out.",smartshark_2_2,1352,deltaspike,"""[0.9984478950500488, 0.001552088651806116]"""
1277,229536,Support JPA 2.1 EntityGraphs,Support EntityGraphs when running in a JPA 2.1 environment. There shall be no compile-time dependency on JPA 2.1.,smartshark_2_2,1111,deltaspike,"""[0.9981794357299805, 0.001820615492761135]"""
1278,229537,Automatic RequestScope activation for @MBean invocations ,"When a {{@JmxManaged}} method is called (via jconsole for instance) on a CDI-Bean which has been registered as a MBean via {{@MBean}} annotation, the RequestScope is not active.
It would be handy if RequestScope could be actived automatically, perhaps configurable via a new {{@MBean}} attribute and/or {{apache-deltaspike.properties}}.

Workaround: Implement and register an Interceptor which actives/deactivates the scope via {{ContextControl}} around the method invocation.",smartshark_2_2,988,deltaspike,"""[0.998418927192688, 0.0015810619806870818]"""
1279,229538,Upgrade Geronimo to 3.0 and MyFaces Test to 2.3,"Still using geronimo 2.5 and myfaces-test20. Please upgrade to the latest versions. See: [https://github.com/apache/deltaspike/blob/master/deltaspike/modules/jsf/api/pom.xml]

Â ",smartshark_2_2,1333,deltaspike,"""[0.9985166192054749, 0.0014833783498033881]"""
1280,229539,add BackwardCompatibility profile,"old versions of weld are just available in the jboss maven-repo.
-> we need to add the repo (at least) in a profile to run e.g.
mvn clean install -PWeld -PBackwardCompatibility -Dweld.version=1.1.3.SP1
for old versions of weld at
https://builds.apache.org/view/A-F/view/DeltaSpike/",smartshark_2_2,110,deltaspike,"""[0.9984655380249023, 0.0015344868879765272]"""
1281,229540,DefaultMessage.argument(Serializable...) should guard against empty arrays,"If you have a method with ... parameters, it will be interpreted as an array. If it is passed along to DefaultMessage.argument the object array is interpreted as an object and added as such. Now that I mention it, wouldn't the flow be the same even if the array came populated with data?",smartshark_2_2,252,deltaspike,"""[0.9020540118217468, 0.09794599562883377]"""
1282,229541,variable support in ConfigResolver,"I'd like to implement the following feature in ConfigResolver:
{code}
myapp.someapp.soap.endpoint=${someapp.host.url}/someservice/myendpoint
{code}
and 
{code}
someapp.host.url=http://localhost:8081
{code}

Of course this is a behaviour which needs to be enabled for each config resolving.",smartshark_2_2,1217,deltaspike,"""[0.9984369874000549, 0.0015629669651389122]"""
1283,229542,Integration test profiles for OWB and Weld don't honor category annotations,"It seems like the integration test profiles for OWB and Weld ignore the @Category annotations used to restrict tests to a certain runtime environment like ""WebProfile"" or ""FullProfile"". This problem occurs when running the tests in both ""core-impl"" and ""integration-test"".

To reproduce this just add this failing method to some test class and execute the tests in the default profile OWB:

    @Test 
    @Category(WebProfileCategory.class)
    public void testCategory() {
        fail(""Should not get executed!"");
    }

",smartshark_2_2,32,deltaspike,"""[0.962389349937439, 0.03761063516139984]"""
1284,229543,import JSF-Scope_to_CDI-Scope mapping from CODI,"We had a great Extension in CODI which mapped JSF Scopes to their CDI Scope counterparts if no @javax.faces.bean.ManagedBean annotation is present.

The Extension warns the users in addition to re-mapping. The reason for this Extension being so useful is to prevent unintended auto-imports from the wrong package by the IDEs.",smartshark_2_2,272,deltaspike,"""[0.9985063672065735, 0.001493629184551537]"""
1285,229544,update java-se example,in the java-se example the bootstrap-api should be used,smartshark_2_2,83,deltaspike,"""[0.9967032074928284, 0.0032967436127364635]"""
1286,229545,Exception handlers: support handling unexpected exceptions,"Provide a way to handle exceptions not handled by any other handler.

Currently, a handler like this:
public void unexpectedException(@Handles CaughtException<Throwable> event) {
 if (!event.isMarkedHandled()) {
  log.error(""Unexpected exception"", event.getException());
  messages.error(new BundleKey(MESSAGE_BUNDLE, ""unexpected.exception""));
 }
}
gets in the way when there are other handlers which respond to non-root-cause exceptions.

Some ideas:
- an UnhandledException event fired as a last chance to handle it before re-throwing it as unhandled
- a third traversal type - after BREADTH_FIRST and DEPTH_FIRST

See https://community.jboss.org/thread/198408",smartshark_2_2,229,deltaspike,"""[0.9985094666481018, 0.0014905005227774382]"""
1287,229546,CDI + Blueprint integration,"Description should be enriched by authors (Jason, ...)

{code}
From:        Nodet Guillaume <gnodet@redhat.com>

Such a xml is in the META-INF/beans.xml, right ? So that you can override the behaviour of annotations ?
I'm not sure how / where we could use it, and that does not seem really critical to me anyway.

I think we'd better come to an understanding of the use case we'd want to cover.
I'm thinking about:
 * #1 create beans using the CDI container
 * #2 inject CDI beans into blueprint beans using the blueprint xml
 * #3 inject blueprint beans into CDI beans using @Inject
 * #4 support CDI annotations on blueprint beans (@PostConstruct, @PreDestroy, @Inject)

#1 is obviously needed, it could be done from the blueprint xml using a simple tag, eventually pointing to the beans.xml config file, or inline it (though inlining is not really worth the pain now imho)
>>>>>> <cdi:container xmlns=""â¦"">
>>>>>>     <cdi:beans url=""â¦"" />
>>>>>> </cdi:container>


#2 means being able to use one of the bean created from the CDI container and inject it using the xml blueprint syntax, something like
 <bean â¦.>
   <cdiroperty name=""service""â¦ />
</bean>
Not sure what exactly we'd need in the <cdiroperty/> element, but the idea is to use the bean setters to inject a bean created inside the CDI container

#3 means that we'd need to be able to inject a bean created by the blueprint container using <bean/> into a @Inject annotated property of a CDI bean created by the CDI container.  In blueprint, beans are referred to by name though, so it may require a custom annotation maybe ?

#4 means mixing CDI annotations with blueprint beans.  It's the most complicated case I think, as it needs an even closer cooperation of both containers.
This needs to be triggered either globally or an individual bean using a flag such an xml attribute such as cdirocess=""true"" that could be set on a <bean/> element or a default attribute on the <cdi:container/> element.

Cheers,
Guillaume Nodet
{code}",smartshark_2_2,507,deltaspike,"""[0.998501181602478, 0.001498792553320527]"""
1288,229547,test-utils jar should be added to deployed wars only if the test is a server test,Using an archive appender can solve it properly without needing any hack or boolean to know if it is needed or not.,smartshark_2_2,356,deltaspike,"""[0.9975973963737488, 0.002402546815574169]"""
1289,229548,Create build-managed test profiles for Glassfish,"We should create ""build-managed"" profiles for Glassfish 3.1 and 4.0",smartshark_2_2,463,deltaspike,"""[0.9966590404510498, 0.003340943483635783]"""
1290,229549,rename PersistenceStrategy to TransactionStrategy,see http://s.apache.org/WRT,smartshark_2_2,187,deltaspike,"""[0.9984858632087708, 0.0015141561161726713]"""
1291,229550,WindowHandler: Avoid windowhandler streaming in some redirect cases,"to avoid streaming of the windowhandler in following cases:
- Non-Ajax-Get request with redirect e.g. in f:viewAction
- Post-Redirect-Get with ajax.


DefaultClientWindowConfig#isClientWindowStoreWindowTreeEnabledOnAjaxRequest can also be removed then.",smartshark_2_2,995,deltaspike,"""[0.9857586622238159, 0.014241376891732216]"""
1292,229551,QBE without attributes should search filled parameters in example,"In method executeExampleQuery from EntityRepositoryHandler there is this comment:

{code:java}
// Not sure if this should be the intended behaviour
// when we don't get any attributes maybe we should
// return a empty list instead of all results
{code}

I think the desired behavior would be to search the example object for filled attributes and do the query with these attributes.

I made a patch with my idea, but it creates a dependency with commons-beanutils. I don't know if there is some alternative to this.
",smartshark_2_2,1245,deltaspike,"""[0.9576184153556824, 0.04238158464431763]"""
1293,229552,Propagation of basic servlet events to the CDI event bus,"The servlet module should send events when the following servlet objects are created and/or destroyed:

* HttpServletRequest
* HttpServletResponse
* HttpSession
* ServletContext",smartshark_2_2,336,deltaspike,"""[0.9956817626953125, 0.0043182107619941235]"""
1294,229553,coordinator bug in alert engine,"Coordinator has bugs below:
1. When I disable a policy, coordinator does not clear allocated queue
2. Coordinator can not schedule policies while there are alert bolts with no resources
3. When I create some policies with stream A and B, coordinator can not schedule policy with stream C although there are free resources
How to reproduce:
Create 8 or more policies with stream A and some policies with Stream B, then create policy with steam C, will reproduce 2 and 3(the number may depends). When you see all the alert bolts contains stream A, then disable policy with stream A, it will reproduce 1 ",smartshark_2_2,702,eagle,"""[0.07522038370370865, 0.9247795939445496]"""
1295,229554,alert engine could not reduce alert bolt number when parallelism of policy reduces,alert engine could not reduce alert bolt number when parallelism of policy reduces,smartshark_2_2,537,eagle,"""[0.1411672979593277, 0.8588327169418335]"""
1296,229555,Job Comparison Page throw error in dev console,Job Comparison Page throw error in dev console when click from or to Job ID url,smartshark_2_2,816,eagle,"""[0.46031635999679565, 0.5396836400032043]"""
1297,229556,Configuration parsing exception in SparkHistoryJob app,"{code}
java.lang.NumberFormatException: For input string: ""2g""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_91]
        at java.lang.Integer.parseInt(Integer.java:580) ~[na:1.8.0_91]
        at java.lang.Integer.parseInt(Integer.java:615) ~[na:1.8.0_91]
        at org.apache.eagle.jpm.util.Utils.parseMemory(Utils.java:78) ~[stormjar.jar:na]
        at org.apache.eagle.jpm.spark.history.crawl.JHFSparkEventReader.getMemoryOverhead(JHFSparkEventReader.java:490) ~[stormjar.jar:na]
        at org.apache.eagle.jpm.spark.history.crawl.JHFSparkEventReader.clearReader(JHFSparkEventReader.java:449) ~[stormjar.jar:na]
        at org.apache.eagle.jpm.spark.history.crawl.JHFSparkParser.parse(JHFSparkParser.java:53) ~[stormjar.jar:na]
        at org.apache.eagle.jpm.spark.history.crawl.SparkFilesystemInputStreamReaderImpl.read(SparkFilesystemInputStreamReaderImpl.java:47) ~[stormjar.jar:na]
        at org.apache.eagle.jpm.spark.history.storm.SparkHistoryJobParseBolt.execute(SparkHistoryJobParseBolt.java:98) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__5697$tuple_action_fn__5699.invoke(executor.clj:659) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__5620.invoke(executor.clj:415) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.disruptor$clojure_handler$reify__1741.onEvent(disruptor.clj:58) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.daemon.executor$fn__5697$fn__5710$fn__5761.invoke(executor.clj:794) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at backtype.storm.util$async_loop$fn__452.invoke(util.clj:465) [storm-core-0.9.3.2.2.0.0-2041.jar:0.9.3.2.2.0.0-2041]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
{code}",smartshark_2_2,898,eagle,"""[0.16707982122898102, 0.8329201340675354]"""
1298,229557,Column not found to EAGLE_METRIC when using JDBC,"Some of the column are missing in table eagle_metric, site, application, policyId, there might be others :                URL:                ../rest/entities?query=GenericMetricService[@application=""dgCassandraLogMonitor"" AND @policyId=""dg_csd_user_policy""]<@site>{sum(value)}&pageSize=100000&timeSeries=true&intervalmin=1440&startTime=2016-02-23 00:00:00&endTime=2016-03-24 00:00:00&metricName=eagle.alert.fail.count                  Params:                undefined                  Exception:                java.io.IOException: org.apache.torque.TorqueException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'eagle_metric.site' in 'field list'                org.apache.eagle.storage.jdbc.JdbcStorage.query(JdbcStorage.java:179)                org.apache.eagle.storage.operation.QueryStatement.execute(QueryStatement.java:47) org.apache.eagle.service.generic.GenericEntityServiceResource.search(GenericEntityServiceResource.java:443)                sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source)        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)                java.lang.reflect.Method.invoke(Method.java:606) com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185) com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)               com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)  ",smartshark_2_2,128,eagle,"""[0.3295460641384125, 0.6704539060592651]"""
1299,229558,"PolicyValidation is insufficient, and will cause blocking for extended handler","Policy validation is hard code to siddhi ql without checking the policy type. Also for complex query, we need to make sure leftjoin query things also works",smartshark_2_2,581,eagle,"""[0.30612698197364807, 0.6938730478286743]"""
1300,229559,Multiple policies in one alert bolt produces duplicated tuples to publisher,"Multiple policies in one alert bolt will cause each policy in this bolt produce the tuple and emit tuple into publisher, the publisher will got multiple duplicated tuples.",smartshark_2_2,681,eagle,"""[0.26551929116249084, 0.7344807386398315]"""
1301,229560,Fix duplicated view path,"It may cause conflict problems if different applications declare same view path,  for example: `MRHistoryJobApplicationProvider ` and `GCLogApplicationProvider `, so we should check view path duplication and throw exception if found.",smartshark_2_2,346,eagle,"""[0.25219666957855225, 0.7478033304214478]"""
1302,229561,Data skews in KafakStreamSink,"When data skews in Kafka sink topic, the alert engine may consumes the data evenly. ",smartshark_2_2,631,eagle,"""[0.20249620079994202, 0.7975037693977356]"""
1303,229562,Application can't load application.properties when running local mode,"When running local mode, jpm app can't get property ""job.name.normalization.rules.key"", then get exception
{code}
ERROR [2016-11-02 15:18:04,382] org.apache.eagle.jpm.mr.history.crawler.AbstractJobHistoryDAO: fail reading job history file
! java.lang.NoClassDefFoundError: Could not initialize class org.apache.eagle.jpm.util.JobNameNormalization
! at org.apache.eagle.jpm.mr.history.parser.JHFEventReaderBase.handleJob(JHFEventReaderBase.java:199) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.parser.JHFMRVer2EventReader.handleJobSubmitted(JHFMRVer2EventReader.java:187) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.parser.JHFMRVer2EventReader.handleEvent(JHFMRVer2EventReader.java:53) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.parser.JHFMRVer2Parser.parse(JHFMRVer2Parser.java:61) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.crawler.DefaultJHFInputStreamCallback.onInputStream(DefaultJHFInputStreamCallback.java:58) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.crawler.AbstractJobHistoryDAO.readFileContent(AbstractJobHistoryDAO.java:123) ~[eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.crawler.JHFCrawlerDriverImpl.crawl(JHFCrawlerDriverImpl.java:166) [eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at org.apache.eagle.jpm.mr.history.storm.JobHistorySpout.nextTuple(JobHistorySpout.java:160) [eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at backtype.storm.daemon.executor$fn__3373$fn__3388$fn__3417.invoke(executor.clj:565) [storm-core-0.9.3.jar:0.9.3]
! at backtype.storm.util$async_loop$fn__464.invoke(util.clj:463) [storm-core-0.9.3.jar:0.9.3]
! at clojure.lang.AFn.run(AFn.java:24) [eagle-topology-0.5.0-incubating-SNAPSHOT-assembly.jar:na]
! at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
{code}
",smartshark_2_2,587,eagle,"""[0.24984592199325562, 0.7501540780067444]"""
1304,229563,java.lang.IncompatibleClassChangeError: class net.sf.extcos.internal.JavaResourceAccessor$AnnotatedClassVisitor has interface org.objectweb.asm.ClassVisitor as super class,"h1. Exception
{code}
21399 [eXtcos managed thread 1] ERROR org.apache.storm.zookeeper.server.NIOServerCnxnFactory - Thread Thread[eXtcos managed thread 1,5,main] died
java.lang.IncompatibleClassChangeError: class net.sf.extcos.internal.JavaResourceAccessor$AnnotatedClassVisitor has interface org.objectweb.asm.ClassVisitor as super class
	at java.lang.ClassLoader.defineClass1(Native Method) ~[na:1.7.0_75]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800) ~[na:1.7.0_75]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[na:1.7.0_75]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[na:1.7.0_75]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[na:1.7.0_75]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[na:1.7.0_75]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_75]
	at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_75]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_75]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_75]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[na:1.7.0_75]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_75]
	at java.lang.ClassLoader.defineClass1(Native Method) ~[na:1.7.0_75]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800) ~[na:1.7.0_75]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[na:1.7.0_75]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[na:1.7.0_75]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[na:1.7.0_75]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[na:1.7.0_75]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_75]
	at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_75]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_75]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_75]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[na:1.7.0_75]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_75]
	at net.sf.extcos.internal.JavaResourceAccessor.readClassData(JavaResourceAccessor.java:363) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.JavaResourceAccessor.setResourceUrl(JavaResourceAccessor.java:333) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.URLResource.getResourceAccessor(URLResource.java:93) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.URLResource.isClass(URLResource.java:126) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.RootFilter.filter(RootFilter.java:22) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.AbstractChainedFilter.filter(AbstractChainedFilter.java:89) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at net.sf.extcos.internal.ThreadingFilterInterceptor$1.run(ThreadingFilterInterceptor.java:48) ~[eagle-topology-0.3.0-incubating-assembly.jar:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_75]
{code}

h1. Root Cause
Included ""asm"" or ""asm-all"" in ""eagle-topology-assembly"" by following dependencies:
{code}
<dependency>
    <groupId>org.apache.eagle</groupId>
    <artifactId>eagle-metric-collection</artifactId>
    <version>${project.version}</version>
 </dependency>
{code}

h1. Validation
It will cause above problem if the jar  contains class ""org.objectweb.asm.ClassVisitor""
{code}
jar -tf lib/topology/eagle-topology-0.3.0-incubating-assembly.jar  | grep org.objectweb.asm.ClassVisitor
{code}

h1. Solution
{code}
<exclusions>
    <exclusion>
        <groupId>org.ow2.asm</groupId>
        <artifactId>asm-all</artifactId>
    </exclusion>
    <exclusion>
        <groupId>org.ow2.asm</groupId>
        <artifactId>asm</artifactId>
    </exclusion>
    <exclusion>
        <groupId>asm</groupId>
        <artifactId>asm</artifactId>
    </exclusion>
    <exclusion>
        <groupId>asm</groupId>
        <artifactId>asm-all</artifactId>
    </exclusion>
</exclusions>
{code}",smartshark_2_2,321,eagle,"""[0.7749497890472412, 0.22505024075508118]"""
1305,229564,using Email Notification Feature throws IncompatibleClassChangeError,"{code}
2017-03-19 20:42:56.369 o.a.e.a.e.p.e.AlertEmailGenerator Thread-46-alertPublishBolt-executor[17 17] [ERROR] Failed to send email to ****@gmail.com,****@gmail.com, due to:java.util.concurrent.ExecutionException: java.lang.IncompatibleClassChangeError: Implementing class
java.util.concurrent.ExecutionException: java.lang.IncompatibleClassChangeError: Implementing class
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_102]
	at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_102]
	at org.apache.eagle.alert.engine.publisher.email.AlertEmailGenerator.sendAlertEmail(AlertEmailGenerator.java:89) [stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.email.AlertEmailGenerator.sendAlertEmail(AlertEmailGenerator.java:55) [stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.impl.AlertEmailPublisher.onAlert(AlertEmailPublisher.java:130) [stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.impl.AlertPublisherImpl.notifyAlert(AlertPublisherImpl.java:88) [stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.impl.AlertPublisherImpl.nextEvent(AlertPublisherImpl.java:70) [stormjar.jar:?]
	at org.apache.eagle.alert.engine.runner.AlertPublisherBolt.execute(AlertPublisherBolt.java:92) [stormjar.jar:?]
	at org.apache.storm.daemon.executor$fn__4973$tuple_action_fn__4975.invoke(executor.clj:727) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__4894.invoke(executor.clj:459) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.disruptor$clojure_handler$reify__4409.onEvent(disruptor.clj:40) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:453) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:432) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.daemon.executor$fn__4973$fn__4986$fn__5039.invoke(executor.clj:846) [storm-core-1.0.3.jar:1.0.3]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.0.3.jar:1.0.3]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
Caused by: java.lang.IncompatibleClassChangeError: Implementing class
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.8.0_102]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763) ~[?:1.8.0_102]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.8.0_102]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) ~[?:1.8.0_102]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73) ~[?:1.8.0_102]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368) ~[?:1.8.0_102]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362) ~[?:1.8.0_102]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361) ~[?:1.8.0_102]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_102]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_102]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_102]
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:118) ~[stormjar.jar:?]
	at org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85) ~[stormjar.jar:?]
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157) ~[stormjar.jar:?]
	at org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269) ~[stormjar.jar:?]
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871) ~[stormjar.jar:?]
	at org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262) ~[stormjar.jar:?]
	at org.apache.velocity.app.VelocityEngine.init(VelocityEngine.java:93) ~[stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.email.EagleMailClient.<init>(EagleMailClient.java:57) ~[stormjar.jar:?]
	at org.apache.eagle.alert.engine.publisher.email.AlertEmailSender.run(AlertEmailSender.java:87) ~[stormjar.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_102]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_102]
	... 1 more
{code}
",smartshark_2_2,1046,eagle,"""[0.21495488286018372, 0.7850450873374939]"""
1306,229565,update packaging to use new configs,"Eagle 0.5 packaging is still using old config files located at eagle-server-assembly/src/main/conf

right config files being used are located at eagle-server/src/main/resources

This task is to update eagle 0.5 packaging.

Feel free to provide all the changes/refactoring that should be done as part of packaging update. 

Hao, Thanks for following suggestions, that is also incorporated into this change.
1) rename eagle-server-assembly to eagle-assembly
2) add doc dir in distribution

",smartshark_2_2,824,eagle,"""[0.998322069644928, 0.001677894964814186]"""
1307,229566,Alert mongodb storage refine,"Currently, some Alert Mongo Collection is too large. Consider future increase, need to split large collection to some small collections.

To back compatible, need to keep the interface as the same as before. ",smartshark_2_2,528,eagle,"""[0.9984099864959717, 0.0015899749705567956]"""
1308,229567,UI support Alert display,For backend will provide the alert api. UI will also support it.,smartshark_2_2,536,eagle,"""[0.9983847141265869, 0.0016152512980625033]"""
1309,229568,convert eagle-gc app to use new app framework,eagle-gc app should use new app framework to manage its lifecycle,smartshark_2_2,957,eagle,"""[0.9959235191345215, 0.00407650601118803]"""
1310,229569,ElasticSearch Alert Publisher,ElasticSearch Alert Publisher,smartshark_2_2,965,eagle,"""[0.9970408082008362, 0.0029591883067041636]"""
1311,229570,Policy detail page support alert list display,"As a user, want to view the alert list content in the policy page directly.",smartshark_2_2,264,eagle,"""[0.9984982013702393, 0.00150178256444633]"""
1312,229571,policy cost recorded into policy statistics,policy evaluation wall time can be one of metrics for policy statistics.,smartshark_2_2,64,eagle,"""[0.9973726272583008, 0.0026273445691913366]"""
1313,229572,track the work before moving code to apache site,"1. resolve all existing pull requests
2. change version to 0.3.0
3. change all package name to prefixed by ""org.apache""
",smartshark_2_2,6,eagle,"""[0.9964873790740967, 0.00351262791082263]"""
1314,229573,Support activity monitoring for Knox,The Knox Gateway provides a single access point for all REST interactions with Hadoop clusters. It will be valuable to monitor the access events happening in knox gateway and see if there is an anomaly and generate an alert.,smartshark_2_2,763,eagle,"""[0.9983817338943481, 0.0016182729741558433]"""
1315,229574,Improve coordinator schedule strategy to reuse alert work slot,"We catch with some critical problem about alert engine policy schedule strategy: 

For example for an alert topology with 20 alert bolts , and after boarding 4 policies and each assigning 5 slots, then the current alert coordinator will treat the work slots as used up and be unable to schedule any more new policies.

But in fact some typical eagle deployment will have at least 20+ streams with lots of different partition requirements, which means we could just define very few policies under such scheduling strategy and may waste slot resources of eagleâs deployment infrastructure.

In original design, we should have designed to reused slot queue among in-conflict monitored stream (stream-partition-sort), then eagle could try to reuse alert slots if possible to improve resource utilization, and  it should be time to implement it now.",smartshark_2_2,660,eagle,"""[0.9984108209609985, 0.00158912583719939]"""
1316,229575,Eagle framework should support custom group by function in addition to group by fields,"In some situation Like EAGLE-24, user need to define custom group by methods to do load balance

storm support user to implement a custom stream grouping by implementing the CustomStreamGrouping interface and declare it in InputDeclarer, The following is InputDeclarer interface

Eagle should add interface for passing custom group by methods in high level DSL

{code}
public interface InputDeclarer<T extends InputDeclarer>
{ 
	public T fieldsGrouping(String componentId, Fields fields); 

	public T fieldsGrouping(String componentId, String streamId, Fields fields); 

	public T globalGrouping(String componentId); public T globalGrouping(String componentId, String streamId); 

	public T shuffleGrouping(String componentId); public T shuffleGrouping(String componentId, String streamId); 

	public T localOrShuffleGrouping(String componentId); public T localOrShuffleGrouping(String componentId, 
String streamId); 

	public T noneGrouping(String componentId); public T noneGrouping(String componentId, String streamId); 

	public T allGrouping(String componentId); public T allGrouping(String componentId, String streamId); 

	public T directGrouping(String componentId); public T directGrouping(String componentId, String streamId); 

	public T customGrouping(String componentId, CustomStreamGrouping grouping); 

	public T customGrouping(String componentId, String streamId, CustomStreamGrouping grouping); 

	public T grouping(GlobalStreamId id, Grouping grouping); 
}
{code}",smartshark_2_2,11,eagle,"""[0.9984471201896667, 0.001552920206449926]"""
1317,229576,UI support metric preview,"For backend check, need preview for the metric state.",smartshark_2_2,712,eagle,"""[0.9986288547515869, 0.0013711457140743732]"""
1318,229577,Support typeahead in Eagle UI,Support typeahead in Eagle UI,smartshark_2_2,1035,eagle,"""[0.9977476000785828, 0.002252412959933281]"""
1319,229578,Front end documentation,Front end documentation,smartshark_2_2,602,eagle,"""[0.9972827434539795, 0.002717301482334733]"""
1320,229579,HDFS Audit log traffic monitoring,"HDFS Audit log traffic monitoring
* support metric trend. 
* support alerting when the metric value trigger the threshold.",smartshark_2_2,875,eagle,"""[0.998289167881012, 0.0017108600586652756]"""
1321,229580,"Convert fid,uid in MapR's audit log to FIle/folder name, user name","FIle/folder, user's names are not saved in MapR's audit log file, instead, the audit log file keeps FID, UID. 
Need a converter to resolve the relationship between ids and their actual names.",smartshark_2_2,277,eagle,"""[0.9945918917655945, 0.005408043973147869]"""
1322,229581,Clean up embedded tomcat dependency,"Embedded tomcat in fact is not being used in eagle-server, so we should clean it.",smartshark_2_2,467,eagle,"""[0.9981611371040344, 0.0018388964235782623]"""
1323,229582,Integrate application streams as Alerting data source (managed kafka topic) & stream,"* *Kafka2TupleMetadata*: any integrated streams with kafka information
* *StreamDefinition*: Integrated StreamDefinition togather with defined StreamDefinition should be part of alerting streams.",smartshark_2_2,958,eagle,"""[0.9985490441322327, 0.001450947718694806]"""
1324,229583,Allow user to customize alert mailing template,"When defining alert policies, eagle should support user to customize alert mailing template.",smartshark_2_2,5,eagle,"""[0.9980972409248352, 0.0019027538364753127]"""
1325,229584,UI adjust,"* Remove 'Back' in site level view.
* Move integration out of user panel.",smartshark_2_2,788,eagle,"""[0.9984450936317444, 0.001554954214952886]"""
1326,229585,Document service health check application,"Application description along with:
-  what to be monitored
-  how to collect data, any expected impact to monitored service
-  how to define a policy, especially on how to extract the fields from stream?
-  as much as supported policy examples
-  latency expected for alert
-  how to troubleshooting an application if any failure happens?
-  ......",smartshark_2_2,610,eagle,"""[0.9983781576156616, 0.0016218610107898712]"""
1327,229586,revisit tomcat package and remove tomcat dependency,"Eagle use maven to package tomcat dependency into binary bits, but we don't need this because of the following reasons.
1. for developers, they can launch eagle service within IDE
2. for users, they can download binary bits which includes apache tomcat distribution

If developers really want to launch eagle service within sandbox, we can provide war file which can be deployed to installed tomcat in sandbox",smartshark_2_2,122,eagle,"""[0.9982395172119141, 0.0017604352906346321]"""
1328,229587,support new Siddhi features by upgrading siddhi version to 4.0,"latest Siddhi has lot of new feature that would be very useful for eagle to define new kinds of policies, specially in the area of absence, patterns and windows.

seems like there are also API changes, we may have to do lot of code change with this upgrade.
Although we need to wait for its stable version release : https://github.com/wso2/siddhi , I am creating this task to take care of this important ( by feature list) and major siddhi upgrade.
",smartshark_2_2,1028,eagle,"""[0.9983291029930115, 0.0016708782641217113]"""
1329,229588,Enable publish bolt parallelism,"Currently the publish is using shuffle grouping, we cannot enable parallelism for publish since we may have local cache which is unavailable across the storm cluster. 
We are going to use the same strategy as router bolts to alert bolts, which is using field grouping (define empty list of field) and dispatch tuple according to group by fields hashing.",smartshark_2_2,673,eagle,"""[0.9983687996864319, 0.0016311915824189782]"""
1330,229589,Chart component display date info,Current chart don't provide date info which is not user friendly,smartshark_2_2,910,eagle,"""[0.9977346658706665, 0.0022653338965028524]"""
1331,229590,Running queue metrics monitoring ,"Hadoop running queue monitoring aims to collect some queue related metrics. Currently we only fetch data from the following three apis. 
* CLUSTER_SCHEDULER_URL = ""ws/v1/cluster/scheduler""
* CLUSTER_METRICS_URL = ""ws/v1/cluster/metrics""
* CLUSTER_RUNNING_APPS_URL = ""ws/v1/cluster/apps?state=RUNNING""

Sample queries

http://localhost:9098/eagle-service/rest/entities?query=RunningQueueService[@site=""sandbox""]{*}&pageSize=1000
http://localhost:9098/eagle-service/rest/entities?query=GenericMetricService[@site=""sandbox""]{*}&metricName=hadoop.queue.usedcapacity&pageSize=1000
",smartshark_2_2,353,eagle,"""[0.9973425269126892, 0.002657511970028281]"""
1332,229591,Remove dependency of AlertExecutorId in metadata while partitioning policy,Remove dependency of AlertExecutorId in metadata while partitioning policy,smartshark_2_2,946,eagle,"""[0.998023271560669, 0.001976731466129422]"""
1333,229592,Topology run in storm local mode should not terminate after Integer.MAX_VALUE milliseconds,"Today when eagle topology run in storm local mode, it will terminate after 2147483647 seconds, which is about 24.85 days(2147483647/1000/60/60/24=24.85)

we should sleep it for ever after it submit the topology

https://github.com/eBay/Eagle/blob/d97273dc01d4a1a64f8d83fccf9b7ff88a6d8058/eagle-core/eagle-data-process/eagle-stream-process-api/src/main/scala/eagle/datastream/StormTopologyExecutorImpl.scala#L42",smartshark_2_2,93,eagle,"""[0.8656812310218811, 0.13431870937347412]"""
1334,229593,JPM statistic need remove jobType,JPM statistic need remove jobType,smartshark_2_2,584,eagle,"""[0.9851316809654236, 0.014868328347802162]"""
1335,229594,Policy detail UI update,Policy detail UI update,smartshark_2_2,592,eagle,"""[0.9981655478477478, 0.0018344869604334235]"""
1336,229595,Fix TestServiceAppWithZk test cause failing due to port conflict,"`TestServiceAppWithZk` often failed with follow exception:

{code}
Regression

com.apache.eagle.service.app.TestServiceAppWithZk.testMain

Failing for the past 1 build (Since Failed#127 )
Took 5.7 sec.
Error Message

java.net.BindException: Address already in use
Stacktrace

java.lang.RuntimeException: java.net.BindException: Address already in use
	at org.eclipse.jetty.setuid.SetUIDListener.lifeCycleStarting(SetUIDListener.java:213)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.setStarting(AbstractLifeCycle.java:187)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at io.dropwizard.cli.ServerCommand.run(ServerCommand.java:43)
	at io.dropwizard.cli.EnvironmentCommand.run(EnvironmentCommand.java:43)
	at io.dropwizard.cli.ConfiguredCommand.run(ConfiguredCommand.java:76)
	at io.dropwizard.cli.Cli.run(Cli.java:70)
	at io.dropwizard.Application.run(Application.java:72)
	at com.apache.eagle.service.app.TestServiceAppWithZk.setUp(TestServiceAppWithZk.java:69)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:103)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:264)
	at org.eclipse.jetty.setuid.SetUIDListener.lifeCycleStarting(SetUIDListener.java:200)
	... 36 more
{code}",smartshark_2_2,459,eagle,"""[0.9379676580429077, 0.0620323084294796]"""
1337,229596,Integrate Application start/stop/status interface with TopologyMgmtResourceImpl,Integrate Application start/stop/status interface with TopologyMgmtResourceImpl,smartshark_2_2,624,eagle,"""[0.9976147413253784, 0.002385320607572794]"""
1338,229597,Apache Eagle should not use com.sun.* packages ,"com.sun packages are not officially supported, should use javax packages instead: refer: http://www.oracle.com/technetwork/java/faq-sun-packages-142232.html",smartshark_2_2,818,eagle,"""[0.9980636239051819, 0.0019363370956853032]"""
1339,229598,Application UI add confirm dialog,Application UI add confirm dialog,smartshark_2_2,596,eagle,"""[0.9969884753227234, 0.003011495340615511]"""
1340,229599,Metric Explorer and Customized Dashboard,https://cwiki.apache.org/confluence/display/EAG/EP-18+Metric+Explorer+and+Customized+Dashboard#EP-18MetricExplorerandCustomizedDashboard-MetadataDesign,smartshark_2_2,789,eagle,"""[0.997103750705719, 0.0028962052892893553]"""
1341,229600,migration eagle-jpm-aggregation to application framework ,"We need migrate this application so that user can install it via ui.
What need to do is add provider xml and spi to eagle-topology-assembly",smartshark_2_2,487,eagle,"""[0.9976156949996948, 0.002384304068982601]"""
1342,229601,UI update: metadata/policies/policy,metadata/policies/policy updated from array to entity. UI also need change.,smartshark_2_2,456,eagle,"""[0.9982738494873047, 0.0017261271132156253]"""
1343,229602,JBDC Metadata Storage Extension,"JDBC as metadata storage maybe more convenient and light-weight for user to use.

* Support Derby in Memory: The JDBC can support derby and mysql completely, and use derby in memory by default, so that user donât have to install any additional dependencies even mysql to get started, and also our unit test could easily pass without any tricky modifications.
* Automatic DDL Schema Manager: JdbcEntitySchemaManager could automatically create all required tables if not existing when application is started.
* Fixed JDBC reserved field name like desc, group and so on
* Force update while inserting failed due to duplicated key exception",smartshark_2_2,127,eagle,"""[0.9982606768608093, 0.0017393036978319287]"""
1344,229603,Refactor message format in the email template.,"Make the message in the email look nicer and clearer.

The Policy ""testLevel"" has been detected with the below information: message=""org.apache.hive.hcatalog.common.HCatException : 9001 : Exception occurred while processing HCat request : NoSuchObjectException while fetching table.. Cause : NoSuchObjectException(message:default.test3 table not found)"" time=""2016-05-17 21:45:32.654+0000"" sendTo="""" level=""ERROR"" host="""" sendFrom="""" actionSuggestion="""" role=""WebHCat Server"" cluster=""""",smartshark_2_2,231,eagle,"""[0.9980465173721313, 0.0019535080064088106]"""
1345,229604,Refactor jdbcMetadataDaoImpl of alert engine metadata,"1. add DDL into creatTables.sql
2. rewrite jdbcMetadataImpl of alert engine metadata",smartshark_2_2,690,eagle,"""[0.9982476234436035, 0.0017524042632430792]"""
1346,229605,Support to fetch audit log from http://localhost:50070/logs/hdfs-audit.log for quick preview,Support to fetch audit log from http://localhost:50070/logs/hdfs-audit.log for quick preview,smartshark_2_2,996,eagle,"""[0.9984633922576904, 0.0015366656007245183]"""
1347,229606,AlertEngine: bolt should check ZK for latest spec when start,"Currently, bolt use zk listening for metadata reload only.

This require zk to be periodically updated. In the case of bolt down, and storm start bolt another place(vm, process), if the zk is not updated, then new bolt possibly can not get the metadata to work well.

",smartshark_2_2,545,eagle,"""[0.868893027305603, 0.13110698759555817]"""
1348,229607,UI remove useless css import,index.html template include one project.min.css which can be ignored,smartshark_2_2,784,eagle,"""[0.9964718818664551, 0.003528113942593336]"""
1349,229608,Add multiple hadoop version assembly application (topology) package,"Add multiple hadoop version application (topology) assembly package, for example:

{code}
lib/
* eagle-topology-assembly-hadoop-2.6.jar
* eagle-topology-assembly-hadoop-2.7.jar
* eagle-topology-assembly-hadoop-X.X.jar
{code}

and add version-based building profile for different version:
{code}
mvn package -Phadoop-2.6
{code}",smartshark_2_2,940,eagle,"""[0.9982179999351501, 0.0017819791100919247]"""
1350,229609, Oozie auditlog integration for Oozie security monitoring,"Oozie audit log data source is good for monitoring Oozie actitivities, we should integrate that into Eagle framework",smartshark_2_2,185,eagle,"""[0.9954343438148499, 0.004565587267279625]"""
1351,229610,convert eagle-hdfs-auditlog app to use new app framework,eagle-hdfs-auditlog should use new app framework to manage its lifecycle,smartshark_2_2,1040,eagle,"""[0.9965464472770691, 0.0034535876475274563]"""
1352,229611,CEP Based Aggregate Framework,Implementation of CEP based aggregate framework ,smartshark_2_2,689,eagle,"""[0.9974181652069092, 0.002581824781373143]"""
1353,229612,Connector to be used by other service to interact with Eagle ,Create a class that can be used by other program's CRUD operations about policies  and sensitivity metadata in Eagle Service.,smartshark_2_2,170,eagle,"""[0.9979250431060791, 0.002074964577332139]"""
1354,229613,Compilation failure in develop branch - Copyright missing,"Apache copyright is missing in file eagle-examples/eagle-app-example/src/main/java/org/apache/eagle/app/example/ExampleStormConfig.java
",smartshark_2_2,265,eagle,"""[0.9833131432533264, 0.016686901450157166]"""
1355,229614,Eagle 0.5 release tasks checklist,This ticket should include all the required tasks that needs to be complete in order to release 0.5,smartshark_2_2,835,eagle,"""[0.996791660785675, 0.003208358306437731]"""
1356,229615,Refactor application shared service registry framework,"Refactor application shared service registry framework:

* Add `Optional<List<Service>> getSharedServices(Config envConfig)` in `ApplicationProvider`
* Move `MRHistoryJobDailyReporter` registry to `MRHistoryJobApplicationProvider` from `ServerApplication`
* Register `getSharedServices` from `ApplicationProvider` to `Environment` in `ServerApplication`




https://issues.apache.org/jira/browse/EAGLE-843",smartshark_2_2,876,eagle,"""[0.9984068274497986, 0.0015931816305965185]"""
1357,229616,application health check support notification,"Currently, application health check only supports logging. However, we should support notification like email/slack and so on.",smartshark_2_2,655,eagle,"""[0.9984585046768188, 0.0015414701774716377]"""
1358,229617,Dynamic security event correlation in Eagle,"The idea is to develop security event correlation platform for user to easily onboard new metric and model metric correlation.

Eagle project was invited to HackIllinois event https://www.hackillinois.org/
Thanks Tyler Brown, Subasree Venkatsubhramaniyen, Yunsheng Wei to make this Hackathon amazing ...
Rest API : Tyler Browner (University of Missouri)
Spout routing: Yunsheng Wei (Urbana-Champaign of Illinois University)
Bolt dispatching: Subasree Venkatsubhramaniyen (University of Wisconsin-Madison)

https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61340204",smartshark_2_2,201,eagle,"""[0.9983153343200684, 0.0016846915241330862]"""
1359,229618,narrow unit test scope of pr reviewer ci job so that it only test the affections in the corresponding pr,"Currently, the job checking PR is running all the unit test cases as a whole, we have to narrow down the scope to focus on the things affected only in the PR.

This may not be accomplishable based on what we now have, but we should try it.",smartshark_2_2,42,eagle,"""[0.9984691739082336, 0.0015307859284803271]"""
1360,229619,update site/doc with new content of 0.4.0-incubating,"base code repository is: https://github.com/eaglemonitoring/eaglemonitoring.github.io

we'd deploy the new content after 0.4.0-incubating release announced.",smartshark_2_2,249,eagle,"""[0.997479259967804, 0.0025207248982042074]"""
1361,229620,Use Annotation to describe application metadata in ApplicationProvider.xml,"Current application framework only supports to define application metadata in XML descriptor file, it may be more convenient to define metadata directly with Annotation in code.",smartshark_2_2,951,eagle,"""[0.9973623156547546, 0.002637705998495221]"""
1362,229621,UI merge side bar item if same name,UI merge side bar item if same name,smartshark_2_2,770,eagle,"""[0.9918729662895203, 0.008126987144351006]"""
1363,229622,Unit test case failed: org.apache.eagle.alert.cep.TestSiddhiEvaluator ,Unit test case failed: org.apache.eagle.alert.cep.TestSiddhiEvaluator,smartshark_2_2,470,eagle,"""[0.9716074466705322, 0.028392581269145012]"""
1364,229623,oozie auditlog parser don`t deal with NullPointerException,"entity = parser.parse(logLine); object entity maybe is null.
It will cause NullPointerException if there is no processing.
",smartshark_2_2,329,eagle,"""[0.3269210755825043, 0.6730788946151733]"""
1365,229624,Integrate  Eagle Service with  Dataguise's product DgSecure,DgSecure can be used to discover sensitive metadata in hadoop cluster. These metadata should be sent to Eagle Service periodically and be used to create Eagle Policies.,smartshark_2_2,166,eagle,"""[0.9980399012565613, 0.001960120862349868]"""
1366,229625,create jekyll-infra for eagle version-based page,"based on branch ""master""",smartshark_2_2,246,eagle,"""[0.9964039325714111, 0.003596134018152952]"""
1367,229626,Run eagle on flink/samza,Eagle is running on storm. One might want to do some interesting port to other stream framework like flink/samza.,smartshark_2_2,1022,eagle,"""[0.9978210926055908, 0.0021789665333926678]"""
1368,229627,tomcat jar dependencies need to be cleaned up,"There are 3 jars needing to be cleaned up in directory eagle-assembly/src/main/lib/tomcat/bin, they are:
 - bootstrap.jar
 - commons-daemon.jar
 - tomcat-juli.jar

Currently, we use an approach to remove it before packaging a source code tar-ball, and give customers a patch url to apply them back before building from source. We'd better clean it up and form a smoother way in version 0.5.0-incubating.",smartshark_2_2,995,eagle,"""[0.9979779124259949, 0.0020221180748194456]"""
1369,229628,Monitor jobs with high RPC throughput ,"We've identified some jobs with high RPC throughput which causes the NN heavy RPC overhead. These jobs has requested extremely large HDFS operations in a very short window (2 mins).

So we tend to capture those jobs with:
a) the job has very large RPC throughput, using the job total HDFS ops/the job duration, if the throughput is larger than 1000
b) and if the HDFS ops per task is larger than 25
Then send out the alert out. Later, we will notify the users to optimize their jobs.",smartshark_2_2,897,eagle,"""[0.9983991980552673, 0.0016008225502446294]"""
1370,229629,Integrate typesafe-config with DropWizard,"Eagle server mix YAML and typesafe-config, it's a little tricky, better solution is to use consistent configuration library.",smartshark_2_2,986,eagle,"""[0.9978602528572083, 0.0021397501695901155]"""
1371,229630,Update Alert page for display more infomation,"As a user, we want to view more information in the alert list page & alert detail page.

For alert list page:
Add message time

For alert detail page:
Add message time
Add user
Add host info",smartshark_2_2,86,eagle,"""[0.9982719421386719, 0.0017280499450862408]"""
1372,229631,Generate sortSpec only on externalWindow,Generate sortSpec only on externalWindow,smartshark_2_2,617,eagle,"""[0.8121154308319092, 0.1878845989704132]"""
1373,229632,WebService Query StartTime & EndTime Support LONG timestamp,"WebService Entity Query currently only support date string in format of YYYY-MM-DD HH-mm-SS, we should also StartTime & EndTime Support in LONG timestamp",smartshark_2_2,263,eagle,"""[0.9982989430427551, 0.0017010882729664445]"""
1374,229633,convert JobHistoryZKStateManager to singleton,"One feeder component only needs one instance of JobHistoryZKStateManager, so it should be singleton",smartshark_2_2,362,eagle,"""[0.9983426332473755, 0.0016573367174714804]"""
1375,229634,Policy UI adv mode support sql highlight,Policy UI adv mode support sql highlight,smartshark_2_2,595,eagle,"""[0.9982174038887024, 0.0017825373215600848]"""
1376,229635,Fix UnitTest Error caused by streamDef.json and TestApplicationImpl,Fix UnitTest Error caused by streamDef.json and TestApplicationImpl,smartshark_2_2,623,eagle,"""[0.9281384348869324, 0.07186153531074524]"""
1377,229636,Fetch accepted MR jobs to assist queue analysis,"If a job is in accepted state for a long time, the submitted queue is considered unhealthy",smartshark_2_2,874,eagle,"""[0.9983460903167725, 0.0016538689378648996]"""
1378,229637,Create Eagle release 0.3.0,"There is some initial discussion on creating first release for Apache Eagle, Eagle mentors and contributors strongly suggested we go through release process.

This ticket is for tracking all activities for creating this release Eagle 0.3.0.",smartshark_2_2,132,eagle,"""[0.9978019595146179, 0.0021980362944304943]"""
1379,229638,VertexInputPath in GiraphRunner refers to EdgeInputPath,"There seems to be a typo in GiraphRunner that prevents me from using it currently. The edge input path is read instead of the vertex input path:

{noformat}
      if (cmd.hasOption(""vip"")) {
        GiraphFileInputFormat.addVertexInputPath(job.getInternalJob(),
            new Path(cmd.getOptionValue(""eip"")));
{noformat}",smartshark_2_2,452,giraph,"""[0.14677581191062927, 0.8532242178916931]"""
1380,229639,createZooKeeperServerList should use task instead of port number,"There was an issue in the recent change: https://issues.apache.org/jira/browse/GIRAPH-1068

createZooKeeperServerList() should actually store task number instead of port and the port should only be leveraged by onlineZooKeeperServer() ",smartshark_2_2,984,giraph,"""[0.6063083410263062, 0.3936917185783386]"""
1381,229640,NPE in HiveGiraphRunner when the vertex output format is not defined.,"13/03/22 09:41:53 ERROR mapred.CoronaJobTracker: UNCAUGHT: Thread main got an uncaught exception
java.lang.NullPointerException
	at org.apache.giraph.hive.HiveGiraphRunner.logOptions(HiveGiraphRunner.java:683)
	at org.apache.giraph.hive.HiveGiraphRunner.run(HiveGiraphRunner.java:265)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:68)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
	at com.facebook.digraph.hellopagerank.HelloPageRank.main(HelloPageRank.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)",smartshark_2_2,530,giraph,"""[0.06533317267894745, 0.9346668720245361]"""
1382,229641,Fix stopping jmap histo thread,Currently if jmap histo frequency is set to long period we end up stuck in the end of the job for a long time waiting on jmap histo thread,smartshark_2_2,1091,giraph,"""[0.14110085368156433, 0.8588991761207581]"""
1383,229642,ByteArrayEdges.iterator() throws NPE when no edges are present,"The ByteArrayEdges.iterator() method throws the following NPE when no edges are present:

2014-03-31 14:44:46,273 ERROR org.apache.giraph.utils.LogStacktraceCallable: Execution of callable failed
java.lang.NullPointerException
	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:120)
	at org.apache.giraph.utils.ExtendedByteArrayDataInput.<init>(ExtendedByteArrayDataInput.java:50)
	at org.apache.giraph.conf.ImmutableClassesGiraphConfiguration.createExtendedDataInput(ImmutableClassesGiraphConfiguration.java:1009)
	at org.apache.giraph.edge.ByteArrayEdges$ByteArrayEdgeIterator.<init>(ByteArrayEdges.java:141)
	at org.apache.giraph.edge.ByteArrayEdges$ByteArrayEdgeIterator.<init>(ByteArrayEdges.java:138)
	at org.apache.giraph.edge.ByteArrayEdges.iterator(ByteArrayEdges.java:167)

Simply checking the edgeCount before constructing a new ByteArrayEdgeIterator solves the problem. ",smartshark_2_2,759,giraph,"""[0.05716424807906151, 0.942835807800293]"""
1384,229643,Number of containers required for a job,"Java 1.6.x
Giraph trunk - revert java 1.7 support.
Hadoop 2.2.0.x

Job submission fails due to:
{noformat}
13/11/28 12:02:14 INFO yarn.GiraphYarnClient: Running Client
13/11/28 12:02:14 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.100:8045
13/11/28 12:02:15 INFO yarn.GiraphYarnClient: Got node report from ASM for, nodeId=kreator:46477, nodeAddresskreator:8042, nodeRackName/default-rack, nodeNumContainers7
13/11/28 12:02:15 INFO yarn.GiraphYarnClient: Got node report from ASM for, nodeId=exotica:46645, nodeAddressexotica:8042, nodeRackName/default-rack, nodeNumContainers8
Exception in thread ""main"" java.lang.RuntimeException: Giraph job requires 2 containers to run; cluster only hosts 15
	at org.apache.giraph.yarn.GiraphYarnClient.checkPerNodeResourcesAvailable(GiraphYarnClient.java:230)
	at org.apache.giraph.yarn.GiraphYarnClient.run(GiraphYarnClient.java:125)
{noformat}",smartshark_2_2,715,giraph,"""[0.15097297728061676, 0.8490270376205444]"""
1385,229644,ClassCastException when giraph.SplitMasterWorker=false,"Using -Dgiraph.SplitMasterWorker=false with a recent trunk (r1401165) caused the machine who is playing the master role (showing itself as {{ALL}} from the task tracker page) to throw ClassCastException (SendVertexRequest -> MasterRequest) from MasterRequestServerHandler class.  I'm trying to use as many machines as possible for actual computation (can't afford to waste one for master+ZK).  This worked fine with r1388628 (roughly a month ago), so a recent change must've broken something.  Here's the relevant log I captured:

{code}
2012-10-24 23:08:02,152 WARN org.apache.giraph.comm.netty.handler.RequestServerHandler: exceptionCaught: Channel failed with remote address /10.x.y.z:41780
java.lang.ClassCastException: org.apache.giraph.comm.requests.SendVertexRequest cannot be cast to org.apache.giraph.comm.requests.MasterRequest
	at org.apache.giraph.comm.netty.handler.MasterRequestServerHandler.processRequest(MasterRequestServerHandler.java:25)
	at org.apache.giraph.comm.netty.handler.RequestServerHandler.messageReceived(RequestServerHandler.java:100)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:71)
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",smartshark_2_2,383,giraph,"""[0.06565210223197937, 0.9343478679656982]"""
1386,229645,HBaseVertexInput/Output formats handle setConf() internally,"My earlier patch GIRAPH-350 added more responsibility with the client and did not address the problem in split master/worker mode. My tests unfortunately do not cover pseudo/fully distributed scenarios. I noticed this condition in one of my HBase jobs and fixed it. 

Internally the HBaseVertexInput/Output formats now setConf() when appropriate, instead of forcing users to remember to call the static setConf() on the abstract class. 

In a separate ticket I will test the Accumulo I/O to see if it suffers from similar limitations.

This patch fixes the HBase I/O formats. ",smartshark_2_2,328,giraph,"""[0.22271910309791565, 0.777280867099762]"""
1387,229646,Worker crashes if its vertices have no edges when using edge input,"We get IllegalArgumentException in moveEdgesToVertices, since ArrayBlockingQueue can't be created with size 0.",smartshark_2_2,570,giraph,"""[0.07220358401536942, 0.9277963638305664]"""
1388,229647,ArrayOutOfBoundsException in org.apache.giraph.worker.InputSplitPathOrganizer,"I ran into a strange exception when testing the RandomWalkVertex on a cluster of 26 machines running Hadoop 1.0.4

{noformat}
java.lang.IllegalStateException: run: Caught an unrecoverable exception Index: 225, Size: 205
	at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:735)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.IndexOutOfBoundsException: Index: 225, Size: 205
	at java.util.ArrayList.rangeCheckForAdd(ArrayList.java:612)
	at java.util.ArrayList.addAll(ArrayList.java:554)
	at org.apache.giraph.worker.InputSplitPathOrganizer.prioritizeLocalInputSplits(InputSplitPathOrganizer.java:140)
	at org.apache.giraph.worker.InputSplitPathOrganizer.<init>(InputSplitPathOrganizer.java:93)
	at org.apache.giraph.worker.InputSplitsCallable.<init>(InputSplitsCallable.java:140)
	at org.apache.giraph.worker.VertexInputSplitsCallable.<init>(VertexInputSplitsCallable.java:97)
	at 
org.apache.giraph.worker.VertexInputSplitsCallableFactory.newCallable(VertexInputSplitsCallableFactory.java:86)
	at org.apache.giraph.worker.BspServiceWorker.loadInputSplits(BspServiceWorker.java:266)
	at org.apache.giraph.worker.BspServiceWorker.loadVertices(BspServiceWorker.java:310)
	at org.apache.giraph.worker.BspServiceWorker.setup(BspServiceWorker.java:483)
	at org.apache.giraph.graph.GraphMapper.map(GraphMapper.java:525)
	at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:723)
	... 7 more
{noformat}",smartshark_2_2,451,giraph,"""[0.06933476030826569, 0.9306652545928955]"""
1389,229648,giraph bin script should pick up giraph-core jar to pass to hadoop,bin script picks up the first jar file in the GIRAPH_HOME that starts with giraph*,smartshark_2_2,685,giraph,"""[0.9931738972663879, 0.006826084107160568]"""
1390,229649,Fix multithreaded output,Multithreaded output is broken - each thread writes all the partitions out.,smartshark_2_2,847,giraph,"""[0.1705721765756607, 0.8294277787208557]"""
1391,229650,HiveGiraphRunner picks -D options too late,"In HiveGiraphRunner, options passed as '-D' are picked up in the end, causing problems with for example initializing IO formats which need these options.",smartshark_2_2,839,giraph,"""[0.19190713763237, 0.8080928921699524]"""
1392,229651,More flexible Hive output,Currently with Hive output formats it's only possible to write single row per vertex. We should support variable number of rows per vertex (zero or multiple).,smartshark_2_2,532,giraph,"""[0.9983420372009277, 0.0016579506918787956]"""
1393,229652,Script for running on all profiles,"Script I have been using to test on all profiles, figured others might benefit from it.",smartshark_2_2,345,giraph,"""[0.9979209303855896, 0.0020790693815797567]"""
1394,229653,"Saving vertices has no status report, making it hard to find DFS issues","Adds status updates to saving of the vertices and improve the overall logic of when to print status of loading/storing the graph (every 250k vertices or 15 secs).  This will help us to see which are the slow workers when saving output.  This updates the Hadoop status messages as well as prints to the task log.  I also made this consistent for the loading.

Task log messages look like the following:

org.apache.giraph.worker.BspServiceWorker  - saveVertices: Saved 598753 out of 1000000 vertices, on partition 14 out of 24
INFO    2013-01-29 12:54:02,072 [main] org.apache.giraph.worker.BspServiceWorker  - saveVertices: Saved 848754 out of 1000000 vertices, on partition 20 out of 24
",smartshark_2_2,455,giraph,"""[0.987375020980835, 0.012624960392713547]"""
1395,229654,Incorrect Logging Statement on #255 in GiraphYarnClient,"Summary :

In-case, Yarn-Heap-Size passed in Giraph Configuration exceeds max available capacity, we downgrade its value to max capacity (Lines #253-#257).

Currently, the logger message associated with this conditional statementÂ incorrectly states ""minimum"" instead of ""maximum"", which is misleading.

This change fixes it.

Pull RequestÂ :Â [https://github.com/apache/giraph/pull/78]",smartshark_2_2,1208,giraph,"""[0.964958906173706, 0.035041045397520065]"""
1396,229655,Provide a way to halt running application,"It would be good to have a way to tell a running application to halt after current superstep is done instead of just killing it. For example, it would be useful when we have a long running application, but we need resources for another job, or we see that the result is good enough, we can just halt it and still get the output.",smartshark_2_2,952,giraph,"""[0.9983959794044495, 0.0016040204791352153]"""
1397,229656,Support vertex combiner in TestGraph,"TestGraph should use vertex combiner which is specified in the conf passed, instead of replacing the vertex with latest added.",smartshark_2_2,1086,giraph,"""[0.998505711555481, 0.0014942580601200461]"""
1398,229657,Remove duplicate BspCase in giraph-formats-contrib (and fix GIRAPH-382 breaking the build),We should use the tests jar instead of duplicating BspCase.,smartshark_2_2,371,giraph,"""[0.9985513091087341, 0.0014486254658550024]"""
1399,229658,implement degree distribution application on giraph,"It computes a histogram of degree distributions: in-degree, out-degree as well as undirected-degree (i.e., count of neighbors that lie in the intersections of in-neighborhood as  well as out-neighborhood for a given vertex)",smartshark_2_2,752,giraph,"""[0.998430073261261, 0.00156990101095289]"""
1400,229659,implement random sampling for input splits,"Currently if we are reading vertex/edge data from multiple tables, and we only want to read a fraction of data (with giraph.inputSplitSamplePercent conf option), we'll always get the first inputSplitSamplePercent of the input slits. We should instead use a random sample of input splits so testing on sample of data would look closer to actual full data run.",smartshark_2_2,1178,giraph,"""[0.9985686540603638, 0.0014313305728137493]"""
1401,229660,Improve memory usage of SendWorkerOneMessageToManyRequest ,"The current implementation takes incoming messages stored as ByteArrayOneMessageToManyIds and prepares them as a map from partition id to a ByteArrayVertexIdMessages, which holds the messages for the corresponding partition. It then adds these to the message store.

However, it is possible that these intermediate lists of message get big before they are added to the message store. If they reach the capacity of the underlying buffers, the job fails. This can be avoided if we push these lists to the message store before the get big. This is mostly beneficial when we use a combiner in which case the message store keeps only one value per vertex.",smartshark_2_2,1164,giraph,"""[0.9984062314033508, 0.0015937930438667536]"""
1402,229661,"We should check input splits status from zookeeeper once per worker, not once per split thread","When using a lot of workers and a lot of input split threads, checking that all input splits are finished after the reading is done takes a long time, since we check every input split once per thread.",smartshark_2_2,460,giraph,"""[0.9659005999565125, 0.03409946709871292]"""
1403,229662,HiveJythonRunner with support for pure Jython value types.,"This adds support for pure Jython jobs. Currently this runner is hooked up to work with Hive. I'll make it more generic later.

Running a Jython job is simply:

HIVE_HOME=<x>
HADOOP_HOME=<y>
$HIVE_HOME/bin/hive --service jar <giraph-hive-jar> org.apache.giraph.hive.jython.HiveJythonRunner jython1.py [jython2.py] ...

You can pass in any number of scripts. They will be parsed in order and sent to all the workers using DistributedCache.

There are examples and testsÂ in the diff. Here is one example:
launcher: https://gist.github.com/nitay/a62e0a5d369a5e701fa3
worker: https://gist.github.com/nitay/7834fd2b059527e65a36

There are a few pieces to a Jython job, I'll go over each part here.

The HiveJythonRunner will call a function called ""prepare(job)"" from the Jython scripts. This is the entry point for configuring your job.

In this configuration you setup everything, such as your graph types (those IVEMM writables) and sets up the Hive vertex/edge inputs and output. Each graph type is one of the following:
1) A Java type. For example the user can specify simply IntWritable
2) A Jython type that implements Writable. In the example above the message value implements Writable.
3) A pure Jython type. The Java code will wrap these objects in a Writable wrapper that serializes Jython values using Pickle (jython IO framework).

Your computation must implement JythonComputation. Note that this does not actually implement Computation, but rather is a separate class so that we can wrap all the types passed in with a wrapper that implements Writable. The methods are named the same so that the user does not notice anything.

For Hive usage - if your value type is a primitive e.g. IntWritable or LongWritable, then you need not do anything. The Java code will automatically read/write the Hive table specified and convert between Hive types and the primitive Writable. The vertex_id type in the example works like this.
IfÂ your value is a custom Jython type, you must create classes which implement JythonHiveReader/JythonHiveWriter (or JythonHiveIO which is both). These objects read/write Jython types from Hive. There are wrappers in the Java code which take HiveIO data normally used in giraph-hive and turns them into Jython types. This means, for example, that getMap() will return a Jython dictionary instead of a Java Map.

There is also a PageRankBenchmark (from previous diff) implemented in Jython. Here's a run for comparison / sanity check:

PageRankBenchmark with 10 workers, 100M vertices, 10B edges, 10 compute threads
trunk:
  https://gist.github.com/nitay/3170fa3b575d4d2e22a9
  total time: 302466
with this diff:
  https://gist.github.com/nitay/a52b6d1d64e50ab9829e
  total time: 306517
in jython:
  https://gist.github.com/nitay/3f2e758b2933c3521727
  total time: 434730

So we see that existing things are not affected (is there something else I should test?) and that Jython has around 40% overhead.

ReviewBoard: https://reviews.apache.org/r/12543/ (Sorry it's a big one, hard to split up :/)",smartshark_2_2,837,giraph,"""[0.9983344674110413, 0.0016655449289828539]"""
1404,229663,Bring back FindBugs,"Currently our FindBugs config excludes a lot of patterns. We should bring them back and ignore the patterns for specific classes where needed so that future code is still warned about problems.

https://reviews.apache.org/r/13447/",smartshark_2_2,826,giraph,"""[0.9983310103416443, 0.0016689951298758388]"""
1405,229664,Move aggregators to a separate sub-package,"Since aggregators will be re-used throughout many projects and algorithms, it makes sense to implement the most common ones in a separate sub-package. This will reduce the time required for users when they implement their projects based on Giraph, because the required aggregators are already in place. I implemented the following ones:
for int/long/float/double: min, max, product, sum, overwrite
for boolean: and, or, overwrite

Most of them speak for themselves, except for the overwrite one. This aggregator simply overwrites the stored value when a new value is aggregated. This is useful when one node is in some way a master node (for example a source node in an routing algorithm), and this node wants to broadcast a value to all other nodes.

Attached is a patch against trunk implementing the aggregators and patching some existing files so they use the .aggregators package instead of the .examples one.",smartshark_2_2,172,giraph,"""[0.9983569979667664, 0.0016429994720965624]"""
1406,229665,enable creation of javadoc and sources jars,"It is pretty useful to enable the creation if javadoc and sources jars during the build, so that people using IDEs like eclipse can easily jump into the code.",smartshark_2_2,44,giraph,"""[0.9965141415596008, 0.0034858682192862034]"""
1407,229666,Remove zookeeper from input splits handling,"Currently we use zookeeper for handling input splits, by having each worker checking each split, and when a lot of splits are used this becomes very slow. We should have master coordinate input splits allocation instead, making the complexity proportional to #splits instead of #workers*#splits.",smartshark_2_2,1053,giraph,"""[0.998299777507782, 0.0017002716194838285]"""
1408,229667,Support use of counters in InputFormats,"I'm working on a custom EdgeInputFormat where I'd like to use map reduce counters. I've noticed these don't work, because the TaskAttemptContext contains a DummyReporter.

This is because WrappedEdgeInputFormat.createEdgeReader() creates a different TaskAttemptContext to pass to the my reader, I assume because of threadsafety considerations.

I patched HadoopUtils.makeTaskAttemptContext() to support counters, but only in case HADOOP_NON_JOBCONTEXT_IS_INTERFACE, because only in that case it possible to pass on a StatusReporter to the TaskAttemptContextImpl.",smartshark_2_2,1012,giraph,"""[0.9704588055610657, 0.029541224241256714]"""
1409,229668,Log amount of free memory on the command line,Very useful when checking whether application is healthy is to know if some of the workers has memory issues - we should track minimal amount of free memory on the command line.,smartshark_2_2,742,giraph,"""[0.9984733462333679, 0.001526624895632267]"""
1410,229669,'mvn clean test' fails for rexster,"This used to work in the past.  Can the rexster guys [~cmartella]] [~armax00]] please take a look?

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Giraph Parent .............................. SUCCESS [49.819s]
[INFO] Apache Giraph Core ................................ SUCCESS [6:40.342s]
[INFO] Apache Giraph Examples ............................ SUCCESS [9:06.653s]
[INFO] Apache Giraph Accumulo I/O ........................ SUCCESS [42.679s]
[INFO] Apache Giraph HBase I/O ........................... SUCCESS [2:54.177s]
[INFO] Apache Giraph HCatalog I/O ........................ SUCCESS [56.357s]
[INFO] Apache Giraph Hive I/O ............................ SUCCESS [5:53.348s]
[INFO] Apache Giraph Gora I/O ............................ SUCCESS [1:45.304s]
[INFO] Apache Giraph Rexster I/O ......................... SUCCESS [1.250s]
[INFO] Apache Giraph Rexster Kibble ...................... SUCCESS [10.493s]
[INFO] Apache Giraph Rexster I/O Formats ................. FAILURE [18.005s]
[INFO] Apache Giraph Distribution ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 29:20.210s
[INFO] Finished at: Tue Feb 25 12:09:17 PST 2014
[INFO] Final Memory: 77M/1842M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project giraph-rexster-io: Could not resolve dependencies for project org.apache.giraph:giraph-rexster-io:jar:1.1.0-SNAPSHOT: Could not find artifact org.apache.giraph:giraph-kibble:jar:1.1.0-SNAPSHOT at specified path /data/users/aching/giraph/giraph-rexster/giraph-rexster-io/../../giraph-rexster/giraph-kibble/target/giraph-kibble-1.1.0-SNAPSHOT.jar -> [Help 1]",smartshark_2_2,896,giraph,"""[0.9795740842819214, 0.02042589895427227]"""
1411,229670,Reduce ZooKeeper output in tests by changing log level from INFO to ERROR,"Right now we generate a lot of output in the test, we can reduce a lot of useless output from ZooKeeper in testing by changing the log level.  In practice in real applications, this isn't much of an issue, so only changing this for tests.",smartshark_2_2,848,giraph,"""[0.9981903433799744, 0.0018096363637596369]"""
1412,229671,Website Documentation: Page Rank Example,Describe how to make a new page rank application for Giraph.,smartshark_2_2,583,giraph,"""[0.9982105493545532, 0.0017894420307129622]"""
1413,229672,Multithreading should intelligently allocate the thread pools,"Even if the user suggests a very high number of threads, the input split threads should not exceed the number splits divided by the number of  workers.

The number of compute threads should not be greater than the number of partitions on that worker.",smartshark_2_2,354,giraph,"""[0.9978063702583313, 0.002193673513829708]"""
1414,229673,Add Blocks Framework,"This will be a set of diffs adding Blocks Framework, summary on each diff will explain it's contents.

More context in:
http://mail-archives.apache.org/mod_mbox/giraph-dev/201506.mbox/%3CCABJ-n3v-24YLzgNmrT3TZT6R8t4Vw1hrBcWWTghG_XgaC%3DYqrg%40mail.gmail.com%3E",smartshark_2_2,1193,giraph,"""[0.9983718991279602, 0.0016280992422252893]"""
1415,229674,Create EdgeInputFormat from Apache Gora,"Currently we support reading a graph from NoSQL stores HBase and Accumulo. The contribs are similar but they are two different codebases for us. Apache Gora provides a general API to access data stores such as HBase and Accumulo (and other NoSQL stores like Cassandra, and relational databases as well). It could make sense to merge the current support for NoSQL stores into a single contrib through Gora, to minimize code duplication and win support more stores.",smartshark_2_2,936,giraph,"""[0.9983349442481995, 0.0016650931211188436]"""
1416,229675,Adding observers with references to BspServiceMaster / BspServiceWorker,"Observers are added which will have access to WorkerAggregatorHandler and GlobalCommHandler, to be used for metrics aggregated across workers.",smartshark_2_2,1189,giraph,"""[0.9983260035514832, 0.0016740408027544618]"""
1417,229676,Site build error with maven 3.1,Site does not build with maven 3.1 and maven-site-plugin 3.2.,smartshark_2_2,804,giraph,"""[0.9931649565696716, 0.006834976375102997]"""
1418,229677,Add new RPC call (putVertexIdMessagesList) to batch putMsgList RPCs together,Right now messages are sent to a vertex one at a time.  It would be good to have a putMsgs call that could send messages to multiple vertices (all hosted on the same worker).  We'd save a huge number of individual RPC calls at the expense of having smaller calls with larger payloads.,smartshark_2_2,36,giraph,"""[0.9983252882957458, 0.0016747588524594903]"""
1419,229678,Perform single and multi-seed Breadth First Search with a single algorithm.,"The idea is to have a single Breadth First Search implementation which can perform both single-seed and multi-seed computation with a single algorithm. Multi-seed version maintains not only the distance from the closest seed but also the closest seed. This can be used for performing clustering, for example. In both versions, current distance from any of the sources can be maintained as a global writable. In the multi-seed version, the closest source can be communicated in messages while the single-seed version requires only empty messages.

This general implementation can accept two functions from the user:
- a function called initializeVertex (vertex, value) -> message that initializes the vertex value and returns the message to be sent to its neighbors.
- a function called traverseVertex (vertex, value, messages) -> message that gets at most once on each vertex. This function is called when the BFS traverses the vertex. 
Additionally, the implementation can accept the class type of the messages. This implementation needs to be backward-compatible and support applications that were using previous BFS interface.",smartshark_2_2,976,giraph,"""[0.9984754920005798, 0.0015245134709402919]"""
1420,229679,Ensure we get the original exception from GraphMapper#run(),"We can lose the original exception if failureCleanup() fails.

I.e.

INFO    2012-10-18 14:23:25,417 [main] org.apache.giraph.graph.WorkerAggregatorHandler  - marshalAggregatorValues: Finished assembling aggregator values
INFO    2012-10-18 14:23:25,451 [main-SendThread(xxx.machine.xxx:22181)] org.apache.zookeeper.ClientCnxn  - Unable to read additional data from server sessionid 0x13a75baca440014, likely server has closed socket, closing socket c\
onnection and attempting reconnect
ERROR   2012-10-18 14:23:25,552 [main] org.apache.giraph.graph.BspServiceWorker  - unregisterHealth: Got failure, unregistering health on /_hadoopBsp/job_201209271814.8652_0001/_applicationAttemptsDir/0/_superstepDir/1/_workerHea\
lthyDir/xxx.machine.xxx_9 on superstep 1
WARN    2012-10-18 14:23:25,554 [main-EventThread] org.apache.giraph.graph.BspService  - process: Disconnected from ZooKeeper (will automatically try to recover) WatchedEvent state:Disconnected type:None path:null
INFO    2012-10-18 14:23:26,916 [main-SendThread(xxx.machine.xxx:22181)] org.apache.zookeeper.ClientCnxn  - Opening socket connection to server xxx.machine.xxx/10.174.108.77:22181
INFO    2012-10-18 14:23:26,917 [main-SendThread(xxx.machine.xxx:22181)] org.apache.zookeeper.ClientCnxn  - Socket connection established to xxx.machine.xxx/10.174.108.77:22181, initiating session
WARN    2012-10-18 14:23:26,977 [main-SendThread(xxx.machine.xxx:22181)] org.apache.zookeeper.ClientCnxn  - Session 0x13a75baca440014 for server xxx.machine.xxx/10.174.108.77:22181, unexpected error, closing socket connection and\
 attempting reconnect
java.io.IOException: Connection reset by peer
at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
at sun.nio.ch.IOUtil.read(IOUtil.java:186)
at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:858)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1130)
WARN    2012-10-18 14:23:27,082 [main] org.apache.hadoop.mapred.Child  - Error running child
java.lang.IllegalStateException: unregisterHealth: KeeperException - Couldn't delete /_hadoopBsp/job_201209271814.8652_0001/_applicationAttemptsDir/0/_superstepDir/1/_workerHealthyDir/xxx.machine.xxx_9
at org.apache.giraph.graph.BspServiceWorker.unregisterHealth(BspServiceWorker.java:582)
at org.apache.giraph.graph.BspServiceWorker.failureCleanup(BspServiceWorker.java:590)
at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:608)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:632)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
at org.apache.hadoop.mapred.Child.main(Child.java:171)
Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /_hadoopBsp/job_201209271814.8652_0001/_applicationAttemptsDir/0/_superstepDir/1/_workerHealthyDir/xxx.machine.xxx_9
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:728)
at org.apache.giraph.graph.BspServiceWorker.unregisterHealth(BspServiceWorker.java:576)
... 5 more
",smartshark_2_2,339,giraph,"""[0.26078325510025024, 0.7392167448997498]"""
1421,229680,update hive-io dependency,"General cleanup of Hive related things.

https://reviews.apache.org/r/10051/",smartshark_2_2,529,giraph,"""[0.9983261227607727, 0.0016738142585381866]"""
1422,229681,Create a testing framework that doesn't require I/O formats,"Create a TestGraph class to conveniently build graphs stored in memory.
Add appropriate input/output formats to be used in InternalVertexRunner.",smartshark_2_2,558,giraph,"""[0.9978744983673096, 0.0021255628671497107]"""
1423,229682,Performance regression after GIRAPH-1068,We noticed significant performance regression caused by GIRAPH-1068 for jobs that have a lot of supersteps. This is likely caused by some missing zookeeper options. ,smartshark_2_2,1103,giraph,"""[0.997655987739563, 0.002344016684219241]"""
1424,229683,Define zookeeper version in a property to allow build time override,Consider supporting a build time override of the ZooKeeper dependency version specified in the POM.,smartshark_2_2,806,giraph,"""[0.9981306195259094, 0.0018693996826186776]"""
1425,229684,CommunicationsInterface shouldn't implement VersionedProtocol,"Currently, CommunicationsInterface, the base interface for peer-peer communication implements as part of its definition Hadoop's VersionedProtocol, which is part of Hadoop's RPC stack.  Other RPC implementations need to implement CommunicationsInterface, but shouldn't need to implement an HRPC specific interface.",smartshark_2_2,167,giraph,"""[0.9963827133178711, 0.0036172557156533003]"""
1426,229685,Allow configuration of arbitrary objects via reflection,"Currently we need to pull data from configuration object by using various getSmth methods. Each get call has to go through synchronized call to Hashtable and then parse string value. It would be convenient to be able to load configuration values into arbitrary object with typed fields and annotated with special annotations. 
",smartshark_2_2,969,giraph,"""[0.9981085062026978, 0.0018915163818746805]"""
1427,229686,implement vertex-subgraph on giraph,"The idea is to create a vertex based subgraph given an input consisting of sampled/selected vertices & the entire graph as edge input.

It filters out all edges that originate at a vertex that's not sampled/selected and writes back to output",smartshark_2_2,751,giraph,"""[0.998472273349762, 0.0015277463244274259]"""
1428,229687,mvn rat doesn't like .git or .idea,mvn verify fails for me due to the existence of .git and .idea (IntelliJ Idea's tracking folder). It fails all the files within those directories.,smartshark_2_2,76,giraph,"""[0.9750396013259888, 0.02496035397052765]"""
1429,229688,Website Documentation: Missing presentations,"I gave two talks on Giraph last years, that might be added to the presentations list on the website.",smartshark_2_2,603,giraph,"""[0.9983160495758057, 0.0016840139869600534]"""
1430,229689,In-proc ZooKeeper server with Master process,"Currently by default zookeeper runs as a separate java process, on the same server where master runs. This prevents us from seeing zookeeper logs and makes it harder to debug memory issues.  We should be able to run zookeeper inside Master process and perhaps this should be default. ",smartshark_2_2,1060,giraph,"""[0.9984893798828125, 0.0015105969505384564]"""
1431,229690,Remove limit on the number of partitions,Currently we have a limit on how many partitions we can have because we write all partition information to Zookeeper. We can instead send this information in requests and remove the hard limit.,smartshark_2_2,1006,giraph,"""[0.9984424710273743, 0.0015575992874801159]"""
1432,229691,Simplify boolean expressions in BspRecordReader,Twice in BspRecordReader boolean expressions are evaluated with == and can be simplified to just one liners or variable evaluation.,smartshark_2_2,48,giraph,"""[0.9982367753982544, 0.0017632246017456055]"""
1433,229692,Remove unused reuseIncomingEdgeObjects option,"As part of the OutEdges redesign, this option was replaced by the ReuseObjectsOutEdges interface. I was pretty sure I had removed it, but for some reason it's still there.",smartshark_2_2,829,giraph,"""[0.9984434247016907, 0.0015566087095066905]"""
1434,229693,Always use job Configuration to create Configuration,"There are a few places where we create new Configuration from scratch, which causes problems because it ignores job specific options.",smartshark_2_2,929,giraph,"""[0.971278190612793, 0.028721798211336136]"""
1435,229694,Support for the Facebook Hadoop branch,"I've been working with Joe Xie on support to get Giraph running on the Facebook Hadoop branch.  He verified today that the examples worked on their cluster.  I need to clean up my changes a little, but otherwise, will submit a cleaned up diff.  As a side note, does anyone know how we can get Hudson support for Giraph?",smartshark_2_2,130,giraph,"""[0.9984650611877441, 0.0015350014436990023]"""
1436,229695,Add additional Examples ,"I'm new to Giraph and one of the things that would have been helpful would had been more examples.  

Does anyone mind if I add an example for KMeans and Tree Building?

My first draft of these implementation are as follows:
1. https://github.com/tmalaska/Giraph.KMeans.Example
2. https://github.com/tmalaska/Giraph.TreeRooter.Example

If Giraph is interested I will make a patch that includes:
1. Full implemented and commented examples
2. Sample data
3. Documentation
",smartshark_2_2,707,giraph,"""[0.9984080195426941, 0.0015920200385153294]"""
1437,229696,Speedup unittesting,It would be awesome if someone could create a Giraph simulator that ran a majority of our unittests and we only had a few that launched with LocalJobRunner (very slow).  This would increase productivity.,smartshark_2_2,745,giraph,"""[0.9981942772865295, 0.0018057645065709949]"""
1438,229697,Improve ZooKeeper issues,"Currently, if the ZooKeeper process fails, we have little information on why and what happened.  This patch addresses this by keeping the last 100 log lines and dumps when the map fails under a RuntimeException.

Here is an example of a master task failure when there is an invalid JVM argument passed to ZooKeeper.  The error is much for obvious now.

2012-10-04 15:05:28,916 WARN org.apache.giraph.zk.ZooKeeperManager: logZooKeeperOutput: Dumping up to last 100 lines of the ZooKeeper process STDOUT and STDERR.
2012-10-04 15:05:28,916 WARN org.apache.giraph.zk.ZooKeeperManager$StreamCollector: Unrecognized option: -BadOpt
2012-10-04 15:05:28,916 WARN org.apache.giraph.zk.ZooKeeperManager$StreamCollector: Could not create the Java virtual machine.
2012-10-04 15:05:28,919 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
2012-10-04 15:05:28,959 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.IllegalStateException: run: Caught an unrecoverable exception onlineZooKeeperServers: Failed to connect in 5 tries!
                                 at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:591)
                                 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
                                 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
                                 at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
                                 at java.security.AccessController.doPrivileged(Native Method)
                                 at javax.security.auth.Subject.doAs(Subject.java:396)
                                 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
                                 at org.apache.hadoop.mapred.Child.main(Child.java:253)
Caused by: java.lang.IllegalStateException: onlineZooKeeperServers: Failed to connect in 5 tries!
       at org.apache.giraph.zk.ZooKeeperManager.onlineZooKeeperServers(ZooKeeperManager.java:721)
       at org.apache.giraph.graph.GraphMapper.setup(GraphMapper.java:328)
       at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:573)
       ... 7 more
2012-10-04 15:05:28,963 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task

",smartshark_2_2,317,giraph,"""[0.9981369972229004, 0.001863025943748653]"""
1439,229698,Loading vertex ranges from HBase,"Loading vertices from an HTable would be an option.

A possible schema for storing the graph would be Hexastore (http://www.vldb.org/pvldb/1/1453965.pdf). 
Also, as vertices whom messages are sent to get created on the fly (if they don't exist already), we could potentially have a HBaseVertex that fetches the adjacency list + vertex value from HBase. That would be kind of a Lazy-load approach, if you can define the initial split as an HBase query.
",smartshark_2_2,152,giraph,"""[0.9984258413314819, 0.0015742157120257616]"""
1440,229699,Improve internal zookeeper launching,"With the most up to date trunk, internal zookeeper launching only appears to work with Hadoop 1.x.x MR1.

With Hadoop 2.x.x MR2, trying to run a job without specifying an external zookeeper location results in a failed job with the following in the logs:

{code}
2014-02-12 17:30:30,281 INFO [main] org.apache.giraph.zk.ZooKeeperManager: onlineZooKeeperServers: Attempting to start ZooKeeper server with command [/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre/bin/java, -Xmx512m, -XX:ParallelGCThr
eads=4, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=70, -XX:MaxGCPauseMillis=100, -cp, /tmp/hadoop-yarn/staging/b.ajf/.staging/job_1392221733726_0002/job.jar, org.apache.zookeeper.server.quorum.QuorumPeerMain, /tmp/hadoop-b
.ajf/nm-local-dir/usercache/b.ajf/appcache/application_1392221733726_0002/work/_bspZooKeeper/zoo.cfg] in directory /tmp/hadoop-b.ajf/nm-local-dir/usercache/b.ajf/appcache/application_1392221733726_0002/work/_bspZooKeeper
(...)
2014-02-12 17:30:30,285 INFO [main] org.apache.giraph.zk.ZooKeeperManager: onlineZooKeeperServers: Connect attempt 0 of 10 max trying to connect to igraph-02.hi.inet:22181 with poll msecs = 3000
2014-02-12 17:30:30,289 WARN [main] org.apache.giraph.zk.ZooKeeperManager: onlineZooKeeperServers: Got ConnectException
java.net.ConnectException: Connection refused
(...)
2014-02-12 17:30:30,413 INFO [org.apache.giraph.zk.ZooKeeperManager$StreamCollector] org.apache.giraph.zk.ZooKeeperManager$StreamCollector: readLines: Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain
(...)
{code}

It clearly is unable to launch Zookeeper as it can't find the necessary class in the classpath. Looking at the command with which it tries to launch Zookeeper, we can see that it has specified a classpath of:

{code}
-cp, /tmp/hadoop/yarn/staging/b.ajf/.staging/job_1392221733726_0002/job.jar
{code}

which is a HDFS location.

It seems that with Hadoop 2.x.x, the function Job.getJar() returns a HDFS path to the jar instead of the path to the local copy of the jar in the DirectoryCache. Hadoop 1.x.x appears to return a correct path as I didn't detect any problem there.

The whole logic of finding the Zookeeper classpath seems extremely convoluted to me (not to mention broken as just shown for both MR2 and YARN). Since the currently running Java process has to have the zookeeper classes in its classpath anyway (because some of the classes in Giraph refer to Zookeeper classes), wouldn't it make more sense to just have the child java process starting Zookeeper simply inherit the classpath?",smartshark_2_2,766,giraph,"""[0.9977421760559082, 0.002257801126688719]"""
1441,229700,Giraph build breaks on trunk over Hive IO jar location issue?,"Hi Folks. When I build trunk after pulling the repo today I get this action:

{code}

[INFO] >>> findbugs-maven-plugin:2.5.1:check (default) @ giraph-core >>>
[INFO] 
[INFO] --- findbugs-maven-plugin:2.5.1:findbugs (findbugs) @ giraph-core ---
[INFO] Fork Value is true
[INFO] Done FindBugs Analysis....
[INFO] 
[INFO] <<< findbugs-maven-plugin:2.5.1:check (default) @ giraph-core <<<
[INFO] 
[INFO] --- findbugs-maven-plugin:2.5.1:check (default) @ giraph-core ---
[INFO] BugInstance size is 0
[INFO] Error size is 0
[INFO] No errors/warnings found
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Giraph Hive I/O 0.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[WARNING] The POM for com.facebook.giraph.hive:hive-io-experimental:jar:0.2-SNAPSHOT is missing, no dependency information available
Downloading: https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/avro/avro/1.7.1/avro-1.7.1.jar
Downloading: https://repository.apache.org/content/repositories/releases/org/apache/avro/avro/1.7.1/avro-1.7.1.jar
Downloaded: https://repository.apache.org/content/repositories/releases/org/apache/avro/avro/1.7.1/avro-1.7.1.jar (291 KB at 1149.7 KB/sec)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Giraph Parent .............................. SUCCESS [0.432s]
[INFO] Apache Giraph Core ................................ SUCCESS [1:37.413s]
[INFO] Apache Giraph Hive I/O ............................ FAILURE [1.082s]
[INFO] Apache Giraph Examples ............................ SKIPPED
[INFO] Apache Giraph Accumulo I/O ........................ SKIPPED
[INFO] Apache Giraph HBase I/O ........................... SKIPPED
[INFO] Apache Giraph HCatalog I/O ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:39.263s
[INFO] Finished at: Mon Mar 11 16:38:32 PDT 2013
[INFO] Final Memory: 35M/110M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project giraph-hive: Could not resolve dependencies for project org.apache.giraph:giraph-hive:jar:0.2-SNAPSHOT: Failure to find com.facebook.giraph.hive:hive-io-experimental:jar:0.2-SNAPSHOT in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache snapshots has elapsed or updates are forced -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :giraph-hive
{code}",smartshark_2_2,533,giraph,"""[0.9388739466667175, 0.06112603843212128]"""
1442,229701,Support for Watts Strogatz VertexInputFormat,"We currently support random graphs (Erdos-Renyi) only to generate random graphs. We sometimes want a higher average clustering coefficient while keeping a low average path length. The Watts-Strogatz model creates random graphs with a similar degree distribution to the E-R, but with high clustering coefficient and low average path length.",smartshark_2_2,789,giraph,"""[0.9984803795814514, 0.001519676297903061]"""
1443,229702,Adding an argument to GiraphRunner for Master Compute classes,"Currently, GiraphRunner does not support adding a custom master compute class through the command line prompt.

This issue is to add a simple master compute argument to giraph runner, so that the user can easily configure the master compute class when using the giraph binary file.",smartshark_2_2,260,giraph,"""[0.9983987212181091, 0.0016012662090361118]"""
1444,229703,remove hadoop RPC and keep just netty,"Given the netty communication behaves faster than rpc, we should just keep netty-based and drop the older one.",smartshark_2_2,159,giraph,"""[0.9984670281410217, 0.0015329541638493538]"""
1445,229704,"Factor creation of job-local ZooKeeper instances out of ZKManager to enable YARN instantiation, etc.","When the user does not specify a ZooKeeper quorum to run Giraph on, the ZooKeeperManager creates one. This factors out the boilerplate that handles this additional process and its lifecycle so that various pluggable versions of this process instantiation can be implemented and fed to the ZKManager via a factory depending on the nature of the underlying cluster.

By implementing the ZooKeeperProcessBuilder interface, one will now be able to implement the creation/mangement of a new, job-local ZK instance using any underlying resourcing mechanism one chooses (including, I hope, YARN.)

Passes mvn verify, etc.

Review board link:
https://reviews.apache.org/r/8948/
",smartshark_2_2,507,giraph,"""[0.9984579086303711, 0.0015420537674799562]"""
1446,229705,Need a callback that will get triggered when job actually starts,Giraph jobs only start after they get all mappers. It may take a while for job to start after being assigned a first mapper. For our project we need a callback that will get triggered when job actually starts that is gets all mappers,smartshark_2_2,1062,giraph,"""[0.9968624114990234, 0.003137632505968213]"""
1447,229706,Giraph Debugger,"Four of us at Stanford (Vikesh Khanna, Semih Salihoglu, Jaeho Shin, and Brian Ba Quan Truong) developed a debugger for Giraph, named Graft, and we hope to integrate our code into Giraph trunk.  It is able to launch Giraph jobs in debugging mode to capture traces of certain vertices and MasterCompute at particular supersteps, requiring almost no code change by the user.  From the captured traces, it can generate JUnit tests to replicate the contexts under which compute() function was running for the user to reproduce bugs.  You can read more about it at our GitHub repository: https://github.com/semihsalihoglu/graft",smartshark_2_2,954,giraph,"""[0.9974907636642456, 0.0025092042051255703]"""
1448,229707,Maven archetype for using Giraph from a separate project,"Provide a Maven archetype for new users to quickly get started with writing vertex code outside the Giraph source tree and running it on their cluster.  It is a painful process for newbies to run their own vertex code outside Giraph source tree without proper knowledge of Maven plugins or how class loading is done in Hadoop and Java.

The archetype can be derived from an existing project, so moving or copying some of the example vertices and input/output formats into a subproject could be the first step.  Then, the subproject should assemble a fat-jar including giraph and optionally giraph-formats-contrib into it.  The quick start guide should be updated to recommend using the archetype to create a fresh project and run the self-contained fat-jar with either the hadoop or giraph command.

See-Also: [comments on GIRAPH-180|https://issues.apache.org/jira/browse/GIRAPH-180?focusedCommentId=13421864&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13421864]",smartshark_2_2,217,giraph,"""[0.9976050853729248, 0.0023949143942445517]"""
1449,229708,Track and log versions of dependencies,"Diff: https://reviews.apache.org/r/13751/

See http://stackoverflow.com/questions/3697449/retrieve-version-from-maven-pom-xml-in-code for background info.

This diff integrates pom dependency versions into the Java code so that we can track it and log it in the job. Here's an example of what gets logged in PageRankBenchmark: https://gist.github.com/nitay/adf6cefa473290d2fb79",smartshark_2_2,857,giraph,"""[0.9984797835350037, 0.001520259422250092]"""
1450,229709,Javadoc fails build with Java 8,"Java 8 javadoc has stricter checking, which results in mvn javadoc:javadoc failing:

Example:

100 errors
200 warnings
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Giraph Parent ............................... SUCCESS [  1.196 s]
[INFO] Apache Giraph Core ................................. FAILURE [  9.583 s]
[INFO] Apache Giraph Blocks Framework ..................... SKIPPED
[INFO] Apache Giraph Examples ............................. SKIPPED
[INFO] Apache Giraph Accumulo I/O ......................... SKIPPED
[INFO] Apache Giraph HBase I/O ............................ SKIPPED
[INFO] Apache Giraph HCatalog I/O ......................... SKIPPED
[INFO] Apache Giraph Gora I/O ............................. SKIPPED
[INFO] Apache Giraph Rexster I/O .......................... SKIPPED
[INFO] Apache Giraph Rexster Kibble ....................... SKIPPED
[INFO] Apache Giraph Rexster I/O Formats .................. SKIPPED
[INFO] Apache Giraph Distribution ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 11.266 s
[INFO] Finished at: 2016-04-22T16:08:13-07:00
[INFO] Final Memory: 40M/944M",smartshark_2_2,1031,giraph,"""[0.9940974712371826, 0.005902523640543222]"""
1451,229710,Random Walks on Graphs,"Implementing RWR on Giraph should be a very simple modification of the SimplePageRankVertex code.

{code}
if ( myID == sourceID )
      DoubleWritable vertexValue = new DoubleWritable((0.15f + 0.85f * sum);
else
      DoubleWritable vertexValue = new DoubleWritable(0.85f * sum);
{code}

It would be nice to make it as configurable as possible by using parametric damping factors, preference vectors, strongly preferential, etc...
More or less along these lines:
http://law.dsi.unimi.it/software/docs/it/unimi/dsi/law/rank/PageRank.html",smartshark_2_2,278,giraph,"""[0.9983736276626587, 0.0016263856086879969]"""
1452,229711, Make MessageStoreFactory configurable,"As mentioned in the task: 4213037

Related to the Task: https://our.intern.facebook.com/intern/tasks/?s=754080093&t=4213037

Changed the MessageStoreFactory interface and added a new method ""initialize(service, config)"" in order to do the setting up of message store factory. In order to invoke the class (as per the configuration) on run time, reflection is used for the class invocation and then initialize() method is called on the instance.

There were few classes which had private factory implementation, for these classes there is no need to implement the initialize() method as the class is never exposed and can't be passed as a configuration.

Also, in order to make the DiskBackedMessageStore.class value to be passed from the configuration, I moved the implementation to the new file and made this class as a public class. 

Modified a test case for testing this configuration(DiskBased/InMemory Backed Message Stores). 

Review Link: https://reviews.apache.org/r/20915/",smartshark_2_2,808,giraph,"""[0.9985073208808899, 0.001492667361162603]"""
1453,229712,Refactor I/O to be independent of Map/Reduce,The I/O mechanisms should probably be abstracted entirely from Map/Reduce in order to support making Giraph an independent framework.,smartshark_2_2,41,giraph,"""[0.9977407455444336, 0.002259236527606845]"""
1454,229713,add options to control Netty's per-channel receive and send buffer sizes,"Avery added these as part of GIRAPH-262, but I think they merit their own JIRA with a patch extracted from his GIRAPH-262 patch.",smartshark_2_2,240,giraph,"""[0.9984286427497864, 0.0015713074244558811]"""
1455,229714,Change PageRankBenchmark to be accessible via bin/giraph,"Currently the PageRankBenchmark has its own main and tool implementation and is difficult to access from the bin/giraph script.  It would be better if everything were accessible via bin/giraph.  The benchmark is particularly problematic because it uses inner classes for its two actual Vertex implementations, which have to be specified on the command line as their .class name(ie org.apache.giraph.benchmark.PageRankBenchmark$PageRankHashMapVertex) rather than just with dots, as one would expect.",smartshark_2_2,106,giraph,"""[0.998404324054718, 0.0015957331052049994]"""
1456,229715,Case insensitive file/directory name matching will produce errors on M/R jar unpack. ,"This only seems to affect platforms where there can be a file/directory naming conflicts
from case insensitive matches. 
 
I was able to reproduce running the pseudo-distributed unit tests within OSX.

This has affected other projects: 
https://issues.apache.org/jira/browse/MAHOUT-780

I've been able to reproduce this on my local OSX install with the following error:
https://groups.google.com/a/cloudera.org/group/cdh-user/browse_thread/thread/a201218000e956d3/cc6eca3ef9f80ff8

Since LICENSE.txt contains the same content as the file LICENSE, I propose we exclude any LICENSE matches found in the unpacked dependency jars
when the maven assembly phase hits 'jar-with-dependencies'. 

I have a patch which moves the 'jar-with-dependencies' descriptor to an external compile.xml file which has the proper excludes. This might also
come in handy down the road should any additional tweaks be needed to the compile phase. 


",smartshark_2_2,117,giraph,"""[0.9655333757400513, 0.034466590732336044]"""
1457,229716,Refactor worker logic from GraphMapper,"The plumbing around executing vertices is hosted within the mapper, but could be extracted to its own class and executed from the Mapper directly.  This would ease testing and make it easier to host in the new YARN infrastructure.  There's nothing mapper specific about this code.",smartshark_2_2,429,giraph,"""[0.9978723526000977, 0.00212766882032156]"""
1458,229717,More efficient and flexible edge-based input,"The current implementation of edge-based input using mutations is not as memory-efficient as it could be, and also can't be used with immutable vertex classes.
By having an ad-hoc code path for edges sent during input superstep, we can achieve both memory efficiency and eliminate the current restriction.",smartshark_2_2,487,giraph,"""[0.998158872127533, 0.0018410856137052178]"""
1459,229718,Properly size netty buffers when encoding requests,"For some type of requests, it's easy to calculate the exact size of requests, and create the buffers as big as we need.",smartshark_2_2,401,giraph,"""[0.9974508881568909, 0.002549039665609598]"""
1460,229719,Remove support for Hadoop 1 and older,"Due to all the munge logic and support for nearly 10 year old APIs, Giraph feels much harder to maintain than it should be.

The majority of projects in this large scale Hadoop based compute space maintain a much shorter compatibility window. I'm not sure if anyone is still running installations of Hadoop 0.20 but it doesn't seem reasonable to continue to support them if it adds a maintenance burden.

Removing support for, at minimum, Hadoop versions before 1.0 would start to reduce the amount of munging that is needed andÂ likely make it easier toÂ maintain and contribute to the project.

Removing support for Hadoop 1.X versions and support for legacy MapReduce 1 mightÂ remove even more complexity.

If there is concern about users of ancient Hadoop releases who are currently building from trunk and don't have the option of a production release, we could do this following a 1.3.0 release which could be declared as the last release to support Hadoop releases prior to 2.0.

Â ",smartshark_2_2,1195,giraph,"""[0.9982571005821228, 0.0017428409773856401]"""
1461,229720,Fix naming of input superstep counter,"When one of the metrics patches was rebased on GIRAPH-155, the name of the input superstep counter got changed back to ""Vertex input superstep"", which is now outdated (since we have both vertex and edge based input).",smartshark_2_2,394,giraph,"""[0.9914324283599854, 0.008567580953240395]"""
1462,229721,Partitioning outgoing graph data during INPUT_SUPERSTEP by # of vertices results in wide variance in RPC message sizes,"This relates to GIRAPH-247. The unfortunately named ""MAX_VERTICES_PER_PARTITION"" fooled me into thinking this value was regulating the size of initial Partition objects as they were composed during INPUT_SUPERSTEP from InputSplits each worker reads.

In fact this configuration option only regulates the size of the outgoing RPC messages, stored locally in Partition objects but decomposed into Collections of BasicVertex for transfer to their eventual homes on another (or this) worker. There they are combined into the actual Partitions they will exist in for the job run.

By partitioning these outgoing messages by # of vertices, metrics load tests have shown the size of the average message is not well regulated and can create overloads on either side of these transfers. This is important because:

1. Throughput and memory are at a premium during INPUT_SUPERSTEP.
2. Only one crashed worker in a Giraph job causes cascading job failure, even in an otherwise healthy workflow.

This JIRA renames the offending variables/config options and further regulates outgoing graph data in INPUT_SUPERSTEP by the # of edges and THEN the # of vertices in a candidate for transfer. This much more effectively regulates message size for typical social graph data and has been show in testing to greatly improve the amount of load-in data Giraph can handle without failure given fixed memory and worker limits.
",smartshark_2_2,248,giraph,"""[0.4569113850593567, 0.5430886149406433]"""
1463,229722,LocalTestMode's zookeeper directory is not being cleaned up after job runs,"Discovered bug while running PageRankBenchmark in localTestMode, with -Phadoop_1.0, with the following giraph-site.xml:

{code}
<configuration>
  <property>
    <name>giraph.SplitMasterWorker</name>
    <value>false</value>
  </property>
  <property>
    <name>giraph.localTestMode</name>
    <value>true</value>
  </property>
  <property>
    <name>giraph.zkJar</name>
    <value>/home/eugene/giraph/target/giraph-0.2-SNAPSHOT-jar-with-dependencies.jar</value>
  </property>
</configuration>
{code}

With this configuration, I ran PageRankBenchmark as follows:

{code}
java -cp (all the jars..) org.apache.giraph.benchmark.PageRankBenchmark -c 0 -e 3 -s 5 -v -w 1 -V 10
{code}

This worked the first time:

{code}
12/06/18 15:33:51 INFO mapred.JobClient: Job complete: job_local_0001
12/06/18 15:33:51 INFO mapred.JobClient: Counters: 31
12/06/18 15:33:51 INFO mapred.JobClient:   Giraph Timers
12/06/18 15:33:51 INFO mapred.JobClient:     Total (milliseconds)=5361
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 3 (milliseconds)=305
12/06/18 15:33:51 INFO mapred.JobClient:     Vertex input superstep (milliseconds)=207
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 4 (milliseconds)=317
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 10 (milliseconds)=297
12/06/18 15:33:51 INFO mapred.JobClient:     Setup (milliseconds)=459
12/06/18 15:33:51 INFO mapred.JobClient:     Shutdown (milliseconds)=875
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 7 (milliseconds)=305
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 0 (milliseconds)=553
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 8 (milliseconds)=304
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 9 (milliseconds)=306
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 6 (milliseconds)=339
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 5 (milliseconds)=268
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 2 (milliseconds)=313
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep 1 (milliseconds)=503
12/06/18 15:33:51 INFO mapred.JobClient:   File Output Format Counters 
12/06/18 15:33:51 INFO mapred.JobClient:     Bytes Written=0
12/06/18 15:33:51 INFO mapred.JobClient:   Giraph Stats
12/06/18 15:33:51 INFO mapred.JobClient:     Aggregate edges=100
12/06/18 15:33:51 INFO mapred.JobClient:     Superstep=11
12/06/18 15:33:51 INFO mapred.JobClient:     Current workers=1
12/06/18 15:33:51 INFO mapred.JobClient:     Last checkpointed superstep=0
12/06/18 15:33:51 INFO mapred.JobClient:     Current master task partition=0
12/06/18 15:33:51 INFO mapred.JobClient:     Sent messages=0
12/06/18 15:33:51 INFO mapred.JobClient:     Aggregate finished vertices=10
12/06/18 15:33:51 INFO mapred.JobClient:     Aggregate vertices=10
12/06/18 15:33:51 INFO mapred.JobClient:   File Input Format Counters 
12/06/18 15:33:51 INFO mapred.JobClient:     Bytes Read=0
12/06/18 15:33:51 INFO mapred.JobClient:   FileSystemCounters
12/06/18 15:33:51 INFO mapred.JobClient:     FILE_BYTES_READ=88
12/06/18 15:33:51 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=32525
12/06/18 15:33:51 INFO mapred.JobClient:   Map-Reduce Framework
12/06/18 15:33:51 INFO mapred.JobClient:     Map input records=1
12/06/18 15:33:51 INFO mapred.JobClient:     Spilled Records=0
12/06/18 15:33:51 INFO mapred.JobClient:     SPLIT_RAW_BYTES=44
12/06/18 15:33:51 INFO mapred.JobClient:     Map output records=0
{code}




 but trying to run it again yields the following:

{code}
12/06/18 15:35:01 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
12/06/18 15:35:01 WARN mapred.FileOutputCommitter: Output path is null in cleanup
12/06/18 15:35:02 INFO mapred.JobClient:  map 100% reduce 0%
12/06/18 15:35:02 INFO mapred.JobClient: Job complete: job_local_0001
12/06/18 15:35:02 INFO mapred.JobClient: Counters: 8
12/06/18 15:35:02 INFO mapred.JobClient:   File Output Format Counters 
12/06/18 15:35:02 INFO mapred.JobClient:     Bytes Written=0
12/06/18 15:35:02 INFO mapred.JobClient:   File Input Format Counters 
12/06/18 15:35:02 INFO mapred.JobClient:     Bytes Read=0
12/06/18 15:35:02 INFO mapred.JobClient:   FileSystemCounters
12/06/18 15:35:02 INFO mapred.JobClient:     FILE_BYTES_READ=88
12/06/18 15:35:02 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=32493
12/06/18 15:35:02 INFO mapred.JobClient:   Map-Reduce Framework
12/06/18 15:35:02 INFO mapred.JobClient:     Map input records=1
12/06/18 15:35:02 INFO mapred.JobClient:     Spilled Records=0
12/06/18 15:35:02 INFO mapred.JobClient:     SPLIT_RAW_BYTES=44
12/06/18 15:35:02 INFO mapred.JobClient:     Map output records=0
Disconnected from the target VM, address: '127.0.0.1:33268', transport: 'socket'
{code}

which is wrong because the Giraph mapper never got called (note the lack of Superstep timers and ""Giraph Stats"" section in the above)

A workaround for this bug is to run ""rm -rf ~/giraph/_bsp/_defaultZkManagerDir"" before re-running PageRankBenchmark - then it will run correctly afterwards.

The problem in the code is that the ZookeeperManager's directory is not being removed as it should be. This is because the zkDirDefault in ZooKeeperManager.java is not being set correctly - it is currently: 

{code}
System.getProperty(""user.dir"") + ""/_bspZooKeeper"";
{code}

but it should be:

{code}
System.getProperty(""user.dir"") + GiraphJob.ZOOKEEPER_MANAGER_DIR_DEFAULT;
{code}",smartshark_2_2,164,giraph,"""[0.9568809866905212, 0.04311904311180115]"""
1464,229723,Create documentation for the projects integration,This projects integration should have the necessary documentation for users to be able to use and try this new feature.,smartshark_2_2,675,giraph,"""[0.9972348809242249, 0.0027651384007185698]"""
1465,229724,Remove final modifier from SimpleHiveToEdge.initializeRecords,We might want to modify the record iterator in implementations.,smartshark_2_2,849,giraph,"""[0.9984011054039001, 0.0015988483792170882]"""
1466,229725,Fix connection retry logic,"Currently when we fail to connect to a channel we retry immediately and that retry most often fails. Add a short wait between retries, and improve the check for whether the channel connected successfully.",smartshark_2_2,1070,giraph,"""[0.9656633138656616, 0.034336723387241364]"""
1467,229726,add support for EdgeOutputFormat also for giraph-hive,should be able to write edges through EdgeOutputFormat to Hive tables.,smartshark_2_2,764,giraph,"""[0.9982830286026001, 0.0017170077189803123]"""
1468,229727,Provide input format for reading graphs stored as adjacency lists,"We should provide as many options for getting data in/out of Giraph as possible.  One common format is adjacency lists, with each edge id, its value, and its edges and their values per line.  It would be good to provide inputformats to handle this type of data with minimal effort from the user.",smartshark_2_2,14,giraph,"""[0.9982478618621826, 0.0017521673580631614]"""
1469,229728,Text Vertex Input/Output Format base classes overhaul,"The current way of implementing {{VertexInputFormat}} and {{VertexReader}} had bad smell.  It required users to understand how these two classes are glued together, and forced similar codes to be duplicated in every new input format.  (Similarly for the VertexOutputFormat and VertexWriter.)  Anyone who wants to create a new format should create an underlying record reader or writer at the right moment and delegate some calls to it, which seemed unnecessary detail being exposed.  Besides, type parameters had to appear all over every new format code, which was extremely annoying for both reading existing code and writing a new one.  I was very frustrated writing my first format code especially when I compared it to writing a new vertex code.  I thought writing a new input/output format should be as simple as vertex.

So, I have refactored {{TextVertexInputFormat}} and {{OutputFormat}} into new forms that have no difference in their interfaces, but remove a lot of burden for subclassing.  Instead of providing static VertexReader base classes, I made it a non-static inner-class of its format class, which helps eliminate the repeated code for gluing these two, already tightly coupled classes.  This has additional advantage of eliminating all the Generics type variables on the VertexReader side, which makes overall code much more concise.  I added several useful TextVertexReader base classes that can save efforts for implementing line-oriented formats.

Please comment if you see my proposed change have any impact on other aspects.  I'm unsure of how these additional layers of abstraction could affect performance.",smartshark_2_2,284,giraph,"""[0.9982243180274963, 0.00177567801438272]"""
1470,229729,PageRankBenchmark User Problem - failed to report status for 600 seconds. Killing!,"Hi : )

I'm newbie to Giraph. I installed Giraph by following its ReadMe instructions ( no Zookeeper installation explicitly, is it packed with Giraph?). 
However, I got problem while running the PageRankBenchmark( -w 2 ), only one worker is running  and the following message shown:
""failed to report status for 600 seconds. Killing!""
Hadoop map task list:
Master_Zookeeper_Only -0 finished 1 0n superstep -1 | complete 100%
setup: connected to Zookeeper service | complete 0% 

It has been run on single machine.

Thanks in advance!",smartshark_2_2,147,giraph,"""[0.8283695578575134, 0.17163041234016418]"""
1471,229730,Metrics Update,"Updating metrics to be more useful.

https://reviews.apache.org/r/7900/",smartshark_2_2,372,giraph,"""[0.9982119798660278, 0.0017879996448755264]"""
1472,229731,Generate primitive type specific code,"It is painful to manually write and maintain type specific code, it can all be generated. This allows us to easily write more powerful things in the future.",smartshark_2_2,980,giraph,"""[0.9982068538665771, 0.0017931038746610284]"""
1473,229732,"In PageRankBenchmark, remove unneeded handling of -t 2","PagerankBenchmark accepts, among other options, -t. For the usage message, it's treated as:

t=0 (no combiner)
t=1 (DoubleSumCombiner (default))

However the code mentions a t=2, but this ends up being treated the same as t=1:

{code}
if (!cmd.hasOption('t') ||
  (Integer.parseInt(cmd.getOptionValue('t')) == 2)) {
     configuration.setVertexCombinerClass(
          DoubleSumCombiner.class);
    } else if (Integer.parseInt(cmd.getOptionValue('t')) == 1) {
     configuration.setVertexCombinerClass(
          DoubleSumCombiner.class);
    }
{code}

We should make the code usage conform to the usage message and remove the extraneous -t=2 handling.",smartshark_2_2,461,giraph,"""[0.9969561100006104, 0.003043950302526355]"""
1474,229733,Create documentation for the projects integration,This projects integration should have the necessary documentation for users to be able to use and try this new feature.,smartshark_2_2,672,giraph,"""[0.9972348809242249, 0.0027651384007185698]"""
1475,229734,ListGenericArray's hashCode causes StackOverflowError,"  public static void main(String[] args) throws Exception {
    Type type = STRING;
    Schema schema = Schema.create(type);
    GenericArray array = new ListGenericArray(schema);
    array.add(new Utf8(""array test""));
    System.out.println(""hashCode="" + array.hashCode());
  }

Exception in thread ""main"" java.lang.StackOverflowError
	at org.apache.avro.generic.GenericData.get(GenericData.java:39)
	at org.apache.gora.persistency.ListGenericArray.hashCode(ListGenericArray.java:86)
	at org.apache.avro.generic.GenericData.hashCode(GenericData.java:434)
	at org.apache.gora.persistency.ListGenericArray.hashCode(ListGenericArray.java:86)
	at org.apache.avro.generic.GenericData.hashCode(GenericData.java:434)
	at org.apache.gora.persistency.ListGenericArray.hashCode(ListGenericArray.java:86)
	...
",smartshark_2_2,73,gora,"""[0.06440870463848114, 0.9355912804603577]"""
1476,229735,Properly escaping spaces of GORA_HOME in bin/gora,"When executing bin/gora in a folder that contains space(s) it fails with the following exception:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/avro/Schema
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2451)
	at java.lang.Class.getMethod0(Class.java:2694)
	at java.lang.Class.getMethod(Class.java:1622)
	at sun.launcher.LauncherHelper.getMainMethod(LauncherHelper.java:494)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:486)
Caused by: java.lang.ClassNotFoundException: org.apache.avro.Schema
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	... 6 more

The problem is the mishandling of spaces when building the CLASSPATH in bin/gora.


",smartshark_2_2,228,gora,"""[0.2710476815700531, 0.7289522886276245]"""
1477,229736,"GoraCompiler needs to add ""import FixedSize"" statement for FIXED type","Since GoraCompiler does not write the following import statement:
import org.apache.avro.specific.FixedSize;
for FIXED type like:
    { ""name"": ""md5"",       ""type"": { ""type"" : ""fixed"", ""size"" : 16, ""name"" : ""MD5"" } },
javac cannot compile the FIXED type class generated by GoraCompiler like:

src/MD5.java:24: cannot find symbol
symbol: class FixedSize
@FixedSize(16)
 ^

GoraCompiler should write:
import org.apache.avro.specific.FixedSize;
",smartshark_2_2,76,gora,"""[0.7493361234664917, 0.2506638467311859]"""
1478,229737,CassandraStore's schemaExists() method always returns false,"CassandraStore's schemaExisst() method always returns false.

  public boolean schemaExists() throws IOException {
    LOG.info(""schema exists"");
    return false;
  }

It makes gora-cassandra testSchemaExists fail.
https://issues.apache.org/jira/browse/GORA-53?focusedCommentId=13414694#comment-13414694

testSchemaExists(org.apache.gora.cassandra.store.TestCassandraStore) Time elapsed: 0.883 sec <<< FAILURE! 
junit.framework.AssertionFailedError 
        at junit.framework.Assert.fail(Assert.java:48) 
        at junit.framework.Assert.assertTrue(Assert.java:20) 
        at junit.framework.Assert.assertTrue(Assert.java:27) 
        at org.apache.gora.store.DataStoreTestUtil.testSchemaExists(DataStoreTestUtil.java:131) 
        at org.apache.gora.store.DataStoreTestBase.testSchemaExists(DataStoreTestBase.java:164)
",smartshark_2_2,117,gora,"""[0.10410025715827942, 0.8958997130393982]"""
1479,229738,AvroUtils.deepClonePersistent needs to flush BinaryEncoder,"Calling AvroUtils.deepClonePersistent on a smallish object (e.g. an unfetched Nutch WebPage) results in the following exception:

{code:java}
java.lang.RuntimeException: Unable to deserialize avro object from byte buffer - please report this issue to the Gora bugtracker or your administrator.
        at org.apache.gora.util.AvroUtils.deepClonePersistent(AvroUtils.java:125)
{code}

The method clones its argument by writing it to a wrapped byte array and deserializing it from there. However, the output is buffered in the BinaryEncoder and doesn't appear in the OutputStream. Adding {{enc.flush()}} after {{writer.write}} seems to fix the bug.

The attached patch contains a test that demonstrates the bug.",smartshark_2_2,273,gora,"""[0.09614986181259155, 0.9038501381874084]"""
1480,229739,"delete() method is not implemented at CassandraStore, and always returns false or 0","CassandraStore.java has the followings:

  @Override
  public boolean delete(K key) throws IOException {
    LOG.debug(""delete "" + key);
    return false;
  }

  @Override
  public long deleteByQuery(Query<K, T> query) throws IOException {
    LOG.debug(""delete by query "" + query);
    return 0;
  }

It causes junit.framework.AssertionFailedError at gora-cassandra test.

testDelete(org.apache.gora.cassandra.store.TestCassandraStore)  Time elapsed: 0.656 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:48)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertTrue(Assert.java:27)
        at org.apache.gora.store.DataStoreTestUtil.testDelete(DataStoreTestUtil.java:580)
        at org.apache.gora.store.DataStoreTestBase.testDelete(DataStoreTestBase.java:349)
",smartshark_2_2,196,gora,"""[0.17664344608783722, 0.823356568813324]"""
1481,229740,Verify storage and retrieval of Avro Union data type within Gora-HBase,The necessary features should be added to confirm that we are able to support Avro Union data types,smartshark_2_2,71,gora,"""[0.9978390336036682, 0.0021609372925013304]"""
1482,229741,Address flagged Sonar error in Gora package design,"Right now the sonar:sonar job is configured on our trunk build to execute the following target

sonar:sonar -Dsonar.skipPackageDesign=true, this prevents the following error. We need to address this.
BTW, to reproduce this error, you can download sonarqube and run the Sonar analysis from here
http://www.sonarqube.org/downloads/


[ERROR] Failed to execute goal
org.codehaus.sonar:sonar-maven-plugin:4.3:sonar (default-cli) on project
imap-core: Directory contains files belonging to different packages :
/path/to/some/project/sources/package Please fix your source code or use
sonar.skipPackageDesign=true to continue the analysis. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to
execute goal org.codehaus.sonar:sonar-maven-plugin:4.3:sonar
(default-cli) on project imap-core: Directory contains files belonging
to different packages : /path/to/some/project/sources/package Please fix
your source code or use sonar.skipPackageDesign=true to continue the
analysis.
         at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
         at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
         at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
         at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:108)
         at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:76)
         at
org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
         at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:116)
         at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:361)
         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
         at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
         at
sun.reflect.DelegatingMethodAccessorImpl.invoke(Del",smartshark_2_2,335,gora,"""[0.9903383851051331, 0.009661645628511906]"""
1483,229742,Hazelcast Jet execution engine support,"Now Apache Gora supports Map Reduce and Spark engines.Â May we should extend this with Hazelcast Jet. [1][2] Feature should include writing custom source and sink connectors [3]Â which can write / read data from / to Apache Gora data stores.

[1] [https://jet.hazelcast.org/use-cases/fast-batch-processing/]

[2] [https://github.com/hazelcast/hazelcast-jet]

[3]Â https://jet.hazelcast.org/connectors/batch-connectors/",smartshark_2_2,581,gora,"""[0.9983910322189331, 0.0016090308781713247]"""
1484,229743,Define <scope> for * dependencies in pom.xml ,This is a no-brainer. It saves us from stumbling in to class loading issues as well.,smartshark_2_2,369,gora,"""[0.998383641242981, 0.0016163559630513191]"""
1485,229744,Complete Gora-Cassandra site documentation,Issue created to track progress on each of the elements noted.,smartshark_2_2,87,gora,"""[0.998427152633667, 0.0015728409634903073]"""
1486,229745,TestContainers should be moved to test scope,"TestContainers is used for CouchDB testing. It is added with https://github.com/apache/gora/pull/74. TestContainers dependecy should be moved to ""test"" scope.",smartshark_2_2,416,gora,"""[0.9983358979225159, 0.0016641119727864861]"""
1487,229746,Ensure that Gora adheres to ASF branding requirements,As per http://apache.org/foundation/marks/pmcs.html we need to ensure that Gora is branding compliant.,smartshark_2_2,8,gora,"""[0.9982442855834961, 0.0017557439859956503]"""
1488,229747,Where does Gora fit in YARN?,"Question from Lewis:

{quote}
Hi Folks,

Based on this diagram of YARN overview and where 'everything' plugs
together.

!http://tm.durusau.net/wp-content/uploads/2013/06/YARN2.png|width=600!

I have a question... where does Gora fit in here? I have arguments in my
head for many different places where Gora fits in. But the purpose of this
thread is to try and discover from you guys, where you think Gota fits in
for what you are doing (that is of course is your architecture looks
anything like the picture I've posted).

I hope that this thread can be a point of discussion as well as a potential
opportunity to define apotential roadmap for Gora post 0.5 release (which I
would like to push very soon).

Thanks
Lewis
{quote}

You can [read it at mailing list archive|https://mail-archives.apache.org/mod_mbox/gora-dev/201408.mbox/%3CCAGaRif3k9Bkc%2B3QFo6O9Xkr2XN_RHndK-FyfpFmP%3D%2BOCvmA9-A%40mail.gmail.com%3E].

Issue for discussion ideas.",smartshark_2_2,525,gora,"""[0.9981691837310791, 0.0018308548023924232]"""
1489,229748,Using IOUtils methods instead of duplicate code lines,"You can use
{code:borderStyle=solid} 
final SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
          final byte[] byteData = IOUtils.serialize(writer,o);
          m.put(col.getFirst(), col.getSecond(), new Value(byteData));
          count++;
          break;
{code}
instead of 

{code:title=AccumuloStore.java|borderStyle=solid}
case RECORD:
          SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
          ByteArrayOutputStream os = new ByteArrayOutputStream();
          org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
          writer.write(o, encoder);
          encoder.flush();
          m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
          count++;
          break;
{code}

The code lines have already been in org.apache.gora.util.IOUtils",smartshark_2_2,384,gora,"""[0.997822642326355, 0.002177357906475663]"""
1490,229749,Uses ByteBufferSerializer for column value to support various data types rather than StringSerializer,"When I made a patch to GORA-131, I noticed that gora-cassandra uses StringSerializer for column value.
However, since it needs to support various value types, I think ByteBuffer is better than String to/from Hector,
and gora-cassandra layer should convert values between Gora data bean value type and ByteBuffer.",smartshark_2_2,34,gora,"""[0.9667706489562988, 0.033229317516088486]"""
1491,229750,Implement a JSON/Java-to-XML mapping tool to auto-generate gora-*-mapping.xml files,"The idea here is to lower the barrier for entry for folks wishing to get up and running with Gora.
Currently when pulling Gora Maven artifacts, the confiuguration process is 'usually' as follows
* write your gora.properties file
* write your .avsc which effectively determines your data and persistence model and storage characteristics
* generate your data store specific gora-${insert_data_store_here}-mapping.xml file

I think it would be real nice if we could provide a data modelling tool which effectively can run within the GoraCompiler phase of the data modelling and persistent code generation phase of the Gora lifecycle.
Effectively what this would require is some tool which would compliment the GoraCompiler and would auto generate a template for your mapping file with default attributes, values, etc.

I think (and hope) that such a tool would enable us to lower the barrier further for new folk wish to use Gora within their pipeline to persist data somewhere.

On my travels, I came across Xstream [0], which (AFAIK) reads JSON and produces XML. Maybe this could be utilized to achieve the above.  

[0] http://xstream.codehaus.org/",smartshark_2_2,455,gora,"""[0.9978874325752258, 0.002112605143338442]"""
1492,229751,Using Docker For Unit Testing,"I have implemented [couchdb datastore|https://issues.apache.org/jira/browse/GORA-437] on [my repo|https://github.com/cguzel/gora-couchdb] . I want to add test class. So, I must start couchdb server programmatically. But couchdb doesn't support that to start couchdb server programmatically using java. Situations like it may occur in the other datastore implementations. So If docker is used, the datastore starts for testing.

[TestContainers|https://github.com/testcontainers/testcontainers-java] is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases that can run in a Docker container.

So, GORA should support [Test Containers|https://github.com/testcontainers/testcontainers-java] 
There is a docker container of couchdb [here|https://hub.docker.com/r/klaemo/couchdb/]



",smartshark_2_2,576,gora,"""[0.9960585832595825, 0.003941379487514496]"""
1493,229752,Complete Gora-SQL site documentation,Issue filed to address each of the elements ,smartshark_2_2,191,gora,"""[0.9984318614006042, 0.0015681725926697254]"""
1494,229753,Minor document changes for clarity in tutorial,"Hi all,

Was poking through the tutorial and found a few minor typos/clarification opportunities. If I find more as I keep going I'll update the attached diff.

Cheers",smartshark_2_2,252,gora,"""[0.9983556866645813, 0.0016443446511402726]"""
1495,229754,Gora Needs a New Website,We should organize the content and also should have a better UI.,smartshark_2_2,574,gora,"""[0.9971575736999512, 0.0028424053452908993]"""
1496,229755,Make generated data beans more java doc friendly,"Generating code through our Gora Compiler is in pretty good shape but our generated data beans are not so good looking as they don't have any java doc in them. As we are already generating the whole code, I think we could add some java doc to make them look better.",smartshark_2_2,219,gora,"""[0.9981496334075928, 0.0018503987230360508]"""
1497,229756,Run bin/gora commands after maven build,"As we are shifting the full build etc process to Maven, it is essential we can run processes from the terminal.
The forthcoming patch adds some config to the pom.xml which enables us to do this.",smartshark_2_2,148,gora,"""[0.9983457326889038, 0.0016542563680559397]"""
1498,229757,Log error trace as well as error message in GoraRecordWriter,"Right now I am logging a rather annoying error when attempting to flush Super Columns to Cassandra 2.0.7 which reads as follows

2014-09-26 20:43:15,847 WARN  mapreduce.GoraRecordWriter - Exception at GoraRecordWriter.class while closing datastore.InvalidRequestException(why:supercolumn parameter is not optional for super CF sc)

Yes, this is useful, however it would be better (for debugging purposes) if I could catch an entire stack trace as well within the log output.

Patch coming up for GoraRecordWriter which does just this.",smartshark_2_2,312,gora,"""[0.9958602786064148, 0.00413978286087513]"""
1499,229758,Reboot Gora Sonar job on analysis.apache.org,"I've let the Sonar job analysis of our codebase slip into the deep sark depths of hell.
It is my intention to bring it back to the light so we can do high level analysis of the codebase in an attempt to improve it.",smartshark_2_2,282,gora,"""[0.9983401298522949, 0.0016598515212535858]"""
1500,229759,javadoc:jar build error,"Getting failure on master branch :
{code} mvn javadoc:jar {code} 
{code}
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 20.902s
[INFO] Finished at: Tue Aug 30 02:19:49 EEST 2016
[INFO] Final Memory: 40M/489M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8:jar (default-cli) on project gora-dynamodb: MavenReportException: Error while creating archive:
...
[ERROR] ^
[ERROR] /home/cihat/Projects/github/gora/gora-dynamodb/src/main/java/org/apache/gora/dynamodb/compiler/GoraDynamoDBCompiler.java:49: error: reference not found
[ERROR] * Different from the {@link org.apache.gora.compiler.GoraCompiler},
...
{code}",smartshark_2_2,397,gora,"""[0.965256929397583, 0.03474303334951401]"""
1501,229760,Make sure gora-core test dependency is always generated when packaging.,The trivial addition of maven jar plugin testing goal will ensure that the test dependency is always produced for gora-core. This is important.,smartshark_2_2,167,gora,"""[0.9976930022239685, 0.0023069484159350395]"""
1502,229761,Gora should make use of QueryBase.getLimit() in setting scanner.cache in creating scans,"  public ResultScanner createScanner(Query<K, T> query)
  throws IOException {
    final Scan scan = new Scan();
+  scan.setCaching(query.getLimit())
    if (query.getStartKey() != null) {
      scan.setStartRow(toBytes(query.getStartKey()));
    }
    if (query.getEndKey() != null) {
      scan.setStopRow(toBytes(query.getEndKey()));
    }
    addFields(scan, query);

    return table.getScanner(scan);
  }",smartshark_2_2,470,gora,"""[0.8608173131942749, 0.1391826868057251]"""
1503,229762,Make Cassandra keyspace consistency configurable within gora.properties,"Current in CassandraClient#checkKeyspace() consistency is hard coded such that consistency level is .ONE which permits consistency to wait until one replica has responded. This could be improved to enable users to specify other consistency profiles e.g. 


        ANY: Wait until some replica has responded.
        ONE: Wait until one replica has responded.
        TWO: Wait until two replicas have responded.
        THREE: Wait until three replicas have responded.
        LOCAL_QUORUM: Wait for quorum on the datacenter the connection was stablished.
        EACH_QUORUM: Wait for quorum on each datacenter.
        QUORUM: Wait for a quorum of replicas (no matter which datacenter).
        ALL: Blocks for all the replicas before returning to the client.

Configuration should be made available through gora.properties",smartshark_2_2,277,gora,"""[0.998498797416687, 0.0015012179501354694]"""
1504,229763,Upgrade to Accumulo 1.8.0,When Accumulo 1.8.0 is released it will have tablet location code in the public API.  Currently Gora leverages tablet location code in earlier versions of Accumulo thats not in the public API.  Using Accumulo's public API to resolve tablet locations will make Gora more stable against Accumulo versions going forward.,smartshark_2_2,412,gora,"""[0.9982898831367493, 0.0017100524855777621]"""
1505,229764,Add support for Solr 5.x,Add Store implementation that supports SOLR 5.x,smartshark_2_2,376,gora,"""[0.9976212382316589, 0.0023787016980350018]"""
1506,229765,Add support for MongoDB to GoraExplorer,"After merging GoraExplorer interfaces and methods for the core, MongoDB module has to be updated with the following:

- Allow to load mappings from properties instead only from files. [Example at HBaseStore|https://github.com/apache/gora/blob/6ce47dcdea60e4b0d3abe03ddac01a65716cc173/gora-hbase/src/main/java/org/apache/gora/hbase/store/HBaseStore.java#L145] (where the static final constant much probably will have to be refactorized).
- Implement the retrieval of native information from MongoDB. ([Technical details|https://translate.google.es/translate?sl=es&tl=en&js=y&prev=_t&hl=es&ie=UTF-8&u=http%3A%2F%2Fredmine.nishilua.com%2Fprojects%2Fgoraexplorer%2Fwiki%2FObtenci%25C3%25B3n_de_metadatos_de_las_BBDD_nativas&edit-text=])

A plus would be implement the support in [GoraExplorer|https://www.goraexplorer.com] so it will be possible to manage MongoDB data visually.",smartshark_2_2,562,gora,"""[0.9985314607620239, 0.0014685223577544093]"""
1507,229766,Add support to access authenticated servers in Aerospike,"Currently, if the Aerospike server has restricted access with authentication needed for access, gora-aerospike module has no support to provide the credentials for the server.",smartshark_2_2,432,gora,"""[0.9984822869300842, 0.001517776632681489]"""
1508,229767,Remove @Deprecated logic for hbase-mapping.xml,"We have some useless checking and log statements in HBaseStore which relate to a very old hbase-mapping.xml.
These should just be removed.
{code}
  @Deprecated
  private static final String DEPRECATED_MAPPING_FILE = ""hbase-mapping.xml"";
...
      try {
        mapping = readMapping(getConf().get(PARSE_MAPPING_FILE_KEY, DEPRECATED_MAPPING_FILE));
        LOG.warn(""{} is deprecated, please rename the file to {}"", DEPRECATED_MAPPING_FILE, DEFAULT_MAPPING_FILE);
      } catch (FileNotFoundException ex1) {
        LOG.error(ex1.getMessage(), ex1);
        //throw (ex1); //throw the original exception
      } catch (Exception ex1) {
        LOG.warn(""{} is deprecated, please rename the file to {}"", DEPRECATED_MAPPING_FILE, DEFAULT_MAPPING_FILE);
        throw new RuntimeException(ex1);
      } 
{code}
etc",smartshark_2_2,377,gora,"""[0.9984973669052124, 0.0015026896726340055]"""
1509,229768,Docker File for Gora Continues Integration,I have some testing problem same as GORA-367. I can fix it. Because of that I decide creating a gora docker for testing. It fetch gora master from git. If you have development patch it can apply and start tests. Maybe we can use for CI. Docher Hub listen github commit. If repo have new commit it will start building new image.,smartshark_2_2,454,gora,"""[0.9979197382926941, 0.0020802428480237722]"""
1510,229769,Merge goraci testing suite with master branch,"Submit goraci as a contrib to gora.

https://github.com/keith-turner/goraci",smartshark_2_2,63,gora,"""[0.9983701109886169, 0.0016299535054713488]"""
1511,229770,Support creation of dynamic columns within Gora datastore mapping designs,"The conversation taking place on [dynamically generating HBase columns|http://www.mail-archive.com/dev%40gora.apache.org/msg05754.html] has raised an issue that new functionality needs to be added in order to achieve this.
The main driver for this issue coming to light is that Chukwa logs need to dynamically create many many columns over time directly dependent on the number of data chunks we get. Each data chunk has a [Sequence ID], this sequenceID should be the column name.

The table design will look like this

{code}

Row Key: [Invert Date]:[Data Type]:[Primary Key]
Column Family: log
Column Name: [Sequence ID]
Timestamp: [log entry timestamp]

Example:

Row Key: 2132013102:TT:host1.example.com
Column Family: log
Column Name: 1230
Cell Value: 2013-01-23 12:01:30 INFO This is a log entry.
Timestamp: 1358942490
{code}

The inverted date allow the table to be partitioned by hour or day of the month or month more easily.
The usage of column name for consecutive sequence to allow fast retrieval in a linear scan. This format is typically good for retrieve a hour worth of logs fast for a node. Hence, if we are doing batch scanning of the table in a rolling window via map reduce job at every hour interval, we get a even spread the work load to multiple map reduce tasks.",smartshark_2_2,469,gora,"""[0.9984422326087952, 0.0015577488811686635]"""
1512,229771,Dedup CassandraMapping and CassandraMappingManager,"We have a pile of what looks lie deduplication between these two classes.
We should make a determination of what is required and then document it within the appropriate class.
This will enable easy navigation of keyspace definition etc. from within gora-cassandra. ",smartshark_2_2,446,gora,"""[0.9983592629432678, 0.001640781294554472]"""
1513,229772,Python binding for Gora,"[~mjoyce] and myself were talking about bindings for Gora.
This issue is extremely self explanatory.
We came across http://py4j.sourceforge.net/ as a possible candidate for implementing Python wrapper for Gora.",smartshark_2_2,464,gora,"""[0.9983415603637695, 0.0016583785181865096]"""
1514,229773,Show better errors when a field is missing in HBase mapping,"When a field is wrong typed or missing in gora-hbase-mapping.xml, a NullPointerException is raised in org.apache.gora.hbase.store.HBaseStore:235
Just control this to know which field is missing/wrong.",smartshark_2_2,130,gora,"""[0.9817160964012146, 0.018283933401107788]"""
1515,229774,TestIOUtils to print objects with native data values as oppose to Bytes,"Right now TestIOUtils returns the following (horrible) logging when tests are run against it.

346 [main] INFO org.apache.gora.util.TestIOUtils - After : org.apache.gora.util.TestIOUtils$StringArrayWrapper@ff94b1
347 [main] INFO org.apache.gora.util.TestIOUtils - Before: org.apache.gora.util.TestIOUtils$StringArrayWrapper@17b0998
347 [main] INFO org.apache.gora.util.TestIOUtils - After : org.apache.gora.util.TestIOUtils$StringArrayWrapper@b30913
348 [main] INFO org.apache.gora.util.TestIOUtils - Before: org.apache.gora.util.TestIOUtils$StringArrayWrapper@18f51f
348 [main] INFO org.apache.gora.util.TestIOUtils - After : org.apache.gora.util.TestIOUtils$StringArrayWrapper@84ce7a

This is useless... we ought to log objects with their native data values.

https://svn.apache.org/repos/asf/gora/trunk/gora-core/src/test/java/org/apache/gora/util/TestIOUtils.java",smartshark_2_2,266,gora,"""[0.9848228693008423, 0.015177142806351185]"""
1516,229775,Make LogManager tutorial easier to consume,"Right now it is extremely time consuming to get through the LogManager tutorial.
Although it is an excellent tutorial it is lengthy... in f2f discussions, I've heard people saying that it is too complex.
We can discuss whether we want to create an easier tutorial or alternatively simplify LogManager.",smartshark_2_2,457,gora,"""[0.9974249601364136, 0.0025750212371349335]"""
1517,229776,Organize source code for Apache,"Now that the initial import is complete, we need to organize the source code for the apache namespace and conventions. 

Things to do: 
- move package names to org.apache.gora 
- add license headers
- Edit Readme 
",smartshark_2_2,101,gora,"""[0.9967339038848877, 0.0032661359291523695]"""
1518,229777,Complete Gora-HBase site documentation,Issue created to track progress on each of the elements noted.,smartshark_2_2,188,gora,"""[0.9984239339828491, 0.0015760994283482432]"""
1519,229778,Make Solrj solr server impl configurable from within gora.properties,"From Solrj [0], I don't always wish to use HttpSolrServer [1] to map my data down into Solr. Right now I need to use CloudSolrServer [2]. There are several other alternatives as well which should be exposed via gora.properties.

[0] http://lucene.apache.org/solr/4_3_1/solr-solrj/index.html?org/apache/solr/client/solrj/impl/package-summary.html
[1] http://lucene.apache.org/solr/4_3_1/solr-solrj/index.html?org/apache/solr/client/solrj/impl/HttpSolrServer.html
[2] http://lucene.apache.org/solr/4_3_1/solr-solrj/index.html?org/apache/solr/client/solrj/impl/CloudSolrServer.html",smartshark_2_2,245,gora,"""[0.998559296131134, 0.0014407273847609758]"""
1520,229779,Pig Adapter for Gora,"This has been on the agenda from the start and I suppose will be purely use case driven. Persoanlly I haven't touched Pig as I don't have a use-case driven requirement to do so, however there is no harm in this being logged if in the future it attracts interest.  ",smartshark_2_2,236,gora,"""[0.9984089732170105, 0.001591074513271451]"""
1521,229780,Upgrade Apache Avro from 1.8.1-->1.8.2,"A newer version of Avro was released some time ago, Gora should upgrade
https://search.maven.org/artifact/org.apache.avro/avro/1.8.2/bundle",smartshark_2_2,582,gora,"""[0.9984079003334045, 0.0015921139856800437]"""
1522,229781,Remove sqlbuilder library,"We have to remove the library sqlbuilder, which is used by gora-sql, to construct queries from code. The reasons are 
 - It is LGPL
 - We have very little need for this library. ",smartshark_2_2,291,gora,"""[0.9982913136482239, 0.0017087042797356844]"""
1523,229782,UTF-8 chars not correctly rendered if served by JBoss,"I download lastest version of jspwiki and run in tomcat with UTF-8 encoding configured.

but when I entered home page of jspwiki all chars malformed.

my platofom 's encoding is -Dfile.encoding=GBK.

tips.when I set wiki's encoing =iso8859-1 and rerun indexpage also malformed,but when i switch broswer's charset to gbk index page look well.",smartshark_2_2,710,jspwiki,"""[0.08370709419250488, 0.9162928462028503]"""
1524,229783,Ounce Labs Security Finding: DOS - Database Connection Close MisUse Pattern ,"Description: 
The application does not close its database connections properly.  Typical best practices indicate the try/catch/finally pattern, where the close connections are in the finally block.

Recommendation: 
Follow the appropriate database connection close pattern to avoid potential DOS vectors.

Related Code Locations: 
4 findings:
  Name:           com.ecyrd.jspwiki.auth.authorize.JDBCGroupDatabase.initialize(com.ecyrd.jspwiki.WikiEngine;java.util.Properties):void
  Type:           Vulnerability.AppDOS.ConnectionClose
  Severity:       Medium
  Classification: Vulnerability
  File Name:      Z:\jspwiki\JSPWiki_2_4_104\JSPWiki-src\src\com\ecyrd\jspwiki\auth\authorize\JDBCGroupDatabase.java
  Line / Col:     387 / 0
  Context:        conn . java.sql.Connection.close ()
     -----------------------------------
  Name:           com.ecyrd.jspwiki.auth.user.JDBCUserDatabase.initialize(com.ecyrd.jspwiki.WikiEngine;java.util.Properties):void
  Type:           Vulnerability.AppDOS.ConnectionClose
  Severity:       Medium
  Classification: Vulnerability
  File Name:      Z:\jspwiki\JSPWiki_2_4_104\JSPWiki-src\src\com\ecyrd\jspwiki\auth\user\JDBCUserDatabase.java
  Line / Col:     432 / 0
  Context:        conn . java.sql.Connection.close ()
  Notes:	  Description: 
   -----------------------------------
  Name:           com.ecyrd.jspwiki.auth.authorize.JDBCGroupDatabase.initialize(com.ecyrd.jspwiki.WikiEngine;java.util.Properties):void
  Type:           Vulnerability.AppDOS.ConnectionClose
  Severity:       Medium
  Classification: Vulnerability
  File Name:      Z:\jspwiki\JSPWiki_2_4_104\JSPWiki-src\src\com\ecyrd\jspwiki\auth\authorize\JDBCGroupDatabase.java
  Line / Col:     367 / 0
  Context:        conn . java.sql.Connection.close ()
    -----------------------------------
  Name:           com.ecyrd.jspwiki.auth.user.JDBCUserDatabase.initialize(com.ecyrd.jspwiki.WikiEngine;java.util.Properties):void
  Type:           Vulnerability.AppDOS.ConnectionClose
  Severity:       Medium
  Classification: Vulnerability
  File Name:      Z:\jspwiki\JSPWiki_2_4_104\JSPWiki-src\src\com\ecyrd\jspwiki\auth\user\JDBCUserDatabase.java
  Line / Col:     412 / 0
  Context:        conn . java.sql.Connection.close ()
    -----------------------------------
",smartshark_2_2,485,jspwiki,"""[0.12375067919492722, 0.8762493133544922]"""
1525,229784,ShortURLConstructor causes Syntax Error when loading jspwiki-edit.js,"I tried using {{jspwiki.urlConstructor = ShortURLConstructor}} and it took me a quite amount of time to see that this causes a problem:

The editor toolbar is not working anymore, the quick preview under the editing text area eighter.
The web console of firefox shows that there is a syntax error in line 13 in {{jspwiki-edit.js}}.

The line highlighted is:

{{<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"">}}

Using the {{ShortViewURLConstructor}} or the default ""fixes"" the problem.",smartshark_2_2,614,jspwiki,"""[0.11380984634160995, 0.8861901760101318]"""
1526,229785,FCK editor duplicates permalink anchors (#),"When editing with the FCK editor (2.6.2), permalink anchors appended to title text by JSPWiki are duplicated on each edit.

",smartshark_2_2,108,jspwiki,"""[0.12775851786136627, 0.8722414970397949]"""
1527,229786,My Prefs : Editor choice not saved when closing the navigator.,"In User Preferences the Editor choice is not saved, it always returns to plain. ",smartshark_2_2,8,jspwiki,"""[0.2665309011936188, 0.7334691286087036]"""
1528,229787,nested tabbed sections fail to display correctly ,";tabbedSections that are nested in a tab... do not work: Here is an example copy and pasted from http://www.jspwiki.org/wiki/TabbedSections. 

An extra tab was added to the example, and the full tabbedSection was pasted into the new tab.

When a user clicks on one of the tabs the display breaks and the content of the tabs disappear.

{{{
%%tabbedSection

%%tab-LoremIpsum
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Donec dictum velit eget risus. Fusce ligula. Maecenas vitae velit eget odio pulvinar aliquet. Quisque ultricies mollis lorem. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Praesent dapibus odio. Nullam sodales erat vel pede. Morbi ut turpis. Sed sed metus. Donec ut dui. Duis gravida risus non nibh. Aliquam erat volutpat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aliquam dolor urna, tincidunt eget, posuere nec, suscipit quis, arcu. Proin pede risus, placerat id, tincidunt eu, consequat in, sem. Sed eu sapien. Vestibulum turpis. Sed fringilla odio vel eros. Mauris in libero sed sapien mattis pellentesque. Cras aliquet nibh sit amet tortor. Nam nunc.
/%

%%tab-NullamSodales
Nullam sodales erat vel pede. Morbi ut turpis. Sed sed metus. Donec ut dui. Duis gravida risus non nibh. Aliquam erat volutpat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aliquam dolor urna, tincidunt eget, posuere nec, suscipit quis, arcu. Proin pede risus, placerat id, tincidunt eu, consequat in, sem. Sed eu sapien. Vestibulum turpis. Sed fringilla odio vel eros. Mauris in libero sed sapien mattis pellentesque. /%

%%tab-Cras
Cras aliquet nibh sit amet tortor. Nam nunc.
/%

%%tab-NestedTabbedSections
%%tabbedSection

%%tab-LoremIpsum
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Donec dictum velit eget risus. Fusce ligula. Maecenas vitae velit eget odio pulvinar aliquet. Quisque ultricies mollis lorem. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Praesent dapibus odio. Nullam sodales erat vel pede. Morbi ut turpis. Sed sed metus. Donec ut dui. Duis gravida risus non nibh. Aliquam erat volutpat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aliquam dolor urna, tincidunt eget, posuere nec, suscipit quis, arcu. Proin pede risus, placerat id, tincidunt eu, consequat in, sem. Sed eu sapien. Vestibulum turpis. Sed fringilla odio vel eros. Mauris in libero sed sapien mattis pellentesque. Cras aliquet nibh sit amet tortor. Nam nunc.
/%

%%tab-NullamSodales
Nullam sodales erat vel pede. Morbi ut turpis. Sed sed metus. Donec ut dui. Duis gravida risus non nibh. Aliquam erat volutpat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aliquam dolor urna, tincidunt eget, posuere nec, suscipit quis, arcu. Proin pede risus, placerat id, tincidunt eu, consequat in, sem. Sed eu sapien. Vestibulum turpis. Sed fringilla odio vel eros. Mauris in libero sed sapien mattis pellentesque. /%

%%tab-Cras
Cras aliquet nibh sit amet tortor. Nam nunc.
/%
/%

/%
/%
}}}",smartshark_2_2,629,jspwiki,"""[0.07661382853984833, 0.9233862161636353]"""
1529,229788,non-functioning links on search result pagination,"After performing a search, if the results are paginated the page displays a list of page links, e.g., ""1 | 2 | 3 | 4 | Next"" below and above the search results. Hovering over these links the cursor remains as an insertion point and the links cannot be clicked on. This prevents users from viewing anything but the first page of search results.

Notably, Janne has reported that in FF on OSX the browser displays an insertion cursor but the links can be clicked on, so this is cosmetic on FF/OSX but broken on Linux or Windows XP. Not tested on Vista. 

This is likely a Javascript problem with the PlainVanilla template as used on jspwiki.org. Notably, this is not occurring in templates that were derived from PlainVanilla (i.e., the ones I'm using).
",smartshark_2_2,366,jspwiki,"""[0.09804542362689972, 0.9019545912742615]"""
1530,229789,URL in password recovery mail is relative while it should be absolute ,"After entering the password recovery dialog you can get an email sent to you with a new generated password.

This email used to contain a link to the login URL of your wiki, but after JSPWIKI-1035 this URL is relative.

Example
{noformat}
As requested, your new password for login ""EricKrauÃer"" is ""JpwwzA5G""
You may log in at /wiki/Login.jsp.
{noformat}
",smartshark_2_2,911,jspwiki,"""[0.1643395572900772, 0.8356603980064392]"""
1531,229790,(Haddock template) popups dissappear when trying to get your mouse to it,"See attached video.  When hovering the mouse over the ""More"" ""Info"" and Search field, you get a nice popup, but when trying to move your mouse pointer there, it dissappears.",smartshark_2_2,841,jspwiki,"""[0.19876785576343536, 0.8012321591377258]"""
1532,229791,fallback on default template doesn't work,"I've created a custom template which consists of the following files only:

* images/bnvwiki_logo.png
* jspwiki.css

When setting this template as the default template, the following error occurs:

{quote}
FATAL com.ecyrd.jspwiki.ui.TemplateManager BNV-GZ Wiki:/ BNV-GZ Wiki:http://wiki.bnv-gz.de:8080/ - findJSP() was asked to find a null template or name (bnv-gz,null). JSP page '/'
{quote}",smartshark_2_2,383,jspwiki,"""[0.1408461034297943, 0.8591539263725281]"""
1533,229792,VersioningFileProvider allows creation of pages that start with a dot,"By manipulating the JSP fields directly, it's possible to upload a file (e.g. ""..""), which ends up in the page directory under the name ""..-att"".  This does not otherwise affect JSPWiki operation, but it does make that data inaccessible and invisible.

Proposal is to make sure that dots should also be escaped when saving a file.",smartshark_2_2,467,jspwiki,"""[0.14660927653312683, 0.8533906936645508]"""
1534,229793,web.xml seems to not work in Resin 3.0.18,"After modifying jspwiki.properties to use the windows file system, I start resin and the wiki will not start.

At first I get an error because of the Description tag in the web.xml file.  The page displays as follows:

WEB-INF/web.xml:9: <description> is an unexpected tag (parent <web-app>
starts at 6). Check for duplicate and out-of-order tags.
<web-app> syntax: (@id?, @version, @xsi:schemaLocation?,
                   (<context-param>*
                   & (<description>*, <display-name>?, <icon>?)
                   & <distributable>?
                   & (<env-entry>*, <ejb-ref>*, <ejb-local-ref>*, <service-ref>*, <resource-ref>*, <resource-env-ref>*, <message-destination-ref>*)
                   & <error-page>*
                   & <filter>*
                   & <filter-mapping>*
                   & <jsp-config>?
                   & <listener>*
                   & <locale-encoding-mapping-list>?
                   & <login-config>?
                   & <message-destination>*
                   & <mime-mapping>*
                   & <security-constraint>*
                   & <security-role>*
                   & <servlet>*
                   & <servlet-mapping>*
                   & <session-config>?
                   & <welcome-file-list>?))

 
After removing the Description tag, I get the following in the browser:

com.ecyrd.jspwiki.InternalWikiException: No wiki engine, check logs.
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:340)
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:280)
	at com.ecyrd.jspwiki.WikiServlet.init(WikiServlet.java:54)
	at com.caucho.server.dispatch.ServletConfigImpl.createServlet(ServletConfigImpl.java:600)
	at com.caucho.server.dispatch.ServletManager.init(ServletManager.java:154)
	at com.caucho.server.webapp.Application.start(Application.java:1604)
	at com.caucho.server.deploy.DeployController.startImpl(DeployController.java:621)
	at com.caucho.server.deploy.StartAutoRedeployAutoStrategy.startOnInit(StartAutoRedeployAutoStrategy.java:72)
	at com.caucho.server.deploy.DeployController.startOnInit(DeployController.java:509)
	at com.caucho.server.deploy.DeployContainer.start(DeployContainer.java:158)
	at com.caucho.server.webapp.ApplicationContainer.start(ApplicationContainer.java:652)
	at com.caucho.server.host.Host.start(Host.java:385)
	at com.caucho.server.deploy.DeployController.startImpl(DeployController.java:621)
	at com.caucho.server.deploy.StartAutoRedeployAutoStrategy.startOnInit(StartAutoRedeployAutoStrategy.java:72)
	at com.caucho.server.deploy.DeployController.startOnInit(DeployController.java:509)
	at com.caucho.server.deploy.DeployContainer.start(DeployContainer.java:158)
	at com.caucho.server.host.HostContainer.start(HostContainer.java:501)
	at com.caucho.server.resin.ServletServer.start(ServletServer.java:977)
	at com.caucho.server.deploy.DeployController.startImpl(DeployController.java:621)
	at com.caucho.server.deploy.AbstractDeployControllerStrategy.start(AbstractDeployControllerStrategy.java:56)
	at com.caucho.server.deploy.DeployController.start(DeployController.java:517)
	at com.caucho.server.resin.ResinServer.start(ResinServer.java:485)
	at com.caucho.server.resin.Resin.init(Resin.java)
	at com.caucho.server.resin.Resin.main(Resin.java:624)


Log shows this:

2007-12-04 16:01:25,884 [resin-282] INFO com.ecyrd.jspwiki.auth.AuthenticationManager  - JAAS already configured by some other application (leaving it alone...)
2007-12-04 16:01:25,900 [resin-282] INFO com.ecyrd.jspwiki.auth.authorize.WebContainerAuthorizer  - Examining file:/C:/resin-pro-3.0.18/webapps/wiki/WEB-INF/web.xml
2007-12-04 16:01:25,900 [resin-282] ERROR com.ecyrd.jspwiki.auth.authorize.WebContainerAuthorizer  - Malformed XML in web.xml
org.jdom.JDOMException: http://apache.org/xml/features/validation/schema feature not supported for SAX driver com.caucho.xml.Xml
	at org.jdom.input.SAXBuilder.internalSetFeature(SAXBuilder.java:729)
	at org.jdom.input.SAXBuilder.setFeaturesAndProperties(SAXBuilder.java:671)
	at org.jdom.input.SAXBuilder.createParser(SAXBuilder.java:553)
	at org.jdom.input.SAXBuilder.build(SAXBuilder.java:424)
	at org.jdom.input.SAXBuilder.build(SAXBuilder.java:810)
	at com.ecyrd.jspwiki.auth.authorize.WebContainerAuthorizer.getWebXml(WebContainerAuthorizer.java:390)
	at com.ecyrd.jspwiki.auth.authorize.WebContainerAuthorizer.initialize(WebContainerAuthorizer.java:98)
	at com.ecyrd.jspwiki.auth.AuthorizationManager.initialize(AuthorizationManager.java:402)
	at com.ecyrd.jspwiki.WikiEngine.initialize(WikiEngine.java:532)
	at com.ecyrd.jspwiki.WikiEngine.<init>(WikiEngine.java:386)
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:334)
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:280)
	at com.ecyrd.jspwiki.WikiServlet.init(WikiServlet.java:54)
	at com.caucho.server.dispatch.ServletConfigImpl.createServlet(ServletConfigImpl.java:600)
	at com.caucho.server.dispatch.ServletManager.init(ServletManager.java:154)
	at com.caucho.server.webapp.Application.start(Application.java:1604)
	at com.caucho.server.deploy.DeployController.startImpl(DeployController.java:621)
	at com.caucho.server.deploy.DeployController.restartImpl(DeployController.java:584)
	at com.caucho.server.deploy.StartAutoRedeployAutoStrategy.alarm(StartAutoRedeployAutoStrategy.java:176)
	at com.caucho.server.deploy.DeployController.handleAlarm(DeployController.java:742)
	at com.caucho.util.Alarm.handleAlarm(Alarm.java:350)
	at com.caucho.util.Alarm.run(Alarm.java:320)
	at com.caucho.util.ThreadPool.runTasks(ThreadPool.java:507)
	at com.caucho.util.ThreadPool.run(ThreadPool.java:433)
	at java.lang.Thread.run(Thread.java:595)
2007-12-04 16:01:25,900 [resin-282] FATAL com.ecyrd.jspwiki.WikiEngine  - Failed to start managers.
com.ecyrd.jspwiki.InternalWikiException: org.jdom.JDOMException: http://apache.org/xml/features/validation/schema feature not supported for SAX driver com.caucho.xml.Xml
	at com.ecyrd.jspwiki.auth.authorize.WebContainerAuthorizer.initialize(WebContainerAuthorizer.java:125)
	at com.ecyrd.jspwiki.auth.AuthorizationManager.initialize(AuthorizationManager.java:402)
	at com.ecyrd.jspwiki.WikiEngine.initialize(WikiEngine.java:532)
	at com.ecyrd.jspwiki.WikiEngine.<init>(WikiEngine.java:386)
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:334)
	at com.ecyrd.jspwiki.WikiEngine.getInstance(WikiEngine.java:280)
	at com.ecyrd.jspwiki.WikiServlet.init(WikiServlet.java:54)
	at com.caucho.server.dispatch.ServletConfigImpl.createServlet(ServletConfigImpl.java:600)
	at com.caucho.server.dispatch.ServletManager.init(ServletManager.java:154)
	at com.caucho.server.webapp.Application.start(Application.java:1604)
	at com.caucho.server.deploy.DeployController.startImpl(DeployController.java:621)
	at com.caucho.server.deploy.DeployController.restartImpl(DeployController.java:584)
	at com.caucho.server.deploy.StartAutoRedeployAutoStrategy.alarm(StartAutoRedeployAutoStrategy.java:176)
	at com.caucho.server.deploy.DeployController.handleAlarm(DeployController.java:742)
	at com.caucho.util.Alarm.handleAlarm(Alarm.java:350)
	at com.caucho.util.Alarm.run(Alarm.java:320)
	at com.caucho.util.ThreadPool.runTasks(ThreadPool.java:507)
	at com.caucho.util.ThreadPool.run(ThreadPool.java:433)
	at java.lang.Thread.run(Thread.java:595)
2007-12-04 16:01:25,900 [JSPWiki Lucene Indexer] INFO com.ecyrd.jspwiki.search.LuceneSearchProvider  - Files found in Lucene directory, not reindexing.

I suspect something not being compatible in Resin.",smartshark_2_2,482,jspwiki,"""[0.4490927755832672, 0.5509071946144104]"""
1535,229794,Scorebar invalid in google chrome after search,Please see screenshots,smartshark_2_2,631,jspwiki,"""[0.13181206583976746, 0.8681879043579102]"""
1536,229795,"javascript error while viewing pages: relativeTo.getCoordinates is not a function, jspwiki-common.js: line 2969","On each page view, the following js error pops up :

{noformat}
Error: relativeTo.getCoordinates is not a function
Source File: http://localhost:8080/JSPWiki/scripts/jspwiki-common.js
Line: 2969
{noformat}",smartshark_2_2,264,jspwiki,"""[0.10532429069280624, 0.8946756720542908]"""
1537,229796,ProfanityFilter fails with NPE if profanity.properties not found,"ProfanityFilter attempts to load a list of naughty words from com/ecyrd/jspwiki/filters/profanity.properties. This is done through a static class initializer. However, if the file cannot be found, static String[] array c_profanities is never initialized.

The fix is simply to assign the value of c_profanities to String[0] before the static {} block.
",smartshark_2_2,398,jspwiki,"""[0.06673651933670044, 0.9332634806632996]"""
1538,229797,Link to non-existing page doesn't change if linked page is created,"Background:
Links to non-existing pages in the same wiki look differently than links to existing pages. 
Clicking on a link to a non-existing page leads directly to the ""Edit"" View so that the page can easily be created.

Problem: 
After the non-existing page has been created, the Link to that new page is still shown in the same way as a link to a non-existing page; Clicking the link leads to the Edit view

To Reproduce: 
1) On an existing page (""page1"") create a link to a non-existing page (""page2"")
     --> the link to ""page2"" is visible on ""page1"" and correctly shows up as a link to a non-existing page
2) Create ""page2"", and save it
3) Visit again ""page1""
    Expected behavior: Link to page2 shows up as a normal link; clicking the link leads to the page2 in view mode
    Actual behavior: Link to page2 shows up as a link to a non-existing page; clicking the link leads to the Edit mode of page 2",smartshark_2_2,944,jspwiki,"""[0.2013644576072693, 0.7986355423927307]"""
1539,229798,overflow:auto breaks FullRecentChanges,"html > body #previewcontent, html > body #info, html > body #pagecontent, html > body #attach, html > body #findcontent
{
overflow:auto
}

breaks FullRecentChanges. 

The whole table is generated, but only the first part is displayed correctly.
The page is as long as the whole table, but after the first shown part, all table rows are not visible, so the page is white.

Testing with Firebug and disabling the mentioned css shows the full table. But may break other styles...",smartshark_2_2,309,jspwiki,"""[0.12681004405021667, 0.8731899261474609]"""
1540,229799,Container managed authorization does not work in tomcat,"I have just installed JSPWiki v2.10.0 into a Tomcat 7.0.52.

I enabled comtainer managed authorization through
uncommenting the last section in web.xml and added appropriate
users to the tomcat-user.xml.

But logging in did not work.

So I debugged jspwiki (/tags/jspwiki_2_10_0) and found,
that WebContainerAuthorizer.java contains these lines starting from line 105

      // Add the J2EE 2.4 schema namespace
      m_webxml.getRootElement().setNamespace( Namespace.getNamespace( J2EE_SCHEMA_24_NAMESPACE ) );

      m_containerAuthorized = isConstrained( ""/Delete.jsp"", Role.ALL )
	      && isConstrained( ""/Login.jsp"", Role.ALL );


Unfortunately, the shipped web.xml contains a 2.5 namespace,
therefor the check for container manager authorization failed.

Replacing in web.xml the lines

<web-app xmlns=""http://java.sun.com/xml/ns/javaee""
        xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
        xsi:schemaLocation=""http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd""
        version=""2.5"">

with these

<web-app xmlns=""http://java.sun.com/xml/ns/j2ee""
        xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
        xsi:schemaLocation=""http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/javaee/web-app_2_4.xsd""
        version=""2.4"">

solved the problem - logging into jspwiki worked again.

Suggestion: either ship a web.xml with a 2.4 namespace or improve the namespace handling.",smartshark_2_2,746,jspwiki,"""[0.14106521010398865, 0.8589348196983337]"""
1541,229800,Binary compatibility test suite,"It would be very good to have a binary compatibility test suite - essentially a pre-compiled JAR file with JUnit tests - that would make sure that our binary API does not change from release to release.

I think this would be good to take aboard in 3.0 (though earlier is possible), because we'll be breaking the binary compatibility in 3.0 anyway... It would also be a good starting point for a new, improved JSPWiki API.",smartshark_2_2,956,jspwiki,"""[0.9982600808143616, 0.0017399439821019769]"""
1542,229801,PluginIndex plugin,"From IRC:

florian1978: another question when you weren't there: 14:33 < seapegasus> Hi! in a jspwiki, is there a list of all plugins currently installed? Like the PageIndex, but a PluginIndex ?
[00:56] Ecyrd: Currently there is no way to find the installed plugins
[00:56] Ecyrd: with 3.0 it *will* be possible
[00:57] Ecyrd: would be now pretty easy to write a PluginIndex
[00:57] Ecyrd: just call PluginManager.modules() and list the results
",smartshark_2_2,150,jspwiki,"""[0.9949828386306763, 0.005017135292291641]"""
1543,229802,Provide proper validation mechanisms (for Wiki page names),"Clicking on the rename button without having changed the page name displays an error page saying ""renaming not possible, unknown error""... not very user friendly I think.",smartshark_2_2,139,jspwiki,"""[0.9983190894126892, 0.0016809402732178569]"""
1544,229803,Create default pages for workflow approval pages,"Since version 2.6.0, two standard workflows are integrated in JSPWiki, which show the pages ApprovalRequiredForUserProfiles and ApprovalRequiredForPageChanges to the user. There should be default versions of these pages in the JSPWiki distribution.",smartshark_2_2,130,jspwiki,"""[0.9985529780387878, 0.0014470013557001948]"""
1545,229804,option to upload multiple attachment files using a single zip/rar file,"The process to attach multiple files is very tedious. You have to upload them one by one.
This proposal just adds a checkbox in the upload attachment page. The following code must be modified:
- templates/thetemplate/UploadTemplate.jsp --> Add the checkbox with a description (let us say 'zip/rar file. Each file inside will be attached.')
- src\com\ecyrd\jspwiki\attachment\AttachmentServlet.java --> The code that upload the file and attach it to the wiki page",smartshark_2_2,102,jspwiki,"""[0.9984819293022156, 0.001518106204457581]"""
1546,229805,commons-io needs upgrading from 1.4 to 2.4,Old dependency in JSPWiki,smartshark_2_2,714,jspwiki,"""[0.9970484375953674, 0.0029516262002289295]"""
1547,229806,generate test-jars,"When extending JSPWiki, it would be useful to have the test support classes (f.ex., TestEngine) as a Maven dependency, so they can be used to test custom plugins, filters, page providers, etc.

The suggested approach is to use [jar:test-jar|http://maven.apache.org/plugins/maven-jar-plugin/test-jar-mojo.html].",smartshark_2_2,940,jspwiki,"""[0.998364269733429, 0.0016357501735910773]"""
1548,229807,Remember edit position in plain text editor,It's annoying that the editor doesn't jump to the last edit position but is set to pos1 when either switching between preview and edit or just using one of the formatting buttons under the edit text area.,smartshark_2_2,120,jspwiki,"""[0.9649300575256348, 0.035069920122623444]"""
1549,229808,package.html -files missing for several packages,"The package.html -files are missing from several package directories, which causes the listing at http://people.apache.org/~metskem/JSPWiki/javadoc/ to look a bit gappy.",smartshark_2_2,224,jspwiki,"""[0.9966323971748352, 0.0033676286693662405]"""
1550,229809,style issues,"Some style issues detected with firefox 4 & chrome:
- -quick navigation image is mis-placed outside quick navigation box (see attached p1.png) (may be because of that, horizontal scroll appears on view page)-
- error boxes on new user page get mis-placed (see attached p1.png)
- -edit area should get as wide as it can (see attached p2.png)-
- -horizontal & vertical scrolls appear on edit page-
- -tooltip on quick navigation button refers to ???find.submit.go???-
- tooltip on toggle sidebar refers to ???Click to toggle the sidebar???",smartshark_2_2,763,jspwiki,"""[0.8977355360984802, 0.10226442664861679]"""
1551,229810,German umlauts not displayed properly on OC4J installation,"The German umlauts are shown as chinese (?) letters. 
The problem was found in the MyServletResponseWrapper in WikiJSPFilter.java. The conversion of the outputstream into a CharArrayWriter seams to be implemented differently on OC4J. The use of a ByteArrayOutputStream instead works for OC4J. 
Not tested on any other servlet container.",smartshark_2_2,387,jspwiki,"""[0.8522786498069763, 0.1477213352918625]"""
1552,229811,TitleBox rendering on Haddock,"haddock template is rendering the TitleBox page, if it exists, behind topline div, instead of on top of it (see attached image). Also, this leaves a ~60px whitespace space between the navigation bar and the sidebar/page content.

When editing a page, the TitleBox page is rendered ok.

Current workaround is to delete the TitleBox page (if it's empty the whitespace remains)

Also, not sure if this is intended, the TitleBox page also has a close button, I was expecting it to be permanent, or at least lasting the entire user session.. currently you have to close it every time.",smartshark_2_2,951,jspwiki,"""[0.6802578568458557, 0.3197421431541443]"""
1553,229812,better mobile experience,"Currently JSPWiki is not optimized for mobile devices, it looks the same on mobile as on desktop, with the effect, that the left menu and the tabs on top of the page look very small.
JSPWiki should automatically recognise that it is accessed from a mobile device and format its output for mobile.

see http://mail-archives.apache.org/mod_mbox/jspwiki-dev/201403.mbox/browser",smartshark_2_2,959,jspwiki,"""[0.9983463287353516, 0.0016536357579752803]"""
1554,229813,Password change process should require old password,"UserProfile.jsp does not require you to type in your old password to change the new password.  This can be a problem if you inadvertently leave your computer open and someone gains access to it.

I think the old password should probably be required to change the email address as well, or else it could be used to restore the backend.

(From Ounce)",smartshark_2_2,71,jspwiki,"""[0.8599879741668701, 0.14001205563545227]"""
1555,229814,Change ImagePlugin,"Hi there,
Hope it is the right place to post my question. (I didnt found another place)
I want to change the Imageplugin (com.ecyrd.jspwiki.plugin.Image.java)because i dont like, that a Image is within a table.
I found the right source, but when i change the Java Klass, export the Projekt to ""JSPWiki.jar"", copied the jar on my server in the lib-directory and restart the Server, no of my changes are accepted. 
Instead it stays on the old functions.

Hope somebody can help me.",smartshark_2_2,667,jspwiki,"""[0.9937493801116943, 0.006250665057450533]"""
1556,229815,Integration support for FCKeditor 2.6,"Integration support for FCKeditor 2.6 to take advantage of the bugfixes and new features.

For changes to FCKeditor itself since version 2.4.3, please see its [changelog website|http://www.fckeditor.net/whatsnew/all].",smartshark_2_2,497,jspwiki,"""[0.9982981085777283, 0.0017019235529005527]"""
1557,229816,Allow customisation of CommandResolver (WikiEngine.m_commandResolver),"Currently, the WikiEngine creates its CommandResolver helper object via a call to new CommandResolver().

I would like to customise the way that the CommandResolver maps the incoming url to the actual wiki content-file to load. This is to support a wiki-based webapp help system where the url passed indicates the webapp page for which context-sensitive-help is wanted. If a specific matching page is not present in the wiki I want to ""walk up the directory tree"" to find the best available match. For example, with /Wiki.jsp?path=foo/bar/baz, first look for a foo%2fbar%2fbaz page, then look for a foo%2fbar page, etc. This requires overriding getFinalPageName in the CommandResolver class.

I'm sure there are other uses for the ability to customise the mapping from url to actual content page.

Note that it is possible to modify the url before it is processed by jspwiki, so that it already points to the actual page to display. However certainly in my case that requires looking inside the ""page repository"" of the wiki to determine what pages currently exist and so is best done inside the actual wiki engine.

One possibility would be for WikiEngine to use the ClassUtils.getMappedObject method to retrieve the CommandResolver instance rather than just using new. However this would only be useful if CommandResolver also has the ""final"" attribute removed from the class and methods, or if CommandResolver were an interface and the current concrete class were renamed to CommandResolverImpl or similar.

Alternatively, CommandResolver could itself support a ""helper"" object, like AuthorizationManager has an optional ""Authorizor"" helper. This would be more work though.",smartshark_2_2,20,jspwiki,"""[0.9984997510910034, 0.0015002627624198794]"""
1558,229817,Replace mouse hover effects by clickable effects ,"Currently, hover effects are used on the quick-navigation menu, the more-menu and the category %%dynamic style.
However, these makes these features hardly usable on mobile devices as they do not support the hover event.


Replace hover menu's by clickable popups or menu's :
* CLICK to hide/show more menu  (double click to jump to [MoreMenu] ?)
* FOCUS/BLUR quick-navigation input element to show/hide the navigation dropdown
* CLICK to hide/show the %%category popups.

",smartshark_2_2,100,jspwiki,"""[0.9984419941902161, 0.0015579862520098686]"""
1559,229818,Not localized labels on Edit Group Page,"there are not localized labels in EditGroupContent.jsp in line 61:
       <th>Group Name</th>
and line 65:
      <th><label>Members</label></th>

which should be replaced by:
      <th><fmt:message key=""group.name""/></th>
and
      <th><label><fmt:message key=""group.members""/></label></th>
",smartshark_2_2,415,jspwiki,"""[0.9694380164146423, 0.03056202456355095]"""
1560,229819,display error in default template,"When hovering over the more menu, the content pane gets moved about one pixel down. Moving around the mouse pointer on the content pane leads to the original state somehow.",smartshark_2_2,775,jspwiki,"""[0.9216001033782959, 0.07839991897344589]"""
1561,229820,JSPWiki-API library creation,"The proposal is to create a ""jspwiki-api.jar"", which would contain all the interfaces and classes from ""com.ecyrd.jspwiki.api"" -package. This would be a new package, which contains a set of interfaces (and probably some basic datacontainer classes) to provide access to JSPWiki innards.

The design of the API set is available in http://www.jspwiki.org/wiki/JSPWiki3APIDesignProposal",smartshark_2_2,648,jspwiki,"""[0.9983090162277222, 0.0016910270787775517]"""
1562,229821,time machine,"Versioning and viewing the history is limited to one single page. If you want to see the whole wiki state as of timestamp X, you have to manually open the corresponding history entry of each page. Therefore, a time machine may be an interesting feature.",smartshark_2_2,115,jspwiki,"""[0.998053789138794, 0.0019462504424154758]"""
1563,229822,add tomcat7 plugin additional configuration,"provide additional configuration to tomcat7-maven-plugin so we're able to

* provide an executable war (cfr. http://tomcat.apache.org/maven-plugin-2.1/executable-war-jar.html)

* enable tomcat7:run, so we're able to debug and change code without recompiling the whole source (cfr. http://tomcat.apache.org/maven-plugin-2.1/run-mojo-features.html)",smartshark_2_2,742,jspwiki,"""[0.9985021352767944, 0.0014979209518060088]"""
1564,229823,JSPWiki missing some translations in Russian,"The Russian translation is almost done, only missing 2 keys now:

notification.createUserProfile.accept.content
notification.createUserProfile.accept.subject",smartshark_2_2,684,jspwiki,"""[0.9944222569465637, 0.005577750038355589]"""
1565,229824,Add an option to disable SecurityConfig.jsp in the property file,"The SecurityConfig.jsp should have a on/off mechanism in the property file.  It's too tedious to remove it after installation, as it will pop up again in any upgrade.",smartshark_2_2,487,jspwiki,"""[0.9985635876655579, 0.001436408027075231]"""
1566,229825,Incubator Web Site unavailable,"JIRA suggests Incubator Site ""http://incubator.apache.org/jspwiki  "", but I just get a 404.",smartshark_2_2,12,jspwiki,"""[0.9967759251594543, 0.0032240499276667833]"""
1567,229826,Auto-save during editing,"Allow to auto-save the page during (long) edit-sessions.

This requires the jspwiki-engine to support Minor -Edit [JSPWIKI-435]	such that no new versions are created for each auto-save operations.

The auto-save will be performed through an ajax call to the backend.

{noformat}
POST:
{""id"":nnn,""method"":""edit.saveText"",""params"":[""markup"",xxxx]}

{noformat}

",smartshark_2_2,293,jspwiki,"""[0.9982086420059204, 0.0017913816263899207]"""
1568,229827,Introduction of JSPWiki settings version numbers for being able to update cookies after software or settings have changed,"Initial idea from Dirk Frederickx:
we could introduce the notion of a 'version' number for user preferences. This prefs-version-nbr should then be stored in the cookie. Upstepping this version number could be used to force a rewrite of all cookies, avoiding incompatibilies between server side settings and cookies.
This could be useful for things like timeformats, introduction of new skins, introudction of new editors, etc. Also useful when upgrading JSPWiki from v2.4.x to v2.6.x.

My comment:
I'd refine this to two version numbers: one internal for JSPWiki version changes (automatically trigger a cookies update) and one in the properties file for the site admin to change and therefore trigger a cookies update manually after configuration changes.
",smartshark_2_2,321,jspwiki,"""[0.9983810186386108, 0.0016190129099413753]"""
1569,229828,Load Plugin resources from classpath,"Some plugins require the browser to load files. E.g. the FreeMindPlugin needs the browser to load the applet's classes, or another plugin might need some flash code.
Currently the solution is to attach these files to a page which has the sole purpose of having the attachment. This is kind of awkward.
JSPWiki should have a mechanism (in JSPFilter?) which would load the file from the classpath. So for FreeMind the FreeMindPlugin.jar would additionally contain freemindbrowser.jar. The plugin would generate some markup that would make the Filter recognize that the parameter is to be loaded from classpath, e.g. <wiki:IncludeResource freemindbrowser.jar>
I guess this could be done with a PageFilter, too, but the idea is to make installing plugins easier and having to add a filters.xml would be counterproductive, so the mechanism should go into core.",smartshark_2_2,287,jspwiki,"""[0.9985750913619995, 0.0014248737134039402]"""
1570,229829,Save user preferences on the server side,"Currently user preferences are stored in a cookie.
Unfortunately, our users cannot change the browser cookie settings, which default to the cookies being deleted on closing the browser and JSPWiki forgetting the user preferences.
JSPWiki should introduce some kind of user preferences storage interface, an implementation should be selected in jspwiki.properties. Implementations should be the current cookie based behaviour and storage in a property file (location specified again in jspwiki.properties).",smartshark_2_2,320,jspwiki,"""[0.998484194278717, 0.0015158462338149548]"""
1571,229830,two user preferences cookies are stored,"I've just noticed that my Firefox 3 stores two JSPWikiUserPrefs cookies for jspwiki.org: one has the path ""/"" and the other one has the path ""/wiki/"". Both have different content.

I think this is a bug in conjunction with ShortViewURLConstructor. What do you think?
",smartshark_2_2,379,jspwiki,"""[0.38281768560409546, 0.6171823143959045]"""
1572,229831,to be able to set default content language of a wikipage,"user should be able to declare  the language of the content  in the current wikipage .
",smartshark_2_2,227,jspwiki,"""[0.9983855485916138, 0.001614508219063282]"""
1573,229832,I'd like a counter that increments by some other number than one,"This concerns the Counter plugin:  [{Counter}] 

Sometime I'd like a countdown counter, or a timer that increments by something other than one.  The counting by something more than one can be accomplished by using the counter again as desired (but it would show on the screen each time unless showing the counter was suppressed -- see a different issue for that).

Perhaps something like this: [{Counter name='xyzzy' increment=5}] or this: [{Counter name='xyzzy' increment=-3}]",smartshark_2_2,450,jspwiki,"""[0.9962301850318909, 0.0037697970401495695]"""
1574,229833,Copy page feature,"There should be a feature to create a copy of a wiki page on the info tab: ""Create copy""
The new page should be called ""Copy of Page"", as in Windows Explorer or like Eclipse's copies of classes.

It is much easier for a user to start with a copy than to start from scratch.

""Monkey See/Monkey Do: Always start by copying the structure of a similar plug-in"" 
(Erich Gamma)",smartshark_2_2,315,jspwiki,"""[0.9983001351356506, 0.0016998708015307784]"""
1575,229834,"Click ""Edit"" for an inserted page doesn't open the inserted page in edit mode","Click ""Edit"" for an inserted page doesn't open the inserted page in edit mode; it opens the current page where I don't find the text to change",smartshark_2_2,590,jspwiki,"""[0.14940838515758514, 0.8505916595458984]"""
1576,229835,ChangeLog published on site,"As talked in http://s.apache.org/IP1, in order to give more visibility to contributors.",smartshark_2_2,670,jspwiki,"""[0.9981871247291565, 0.0018128053052350879]"""
1577,229836,Test webapps for web testing don't get undeployed,"Test webapps used for web testing don't get totally or correctly undeployed. Almost all of the webapp gets deleted except WEB-INF/lib/JSPWiki.jar, so after a few seconds it appears as deployed in the tomcat manager. If I try to undeploy them again I get this Exception on Tomcat (tried to undeploy them with Tomcat Manager and with build.xml script, same results on both):

java.lang.IllegalStateException
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1248)
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1208)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
        at org.apache.log4j.spi.LoggingEvent.<init>(LoggingEvent.java:154)
        at org.apache.log4j.Category.forcedLog(Category.java:388)
        at org.apache.log4j.Category.error(Category.java:319)
        at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:172)
Exception in thread ""JSPWiki Lucene Indexer"" java.lang.NoClassDefFoundError: org/apache/log4j/spi/VectorWriter
        at org.apache.log4j.spi.LoggingEvent.<init>(LoggingEvent.java:154)
        at org.apache.log4j.Category.forcedLog(Category.java:388)
        at org.apache.log4j.Category.error(Category.java:319)
        at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:172)
10-nov-2007 17:07:11 org.apache.catalina.loader.WebappClassLoader loadClass

So it seems that this WikiBackgroundThread is not properly closed.. hadn't many time to look into it althought",smartshark_2_2,515,jspwiki,"""[0.9399455785751343, 0.0600544773042202]"""
1578,229837,integrate a Google Wave using a plugin,Is anyone thinking of creating a Google Wave plugin?  I'm not sure what is involved in the process.,smartshark_2_2,579,jspwiki,"""[0.9982072114944458, 0.0017927632434293628]"""
1579,229838,Show Wikipages in Search without Authorization,"I often have the problem that users tell me: ""I canÂ´t find the information in the wiki."" 
But I know that it is actually there. So they donÂ´t have the authorization to view the page and therefore the search filters the page away. 

So here is my question: Why donÂ´t we show the user that there is a page that contains the information he is searching for and he simply does not have the authorization to see it. (see screenshot)


Then he can ask for the permission instead of making stupid stuff like creating a new page for his issue.

",smartshark_2_2,825,jspwiki,"""[0.955729603767395, 0.0442703515291214]"""
1580,229839,Drop JAAS configuration,"JAAS configuration is error-prone, ugly and cumbersome. It would be more convenient to configure security settings on application level ideally with a nice GUI.",smartshark_2_2,350,jspwiki,"""[0.9973860383033752, 0.002613913966342807]"""
1581,229840,Refactor all utility classes to the util-package,To be performed together with the renaming of the packages.,smartshark_2_2,675,jspwiki,"""[0.9974449872970581, 0.0025550175923854113]"""
1582,229841,NL localization needs update,"Attached a patch for etc/i18n/CoreResources_nl.properties 
Attached a brand new src/com/ecyrd/jspwiki/plugin/PluginResources_nl.properties",smartshark_2_2,455,jspwiki,"""[0.9971390962600708, 0.002860916079953313]"""
1583,229842,JSPWiki missing some translations in Finnish,"[Update: Italian, French, German, Russian, Simplified Chinese and Brazilian Portuguese completed, thanks for submissions!]

The following resource strings need to be translated into Finnish.  If you know this language and can translate them please place your translations as a comment below or email them to gmazza at apache dot org.  (Only the right-side of the equals sign needs translating, the left-side is a hardcoded constant string.)

security.error.cannot.rename = Cannot rename: the login name <...> is already taken.

security.error.fullname.taken = Someone with the name of <...> has already registered.

security.error.login.taken = The login name <...> is already taken.

notification.createUserProfile.accept.content = Congratulations! Your new profile on <...> has been created. Your profile details are as follows: 

Login name: <...>
Your name : <...>
E-mail    : <...>

If you forget your password, you can reset it at <...>
notification.createUserProfile.accept.subject = Welcome to JSPWiki!

------------------------------------------------------

Thanks for any translation assistance you can provide!
",smartshark_2_2,683,jspwiki,"""[0.9961053729057312, 0.00389455771073699]"""
1584,229843,ASF style voting adopted,ASF-style voting adopted and standard practice.,smartshark_2_2,223,jspwiki,"""[0.9982491731643677, 0.0017509085591882467]"""
1585,229844,"saving a page without supplying a change note will render a ""null"" change note in PageInfo.jsp","Create a new page, or update an existing page, and do not provide a change note while saving it.
Go to the Info Tab to see the revisions, and you will see a change note ""null"".

I would expect the change note to be empty.",smartshark_2_2,673,jspwiki,"""[0.12128837406635284, 0.8787115812301636]"""
1586,229845,Move jspwiki.org content to ASF-owned VM,"jspwiki.org content needs to be moved over to ASF-hosted place.  Apparently VMs are available.

We can either move jspwiki.org to Apache, or I can keep the domain and point everything to jspwiki.apache.org.",smartshark_2_2,733,jspwiki,"""[0.9982897639274597, 0.001710185781121254]"""
1587,229846,Incorporate Stripes into JSPWiki,"This has already been discussed quite a lot on the mailing lists, but it looks like a JIRA ticket was missing for it.  So here it is :-)

(Stripes provides us a better templating and MVC model than the current structure.  And it means we can get rid of the Command structure, yay!)",smartshark_2_2,165,jspwiki,"""[0.9983963370323181, 0.0016036347951740026]"""
1588,229847,"""Edit Website"" page clarifications","Attached patch (unsure if I can use the CMS Bookmarklet to do the same...?) updates the Edit Website page on the JSPWiki site.  In particular, I removed the partial/incomplete instructions on local site builds to refer the user to the full instructions (updated by me recently) on the Apache CMS site.",smartshark_2_2,603,jspwiki,"""[0.9981807470321655, 0.0018193083815276623]"""
1589,229848,Prepare a charter,"We need to prepare a charter, since JSPWiki should (probably) graduate as a top-level project.

http://incubator.apache.org/guides/graduation.html#tlp-resolution",smartshark_2_2,596,jspwiki,"""[0.998330295085907, 0.0016696554375812411]"""
1590,229849,Startup fails due to jspwiki.log (Permission denied),"When i follwow the [installation instructions|https://jspwiki-wiki.apache.org/Wiki.jsp?page=Getting%20Started#section-Getting+Started-Installation] the JSPWiki webapp fails to start.
{noformat}
log4j:WARN No appenders could be found for logger (org.apache.wiki.util.PropertyReader).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: jspwiki.log (Permission denied)
        at java.io.FileOutputStream.open0(Native Method)
        at java.io.FileOutputStream.open(FileOutputStream.java:270)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:133)
        at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
        at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440)
        at org.apache.wiki.WikiEngine.initialize(WikiEngine.java:464)
        at org.apache.wiki.WikiEngine.<init>(WikiEngine.java:430)
        at org.apache.wiki.WikiEngine.getInstance(WikiEngine.java:370)
        at org.apache.wiki.ui.WikiServletFilter.init(WikiServletFilter.java:82)
        at org.apache.catalina.core.ApplicationFilterConfig.initFilter(ApplicationFilterConfig.java:279)
        at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:260)
        at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:105)
        at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4660)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5281)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:147)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:725)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:701)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:717)
        at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:939)
        at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1812)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

The current dir of the tomcat is {{/var/lib/tomcat8}}. This directory belongs to root and the tomcat user has no write permission.

I would prefer to have a fallback for this situation, eg. initializing log4j with a simple console appender. So the webapp could startup and the log will be catched by tomcat.",smartshark_2_2,917,jspwiki,"""[0.5761169195175171, 0.4238830804824829]"""
1591,229850,"knox renaming ""app"" to ""/gateway/ui_ns1/zeppelin/app""","While running the zeppelin through Knox proxy, the variable ""app"" got explicitly replace with the string ""/gateway/ui_ns1/zeppelin/app"".

Original code: ng-repeat=""app in apps""
Knox code: ng-repeat=""/gateway/ui_ns1/zeppelin/app in apps""

In two files
a] result.html
b] result-chart-selector.html

Due to which the UI breaks
Error:Â [ngRepeat:iidexp|https://hortonworks.jira.com/wiki/display/ngRepeat/iidexp]Â '_item_' in '_item_Â inÂ _collection_' should be an identifier or '(_key_,Â _value_)' expression",smartshark_2_2,720,knox,"""[0.30168551206588745, 0.6983145475387573]"""
1592,229851,Reconcile WebAppSecurity provider X-Frame-Options and X-Content-Type-Options param names,"The X-Frame-OptionsÂ params for the WebAppSec provider do not match what is documented.

Since the implementation isÂ being used (e.g., manager.xml, knoxsso.xml), the appropriate resolution is to correct the docs.

Additionally, since the Admin UI support for this was based on the docs, it also needs to be updated to produce the correct params.

Further, the X-Content-Type-Options param names should be similar in form to the X-Frame-Options param names.

The correct param names are:
 * xframe.options
 * xframe.options.enabled
 * xcontent-type.options
 * xcontent-type.options.enabled

The User Guide must be updated to reflect the correct X-Frame-Options param names; it currently describes xframe-options.enabled and xframe-options.value

Â ",smartshark_2_2,1252,knox,"""[0.9967356324195862, 0.0032643338199704885]"""
1593,229852,KnoxCLI create-master requires existing master file to be deleted,"The current create-master command silently fails when there is an existing {GATEWAY_HOME}/data/security/master file.

We need to be able to replace it or at least provide an error that it must be removed.",smartshark_2_2,1473,knox,"""[0.269804984331131, 0.7301950454711914]"""
1594,229853,Knox DSL - Ensure that HTTP client is closed on shutdown,"The class org.apache.hadoop.gateway.shell.Hadoop used to connect to knox doesn't close opened socket.
The ""shutdown"" method of org...shell.Hadoop doesn't call client.close() and this produces  too much permanent CLOSE_WAIT sockest.

Suggestion:


    public void shutdown() throws InterruptedException {
 -    executor.shutdown();
 +    closeClient();
    }
  
    public boolean shutdown( long timeout, TimeUnit unit ) throws InterruptedException {
-      executor.shutdown();
 +    closeClient();
      return executor.awaitTermination( timeout, unit );
    }
 +  
 +  private void closeClient(){
+      executor.shutdownNow();
 +    if(client!=null){
 +      client.close();
 +    }
 +  }",smartshark_2_2,755,knox,"""[0.18184731900691986, 0.8181527256965637]"""
1595,229854,GatewayDeployFuncTest is broken because of topology validation,"As part of the effort of KNOX-1350 topology validation is enforced which causes GatewayDeployFuncTest test to fail.

Relevant diff snippet from DefaultTopologyService.java for the change.
{code:java}
 try {
       TopologyValidator tv = new TopologyValidator(topology);
 
-      if(tv.validateTopology()) {
+      if(!tv.validateTopology()) {
         throw new SAXException(tv.getErrorString());
       }
{code}",smartshark_2_2,590,knox,"""[0.8830151557922363, 0.1169847920536995]"""
1596,229855,Atlas HA URL Manager breaks if ZooKeeper namespace value begins with /,"AtlasZookeeperURLManager always pre-pends '/' to the value of the atlas.server.ha.zookeeper.zkroot config property. The problem is that the default value of this property is '/apache_atlas', and the addition leading '/' breaks the AtlasZookeeperURLManager. The value should be checked, and the '/' only added if it is missing.",smartshark_2_2,1128,knox,"""[0.08884859830141068, 0.9111513495445251]"""
1597,229856,generated shiro.ini file does not preserve property order,"The shiro.ini file generated by the deployer does not preserve the order of properties in topology file.

This causes error shiro initialiation.

Attaching the topology file (sandbox.xml) and generated shiro.ini.

Start up error reported in console pasted here:

org.apache.shiro.config.ConfigurationException: Property 'contextFactory.systemAuthenticationMechanism' does not exist for object of type org.apache.hadoop.gateway.shirorealm.KnoxLdapRealm.
	at org.apache.shiro.config.ReflectionBuilder.isTypedProperty(ReflectionBuilder.java:252)
	at org.apache.shiro.config.ReflectionBuilder.applyProperty(ReflectionBuilder.java:491)
	at org.apache.shiro.config.ReflectionBuilder.applySingleProperty(ReflectionBuilder.java:203)
	at org.apache.shiro.config.ReflectionBuilder.applyProperty(ReflectionBuilder.java:164)
	at org.apache.shiro.config.ReflectionBuilder.buildObjects(ReflectionBuilder.java:124)
	at org.apache.shiro.config.IniSecurityManagerFactory.buildInstances(IniSecurityManagerFactory.java:170)
	at org.apache.shiro.config.IniSecurityManagerFactory.createSecurityManager(IniSecurityManagerFactory.java:119)
	at org.apache.shiro.config.IniSecurityManagerFactory.createSecurityManager(IniSecurityManagerFactory.java:97)
	at org.apache.shiro.config.IniSecurityManagerFactory.createInstance(IniSecurityManagerFactory.java:83)
	at org.apache.shiro.config.IniSecurityManagerFactory.createInstance(IniSecurityManagerFactory.java:41)
	at org.apache.shiro.config.IniFactorySupport.createInstance(IniFactorySupport.java:123)
	at org.apache.shiro.util.AbstractFactory.getInstance(AbstractFactory.java:47)
	at org.apache.shiro.web.env.IniWebEnvironment.createWebSecurityManager(IniWebEnvironment.java:203)
	at org.apache.shiro.web.env.IniWebEnvironment.configure(IniWebEnvironment.java:99)
	at org.apache.shiro.web.env.IniWebEnvironment.init(IniWebEnvironment.java:92)
	at org.apache.shiro.util.LifecycleUtils.init(LifecycleUtils.java:45)
	at org.apache.shiro.util.LifecycleUtils.init(LifecycleUtils.java:40)
	at org.apache.shiro.web.env.EnvironmentLoader.createEnvironment(EnvironmentLoader.java:226)
	at org.apache.shiro.web.env.EnvironmentLoader.initEnvironment(EnvironmentLoader.java:138)
	at org.apache.shiro.web.env.EnvironmentLoaderListener.contextInitialized(EnvironmentLoaderListener.java:58)
	at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:782)
	at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:424)
	at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:774)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)
	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)
	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.hadoop.gateway.GatewayServer.internalDeploy(GatewayServer.java:310)
	at org.apache.hadoop.gateway.GatewayServer.access$600(GatewayServer.java:63)
	at org.apache.hadoop.gateway.GatewayServer$InternalTopologyListener.handleTopologyEvent(GatewayServer.java:360)
	at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.notifyChangeListeners(FileTopologyProvider.java:164)
	at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.reloadTopologies(FileTopologyProvider.java:128)
	at org.apache.hadoop.gateway.GatewayServer.start(GatewayServer.java:268)
	at org.apache.hadoop.gateway.GatewayServer.startGateway(GatewayServer.java:183)
	at org.apache.hadoop.gateway.GatewayServer.main(GatewayServer.java:102)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.gateway.launcher.Invoker.invokeMainMethod(Invoker.java:64)
	at org.apache.hadoop.gateway.launcher.Invoker.invoke(Invoker.java:37)
	at org.apache.hadoop.gateway.launcher.Command.run(Command.java:101)
	at org.apache.hadoop.gateway.launcher.Launcher.run(Launcher.java:70)
	at org.apache.hadoop.gateway.launcher.Launcher.main(Launcher.java:49)
",smartshark_2_2,1420,knox,"""[0.1620859056711197, 0.8379141092300415]"""
1598,229857,Provider Configuration Reference Isn't Updated If New Reference Is Invalid,"If an existing simple descriptor is modified, such that its new provider configuration reference is invalid, the original reference relationship is (incorrectly) maintained. This relationship should be removed, whether the new reference is valid or not, because the descriptor no longer references the previously-referenced provider config. Maintaining this relationship prevents deletion of the previously-referenced provider configuration via the API (and UI), though manual deletion is still possible.",smartshark_2_2,1094,knox,"""[0.09808644652366638, 0.901913583278656]"""
1599,229858,Websockets connection is terminated when message size is larger than 65536,"In some cases, or e.g. in Zeppelin with large demo Notebooks, Knox will abruptly close connection with error:

[exec] org.eclipse.jetty.websocket.api.MessageTooLargeException: Text message size [1313822] exceeds maximum size [65536]
     [exec] 	at org.eclipse.jetty.websocket.api.WebSocketPolicy.assertValidTextMessageSize(WebSocketPolicy.java:140)
     [exec] 	at org.eclipse.jetty.websocket.common.Parser.assertSanePayloadLength(Parser.java:127)
     [exec] 	at org.eclipse.jetty.websocket.common.Parser.parseFrame(Parser.java:482)
     [exec] 	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:254)
     [exec] 	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:632)
     [exec] 	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:480)
     [exec] 	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
     [exec] 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
     [exec] 	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
     [exec] 	at java.lang.Thread.run(Thread.java:745)
",smartshark_2_2,426,knox,"""[0.1118067279458046, 0.8881932497024536]"""
1600,229859,encryptQueryString Password is Recreated when Topology is Changed.,"URLs with encrypted query strings, which contain sensitive cluster internal info, fail to be decrypted by the gateway after a cluster topology is redeployed in the gateway instance.  Upon redeployment the password for encryptQueryString is being regenerated even though the credential store already exists for a given topology file (cluster). This must be changed to only generate if the alias doesn't already exist inside the credential store.  This only affects Knox HA cluster deployments.  The effect will be that gateway instances will not be able to decrypt URLs generated by other gateway instances after fail-over if the password are not kept in sync after each topology deployment.  The workaround is to manually synchronize the content of the {GATEWAY_HOME}/conf/security folder after topology deployment changes.",smartshark_2_2,1399,knox,"""[0.08478092402219772, 0.9152190685272217]"""
1601,229860,WebAppSecurity wizard produces bad config when invalid params are specified,"If invalid property values are specified for one of the WebAppSecurity provider types, the wizard adds TWO WebAppSec providers. The validation isn't working as expected, and it's not clear why TWO are being added.",smartshark_2_2,1244,knox,"""[0.11943193525075912, 0.8805680274963379]"""
1602,229861,"[NN Federation] Incase of providing invalid Nameservice as param, then service is created with invalid endpoint url"," Incase of providing invalid Nameservice as param, then HDFSUI,Namenode and  webhdfs service is created with invalid endpoint url

Given nameservice as ""ns3"" which is not available, then:

1. The ""NAMENODE"" service is created with endpoint url as ""hdfs://ns3"" which doesnt exist.
2. services like ""HDFSUI"" and ""WEBHDFS"" are created without any endpoint urls

*Descriptor file*

{noformat}
    {
      ""name"": ""WEBHDFS"",
      ""params"": {
        ""discovery-nameservice"": ""ns3""
      }
    },
    {
      ""name"": ""NAMENODE"",
      ""params"": {
        ""discovery-nameservice"": ""ns3""
      }
    },
    {
      ""name"": ""HDFSUI"",
      ""params"": {
        ""discovery-nameservice"": ""ns3""
      }
    },
{noformat}

*Generated Topology:*
{noformat}
    <service>
        <role>NAMENODE</role>
        <url>hdfs://ns3</url>
    </service>
    <service>
        <role>WEBHDFS</role>
    </service>
    <service>
        <role>NAMENODE</role>
        <url>hdfs://ns3</url>
    </service>
{noformat}

",smartshark_2_2,1200,knox,"""[0.1540020853281021, 0.8459978699684143]"""
1603,229862,X-Forwarded-Context header is not rewritten correctly,"In KNOX-1295 a rewrite rule for properly having the X-Forwarded-Context full path was introduced. Unfortunately, there are 2 problems with the current rewrite rule:
 - It should add {{sparkhistory}}, but instead it adds {{sparkhistoryui}};
 - It trims the leading {{/}}, so {{/gateway/default}} becomes {{gateway/default/sparkhistoryui}};
",smartshark_2_2,1283,knox,"""[0.09256186336278915, 0.9074381589889526]"""
1604,229863,Yarn responses with TrackingUrl in the body not getting blanked out,"For example if issuing the following request (after atleast one yarn application has been submitted and run).

curl -ikv -u guest:guest-password -X GET 'https://nn.example.com:8443/gateway/sandbox/resourcemanager/v1/cluster/apps'

The details of app will have content like :

""trackingUrl"":""http://nn.example.com:8088/cluster/app/application_1409008107556_0007""


The gateway log has the following related output, indicating failure running a rewrite rule:

2014-08-27 21:57:34,088 ERROR hadoop.gateway (UrlRewriteProcessor.java:rewrite(160)) - Failed to rewrite URL: http://nn.example.com:8088/cluster/app/application_1409008107556_0008, direction: OUT via rule: RESOURCEMANAGER/resourcemanager/trackingUrlHistory/outbound, status: FAILURE
2014-08-27 21:57:34,088 ERROR hadoop.gateway (JsonFilterReader.java:filterBufferedValue(532)) - Failed to filter value http://nn.example.com:8088/cluster/app/application_1409008107556_0008, rule RESOURCEMANAGER/resourcemanager/trackingUrlHistory/outbound: java.lang.NullPointerException
",smartshark_2_2,15,knox,"""[0.10923000425100327, 0.8907699584960938]"""
1605,229864,Set JSSESSIONID cookie as HttpOnly and Secure.,"Knox, at present, leverages JSESSIONID as session cookie,
This needs to be flagged as httpOnly.

Please see
https://www.owasp.org/index.php/HttpOnly
for some context on httpOnly flag.",smartshark_2_2,1461,knox,"""[0.8237675428390503, 0.1762324869632721]"""
1606,229865,Provider Configuration Wizard HA Provider Validation for ZooKeeper Ensemble is Wrong,The validation regular expression for HA provider zookeeperEnsemble values mistakenly checks for a semicolon delimiter instead of a comma.,smartshark_2_2,1284,knox,"""[0.12358196824789047, 0.8764180541038513]"""
1607,229866,JWT.getExpires() returns null,"JWT.getExpires() returns null even when there is an expiry Date:

SEVERE: Unable to parse JWT token: java.text.ParseException: The ""exp"" claim is not a String
",smartshark_2_2,770,knox,"""[0.08369874209165573, 0.9163012504577637]"""
1608,229867,Pseudo Federation Provider does not Propagate user.name on Redirect,Providing a query parameter of user.name on initial request works fine but the redirect does not result in a request having the parameter.,smartshark_2_2,48,knox,"""[0.11581649631261826, 0.8841835260391235]"""
1609,229868,Ensure cluster topology details are rewritten for HBase/Stargate REST APIs,"From: Vladimir
There are 2 requests where HBase returns internal cluster structure, Region Server address in particular (marked in red). Didn't noticed this earlier. Since it's not http address of Region Server should we actually hide it? If 'yes' then how it should be rewritten in Knox not to break existing HBase/Stargate clients? 

GET http://localhost:8080/status/cluster
{   ""requests"":36125,
   ""LiveNodes"":[
      {
         ""name"":""dev01.hortonworks.com:60020"",
         ""requests"":0,
         ""startCode"":1379004777978,
         ""Region"":[
            {
               ""name"":""YW1iYXJpc21va2V0ZXN0LCwxMzc5MDA1MDIyNjc4LjkzN2M3YTcxODBlNTQ3Y2NiMDQ1ODdlNzA3Y2U1MTIyLg=="",
               ""readRequestsCount"":0,
               ""writeRequestsCount"":1,
               ""stores"":1,
               ""storefiles"":1,
               ""storefileSizeMB"":0,
               ""memstoreSizeMB"":0,
               ""storefileIndexSizeMB"":0,
               ""rootIndexSizeKB"":0,
               ""totalStaticIndexSizeKB"":0,
               ""totalStaticBloomSizeKB"":0,
               ""totalCompactingKVs"":0,
               ""currentCompactedKVs"":0
            },
            ...
         ],
         ""heapSizeMB"":60,
         ""maxHeapSizeMB"":1004
      }
   ],
   ""DeadNodes"":[

   ],
   ""regions"":5,
   ""averageLoad"":5.0
}

GET http://localhost:8080/test_table/regions
{
   ""name"":""test_table"",
   ""Region"":[
      {
         ""endKey"":"""",
         ""id"":1379330509662,
         ""location"":""dev01.hortonworks.com:60020"",
         ""name"":""test_table,,1379330509662.0381e0912d8802b53b3946987736748e."",
         ""startKey"":""""
      }
   ]
}

From: Kevin
I can think of four things we can do. 

    Remove the value: ""name"":""""
    Hash the value: ""name"":""asdkljhasdfjkhasdkjlhsd""
        This would make sense only if we never needed to get the original value back.
    Encrypt the value.
        The value of this beyond hashing would be that we could decrypt on input if required.
        Number 5 below would make this easier though.
    Replace with a URL that has the address encoded/encrypted as part of its query string.
        region://knox-host:8334/gateway/cluster/hbase?_=asdlkjasdlajsdklasdflkjsda
        This would make it easier to detect in incoming URLs and rewrite back to host:port.
        This is similar to how datanode addresses are handled

Note that we can do different things for ""name"" and ""location"" if required.

From Vinay:
I think option 4 should be the default. IMO, it shields the cluster topology from leaking out and with Knox being able to encode/decode (or is it encrypt/decrypt) the internal cluster nodes aren't directly addressable.

This seems like the right behavior to me.",smartshark_2_2,1357,knox,"""[0.27645060420036316, 0.7235494256019592]"""
1610,229869,"Gateway deployment directory value is been hardcoded , instead of value being read from exposed configuration.","Issue1 :
======

During Gateway startup , deployment directory value is been hardcoded , instead of value being read from available configuration .

{noformat}
  @Override
  public String getGatewayDeploymentDir() {
    return getGatewayDataDir() + File.separator + ""deployments"";
  }
{noformat}

Issue2:
======
   Configuration Name in gateway-default.xml is not same as the name used in ""org.apache.hadoop.gateway.config.impl.GatewayConfigImpl""

{noformat}
  private static final String GATEWAY_CONFIG_FILE_PREFIX = ""gateway"";
  public static final String DEPLOYMENT_DIR = GATEWAY_CONFIG_FILE_PREFIX + "".deployment.dir"";
{noformat}

Incorrect Configuration name in gateway-default.xml
{noformat}
    <property>
        <name>gateway.gateway.conf.dir</name>
        <value>deployments</value>
        <description>The directory within GATEWAY_HOME that contains gateway topology deployments.</description>
    </property>
{noformat}





",smartshark_2_2,89,knox,"""[0.11639083176851273, 0.8836091756820679]"""
1611,229870,"Failure while executing a `curl` command using Content-Type: text/xml"" ""Content-Encoding: gzip""","Knox should honor the Content-Encoding header.

Currently Knox only takes Content-Type header into account, there could be a case where

""Content-Type: text/xml"" and ""Content-Encoding: gzip"" Knox fails with the following error
{code:java}
java.lang.RuntimeException: com.ctc.wstx.exc.WstxUnexpectedCharException: Illegal character ((CTRL-CHAR, code 31))
Â at [row,col {unknown-source}]: [1,1]
java.lang.RuntimeException: com.ctc.wstx.exc.WstxUnexpectedCharException: Illegal character ((CTRL-CHAR, code 31))
Â at [row,col {unknown-source}]: [1,1]
Â Â  Â at org.apache.knox.gateway.filter.rewrite.impl.xml.XmlFilterReader.read(XmlFilterReader.java:127)
Â Â  Â at org.apache.commons.io.input.ReaderInputStream.fillBuffer(ReaderInputStream.java:198)
Â Â  Â at org.apache.commons.io.input.ReaderInputStream.read(ReaderInputStream.java:277)
Â Â  Â at org.apache.knox.gateway.filter.rewrite.impl.UrlRewriteRequestStream.read(UrlRewriteRequestStream.java:37)
Â Â  Â at java.io.InputStream.read(InputStream.java:179)
Â Â  Â at java.io.InputStream.read(InputStream.java:101)
Â Â  Â at org.apache.knox.gateway.dispatch.InputStreamEntity.writeTo(InputStreamEntity.java:126)
Â Â  Â at org.apache.http.impl.execchain.RequestEntityProxy.writeTo(RequestEntityProxy.java:121)
Â Â  Â at org.apache.http.impl.DefaultBHttpClientConnection.sendRequestEntity(DefaultBHttpClientConnection.java:156)
Â Â  Â at org.apache.http.impl.conn.CPoolProxy.sendRequestEntity(CPoolProxy.java:160)
Â Â  Â at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:238)
Â Â  Â at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:123)
Â Â  Â at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)
Â Â  Â at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)
Â Â  Â at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)
Â Â  Â at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)
Â Â  Â at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
Â Â  Â at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
Â Â  Â at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108)
Â Â  Â at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
Â Â  Â at org.apache.knox.gateway.dispatch.DefaultDispatch.executeOutboundRequest(DefaultDispatch.java:130)
Â Â  Â at org.apache.knox.gateway.dispatch.DefaultDispatch.executeRequest(DefaultDispatch.java:116)
Â Â  Â at org.apache.knox.gateway.dispatch.DefaultDispatch.doPut(DefaultDispatch.java:295)
Â Â  Â at org.apache.knox.gateway.dispatch.GatewayDispatchFilter$PutAdapter.doMethod(GatewayDispatchFilter.java:184)
Â Â  Â at org.apache.knox.gateway.dispatch.GatewayDispatchFilter.doFilter(GatewayDispatchFilter.java:122)
Â Â  Â at org.apache.knox.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:61)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.identityasserter.common.filter.AbstractIdentityAssertionFilter.doFilterInternal(AbstractIdentityAssertionFilter.java:196)
Â Â  Â at org.apache.knox.gateway.identityasserter.common.filter.AbstractIdentityAssertionFilter.continueChainAsPrincipal(AbstractIdentityAssertionFilter.java:153)
Â Â  Â at org.apache.knox.gateway.identityasserter.common.filter.CommonIdentityAssertionFilter.doFilter(CommonIdentityAssertionFilter.java:90)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.filter.rewrite.api.UrlRewriteServletFilter.doFilter(UrlRewriteServletFilter.java:60)
Â Â  Â at org.apache.knox.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:61)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:91)
Â Â  Â at org.apache.knox.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:88)
Â Â  Â at java.security.AccessController.doPrivileged(Native Method)
Â Â  Â at javax.security.auth.Subject.doAs(Subject.java:422)
Â Â  Â at org.apache.knox.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:143)
Â Â  Â at org.apache.knox.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:75)
Â Â  Â at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
Â Â  Â at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
Â Â  Â at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
Â Â  Â at org.apache.knox.gateway.filter.ShiroSubjectIdentityAdapter.doFilter(ShiroSubjectIdentityAdapter.java:72)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
Â Â  Â at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
Â Â  Â at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
Â Â  Â at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
Â Â  Â at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
Â Â  Â at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
Â Â  Â at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
Â Â  Â at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
Â Â  Â at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
Â Â  Â at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
Â Â  Â at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
Â Â  Â at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.filter.ResponseCookieFilter.doFilter(ResponseCookieFilter.java:50)
Â Â  Â at org.apache.knox.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:61)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.filter.XForwardedHeaderFilter.doFilter(XForwardedHeaderFilter.java:30)
Â Â  Â at org.apache.knox.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:61)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:372)
Â Â  Â at org.apache.knox.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:272)
Â Â  Â at org.apache.knox.gateway.GatewayFilter.doFilter(GatewayFilter.java:171)
Â Â  Â at org.apache.knox.gateway.GatewayFilter.doFilter(GatewayFilter.java:94)
Â Â  Â at org.apache.knox.gateway.GatewayServlet.service(GatewayServlet.java:141)
Â Â  Â at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
Â Â  Â at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1669)
Â Â  Â at org.eclipse.jetty.websocket.server.WebSocketUpgradeFilter.doFilter(WebSocketUpgradeFilter.java:201)
Â Â  Â at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
Â Â  Â at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
Â Â  Â at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
Â Â  Â at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)
Â Â  Â at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)
Â Â  Â at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
Â Â  Â at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
Â Â  Â at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
Â Â  Â at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
Â Â  Â at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
Â Â  Â at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)
Â Â  Â at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
Â Â  Â at org.apache.knox.gateway.trace.TraceHandler.handle(TraceHandler.java:51)
Â Â  Â at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
Â Â  Â at org.apache.knox.gateway.filter.CorrelationHandler.handle(CorrelationHandler.java:41)
Â Â  Â at org.eclipse.jetty.servlets.gzip.GzipHandler.handle(GzipHandler.java:529)
Â Â  Â at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
Â Â  Â at org.apache.knox.gateway.filter.PortMappingHelperHandler.handle(PortMappingHelperHandler.java:152)
Â Â  Â at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)
Â Â  Â at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
Â Â  Â at org.eclipse.jetty.server.Server.handle(Server.java:499)
Â Â  Â at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
Â Â  Â at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:258)
Â Â  Â at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
Â Â  Â at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
Â Â  Â at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
Â Â  Â at java.lang.Thread.run(Thread.java:748)
Caused by: com.ctc.wstx.exc.WstxUnexpectedCharException: Illegal character ((CTRL-CHAR, code 31))
Â at [row,col {unknown-source}]: [1,1]
Â Â  Â at com.ctc.wstx.sr.StreamScanner.throwInvalidSpace(StreamScanner.java:676)
Â Â  Â at com.ctc.wstx.sr.StreamScanner.throwInvalidSpace(StreamScanner.java:661)
Â Â  Â at com.ctc.wstx.sr.StreamScanner.getNextAfterWS(StreamScanner.java:831)
Â Â  Â at com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:2110)
Â Â  Â at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1181)
Â Â  Â at org.codehaus.stax2.ri.Stax2EventReaderImpl.nextEvent(Stax2EventReaderImpl.java:255)
Â Â  Â at org.apache.knox.gateway.filter.rewrite.impl.xml.XmlFilterReader.read(XmlFilterReader.java:120)
Â Â  Â ... 103 more{code}
Â ",smartshark_2_2,708,knox,"""[0.20689396560192108, 0.7931060194969177]"""
1612,229871,Incompatible Dependencies in Pac4j Provider,"From pac4j github pull request - https://github.com/pac4j/pac4j/pull/445:

""xmltooling is a part of OpenSAML v2, and is incompatible with OpenSAML v3 (see: https://issues.shibboleth.net/jira/browse/OSJ-152). It appears that it worked fine for most users in the demo projects, because the classloader order happened to place xmltooling after the other opensaml libraries (such as opensaml-core), causing the correct default-config.xml to be loaded, instead of the xmltooling version of the same file.

The xmltooling library is only used in the pac4j-saml module for a convenience method, which is also implemented in the OpenSAML v3 java-tooling library. This change replaces the call with that newer method, and also removes the xmltooling dependency from the pom.

This change resolves #444.""

From KnoxSSO testing with Ambari 2.2 on centos 6.4:

""2016-01-30 17:05:19,411 ERROR hadoop.gateway (GatewayServlet.java:service(126)) - Gateway processing failed: javax.servlet.ServletException: java.lang.NullPointerException
javax.servlet.ServletException: java.lang.NullPointerException
        at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:70)
        at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:322)
        at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:222)
        at org.apache.hadoop.gateway.GatewayFilter.doFilter(GatewayFilter.java:129)
        at org.apache.hadoop.gateway.GatewayServlet.service(GatewayServlet.java:121)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:370)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ExceptionInInitializerError
        at org.pac4j.saml.client.SAML2Client.<clinit>(SAML2Client.java:104)
        at org.pac4j.config.client.PropertiesConfigFactory.tryCreateSaml2Client(PropertiesConfigFactory.java:146)
        at org.pac4j.config.client.PropertiesConfigFactory.build(PropertiesConfigFactory.java:93)
        at org.apache.hadoop.gateway.pac4j.filter.Pac4jDispatcherFilter.init(Pac4jDispatcherFilter.java:139)
        at org.apache.hadoop.gateway.GatewayFilter$Holder.getInstance(GatewayFilter.java:352)
        at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:321)
        at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:222)
        at org.apache.hadoop.gateway.filter.XForwardedHeaderFilter.doFilter(XForwardedHeaderFilter.java:30)
        at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:61)
        ... 34 more
Caused by: java.lang.ClassCastException: org.opensaml.xml.schema.impl.XSAnyBuilder cannot be cast to org.opensaml.core.xml.XMLObjectBuilder
        at org.opensaml.core.xml.config.XMLConfigurator.initializeObjectProviders(XMLConfigurator.java:238)
        at org.opensaml.core.xml.config.XMLConfigurator.load(XMLConfigurator.java:203)
        at org.opensaml.core.xml.config.XMLConfigurator.load(XMLConfigurator.java:188)
        at org.opensaml.core.xml.config.XMLConfigurator.load(XMLConfigurator.java:162)
        at org.opensaml.core.xml.config.AbstractXMLObjectProviderInitializer.init(AbstractXMLObjectProviderInitializer.java:52)
        at org.opensaml.core.xml.config.XMLObjectProviderInitializer.init(XMLObjectProviderInitializer.java:45)
        at org.opensaml.core.config.InitializationService.initialize(InitializationService.java:56)
        at org.pac4j.saml.util.Configuration.bootstrap(Configuration.java:76)
        at org.pac4j.saml.util.Configuration.<clinit>(Configuration.java:49)
        ... 43 more""
",smartshark_2_2,308,knox,"""[0.9778239727020264, 0.022176047787070274]"""
1613,229872,Inconsistent WebSSOResourceTest whitelist tests,"org.apache.knox.gateway.service.knoxsso.WebSSOResourceTest
 * testDefaultWhitelistLocalhostByAddress
 * testDefaultWhitelistLocalhostByName",smartshark_2_2,617,knox,"""[0.9947742819786072, 0.005225729662925005]"""
1614,229873,Test LDAP Authentication+Authorization from KnoxCLI,"Add a new command to the KnoxCLI that allows a user to connect, authenticate and authorize using a topology configuration settings.

Command usage: auth-test --cluster clustername [--u username] [--p password] [--g] [--d]

Required Argument:
--cluster clustername -- specifies the cluster/topology one wishes to use to connect to the LDAP server

Optional Arguments:
--u: supply a username
--p supply a password
--g specify that you want to return the groups a user is a member of
--d print debug output",smartshark_2_2,142,knox,"""[0.9985532164573669, 0.001446818234398961]"""
1615,229874,Knox doing SPNego with Hadoop for every client request is not scalable,"At present,  Knox talking to Kerberos protected  Hadoop cluster, does SPNegot authentication with Hadodop every request.  This is not scalable and needs to be fixed. I think we could cache hadoop.auth cookie returned by Hadoop service and avoid repeating SPNego authentication.  Requires more research, tests and some POC.",smartshark_2_2,1331,knox,"""[0.908410906791687, 0.09158910810947418]"""
1616,229875,Upgrade jansi to 1.17.1,jansi 1.16 to 1.17.1,smartshark_2_2,865,knox,"""[0.9981456995010376, 0.0018542981706559658]"""
1617,229876,Upgrade easymock to 3.6,easymock 3.5 to 3.6,smartshark_2_2,870,knox,"""[0.9977554678916931, 0.002244518371298909]"""
1618,229877,Upgrade maven-bundle-plugin to 4.0.0 ,maven-bundle-plugin 3.0.1 to 4.0.0,smartshark_2_2,868,knox,"""[0.9979903697967529, 0.0020095950458198786]"""
1619,229878,Attempt to reparse topology files to recover from overlapping write,"In gateway-server/src/main/java/org/apache/hadoop/gateway/topology/file/FileTopologyProvider.java
method loadTopology there needs to be a loop from the digester.parse onward.  Within the loop there should be a try/catch for IOException and SAXException.  If either of these are caught there should be a small sleep (e.g. 50ms) and then the parse should be reattempted.  There should be a timeout for the whole loop such that it gives up within a short period (e.g. 250ms) and rethrows the last IOException or SAXException encountered. ",smartshark_2_2,1485,knox,"""[0.9300549030303955, 0.06994505971670151]"""
1620,229879,Replace redundant types with the diamond operator,Replace redundant types with the diamond operator,smartshark_2_2,1043,knox,"""[0.9982675313949585, 0.0017324257642030716]"""
1621,229880,Improve diagnostic logging of HTTP traffic,Currently it is very difficult to gain visibility into the HTTP traffic on the between the client and Knox.  It would be ideal to have similar wire level logging as is offered by the org.apache.http.wire Log4J loggers.,smartshark_2_2,168,knox,"""[0.9975965619087219, 0.0024033868685364723]"""
1622,229881,Add provider name to test topologies to prevent intermittent test failures,The addition of alternate authentication providers can cause intermittent failures in the functional tests.  The functional tests all need to be updated to specify authentication providers by name.,smartshark_2_2,1575,knox,"""[0.9980401396751404, 0.0019598668441176414]"""
1623,229882,Add unit and functional testing for HBase,Add unit and functional tests for HBase integration.,smartshark_2_2,1322,knox,"""[0.998033344745636, 0.00196670088917017]"""
1624,229883,Guard Against Missing Subject in Identity Assertion,"Within the CommonIdentityAssertionFilter class, it is possible the evaluation of the Subject can return null.  A check should be added for this, error logged and IllegalStateException exception thrown.",smartshark_2_2,930,knox,"""[0.8251770734786987, 0.17482294142246246]"""
1625,229884,"knox needs to support basedn,  search attribute based LDAP authentication","To set the context,  here is the authentication provider specification in a Knox topology file:

 <provider>
            <role>authentication</role>
            <enabled>true</enabled>
            <name>ShiroProvider</name>
            <param>
                <name>main.ldapRealm</name>
                <value>org.apache.shiro.realm.ldap.JndiLdapRealm</value>
            </param>
            <param>
                <name>main.ldapRealm.userDnTemplate</name>
                <value>uid={0},ou=people,dc=hadoop,dc=apache,dc=org</value>
            </param>
            <param>
                <name>main.ldapRealm.contextFactory.url</name>
                <value>ldap://localhost:33389</value>
            </param>
            <param>
                <name>main.ldapRealm.contextFactory.authenticationMechanism</name>
                <value>simple</value>
            </param>
            <param>
                <name>urls./**</name>
                <value>authcBasic</value>
            </param>
        </provider>

This allows configurable userDnTemplate to infer the bindDN based on the  authenticating user name.

However,  in enterprise use cases,  it is not always possible to infer bindDN based on authenticating username using a template like this.
We have to do a search in the directory based on the userName mapped to a configurable attribute name to find the userDN.  This means,  we should add at least one additional configuration parameter such as 
userSearchTemplate.

An example value for userSearchTemplate
(&(uid={0})(objectclass=inetorgperson))

BaseDN for search can be specified as part of

contextFactory.url





",smartshark_2_2,1546,knox,"""[0.8938542008399963, 0.10614577680826187]"""
1626,229885,"Spark page characters ""&amp;"" should be treated as a delimiter","I am a beginner of apache knox. When I use knox to proxy my sparkhistoryui, I meet this problem.

{noformat} <a href=""/history/application_1505879554093_0036/stages/stage?id=0&amp;attempt=0"" class=""name-link"">{noformat}

Some query parameters in the response body of the spark history ui contain a {noformat}&amp;{noformat}
org.apache.hadoop.gateway.util.urltemplate.Parser#consumeQueryToken does not handle this case

{code:java}
StringTokenizer tokenizer = new StringTokenizer( token, ""?&"" );
      while( tokenizer.hasMoreTokens() ) {
        consumeQuerySegment( builder, tokenizer.nextToken() );
      }
{code}
so,I changed the code block as bellows
{code:java}
String[] tokens = token.split(""(&amp;|\\?|&)"");
      if (tokens != null){
        for (String nextToken : tokens){
          consumeQuerySegment(builder,nextToken);
        }
      }
{code}
",smartshark_2_2,788,knox,"""[0.5979676246643066, 0.40203240513801575]"""
1627,229886,Adding example PAM config for Ubuntu in Knox User Guide under PAM Based Authentication section,Current documentation covers an example of PAM configuration on OSX. It will be useful if we can add at least one linux variant example to help deploy Knox in Linux environment configured for PAM based authentication.,smartshark_2_2,504,knox,"""[0.9973869919776917, 0.0026129691395908594]"""
1628,229887,Builds of src distributions result in unexpected result from gateway version API,"When a Knox build is executed outside of a git repo (e.g., src distributions), then the build.hash property in build.properties is not populated with an actual value.

This manifests as an unexpected response to the gateway version API:
{
    ""ServerVersion"" : {
       ""version"" : ""0.14.0"",
       ""hash"" : ""${buildNumber}""
}

This is due to the fact that the buildnumber-maven-plugin attempts to invoke 'git rev-parse --verify HEAD', which fails because it's not in a git repo.

This plugin apparently has a facility for specifying an alternate value when an scm error is encountered. It would be nice if Knox could employ that facility to provide a better value than ${buildNumber} in these cases.",smartshark_2_2,1072,knox,"""[0.9671146273612976, 0.03288538008928299]"""
1629,229888,Test GatewayLdapPosixGroupFuncTest failing intermittently,"The test GatewayLdapPosixGroupFuncTest is failing intermittently with a 403 error.  The failure is below.
{code}
Results :

Failed tests:
  GatewayLdapPosixGroupFuncTest.testGroupMember:286 3 expectations failed.
Expected status code <200> doesn't match actual status code <403>.

Expected content-type ""text/plain"" doesn't match actual content-type ""text/html;charset=ISO-8859-1"".

Response body doesn't match expectation.
Expected: is ""test-service-response""
  Actual: <html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html; charset=ISO-8859-1""/>
<title>Error 403 Forbidden</title>
</head>
<body><h2>HTTP ERROR 403</h2>
<p>Problem accessing /gateway/test-cluster/test-service-path/test-service-resource. Reason:
<pre>    Forbidden</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

</body>
</html>
{code}",smartshark_2_2,288,knox,"""[0.9676826596260071, 0.03231726959347725]"""
1630,229889,Make Topology Instance Available in WebContext for Runtime Access,"There are times when access to the topology and the configured services within it is required by an application, service or provider at runtime.

For instance, we need to be able to check the whitelist expression that is configured in the KnoxSSO service from within the KnoxAuth application. Rather than redundantly configuring it in both places or making a topology specific config item global within gateway-site.xml we should provide access to the topology object itself for all apps, services, providers processing requests within a topology.",smartshark_2_2,648,knox,"""[0.9984702467918396, 0.0015297651989385486]"""
1631,229890,Support JDK 9/10/11,"This Jira is to track supporting JDK 9/10/11 for Apache Knox. With JDK 11 being released GA yesterday 9/25, we should see what needs to be investigated to make Apache Knox work.",smartshark_2_2,761,knox,"""[0.9982994198799133, 0.001700639957562089]"""
1632,229891,Remove javancss-maven-plugin,"JavaNCSS Maven plugin [1] is not used anymore. The plugin hasn't been updated since 2015. ""mvn site"" fails due to this plugin currently so I'm assuming the reports haven't been used recently. The plugin may result in a failure if code complexity is above a 100.Â 
 # http://www.mojohaus.org/javancss-maven-plugin/index.html
 # https://mail-archives.apache.org/mod_mbox/knox-dev/201810.mbox/%3CCAJU9nmiNnuFDjpYOa4_Bh8WM10w4ya4qwsqj7w2WEBL-cD7d5A%40mail.gmail.com%3E",smartshark_2_2,851,knox,"""[0.9982342720031738, 0.0017657402204349637]"""
1633,229892,Create an Admin API to return a topology status,"Currently we have Admin api to return the list of topologies ( which will also return the topology which is not deployed completely as it just list the files in topology directory )

Need to have an Admin api to return the status ofÂ a topology ( WhetherÂ topology is activatedÂ or not)",smartshark_2_2,607,knox,"""[0.9984381794929504, 0.0015618348261341453]"""
1634,229893,Cleanup gateway-group no value errors,"INFO    : expression: gateway-group no value 
This message is printed repeatedly when doing a mvn versions:set. This should be cleanedup. gateway-group can be hardcoded back to org.apache.knox",smartshark_2_2,950,knox,"""[0.9983735084533691, 0.001626471639610827]"""
1635,229894,Make the Default Ephemeral DH Key Size 2048 with Ability to Override,"See description of logjam
""The Logjam Attack""
https://weakdh.org/


To test you should do:
[root@bdvs1392 logs]# openssl s_client -connect bdvs1392.svl.ibm.com:8443 -cipher ""EDH"" | grep ""Server Temp Key""
depth=0 C = US, ST = Test, L = Test, O = Hadoop, OU = Test, CN = bdvs1392.svl.ibm.com
verify error:num=18:self signed certificate
verify return:1
depth=0 C = US, ST = Test, L = Test, O = Hadoop, OU = Test, CN = bdvs1392.svl.ibm.com
verify return:1
Server Temp Key: DH, 768 bits


The key should >= 1024",smartshark_2_2,186,knox,"""[0.9981112480163574, 0.001888787024654448]"""
1636,229895,Add submitSqoop via knox shell api,"Add ability to submitSqoop job via knox since templeton also support this functionality. I have a basic patch that I am trying and will submit it. Would be something like this:

{code:java}
sqoop_command = ""import --connect ...""
jobId = Job.submitSqoop(session) \
            .command(sqoop_command) \
            .statusDir(""${jobDir}/output"") \
            .now().jobId
{code}",smartshark_2_2,477,knox,"""[0.9985796213150024, 0.0014203583123162389]"""
1637,229896,Add test to ensure regex or support in RegexIdentityAssertionFilter,Created a test to ensure that regex or operations worked with both email addresses (e.g. member@apache.org) and simple usernames (e.g. member) for RegexIdentityAssertionFilter.Â  This did work properly and I'd like to add the test to ensure it continues to work.,smartshark_2_2,1228,knox,"""[0.9969614148139954, 0.0030386135913431644]"""
1638,229897,Dispatch Refactoring Breaks Upgrade Compatibility,"The refactoring of the dispatch code introduced an incompatibility for upgrades where the data directory is preserved. The gateway.xml file from the previous version has old classnames configured that are still valid classes but no longer Filters.

We need to continue the refactoring to make the original classes valid Filters that extend the new DispatchFilter and provides whatever is needed in terms of new dispatch-impl config based on the refactoring.",smartshark_2_2,102,knox,"""[0.7975439429283142, 0.20245607197284698]"""
1639,229898,"Hive ""select *"" performance evaluation","While looking at WebHDFS performance inÂ KNOX-1221, I decided to look a bit more into performance for common use cases. Hive performance is another area that could use some research.

Use ""select * ... limit"" to get a comparison of raw return speed from HiveServer2. This should show how fast results can be streamed through HiveServer2 and Knox. Compare the results to ""hdfs dfs -text"" since this will render the data directly from HDFS. This should give comparisons for the difference in overhead between HDFS, HiveServer2 binary, HiveServer2 HTTP, and HiveServer2 HTTP with Knox.",smartshark_2_2,937,knox,"""[0.9983243346214294, 0.0016756406985223293]"""
1640,229899,Rename Hadoop class in KnoxShell to KnoxSession,Refactor KnoxShell Hadoop class to a KnoxSession but maintain backward compatibility for existing scripts and lib usage. Deprecate the Hadoop class to remove later.,smartshark_2_2,1045,knox,"""[0.9980522394180298, 0.0019477561581879854]"""
1641,229900,Optimize File Upload to Webhdfs,"We need to not send ""100 continue"" to a client that sends an Expect 100 continue until after the 307 redirect to the datanode. Otherwise, we end up penalizing performance by sending it more than once.",smartshark_2_2,18,knox,"""[0.998115062713623, 0.001884925295598805]"""
1642,229901,Identity Broker API,"Ability to request a token on behalf of another user w/ cryptographically verifiable trust relationship. This is essentially an extension of the KnoxToken service where the username of the authenticated user is presented and the resulting token should represent
Ability to specify what type of token is being requested. There are use cases where Knox may be expected to interact with another STS service in order to acquire another token. This may need to be addressed as a separate REST resource and API or perhaps it can be a subtype of a more generic token response. Current KnoxToken API response looks something like:

bq. {
""access_token"":""eyJhbGciOiJSUzI1NiJ9.
                 eyJzdWIiOiJndWVzdCIsImF1ZCI6InRva2VuYmFzZWQiLCJpc3MiOiJLTk9YU1NPIiwiZXhwIjoxNDkzNTM4MjY1fQ.
                 FHsIdhlCi_h61PEXoKSbIEp5AlnVe9U5uEgcp7ktmVS8kLClFD2dj0KS-8sSnvNiPYdyZhEvxqNhjmhqXMd2YQz97O6FUSGf69_
                 lmarJPjz9K_6sDgrgpZnVQhUfHUG3k6-zetqzKZhu3gZYVLfu36TXb3C62TfXIrPF2qI9psM"",
  ""target_url"":""https://localhost:8443/gateway/tokenbased"",
  ""token_type"":""Bearer "",
  ""expires_in"":1493538265484
}

It is possible that the above could be used to represent multiple token types by adding additional token_type values for the client to interrogate and handle appropriately.

Of course, a client should request a token from a KnoxToken service that is configured to provide the desired token.
Perhaps, we limit an endpoint to a single token_type or we could configure a single service endpoint to service requests for certain types and rely on the client to ask for the desired one and default to current 'Bearer' type.
Where 'Bearer' implies our Knox JWT token.
Other potential types would include things like S3 or AWS tokens.
We may also want to consider base64 encoding the token for certain types.
Clients would have to know whether it needs to be decoded based on the type.
",smartshark_2_2,605,knox,"""[0.9983727931976318, 0.0016271461499854922]"""
1643,229902,Upgrade eclipselink to 2.7.3,eclipselink 2.5.2 -> 2.7.3,smartshark_2_2,875,knox,"""[0.9979761242866516, 0.0020238650031387806]"""
1644,229903,Knox SSO - some config variables not getting picked up.,"Config variables ""defaultUrl"", ""multiProfile"" and ""renewSession"" are not getting picked up.",smartshark_2_2,1098,knox,"""[0.37960493564605713, 0.6203950047492981]"""
1645,229904,use EXAMPLE.COM instead of sample.com in template files for kerberos relam,"MIT kerberos by default creates krb5.conf with EXAMPLE.COM realm.
So, it is better to use EXAMPLE.COM in our templates instead of SAMPLE.COM in our template files",smartshark_2_2,1298,knox,"""[0.9983254075050354, 0.001674562692642212]"""
1646,229905,User's guide needs update after trying examples,Various examples and some description text need minor tweaks after review.,smartshark_2_2,116,knox,"""[0.9979752898216248, 0.0020247329957783222]"""
1647,229906,Upgrade Jetty,We should upgrade Jetty from 9.2.15.v20160210 to 9.2.22.v20170606.,smartshark_2_2,1263,knox,"""[0.9981552958488464, 0.001844726037234068]"""
1648,229907,gateway-shell-release doesn't include shaded jar,The knoxshell jarÂ is being packaged in the gateway-shell-release by the assembly plugin before it is shaded. This means that there are no classes in the knoxshell jar. The order of the plugins must be shade-plugin -> assembly-plugin. This was broken byÂ KNOX-1463,smartshark_2_2,906,knox,"""[0.9571633338928223, 0.04283665865659714]"""
1649,229908,Reame PostAuthenticationFilter to ShiroSubjectIdentityAdapter,"org.apache.hadoop.gateway.filter.PostAuthenticationFilter - the name does not convey the responsibility it fulfills.

The responsibility this class fulfills is: mapping Shiro Subject into javax.security.auth.Subject. In the light of this,  we would rename the class as org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter
",smartshark_2_2,1416,knox,"""[0.9983822107315063, 0.0016177438665181398]"""
1650,229909,Build break with JVM IBM JAVA,"
The build failed with JVM IBM JAVA : Java version: 1.7.0, vendor: IBM Corporation
[INFO] 30 errors
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] gateway ........................................... SUCCESS [  0.510 s]
[INFO] gateway-test-utils ................................ SUCCESS [  1.631 s]
[INFO] gateway-i18n ...................................... SUCCESS [  0.290 s]
[INFO] gateway-util-common ............................... SUCCESS [  0.566 s]
[INFO] gateway-util-launcher ............................. SUCCESS [  0.190 s]
[INFO] gateway-util-urltemplate .......................... SUCCESS [  0.345 s]
[INFO] gateway-test-ldap ................................. SUCCESS [  0.349 s]
[INFO] gateway-test-ldap-launcher ........................ SUCCESS [  0.030 s]
[INFO] gateway-i18n-logging-log4j ........................ SUCCESS [  0.064 s]
[INFO] gateway-i18n-logging-sl4j ......................... SUCCESS [  0.060 s]
[INFO] gateway-spi ....................................... FAILURE [  0.701 s]
...
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /home/pascal/release/ibmsoe/knox/gateway-spi/src/main/java/org/apache/hadoop/gateway/services/security/impl/BaseKeystoreService.java:[88,9] cannot find symbol
  symbol:   class X509CertInfo
...

On /home/pascal/release/ibmsoe/knox/gateway-spi/src/main/java/org/apache/hadoop/gateway/services/security/impl/BaseKeystoreService.java

import sun.security.x509.*; is not available with the JVM IBM JAVA

",smartshark_2_2,54,knox,"""[0.9792917370796204, 0.020708298310637474]"""
1651,229910,Update samples to be consistent with Hive 0.12 configuration parameters names.,Hive 0.12 release made some changes in configuration parameters names for HTTP transport mode. We need to update samples to be consistent with Hive 0.12.,smartshark_2_2,1464,knox,"""[0.9981949925422668, 0.0018050780054181814]"""
1652,229911,Filter order in generated gateway.xml needs to be consistent,"On reviewing the gateway.xml in the deployed knox webapp, shows filter order is not consistent  across resources.

In most of the resources, rewrite filter comes before idenity-assertion filter. But,  we also see rewrite filer coming after  identity-assertion filter.

This could be pointing to a problem in the logic that builds gateway.xml",smartshark_2_2,112,knox,"""[0.7655209898948669, 0.23447902500629425]"""
1653,229912,Support configuration driven REST API integration (aka Stacks),The gateway server has a framework that allows for plugging in new 'services'.  The framework requires code to be written for every new service that is added in. We need a way to make adding new services easy and make it possible to add a new service without writing any code (using configuration instead) while leaving open the flexibility of being able to write code if necessary.,smartshark_2_2,97,knox,"""[0.9983603358268738, 0.0016396637074649334]"""
1654,229913,Support Providing Your own SSL Certificate,"For 0.3.0 we will support key passphrases that are the same as the keystore password.
In order to generate your own selfsigned cert/keystore and credential store:

Gateway identity keystore:
keytool -genkey -keyalg RSA -alias gateway-identity -keystore gateway.jks -storepass {masterpassword} -validity 360 -keysize 2048

Gateway credential store:
keytool -genkey -alias credstore -keystore __gateway-credentials.jceks -storepass {masterpassword} -validity 360 -keysize 1024 -storetype JCEKS

Place these files in the {GATEWAY_HOME}/conf/security/keystores directory prior to server startup.

NOTE: that the same masterpassword is being used for each store.",smartshark_2_2,1384,knox,"""[0.9985532164573669, 0.0014468211447820067]"""
1655,229914,Constrain cookies added to the HadoopAuthCookieStore,Constrain the cookies added to the HadoopAuthCookieStore to only those associated with the Knox principal.,smartshark_2_2,1279,knox,"""[0.9872654676437378, 0.01273458544164896]"""
1656,229915,knox to uptake Apache DS  2.0.0-M16 or later,"Apache Ranger pointing to ApacheDS bundled with Knox is hitting ApacheDS bug
https://issues.apache.org/jira/browse/DIRSERVER-1917
This seems to be fixed in Apache DS  2.0.0-M16.
Hence, requesting Knox to uptake  Apache DS  2.0.0-M16 or later

Exception stack trace in Apache Ranger logs when it hits the ApacheDS bug:
javax.naming.NamingException: [LDAP: error code 80 - OTHER: failed for MessageType : SEARCH_REQUEST
Message ID : 2
    SearchRequest
        baseDn : 'ou=people,dc=hadoop,dc=apache,dc=org'
        filter : '(objectclass=person)'
        scope : whole subtree
        typesOnly : false
        Size Limit : no limit
        Time Limit : no limit
        Deref Aliases : deref Always
        attributes : 'uid', 'ismemberof', 'memberof'
org.apache.directory.api.ldap.model.message.SearchRequestImpl@7c51bb2    OpaqueControl Control
        Type OID    : '1.2.840.113556.1.4.319'
        Criticality : 'false'
'
: org.apache.directory.api.ldap.codec.BasicControlDecorator cannot be cast to org.apache.directory.api.ldap.codec.controls.search.pagedSearch.PagedResultsDecorator]; remaining name 'ou=people,dc=hadoop,dc=apache,dc=org'
at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3131)
at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3033)
at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2840)
at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1849)
at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1772)
at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:386)
at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:356)
at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:339)
at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)
at com.xasecure.ldapusersync.process.LdapUserGroupBuilder.updateSink(LdapUserGroupBuilder.java:195)
at com.xasecure.usergroupsync.UserGroupSync.run(UserGroupSync.java:59)
at java.lang.Thread.run(Thread.java:745)",smartshark_2_2,381,knox,"""[0.9529806971549988, 0.04701927676796913]"""
1657,229916,Upgrade Knox's Jetty dependency to latest 9.x,There are a number of advantages to Jetty 9 from a configuration and dependency management perspective.  It would be idea to upgrade to take advantage of these as well as making it easier to stay abreast of any fixes that might come from the Jetty community.,smartshark_2_2,322,knox,"""[0.998185932636261, 0.0018141046166419983]"""
1658,229917,Provide Whitelisting for Redirect Destinations for KnoxSSO,"In order for KnoxSSO to redirect back to the originally requested resource after authentication, the participating application provides a query parameter indicating the originalURL. We need to be able to restrict the URLs that can be provided here so that it can't be misused.",smartshark_2_2,189,knox,"""[0.9980974793434143, 0.0019025728106498718]"""
1659,229918,Update Apache Shiro dependency,"The Apache Shiro dependency should be updated due to CVE-2016-4437:

https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-4437",smartshark_2_2,401,knox,"""[0.9982507824897766, 0.0017491807229816914]"""
1660,229919,Improvements for Oozie in the ClientDSL,"Implements: list, kill, change, info, definition, rerun, resume, log, start, dryrun",smartshark_2_2,422,knox,"""[0.9982141256332397, 0.0017858408391475677]"""
1661,229920,Add spark job server as a service to knox gateway,"spark-jobserver (https://github.com/spark-jobserver/spark-jobserver) provides a RESTful interface for submitting and managing Apache Spark jobs, jars, and job contexts.

This patch adds the spark-jobserver rest api as a service to the knox gateway.",smartshark_2_2,1009,knox,"""[0.9984013438224792, 0.0015986633952707052]"""
1662,229921,Upgrade Rest-Assured test dependency to latest version,The version of Rest-Assured being used is currently three years old and no longer matches the easily accessible documentation very well.  Along with all of the other dependency updates it would be nice to update this one as well.,smartshark_2_2,993,knox,"""[0.9983152151107788, 0.0016847653314471245]"""
1663,229922,Proxy support for Solr UI,Provide proxy UI support for the Solr UI. ,smartshark_2_2,490,knox,"""[0.998369038105011, 0.001630957005545497]"""
1664,229923,Should not send Knox stack trace to client in error responses,"Currently if an exception occurs in the processing of a request that exception stack trace is included in the response to the client.  The curl command below was executed against a Knox instance that didn't have any of the backend Hadoop services running.

{code}
curl -k -u guest:guest-password -X GET ""https://localhost:8443/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS""
{code}

{code}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html; charset=ISO-8859-1""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /gateway/sandbox/webhdfs/v1/. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>org.apache.shiro.subject.ExecutionException: java.security.PrivilegedActionException: java.io.IOException: Service connectivity error.
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:385)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter.doFilter(ShiroSubjectIdentityAdapter.java:74)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ResponseCookieFilter.doFilter(ResponseCookieFilter.java:38)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.GatewayFilter.doFilter(GatewayFilter.java:128)
	at org.apache.hadoop.gateway.GatewayServlet.service(GatewayServlet.java:117)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:695)
Caused by: java.security.PrivilegedActionException: java.io.IOException: Service connectivity error.
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:127)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:77)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	... 48 more
Caused by: java.io.IOException: Service connectivity error.
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.executeRequest(HttpClientDispatch.java:120)
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.doGet(HttpClientDispatch.java:241)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch$GetAdapter.doMethod(AbstractGatewayDispatch.java:134)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch.doFilter(AbstractGatewayDispatch.java:64)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.doFilterInternal(AbstractIdentityAssertionFilter.java:209)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.continueChainAsPrincipal(AbstractIdentityAssertionFilter.java:157)
	at org.apache.hadoop.gateway.identityasserter.filter.IdentityAsserterFilter.doFilter(IdentityAsserterFilter.java:55)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.rewrite.api.UrlRewriteServletFilter.doFilter(UrlRewriteServletFilter.java:60)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:93)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:90)
	... 55 more
</pre>
<h3>Caused by:</h3><pre>java.security.PrivilegedActionException: java.io.IOException: Service connectivity error.
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:127)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:77)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter.doFilter(ShiroSubjectIdentityAdapter.java:74)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ResponseCookieFilter.doFilter(ResponseCookieFilter.java:38)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.GatewayFilter.doFilter(GatewayFilter.java:128)
	at org.apache.hadoop.gateway.GatewayServlet.service(GatewayServlet.java:117)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:695)
Caused by: java.io.IOException: Service connectivity error.
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.executeRequest(HttpClientDispatch.java:120)
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.doGet(HttpClientDispatch.java:241)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch$GetAdapter.doMethod(AbstractGatewayDispatch.java:134)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch.doFilter(AbstractGatewayDispatch.java:64)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.doFilterInternal(AbstractIdentityAssertionFilter.java:209)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.continueChainAsPrincipal(AbstractIdentityAssertionFilter.java:157)
	at org.apache.hadoop.gateway.identityasserter.filter.IdentityAsserterFilter.doFilter(IdentityAsserterFilter.java:55)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.rewrite.api.UrlRewriteServletFilter.doFilter(UrlRewriteServletFilter.java:60)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:93)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:90)
	... 55 more
</pre>
<h3>Caused by:</h3><pre>java.io.IOException: Service connectivity error.
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.executeRequest(HttpClientDispatch.java:120)
	at org.apache.hadoop.gateway.dispatch.HttpClientDispatch.doGet(HttpClientDispatch.java:241)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch$GetAdapter.doMethod(AbstractGatewayDispatch.java:134)
	at org.apache.hadoop.gateway.dispatch.AbstractGatewayDispatch.doFilter(AbstractGatewayDispatch.java:64)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.doFilterInternal(AbstractIdentityAssertionFilter.java:209)
	at org.apache.hadoop.gateway.filter.security.AbstractIdentityAssertionFilter.continueChainAsPrincipal(AbstractIdentityAssertionFilter.java:157)
	at org.apache.hadoop.gateway.identityasserter.filter.IdentityAsserterFilter.doFilter(IdentityAsserterFilter.java:55)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.rewrite.api.UrlRewriteServletFilter.doFilter(UrlRewriteServletFilter.java:60)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:93)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain$1.run(ShiroSubjectIdentityAdapter.java:90)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:127)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter$CallableChain.call(ShiroSubjectIdentityAdapter.java:77)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.hadoop.gateway.filter.ShiroSubjectIdentityAdapter.doFilter(ShiroSubjectIdentityAdapter.java:74)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.filter.ResponseCookieFilter.doFilter(ResponseCookieFilter.java:38)
	at org.apache.hadoop.gateway.filter.AbstractGatewayFilter.doFilter(AbstractGatewayFilter.java:60)
	at org.apache.hadoop.gateway.GatewayFilter$Holder.doFilter(GatewayFilter.java:311)
	at org.apache.hadoop.gateway.GatewayFilter$Chain.doFilter(GatewayFilter.java:212)
	at org.apache.hadoop.gateway.GatewayFilter.doFilter(GatewayFilter.java:128)
	at org.apache.hadoop.gateway.GatewayServlet.service(GatewayServlet.java:117)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:695)
</pre>
<hr /><i><small>Powered by Jetty://</small></i><br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

</body>
</html>
{code}",smartshark_2_2,1483,knox,"""[0.9280874133110046, 0.07191260159015656]"""
1665,229924,JWT Cookie Based Federation Provider,"Adding support for JWT tokens via HTTP Cookies will allow for applications that utilize the KnoxSSO WebSSO capabilities to call REST APIs through Knox from javascript in the browser.

This patch will require the acceptance and validation of KnoxSSO cookies/tokens as well as the introduction of CORS capabilities in order to facilitate cross origin resource sharing.",smartshark_2_2,197,knox,"""[0.9982448816299438, 0.0017551197670400143]"""
1666,229925,Enable Jetty's JSP support in Knox hosted applications.,Following the upgrade to Jetty 9.2 it should now possible to enable Jetty's JSP support fairly simply.  This may enable us to host Shibboleth and give is a fairly compelling SAML SSO story at least for demonstration purposes.,smartshark_2_2,336,knox,"""[0.9983052015304565, 0.001694750040769577]"""
1667,229926,Use session instead of hadoop in client DSL samples,Use session instead of hadoop in client DSL samples,smartshark_2_2,1332,knox,"""[0.9970417618751526, 0.0029582767747342587]"""
1668,229927,Monitor Ambari for Cluster Topology changes,"Knox should provide the option to monitor an Ambari cluster for which it has deployed a topology, in order to notice configuration changes that affect the service URLs for that topology. Upon noticing such a change, Knox should respond by updating the deployed topology appropriately.
",smartshark_2_2,780,knox,"""[0.9984470009803772, 0.001552995410747826]"""
1669,229928,Remove references to deprecated httpclient class DefaultHttpClient,"Need to get rid of deprecated DefaultHttpClient class references. Most, if not all, occur in tests at this point since the Dispatch framework has already moved on to the new classes and API from httpclient. 

Besides cleaning up, the tests appear to have some erratic behavior on some OS/jdk combinations with this deprecated class. These random test failures go away when using the new classes.",smartshark_2_2,680,knox,"""[0.9983789920806885, 0.0016209500608965755]"""
1670,229929,gateway-i18n-logging-sl4j refers to Log4jMessageLoggerFactory,"The gateway-i18n-logging-sl4j/src/main/resources/META-INF/services/org.apache.hadoop.gateway.i18n.messages.MessageLoggerFactory file contains:

org.apache.hadoop.gateway.i18n.messages.loggers.sl4j.Log4jMessageLoggerFactory

instead of:

org.apache.hadoop.gateway.i18n.messages.loggers.sl4j.Sl4jMessageLoggerFactory
",smartshark_2_2,83,knox,"""[0.9810096025466919, 0.01899043284356594]"""
1671,229930,Simplify Configuration for Default Topology,"Currently, there is a configuration element for the path to forward the incoming request to. This should be derived from the default topology file name not a separate config element.",smartshark_2_2,33,knox,"""[0.9984363913536072, 0.0015635762829333544]"""
1672,229931,Add signature validation tests for the JWT filters,Currently we don't test signature validation in the JWT filters - this task is to add some tests for this.,smartshark_2_2,684,knox,"""[0.9984203577041626, 0.0015796595253050327]"""
1673,229932,Remove hsso-release module from build,Remove the hsso release artifact from the build as the functionality has migrated to be part of the Knox gateway release artifact.  The hsso release artifact was never released as a public artifact.,smartshark_2_2,289,knox,"""[0.9975047707557678, 0.0024952806998044252]"""
1674,229933,Add README to Samples and Documentation,"We need to ensure better OOTB experience with the samples provided with the distribution. Let's add a README to the samples directory that points to a new section in the users guide.

The samples section in the users guide will describe the assumptions made by the samples and the steps to ensure that they are fully installed and configured in various deployment scenarios.",smartshark_2_2,82,knox,"""[0.9970455765724182, 0.00295439874753356]"""
1675,229934,support websocket endpoint rewrite,"As knox websocket consumer, we have one webapp provide the SQL execution service. We use websocket to provider better experience which will notify customer whenever there is notification.
  We have the websocket backend ws://localhost:11080/message-service/messages. we add some extra path and query to provider different service. For example:
ws://localhost:11080/message-service/messages/subscribe?name=XXXX 
provide message channel subscribe service

ws://localhost:11080/message-service/messages/topic?name=XXXX
provide message channel for specific features

ws://localhost:11080/message-service/messages/unsubscribe?name=XXXX
provide message channel for unsubscribe service

I want to define the rewrite.xml as something like 
<rules>
  <rule dir=""IN"" name=""WSSERVICE/ws/inbound"" pattern=""*://*:*/**/message-service/messages/**?{**}"">
    <rewrite template=""{$serviceUrl[WSSERVICE]}/**?{**}""/>
  </rule>
</rules>

and configure service like 
<service>
        <role>WSSERVICE</role>
        <url>ws://localhost:8000/message-service/messages</url>
</service>
",smartshark_2_2,803,knox,"""[0.998555600643158, 0.0014443540712818503]"""
1676,229935,Bump commons-* dependency versions,"The following commons-* dependencies are out of date:
 * commons-cli - 1.2 -> 1.4
 * commons-codec - 1.7 -> 1.11
 * commons-configuration - 1.6 to 1.10
 * commons-io - 2.4 -> 2.6
 * commons-lang3 - 3.4 -> 3.8
 * commons-net - 1.4.1 -> 3.6

The only big bump is commons-net jumping 2 major versions.Â ",smartshark_2_2,846,knox,"""[0.9982919096946716, 0.0017081382684409618]"""
1677,229936,In Knox can we capture Bytes transfered info in logs,"Hi Knox-team,

In knox can we capture the bytes transferred information along with file name etc. This would help us to track how much transaction each users are doing  via knox , Any method or thoughts on this?

 We make use of audit logs to capture each user how many transaction doing per hr per day. But we like to capture the details of how much bytes per transaction users are doing which will help us for various cases to manage Hadoop and Knox better. Any thoughts on this ? 
",smartshark_2_2,789,knox,"""[0.9984292387962341, 0.0015707112615928054]"""
1678,229937,Complete transition to TLP,"Complete all tasks described in Life After Graduation
http://incubator.apache.org/guides/graduation.html#life-after-graduation",smartshark_2_2,44,knox,"""[0.9983047246932983, 0.0016952842706814408]"""
1679,229938,Upgrade the transitive springframework spring core from pac4j,"We need to upgrade the transitive dependency from pac4j on springframework 4.3.7.RELEASE to 4.3.13.RELEASE. Since, pac4j doesn't have a 2.x.x release with this upgrade we should be able to just exclude it from the pac4j dependency and explicitly upgrade it in the same pom.",smartshark_2_2,1260,knox,"""[0.9982981085777283, 0.0017019161023199558]"""
1680,229939,Update Documentation for Knox 1.0.0,"With the release of 1.0.0 we also need to update the documentation to reflect the changes, things like properties which need to updated from ""org.apache.hadoop.gateway.*"" to ""org.apache.knox.gateway.*""",smartshark_2_2,1080,knox,"""[0.9982941746711731, 0.001705806003883481]"""
1681,229940,Minor code improvements,"This task encapsulates some minor code improvements, mainly relating to the tests:

 - Use !collection.isEmpty() instead collections.size() > 0
 - Use ""literal"".equals(string) instead of string.equals(""literal"")
 - Remove all boolean == true and  == false
 - Remove Vector and HashTable in a few places (test code only) where it is not required.
 - Replace new Integer with Integer.valueOf().",smartshark_2_2,682,knox,"""[0.9982612729072571, 0.0017387294210493565]"""
1682,229941,Admin UI Provider Config wizard URL validation requires a port,"URL values specified in the provider configuration wizard fields are required to have ports in order to be valid. This should not be the case, since a valid URL may omit the port detail, implying the default.",smartshark_2_2,628,knox,"""[0.779056966304779, 0.22094309329986572]"""
1683,229942,Upgrade curator to 4.0.1,curator-client/curator-framework/curator-recipes 4.0.0 to 4.0.1,smartshark_2_2,874,knox,"""[0.9981541037559509, 0.001845909864641726]"""
1684,229943,Service Level Authorization Provider with ACLs,"Provide a straight forward authorization provider that enforces user, group and ipaddress ACLs at the service level.",smartshark_2_2,1337,knox,"""[0.9980270266532898, 0.0019729016348719597]"""
1685,229944,Create windows service template file for LDAP server.,Should end up in <GATEWAY_HOME>/bin,smartshark_2_2,1489,knox,"""[0.9979352951049805, 0.002064683008939028]"""
1686,229945,Add mailing lists to top level pom,"{code:java}
<mailingLists>
	<mailingList>
	<name>Apache Knox User List</name>
	<subscribe>user-subscribe@knox.apache.org</subscribe>
	<unsubscribe>user-unsubscribe@knox.apache.org</unsubscribe>
	<post>user@knox.apache.org</post>
	<archive>https://mail-archives.apache.org/mod_mbox/knox-user/</archive>
	</mailingList>
	<mailingList>
	<name>Apache Knox Development List</name>
	<subscribe>dev-subscribe@knox.apache.org</subscribe>
	<unsubscribe>dev-unsubscribe@knox.apache.org</unsubscribe>
	<post>dev@knox.apache.org</post>
	<archive>https://mail-archives.apache.org/mod_mbox/knox-dev/</archive>
	</mailingList>
</mailingLists>
{code}",smartshark_2_2,767,knox,"""[0.9984130859375, 0.0015869468916207552]"""
1687,229946,Proper error message when root tag of topology file incorrect,"When a topology file is well formed XML but the root element is not <topology> the result is a NullPointerException similar to this at least as it was in the 0.4.0 code line.
{code}
2014-10-30 18:52:20,730 INFO  hadoop.gateway (GatewayServer.java:startGateway(240)) - Starting gateway...
2014-10-30 18:52:21,175 INFO  hadoop.gateway (GatewayServer.java:start(336)) - Loading topologies from directory: /usr/lib/knox/bin/../conf/topologies
2014-10-30 18:52:21,179 DEBUG hadoop.gateway (FileTopologyProvider.java:loadTopology(93)) - Loading topology file: /usr/lib/knox/bin/../conf/topologies/topology.xml
2014-10-30 18:52:21,276 ERROR hadoop.gateway (FileTopologyProvider.java:loadTopologies(143)) - Failed to load topology /usr/lib/knox/bin/../conf/topologies/topology.xml: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.loadTopologyAttempt(FileTopologyProvider.java:122)
        at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.loadTopology(FileTopologyProvider.java:98)
        at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.loadTopologies(FileTopologyProvider.java:134)
        at org.apache.hadoop.gateway.topology.file.FileTopologyProvider.reloadTopologies(FileTopologyProvider.java:154)
        at org.apache.hadoop.gateway.GatewayServer.start(GatewayServer.java:337)
        at org.apache.hadoop.gateway.GatewayServer.startGateway(GatewayServer.java:252)
        at org.apache.hadoop.gateway.GatewayServer.main(GatewayServer.java:112)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:622)
        at org.apache.hadoop.gateway.launcher.Invoker.invokeMainMethod(Invoker.java:70)
        at org.apache.hadoop.gateway.launcher.Invoker.invoke(Invoker.java:39)
        at org.apache.hadoop.gateway.launcher.Command.run(Command.java:101)
        at org.apache.hadoop.gateway.launcher.Launcher.run(Launcher.java:69)
        at org.apache.hadoop.gateway.launcher.Launcher.main(Launcher.java:46)
{code}",smartshark_2_2,114,knox,"""[0.2805202901363373, 0.7194796800613403]"""
1688,229947,Add support for load balancing across HA instances,"Knox should support load balancing across HA service instances rather than just offering failover.

One solution may be to add an option to the HAProvider. ",smartshark_2_2,929,knox,"""[0.9983797073364258, 0.0016203097766265273]"""
1689,229948,SSO topology name is hardcoded in knoxauth.js,"The topology name for Knox SSO is hardcoded in knoxauth.js file:

{code}
var loginURL = ""/gateway/knoxsso/api/v1/websso?originalUrl="";
{code}

If I create a proper topology file for Knox SSO under the name e.g something.xml then it does not work since  the knoxauth.js  always posts data to /gateway/knoxsso/api/v1... instead of /gateway/something/api/v1...

It would be better if the loginURL could be determined  from the actual URL, e.g. if the login page is loaded from:
{code}
/gateway/something/knoxauth/login.html
{code}
then the JS would post the request to:
{code}
/gateway/something/api/v1/websso?originalUrl=""
{code}",smartshark_2_2,491,knox,"""[0.8104557991027832, 0.1895441859960556]"""
1690,229949,"Change ""out of the box"" setup to use sandbox instead of sample",The out of the box deployments/sample.xml has become increasingly Sandbox specific.  We should rename that to sandbox.xml and also have all of the other samples and site use that URL.,smartshark_2_2,1352,knox,"""[0.9984105825424194, 0.0015894208336248994]"""
1691,229950,rats plugin should exclude deb pkg files that are created on the fly,"When building know through bigtop, knox rats plugin complains about the debian pkg files for missing headers

{code}
7 Unknown Licenses

 *******************************

 Unapproved licenses:

   debian/compat
   debian/source/format
   debian/bigtop.bom
   debian/knox.debhelper.log
   .pc/.version
   .pc/.quilt_series
   .pc/.quilt_patches

 *******************************
{code}",smartshark_2_2,1484,knox,"""[0.9969483017921448, 0.0030517044942826033]"""
1692,229951,Knox Gateway Service for ElasticSearch,"We have used a lot of Knox Gateway Services and ElasticSearch service on our Big Data platforms. However there are no Knox Gateway Service for ElasticSearch yet.  In our situation, we need such a Knox Gateway Service for ElasticSearch without Knox to do the â¦
authentication but ElasticSearch Rest Server(s) to do the authentication. As per our use case, we have developed such a Knox Gateway ElasticSearch Service (services/elasticsearch/1.0.0), and we are in a mode to share the code to the Apache Knox community because it has been fully tested for the following scenarios:
(1)	No-LDAP, Local-LDAP or company-specific-LDAP authentication in the Knox gateway;
(2)	Any Elasitcsearch Index - creation, deletion, refresshing and data - writing, updating and retrieval;
(3)	Elasticsearch node root query.",smartshark_2_2,600,knox,"""[0.998439371585846, 0.001560684060677886]"""
1693,229952,Provide rewrite functions that resolve service location information,Need to provide rewrite functions like $service-host[NAMENODE.rpt] so that we can rewrite things like the Oozie workflow config.,smartshark_2_2,1341,knox,"""[0.9984418749809265, 0.001558106392621994]"""
1694,229953,Extend support of OAuth by adding generation of Discovery Document,"To make it easier for clients to use Knox as an IdP, OAuth support can be extended to provide a Discovery Document, similar to what is documented here: https://openid.net/specs/openid-connect-discovery-1_0.html

Google's Discovery Document example: https://developers.google.com/identity/protocols/OpenIDConnect#discovery",smartshark_2_2,691,knox,"""[0.9983953833580017, 0.0016045474912971258]"""
1695,229954,Spelling/grammar fixes,"See the attached patch for two trivial spelling/grammar problems I noticed:

a) Misspelling of ""JASS"" in conf/gateway-site.xml
b) Grammatical error in the user guide",smartshark_2_2,562,knox,"""[0.9954743981361389, 0.004525647033005953]"""
1696,229955,Upgrade ApacheDS for demo LDAP server to ApacheDS 2,The current version of ApacheDS is buggy and generates errors in its logs that are confusing for users.  We should upgrade to ApacheDS 2.x.,smartshark_2_2,1405,knox,"""[0.9983012676239014, 0.0016987263225018978]"""
1697,229956,2-way SSL Truststore and Keystore Improvements,"Currently, the DefaultHttpClientFactory is setting the 2-way SSL for dispatches truststore as gateway.jks. This should be driven by configuration and probably default to cacerts rather than gateway.jks.

The client cert alias inside the keystore should be configurable as well so that we can possibly have different certs representing different topologies.

In addition, the keystore to host the client certs should be configurable.",smartshark_2_2,1262,knox,"""[0.9985106587409973, 0.0014893356710672379]"""
1698,229957,Upgrade buildnumber-maven-plugin to 1.4,buildnumber-maven-plugin 1.3 to 1.4,smartshark_2_2,856,knox,"""[0.997632622718811, 0.0023673376999795437]"""
1699,229958,Support multiple preauth Validators,Currently KNOX-861 supports one validator. This task is to support a chain of validators.,smartshark_2_2,450,knox,"""[0.9982193112373352, 0.0017806377727538347]"""
1700,229959,Enhance LDAP user search configurability,"In very complex organizations the current configuration supported by KnoxLdapRealm my not be sufficient.  Ideally it would be possible to:
1. Configure the LDAP search filter directly
2. Configure the LDAP search scope
3. Have portions of the search base and filter be derived from the input principal.
To clarify this, I'm thinking of provider configurations like these

{code}
<param name=""main.ldapRealm.principalRegex"" value=""(.*?)\\(.*)""/>
<param name=""main.ldapRealm.userDnTemplate"" value=""CN={2},CN={1},DC=qa,DC=company,DC=com""/>
{code}

{code}
<param name=""main.ldapRealm.principalRegex"" value=""(.*?)\\(.*)""/>
<param name=""main.ldapRealm.userSearchBase"" value=""CN={1},DC=qa,DC=company,DC=com""/>
<param name=""main.ldapRealm.userSearchAttributeName"" value=""sAMAccountName""/>
<param name=""main.ldapRealm.userSearchAttributeTemplate"" value=""{2}""/>
{code}

{code}
<param name=""main.ldapRealm.principalRegex"" value=""(.*?)\\(.*)""/>
<param name=""main.ldapRealm.userSearchBase"" value=""CN={1},DC=qe,DC=company,DC=com""/>
<param name=""main.ldapRealm.userSearchFilter"" value=""(&amp;(objectclass=person)(sAMAccountName={2}))""/>
{code}

{code}
<param name=""main.ldapRealm.principalRegex"" value=""(.*?)\\(.*)""/>
<param name=""main.ldapRealm.userSearchBase"" value=""CN={1},DC=qe,DC=company,DC=com""/>
<param name=""main.ldapRealm.userSearchFilter"" value=""(&amp;(objectclass=person)(sAMAccountName={2}))""/>
<param name=""main.ldapRealm.userSearchScope"" value=""onelevel""/>
{code}

{code}
<param name=""main.ldapRealm.principalRegex"" value=""(.*?)\\(.*)""/>
<param name=""main.ldapRealm.userSearchBase"" value=""CN={2},CN={1},DC=qa,DC=company,DC=com""/>
<param name=""main.ldapRealm.userSearchScope"" value=""object""/>
{code}
",smartshark_2_2,341,knox,"""[0.9983604550361633, 0.0016395639395341277]"""
1701,229960,Please create a DOAP file for your TLP,"Please can you set up a DOAP for your project and get it added to files.xml?

Please see http://projects.apache.org/create.html

Once you have created the DOAP and committed it to your source code repository, please submit it for inclusion in the Apache projects listing as per:

http://projects.apache.org/create.html#submit

Remember, if you ever move or rename the doap file in future, please
ensure that files.xml is updated to point to the new location.

Thanks!
",smartshark_2_2,1565,knox,"""[0.9981689453125, 0.0018310395535081625]"""
1702,229961,Support UI HA for resource manager ,"Looks like, Knox doesn't support YARN UI HA. For example, if there is two RM instances running, any access to RM UI redirects ( code 307)  to the active RM UI.
This redirect doesn't work when the actual UI is running behind a firewall (because primary UI is not directly accessible).

However, if KNOX hits the active UI directly - there will be no forwarding and UI is displayed properly.
",smartshark_2_2,527,knox,"""[0.9235357046127319, 0.0764642283320427]"""
1703,229962,Still build dictionary for TopN group by column even using non-dict encoding,"Using latest 2.0 beta, create a cube with TopN, but specifying using non-dict encoding, the JSON is like:
{code}
{
      ""name"": ""TOP_CUSTOMER"",
      ""function"": {
        ""expression"": ""TOP_N"",
        ""parameter"": {
          ""type"": ""column"",
          ""value"": ""P_LINEORDER.V_REVENUE"",
          ""next_parameter"": {
            ""type"": ""column"",
            ""value"": ""P_LINEORDER.LO_CUSTKEY""
          }
        },
        ""returntype"": ""topn(100,4)"",
        ""configuration"": {
          ""topn.encoding.P_LINEORDER.LO_CUSTKEY"": ""integer:4"",
          ""topn.encoding_version.P_LINEORDER.LO_CUSTKEY"": ""1""
        }
      }
{code}

But, in the third step ""Extract Fact Table Distinct Columns"", there is still a reducer running for ""P_LINEORDER.LO_CUSTKEY""; as this column is a UHC, it takes a long time and finally got failed.

Expected behavior: no reducer for this column. 1.6.0 doesn't have this issue; this is a new issue in 2.0 during the upgrade to snowflake.",smartshark_2_2,1339,kylin,"""[0.14847280085086823, 0.8515271544456482]"""
1704,229963,change kylin.job.hive.database.for.intermediatetable cause job to fail,"KYLIN-883 introduces ""kylin.job.hive.database.for.intermediatetable"" so that user can specify a database where kylin's intermediate tables are stored. However the feature is broken and cause cube creation to fail at ""Extract Fact Table Distinct Columns"" step.

{noformat}
[pool-7-thread-2]:[2015-08-28 16:16:27,740][ERROR][org.apache.kylin.job.common.MapReduceExecutable.doWork(MapReduceExecutable.java:116)] - error execute MapReduceExecutable{id=6c0bc4ac-5a94-448a-ae5d-3dd523954a57-01, name=Extract Fact Table Distinct Columns, state=RUNNING}
java.io.IOException: NoSuchObjectException(message:DEFAULT.kylin_intermediate_kylin_sales_cube_desc_19700101000000_20150822000000_6c0bc4ac_5a94_448a_ae5d_3dd523954a57 table not found)
        at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:97)
        at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:51)
        at org.apache.kylin.job.hadoop.cube.FactDistinctColumnsJob.setupMapper(FactDistinctColumnsJob.java:101)
        at org.apache.kylin.job.hadoop.cube.FactDistinctColumnsJob.run(FactDistinctColumnsJob.java:77)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.kylin.job.common.MapReduceExecutable.doWork(MapReduceExecutable.java:113)
        at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
        at org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:51)
        at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
        at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:130)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: NoSuchObjectException(message:DEFAULT.kylin_intermediate_kylin_sales_cube_desc_19700101000000_20150822000000_6c0bc4ac_5a94_448a_ae5d_3dd523954a57 table not found)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1605)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
        at com.sun.proxy.$Proxy50.get_table(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:997)
        at org.apache.hive.hcatalog.common.HCatUtil.getTable(HCatUtil.java:191)
        at org.apache.hive.hcatalog.mapreduce.InitializeInput.getInputJobInfo(InitializeInput.java:117)
        at org.apache.hive.hcatalog.mapreduce.InitializeInput.setInput(InitializeInput.java:86)
        at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:95)
        ... 13 more
{noformat}

The reason is in CubingJobBuilder.java, we didn't pass the configured database name into createFactDistinctColumnsStep.

{code:java}
addCubingSteps(CubeSegment seg, String cuboidRootPath, CubingJob result) {
        // ...code ommitted...
        final CubeJoinedFlatTableDesc intermediateTableDesc = new CubeJoinedFlatTableDesc(seg.getCubeDesc(), seg);
        
        // SHOULE BE dbname.tblname, BUT WAS tblname
        final String intermediateHiveTableName = getIntermediateHiveTableName(intermediateTableDesc, jobId);
        
        // ...code ommitted...
        result.addTask(createFactDistinctColumnsStep(seg, intermediateHiveTableName, jobId));
{code}",smartshark_2_2,504,kylin,"""[0.12337560206651688, 0.876624345779419]"""
1705,229964,No email notification on job failure in some rare cases,"Usually Kylin will send email notification (if configured) on job failure; while today we found there is no email under two job failures; This downgrade the admin's response efficiency, need check the root cause.",smartshark_2_2,1985,kylin,"""[0.6237682104110718, 0.37623175978660583]"""
1706,229965,Deploy coprocessor only this server own the table,"When the table has migrated from test env to prod env and we update the coprocessor in the test env, we should not update the coprocessor of the table has migrated, otherwise the queries to prod env will fail.",smartshark_2_2,840,kylin,"""[0.9343316555023193, 0.06566835939884186]"""
1707,229966,Dimension with all nulls cause BuildDimensionDictionary failed due to FileNotFoundException,"From mailing list
----------------------
{noformat}
I am building a cube with some lookup table in between and getting
exception at third step of cube build i.e Build Dimension Dictionary with
exception saying

java.io.FileNotFoundException: File does not exist:
/tmp/kylin-5a2ea405-24a2-45ed-958e-2a7fddd8cc97/sc_o2s_metrics_verified123455/fact_distinct_columns/SC
at
org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1093)
at
org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1085)
at
org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1085)
at org.apache.kylin.dict.lookup.FileTable.getSignature(FileTable.java:62)
at
org.apache.kylin.dict.DictionaryManager.buildDictionary(DictionaryManager.java:164)
at org.apache.kylin.cube.CubeManager.buildDictionary(CubeManager.java:154)
at
org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:53)
at
org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:42)
at
org.apache.kylin.job.hadoop.dict.CreateDictionaryJob.run(CreateDictionaryJob.java:53)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
at
org.apache.kylin.job.common.HadoopShellExecutable.doWork(HadoopShellExecutable.java:63)
at
org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
at
org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:50)
at
org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
at
org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:132)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
{noformat}

The problem is that FactDistinctColumnsMapper's map method skips null values. As a result, if all values of dimension 'x' are null, FactDistinctColumnsReducer will not create file for 'x', thereafter the following job throws FileNotFoundException.

{code:title=FactDistinctColumnsMapper.java|borderStyle=solid}
public void map(KEYIN key, HCatRecord record, Context context) throws IOException, InterruptedException {
        try {
            // code ommited ...
            for (int i : factDictCols) {
                outputKey.set((short) i);
                fieldSchema = schema.get(flatTableIndexes[i]);
                Object fieldValue = record.get(fieldSchema.getName(), schema);
                // NULL VALUE IS SKIPPED
                if (fieldValue == null)
                    continue;
                // code ommited ...
            }
        } catch (Exception ex) {
            handleErrorRecord(record, ex);
        }
    }
{code}",smartshark_2_2,451,kylin,"""[0.14417997002601624, 0.8558200597763062]"""
1708,229967,"HLLCSerializer, RawSerializer, PercentileSerializer returns shared object in serialize()","This is a bug, which causing incorrect query result. See more in KYLIN-2926",smartshark_2_2,2090,kylin,"""[0.07552269101142883, 0.9244772791862488]"""
1709,229968,Fix Extended column bug in web,The option for {{Extended column on fact table}} should be {{getCommonMetricColumns()}} not {{getAllModelDimColumns()}},smartshark_2_2,1939,kylin,"""[0.11974216997623444, 0.8802578449249268]"""
1710,229969,Should get FileSystem from HBaseConfiguration in HBaseResourceStore,"KYLIN-2351 introduced a bug if User use Standalone HBase Cluster.
{code:java}
   Error while executing SQL ""SELECT SUM(revenue) AS revenue, SUM(profit) AS profit, SUM(repay_profit) AS repayProfit, SUM(fraud_profit) AS fraudProfit, SUM(share_profit) AS shareProfit, SUM(consume) AS consume, SUM(repay_consume) AS repayConsume, SUM(fraud_consume) AS fraudConsume, SUM(share_consume) AS shareConsume, SUM(cost) AS cost, SUM(fraud_cost) AS fraudCost, SUM(repay_cost) AS repayCost, poi_cate2_id AS poiCategory2Id, poi_cate2_name AS poiCategory2Name, main_poi_id AS orgId, main_poi_name AS orgName, COUNT(DISTINCT NEW_OBJECT) AS newDeal, COUNT(DISTINCT ONLINE_OBJECT) AS onlineDeal, partition_date AS dateStr FROM mart_catering.app_shu_v5_trade_view WHERE (bd_id = 2084324 AND c_platform IN ('mt', 'dp') AND partition_date = '2017-05-24') GROUP BY poi_cate2_id, poi_cate2_name, partition_date, main_poi_id, main_poi_name LIMIT 50000"": java.io.FileNotFoundException: File does not exist: /user/kylin2x/prod/kylin2x_metadata_prod/resources/dict/MART_CATERING.APP_SHU_V5_TRADE_VIEW/C_OBJECT_ID/854df823-abc8-4e19-9035-def12f8af3e2.dict at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71) at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1850) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1821) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1729) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:589) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)



at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:793)
        at org.apache.kylin.storage.hbase.HBaseResourceStore.getInputStream(HBaseResourceStore.java:206)
        at org.apache.kylin.storage.hbase.HBaseResourceStore.getResourceImpl(HBaseResourceStore.java:226)
        at org.apache.kylin.common.persistence.ResourceStore.getResource(ResourceStore.java:148)
        at org.apache.kylin.dict.DictionaryManager.load(DictionaryManager.java:448)
        at org.apache.kylin.dict.DictionaryManager$1.load(DictionaryManager.java:105)
        at org.apache.kylin.dict.DictionaryManager$1.load(DictionaryManager.java:102)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.kylin.dict.DictionaryManager.getDictionaryInfo(DictionaryManager.java:122)
{code}",smartshark_2_2,1951,kylin,"""[0.2370789796113968, 0.7629210352897644]"""
1711,229970,ConcurrentModificationException when initializing ResourceStore,"Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)
	at java.util.ArrayList$Itr.next(ArrayList.java:831)
	at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:81)
	at org.apache.kylin.metadata.MetadataManager.getStore(MetadataManager.java:134)
	at org.apache.kylin.metadata.MetadataManager.reloadAllSourceTable(MetadataManager.java:250)
	at org.apache.kylin.metadata.MetadataManager.init(MetadataManager.java:201)
	at org.apache.kylin.metadata.MetadataManager.<init>(MetadataManager.java:113)
	at org.apache.kylin.metadata.MetadataManager.getInstance(MetadataManager.java:85)
	at org.apache.kylin.job.dao.ExecutableDao.<init>(ExecutableDao.java:66)
	at org.apache.kylin.job.dao.ExecutableDao.getInstance(ExecutableDao.java:54)
	at org.apache.kylin.job.manager.ExecutableManager.<init>(ExecutableManager.java:74)
	at org.apache.kylin.job.manager.ExecutableManager.getInstance(ExecutableManager.java:61)
	at org.apache.kylin.rest.service.BasicService.getExecutableManager(BasicService.java:183)
	at org.apache.kylin.rest.service.JobService.listCubeJobInstance(JobService.java:103)
	at org.apache.kylin.rest.service.JobService.listAllJobs(JobService.java:90)
	at org.apache.kylin.rest.service.JobService.listAllJobs(JobService.java:75)
	at org.apache.kylin.rest.service.JobService$$FastClassByCGLIB$$83a44b2a.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
	at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618)
	at org.apache.kylin.rest.service.JobService$$EnhancerByCGLIB$$329bd828.listAllJobs(<generated>)
	at org.apache.kylin.rest.controller.JobController.list(JobController.java:122)
	... 74 more",smartshark_2_2,2519,kylin,"""[0.07555023580789566, 0.9244498014450073]"""
1712,229971,"There is no response when click ""Property"" button at Cube Designer","When create cube, in step 'Configuration Overwrites', you can not create '+property'",smartshark_2_2,2963,kylin,"""[0.4250204563140869, 0.5749795436859131]"""
1713,229972,"Get ""Table 'xxxx' not found while executing SQL"" error after a cube be successfully built","After building a cube successfully, execute a SQL on the ""query"" tab, but got error like:

rom line 1, column 86 to line 1, column 96: Table 'KYLIN_SALES' not found while executing SQL: ""select cal_dt, sum(price) as total_selled, count(distinct seller_id) as sellers from kylin_sales group by cal_dt order by cal_dt LIMIT 50000""

In the left table tree of ""query"" tab, the tables that defined in the cube wasn't displayed as well; But if restart the kylin server, everything goes well.",smartshark_2_2,735,kylin,"""[0.2190844863653183, 0.7809155583381653]"""
1714,229973,StorageCleanupJob may clean a newly created HTable in streaming cube building,"Will add timestamp to the htable descriptor, then when do the cleanup, check whether it was newly created (say in past 48 hours), if yes, will skip it; ",smartshark_2_2,2557,kylin,"""[0.34964504837989807, 0.6503549218177795]"""
1715,229974,List readable project not correct if add limit and offset,"There is one line getReadableProjects in ProjectController:
List<ProjectInstance> projectInstances = projectService.listAllProjects(limit, offset);
That means just the part of projects, then validate the ACL.
If user's project beyond the limit, then he will never get the projects if he put limit and offset.
It should be similar with cube list, get all the projects then validate the ACL, at last get the sub list by limit and offset.",smartshark_2_2,1065,kylin,"""[0.12757200002670288, 0.8724279999732971]"""
1716,229975,Job couldn't stop when hive commands got error with beeline,"Configure Kylin to use beeline as the hive command line; submit a cube build job, the job moves to 100% with success, while I found there was error in the hive related steps, but the error wasn't captured by Kylin;",smartshark_2_2,1135,kylin,"""[0.19270187616348267, 0.8072981238365173]"""
1717,229976,Exception during query on lookup table,"{noformat}
2017-07-18 10:52:49,744 ERROR [Query 49c9601f-017f-44b7-b166-6991527f8903-2192] service.QueryService : Exception when execute sql
java.sql.SQLException: Error while executing SQL ""select
    ""BUYER_ACCOUNT"".""ACCOUNT_BUYER_LEVEL"" as ""c0""
from
    ""DEFAULT"".""KYLIN_ACCOUNT"" as ""BUYER_ACCOUNT""
group by
    ""BUYER_ACCOUNT"".""ACCOUNT_BUYER_LEVEL""
order by
    ""BUYER_ACCOUNT"".""ACCOUNT_BUYER_LEVEL"" ASC
LIMIT 50000"": java.lang.Integer cannot be cast to java.lang.Long
	at org.apache.calcite.avatica.Helper.createException(Helper.java:56)
	at org.apache.calcite.avatica.Helper.createException(Helper.java:41)
	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)
	at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:218)
	at org.apache.commons.dbcp.DelegatingStatement.executeQuery(DelegatingStatement.java:208)
	at org.apache.commons.dbcp.DelegatingStatement.executeQuery(DelegatingStatement.java:208)
	at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:769)
	at org.apache.kylin.rest.service.QueryService.queryWithSqlMassage(QueryService.java:506)
	at org.apache.kylin.rest.service.QueryService.query(QueryService.java:177)
	at org.apache.kylin.rest.service.QueryService.doQueryWithCache(QueryService.java:387)
	at org.apache.kylin.rest.controller2.QueryControllerV2.queryV2(QueryControllerV2.java:81)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:832)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:743)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:961)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:895)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:967)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:869)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:650)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:843)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:731)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at io.kyligence.kap.rest.spring.DoNothingFilter.doFilter(SourceFile:42)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:316)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:126)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:90)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:122)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:169)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:48)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:158)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter.doFilter(DefaultLoginPageGeneratingFilter.java:162)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:205)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:120)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:53)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:213)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:209)
	at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:244)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:956)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:436)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1078)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:625)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
	at Baz$1$1.current(Unknown Source)
	at org.apache.calcite.linq4j.EnumerableDefaults.distinct(EnumerableDefaults.java:450)
	at org.apache.calcite.linq4j.DefaultEnumerable.distinct(DefaultEnumerable.java:206)
	at Baz.bind(Unknown Source)
	at org.apache.calcite.jdbc.CalcitePrepare$CalciteSignature.enumerable(CalcitePrepare.java:331)
	at org.apache.calcite.jdbc.CalciteConnectionImpl.enumerable(CalciteConnectionImpl.java:294)
	at org.apache.calcite.jdbc.CalciteMetaImpl._createIterable(CalciteMetaImpl.java:553)
	at org.apache.calcite.jdbc.CalciteMetaImpl.createIterable(CalciteMetaImpl.java:544)
	at org.apache.calcite.avatica.AvaticaResultSet.execute(AvaticaResultSet.java:193)
	at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:67)
	at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:44)
	at org.apache.calcite.avatica.AvaticaConnection$1.execute(AvaticaConnection.java:607)
	at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:600)
	at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:615)
	at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:148)
	... 91 more
{noformat}
",smartshark_2_2,2124,kylin,"""[0.10231771320104599, 0.8976823091506958]"""
1718,229977,"Dictionary couldn't recognize a value and throw IllegalArgumentException: ""Not a valid value""","I have a cube which have several lookups be left joined with the fact table; I defined one foreign key as a dimension, then that fk column was added into the rowkey, with ""dictinoary"" = ""Y"";

The dictionary was successfully built; while the cube build job was failed, the error trace looks like: 
Caused by: java.lang.IllegalArgumentException: Not a valid value: 00000000000001020
at org.apache.kylin.dict.TrieDictionary.getIdFromValueBytesImpl(TrieDictionary.java:176)
at org.apache.kylin.dict.NumberDictionary.getIdFromValueBytesImpl(NumberDictionary.java:172)
at org.apache.kylin.dict.Dictionary.getIdFromValueBytes(Dictionary.java:138)
at org.apache.kylin.dict.TrieDictionary.getIdFromValueImpl(TrieDictionary.java:161)
at org.apache.kylin.dict.Dictionary.getIdFromValue(Dictionary.java:91)

Then I dumped the dictionary to local and output all its values; In the mean while I run hive SQL to list the distinct values for that column from the flat table; Then I found the 1020 is appeared in hive table but not in the dictionary; ",smartshark_2_2,738,kylin,"""[0.11987745761871338, 0.8801225423812866]"""
1719,229978,HBase Token not added after KYLIN-1007,"The issue is TableMapReduceUtil.initTableMapperJob in hbase 0.98.0 does not init credentials, try manually add it",smartshark_2_2,2655,kylin,"""[0.18130332231521606, 0.8186966180801392]"""
1720,229979,Inaccurate arithmetic operation in LookupTableToHFileJob#calculateShardNum,"There're two issues with the following code:
{code}
    private int calculateShardNum(KylinConfig kylinConfig, long dataSize) {
        long shardSize = kylinConfig.getExtTableSnapshotShardingMB() * 1024 * 1024;
        return dataSize < shardSize ? 1 : (int) (Math.ceil(dataSize / shardSize));
{code}
getExtTableSnapshotShardingMB returns an int. The multiplication is done using 32-bit arithmetic, and then used in a context that expects an expression of type ""long"".

Second, Math.ceil expects a double. The integer division would lose some precision.",smartshark_2_2,1650,kylin,"""[0.4933475852012634, 0.5066524147987366]"""
1721,229980,Fix bug in readLong function in BytesUtil,"A bug was found in readLong function in BytesUtil, which might cause error when 
deserializing a Long type object. ",smartshark_2_2,2112,kylin,"""[0.06623265147209167, 0.9337673187255859]"""
1722,229981,"After refresh the page,the cubes can't sort by 'create_time'","When we open the page 'localhost:7070/kylin/models',the cubes was disordered.Please refer to 01.png.
Therefore,after we add or clone a new cube,then the page will be refreshed,we may not find the cube that just add.

And through debug the code,I find that the cubes were sorted by 'create_time' default.But the cubes' attributes are not contain 'create_time'.
So the cubes were disordered.Please refer to 02.png.

Besides,in the back-end, cubes was sorted by created time from early to late,so the cubes we get were not latest.
At this time,I add 50 cubes,from c1 to c50,add in order,but the cubes we get first were c1-c15,not c36-c50.",smartshark_2_2,1061,kylin,"""[0.17837117612361908, 0.8216288685798645]"""
1723,229982,Hive dependency jars appeared twice on job configuration,"Reported from Yu feng from dev-mailing list: ""it looks like we append kylinHiveDependency to classpath twice""",smartshark_2_2,606,kylin,"""[0.9739070534706116, 0.026092957705259323]"""
1724,229983,"Exception will throw out if dimension is created on a lookup table, then deleting the lookup table.","1. Click Cube.
2. Add 2 lookup table, such as TEST_CATEGORY_GROUPINGS, TEST_KYLIN_FACT, then create join relationship between fact table and lookup table
3. Add 2 dimensions based on the 2 lookup tables.
4. Click Prev to delete one lookup table, such as TEST_CATEGORY_GROUPINGS
5. Click Next
6. Click Auto Generator

Actual Result:
Exception occured
TypeError: Cannot set property '' of undefined
    at http://10.249.65.5:7070/js/scripts.min.0.js:14014:60
    at Array.forEach (native)
    at Object.r [as forEach] (http://10.249.65.5:7070/js/scripts.min.0.js:18:280)
    at http://10.249.65.5:7070/js/scripts.min.0.js:14013:21
    at Array.forEach (native)
    at Object.r [as forEach] (http://10.249.65.5:7070/js/scripts.min.0.js:18:280)
    at h.$scope.initColumnStatus (http://10.249.65.5:7070/js/scripts.min.0.js:14010:17)
    at h.$scope.openAutoGenModal (http://10.249.65.5:7070/js/scripts.min.0.js:14175:16)
    at http://10.249.65.5:7070/js/scripts.min.0.js:180:382
    at http://10.249.65.5:7070/js/scripts.min.0.js:197:390 

Expected Result:
No error and TEST_CATEGORY_GROUPINGS is removed from Dimension page. Or not allow user to delete lookup table if there is one dimension related to the lookup table.",smartshark_2_2,325,kylin,"""[0.09530752152204514, 0.9046924710273743]"""
1725,229984,can not see job step log ,"at [Jobs] page,when we go to one job ,and click each step to see it's log ,

it shows loading always.",smartshark_2_2,448,kylin,"""[0.22423087060451508, 0.7757691144943237]"""
1726,229985,java.lang.StringIndexOutOfBoundsException in org.apache.kylin.storage.hbase.util.StorageCleanupJob,"While running storage cleanup job:

./bin/kylin.sh org.apache.kylin.storage.hbase.util.StorageCleanupJob --delete true

I see Hive tables in form kylin_intermediate_<cube_name>_19700101000000_20160701031500
 in the defaul schema.

While running the above storage cleaner (v.1.5.2.1 - all previously built Cubes Disabled & Dropped) I am getting an error:

2016-06-27 22:28:08,480 INFO  [main StorageCleanupJob:262]: Remove intermediate hive table with job id fc44da88-cffc-4710-8726-ff910cf83451 with job status ERROR
usage: StorageCleanupJob
 -delete <delete>   Delete the unused storage
Exception in thread ""main"" java.lang.StringIndexOutOfBoundsException: String index out of range: -2
        at java.lang.String.substring(String.java:1904)
        at org.apache.kylin.storage.hbase.util.StorageCleanupJob.cleanUnusedIntermediateHiveTable(StorageCleanupJob.java:269)
        at org.apache.kylin.storage.hbase.util.StorageCleanupJob.run(StorageCleanupJob.java:91)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.kylin.storage.hbase.util.StorageCleanupJob.main(StorageCleanupJob.java:308)
2016-06-27 22:28:08,486 INFO  [Thread-0 HConnectionManager$HConnectionImplementation:1907]: Closing zookeeper sessionid=0x154c97461586119
2016-06-27 22:28:08,491 INFO  [Thread-0 ZooKeeper:684]: Session: 0x154c97461586119 closed
2016-06-27 22:28:08,491 INFO  [main-EventThread ClientCnxn:509]: EventThread shut down




",smartshark_2_2,3157,kylin,"""[0.09239745885133743, 0.9076025485992432]"""
1727,229986,kylin TopN only support the first measure ,"when I build cube chose top-N and select the group by column ,like this 

TOP_N	SUM|ORDER BY:CLICK
GROUP BY:MEDIA_SLOT

TOP_N	
SUM|ORDER BY:EREQ
GROUP BY:MEDIA_SLOT

TOP_N	
SUM|ORDER BY:ECLICK
GROUP BY:MEDIA_DOMAIN

TOP_N	
SUM|ORDER BY:REQUEST
GROUP BY:MEDIA_DOMAIN

if I use SQL likes ""select MEDIA_SLOT ,sum(EREQ) ... ...group by (MEDIA_SLOT) order by sum(EREQ) desc /asc limit 100""  or group by(MEDIA_DOMAIN),sum(REQUEST) the result gets none.But if sum(CLICK) group by (MEDIA_SLOT) or sum(ECLICK) group by (MEDIA_DOMAIN) the result return true;
Best whishes.
",smartshark_2_2,3302,kylin,"""[0.36012524366378784, 0.6398747563362122]"""
1728,229987,Unclosed HBaseAdmin in ToolUtil#getHBaseMetaStoreId(),"{code}
    public static String getHBaseMetaStoreId() throws IOException {
        final HBaseAdmin hbaseAdmin = new HBaseAdmin(HBaseConfiguration.create(HadoopUtil.getCurrentConfiguration()));
        final String metaStoreName = KylinConfig.getInstanceFromEnv().getMetadataUrlPrefix();
        final HTableDescriptor desc = hbaseAdmin.getTableDescriptor(TableName.valueOf(metaStoreName));
        return ""MetaStore UUID: "" + desc.getValue(HBaseConnection.HTABLE_UUID_TAG);
{code}
hbaseAdmin should be closed upon return.",smartshark_2_2,2845,kylin,"""[0.1030474305152893, 0.8969525694847107]"""
1729,229988,Negative number in SUM result and Kylin results not matching exactly Hive results,"I have a simple cube with a Fact Table and a Lookup Table. The Fact Table
includes some counter. The Lookup Table includes a region property which is
the highest level of the hierarchy and therefore can accumulate quite a lot
of data when summing the counter by region.

I am doing a query like this:

select sum(MY_COUNTER) as tot_count, REGION

from FACT_TABLE

join LOOKUP_TABLE

on FACT_TABLE.FK = LOOKUP_TABLE.PK

where date = '2015-01-01'

group by REGION


I get the following result:

-917,164,421 EMEA --> negative result!

777,795,808 AMER

514,879,134 APAC

117,814,485 LATAM

I ran the exact same query in Hive and got the following result:

3,381,591,228  EMEA  --> big number but smaller than the BIGINT limit

778,718,563    AMER  --> 922,755 difference with the Kylin result

520,253,610    APAC   --> 5,374,476 difference with the Kylin result

117,913,857    LATAM --> 99,372 difference with the Kylin result

Based on that it seems that the limit is the int limit 2^31-1
(2,147,483,647) and that my negative result comes from an overflow.

In my cube I defined this measure as a bigint:

Expression    Param Type    Param Value       Return Type

SUM              column            AVAIL_COUNT    bigint

The other thing that worries me a bit is that the other numbers are not
100% accurate either (>100,000,000 difference!)


===================================================
reported by alex schufo  alexschufo@gmail.com",smartshark_2_2,478,kylin,"""[0.15812936425209045, 0.8418706655502319]"""
1730,229989,"When we cancel the Topic modification for 'Kafka Setting' of streaming table, the 'Cancel' operation will make a mistake.","There are two bugs in this issue.

First
1. Choose one streaming table, choose the 'Streaming Cluster' tab, then click 'Edit' button, refer to [^edit_streaming_table.png]
2. The edit page will be opened, then you can edit the value of topic of 'Kafka Setting'. As long as you modify the ID, Host, Port value, the original value tag will follow changes, refer to [^edit_kafka_setting.png];
3. When you click 'cancel' button, you will find the old values have been changed to the new values, and if you click the 'submit' button, you will also find the values to be canceled will be submitted, refer to [^cancel_is_not_right.png]
But I think the correct way should be that 'cancel' button will not change any value.

Second
The follow code in streamingConfig.js has a bug, even if ""cluster.newBroker"" object matchs the one element of ""cluster.brokers"" array, this ""if"" decision will return false. Because even though the ""cluster.newBroker"" object has the same attribute values as an element in the array, their storage addresses may be different. The result is that you can add several same record, like [^can_add_the_same_record.png].
{code:java}
  $scope.saveNewBroker = function(cluster) {
    if (cluster.brokers.indexOf(cluster.newBroker) === -1) {
    ......
    }
  }
{code}

So I have repaired these two bugs, please check the patch, thanks!",smartshark_2_2,959,kylin,"""[0.08067101240158081, 0.919329047203064]"""
1731,229990,GarbageCollectionStep dropped Hive Intermediate Table but didn't drop external hdfs path,"In GarbageCollectionStep, the hive intermediate table created in step 1 was dropped. 
As the table is external table, data was stored in a external hdfs path, like '.../kylin-$\{jobId\}/kylin_intermediate_...', which didn't deleted when drop hive table.
Considering the purpose of GarbageCollectionStep, the external data path should also be deleted.",smartshark_2_2,521,kylin,"""[0.23547452688217163, 0.7645255327224731]"""
1732,229991,enable checkbox in auto generator box when delete dimension,"when create cube, at step 'Dimension' if you click 'Auto Generator' and pick up you option, and then you cancel the modal back to dimension page, delete dimension and click 'Auto Generator' you will find the you can't pick up your option again.",smartshark_2_2,1815,kylin,"""[0.8754414916038513, 0.12455856800079346]"""
1733,229992,Sample cube build error,"MapReduce job failed on Build Cube step, the error log is:
Error: java.io.IOException: Failed to build cube in mapper 0 at org.apache.kylin.engine.mr.steps.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:142) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:188) at org.apache.kylin.engine.mr.steps.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:140) ... 8 more Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.AbstractInMemCubeBuilder$1.run(AbstractInMemCubeBuilder.java:76) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.build(DoggedCubeBuilder.java:125) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder.build(DoggedCubeBuilder.java:72) at org.apache.kylin.cube.inmemcubing.AbstractInMemCubeBuilder$1.run(AbstractInMemCubeBuilder.java:74) ... 5 more Caused by: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.abort(DoggedCubeBuilder.java:193) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.checkException(DoggedCubeBuilder.java:166) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.build(DoggedCubeBuilder.java:113) ... 7 more Caused by: java.lang.IllegalStateException at org.apache.kylin.gridtable.GTInfo.validateColumnBlocks(GTInfo.java:188) at org.apache.kylin.gridtable.GTInfo.validate(GTInfo.java:157) at org.apache.kylin.gridtable.GTInfo$Builder.build(GTInfo.java:259) at org.apache.kylin.cube.gridtable.CubeGridTable.newGTInfo(CubeGridTable.java:74) at org.apache.kylin.cube.gridtable.CubeGridTable.newGTInfo(CubeGridTable.java:62) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.newGridTableByCuboidID(InMemCubeBuilder.java:107) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.createBaseCuboid(InMemCubeBuilder.java:327) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.build(InMemCubeBuilder.java:164) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.build(InMemCubeBuilder.java:133) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$SplitThread.run(DoggedCubeBuilder.java:281)",smartshark_2_2,2844,kylin,"""[0.8795536756515503, 0.12044632434844971]"""
1734,229993,possible memory leak for segmentIterator,"SerializedHBaseTupleIterator:

 @Override
    public ITuple next()  {
        //TODO: last not closed

        ITuple t = null;
        while (hasNext()) {
            if (segmentIterator.hasNext()) {
                t = segmentIterator.next();
                scanCount++;
                break;
            } else {
                try {
                    segmentIterator.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
                segmentIterator = segmentIteratorIterator.next();
            }
        }
        return t;
    }
=======================
currently segmentIterator is closed only when segmentIterator.hasNext() returns false. the last segmentIterator will never reach here because the outer while(hasNext()) would exit the whole loop first.
What's more, due to the complexity of ""while(hasNext())"", even non-last segmentIterator might not get the chance to be closed.",smartshark_2_2,2658,kylin,"""[0.10507961362600327, 0.8949204087257385]"""
1735,229994,Incompatible RowKeySplitter initialize between build and merge job,"In class NDCuboidBuilder:
    public NDCuboidBuilder(CubeSegment cubeSegment) {
        this.cubeSegment = cubeSegment;
        this.rowKeySplitter = new RowKeySplitter(cubeSegment, 65, 256);
        this.rowKeyEncoderProvider = new RowKeyEncoderProvider(cubeSegment);
    } 
which will create a bytes array with length 256 to fill in rowkey column bytes.

While, in class MergeCuboidMapper it's initialized with length 255. 
rowKeySplitter = new RowKeySplitter(sourceCubeSegment, 65, 255);

So, if a dimension is encoded in fixed length and the max length is set to 256. The cube building job will succeed. While, the merge job will always fail. Since in class MergeCuboidMapper method doMap:
    public void doMap(Text key, Text value, Context context) throws IOException, InterruptedException {
        long cuboidID = rowKeySplitter.split(key.getBytes());
        Cuboid cuboid = Cuboid.findForMandatory(cubeDesc, cuboidID);

in method doMap, it will invoke method RowKeySplitter.split(byte[] bytes):
        for (int i = 0; i < cuboid.getColumns().size(); i++) {
            splitOffsets[i] = offset;
            TblColRef col = cuboid.getColumns().get(i);
            int colLength = colIO.getColumnLength(col);
            SplittedBytes split = this.splitBuffers[this.bufferSize++];
            split.length = colLength;
            System.arraycopy(bytes, offset, split.value, 0, colLength);
            offset += colLength;
        }
Method System.arraycopy will result in IndexOutOfBoundsException exception, if a column value length is 256 in bytes and is being copied to a bytes array with length 255.

The incompatibility is also occurred in class FilterRecommendCuboidDataMapper, initialize RowkeySplitter as: 
rowKeySplitter = new RowKeySplitter(originalSegment, 65, 255);

I think the better way is to always set the max split length as 256.
And actually dimension encoded in fix length 256 is pretty common in our production. Since in Hive, type varchar(256) is pretty common, users do have not much Kylin knowledge will prefer to chose fix length encoding on such dimensions, and set max length as 256. ",smartshark_2_2,1530,kylin,"""[0.1553967297077179, 0.8446032404899597]"""
1736,229995,Fix the IllegalArgumentException during segments auto merge,"The function of auto merge loses efficacy after we upgrade to 2.2.0, related stack is:

2018-01-14 11:21:10,965 ERROR [Thread-694] service.CacheService:88 : Error in updateOnNewSegmentReady()
java.lang.IllegalArgumentException
 at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)
 at org.apache.kylin.cube.CubeManager.mergeSegments(CubeManager.java:547)
 at org.apache.kylin.rest.service.CubeService.mergeCubeSegment(CubeService.java:646)
 at org.apache.kylin.rest.service.CubeService.updateOnNewSegmentReady(CubeService.java:594)
 at org.apache.kylin.rest.service.CubeService$$FastClassBySpringCGLIB$$17a07c0e.invoke(<generated>)
 at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
 at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:669)
 at org.apache.kylin.rest.service.CubeService$$EnhancerBySpringCGLIB$$eaadc7fa.updateOnNewSegmentReady(<generated>)
 at org.apache.kylin.rest.service.CacheService$1$1.run(CacheService.java:86)",smartshark_2_2,828,kylin,"""[0.11363636702299118, 0.8863635659217834]"""
1737,229996,Parse Boolean type in JDBC driver,"Exception below is thrown when querying kylin with Saiku as front end, or when retrieving value from getBoolean method in jdbc driver.
That's because KylinClient in jdbc module dose not parse value properly when dimension is boolean type.

Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Boolean
        at org.apache.kylin.jdbc.shaded.org.apache.calcite.avatica.util.AbstractCursor$BooleanAccessor.getBoolean(AbstractCursor.java:463)
        at org.apache.kylin.jdbc.shaded.org.apache.calcite.avatica.util.AbstractCursor$BooleanAccessor.getLong(AbstractCursor.java:468)
        at org.apache.kylin.jdbc.shaded.org.apache.calcite.avatica.util.AbstractCursor$AccessorImpl.getInt(AbstractCursor.java:304)
        at org.apache.kylin.jdbc.shaded.org.apache.calcite.avatica.AvaticaResultSet.getInt(AvaticaResultSet.java:252)
        at org.apache.commons.dbcp.DelegatingResultSet.getInt(DelegatingResultSet.java:225)
        at org.apache.commons.dbcp.DelegatingResultSet.getInt(DelegatingResultSet.java:225)
        at mondrian.rolap.SqlStatement$4.get(SqlStatement.java:418)
        at mondrian.rolap.SqlStatement$1.get(SqlStatement.java:394)
        at mondrian.rolap.SqlTupleReader$Target.internalAddRow(SqlTupleReader.java:260)
        at mondrian.rolap.SqlTupleReader$Target.addRow(SqlTupleReader.java:172)
        at mondrian.rolap.SqlTupleReader.prepareTuples(SqlTupleReader.java:545)
        at mondrian.rolap.SqlTupleReader.readTuples(SqlTupleReader.java:690)
        at mondrian.rolap.RolapNativeSet$SetEvaluator.executeList(RolapNativeSet.java:260)
        at mondrian.rolap.RolapNativeSet$SetEvaluator.execute(RolapNativeSet.java:200)
        at mondrian.olap.fun.NonEmptyCrossJoinFunDef$1.evaluateList(NonEmptyCrossJoinFunDef.java:80)
        at mondrian.calc.impl.AbstractListCalc.evaluateIterable(AbstractListCalc.java:71)
        at mondrian.olap.fun.CrossJoinFunDef$CrossJoinIterCalc.evaluateIterable(CrossJoinFunDef.java:184)
        at mondrian.rolap.RolapResult.executeAxis(RolapResult.java:857)
        at mondrian.rolap.RolapResult.evalLoad(RolapResult.java:693)
        at mondrian.rolap.RolapResult.loadMembers(RolapResult.java:649)
        at mondrian.rolap.RolapResult.<init>(RolapResult.java:279)
        at mondrian.rolap.RolapConnection.executeInternal(RolapConnection.java:500)
        ... 7 more",smartshark_2_2,2018,kylin,"""[0.0883064866065979, 0.9116935133934021]"""
1738,229997,bin/find-hive-dependency.sh may not get correct local classpath when configured hive client is beeline,"in bin/find-hive-dependency.sh, if configured hive client is beeline, it will use beeline to execute ""set"" and then to find the classpath; I noticed the result doesn't have ""hive-cli.jar"",  this will cause error in ""load hive table"" action, a NoClassDefFoundError error is thrown;

If ""hive -e set"" will return all the needed hive jars.

{code}
SEVERE: Servlet.service() for servlet [kylin] in context with path [/kylin] threw exception [Handler processing failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/hive/cli/CliSessionState] with root cause
java.lang.ClassNotFoundException: org.apache.hadoop.hive.cli.CliSessionState
	at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1720)
	at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1571)
	at org.apache.kylin.rest.controller.TableController.showHiveDatabases(TableController.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
{code}
",smartshark_2_2,1494,kylin,"""[0.37998414039611816, 0.6200159192085266]"""
1739,229998,Fix issue on mapping LDAP group to the admin group,"The LDAP group named ""admin"" always has ROLE_ADMIN permission, even if ""kylin.security.acl.admin-role"" is set to another group.",smartshark_2_2,2217,kylin,"""[0.11022251844406128, 0.8897775411605835]"""
1740,229999,Fix the conflict between KYLIN-1851 and KYLIN-2135,"In KYLIN-1851, we use a new dictionary(TrieDictionaryForest) to reduce the peek memory usage of the dictionary building procedure. In order to use the new dictionary, we need to make sure that the input data is sorted. But in KYLIN-2135, the output data is split into multi files and values in different files may be out-of-order. So we need to fix the conflict between this two issues.",smartshark_2_2,3235,kylin,"""[0.29601046442985535, 0.703989565372467]"""
1741,230000,Can't build more segments with sample cube,"Create a new Kylin environment, and execute bin/sample.sh, created sample cube 'kylin_sales_cube'.
Build this cube from 2012-01-01 to 2015-09-06 successfully, and build another segment from 2015-09-07 to 2015-09-08 successfully too.

At this moment, click the build/merge on Web UI, a alarm pop with 'There exists a build request of this cube, the job may be running or error. If you need a new build, please wait for its complete or discard it.'.
It's seems that cube state didn't updated after the second build job completed. Maybe because of that the data is empty for the second build job?",smartshark_2_2,522,kylin,"""[0.705130934715271, 0.294869065284729]"""
1742,230001,Didn't deal with the failure of renaming folder in hdfs when running the tool CubeMigrationCLI,"In some cases, the invoking of the ""rename"" method of FileSystem may fail and return false without throwing any Exception. Currently, such case is regarded as renaming successfully, which is not true.",smartshark_2_2,3118,kylin,"""[0.18665002286434174, 0.8133500218391418]"""
1743,230002,Handle the column that all records is null in MergeCuboidMapper,"If all records of one column is null in a segment, there will be a NPE in {{sourceCubeSegment.getDictionary}}",smartshark_2_2,1289,kylin,"""[0.0808284804224968, 0.9191714525222778]"""
1744,230003,Subquery columns not exported in OLAPContext allColumns,"A query like this returns empty value because OLAP context allColumns field isn't set rightly.

```sql
select c.COUNTRY, c.NAME,b.TRANS_ID, b.BUYER_ID from KYLIN_COUNTRY c join (
        select * from KYLIN_SALES s join KYLIN_ACCOUNT a on s.SELLER_ID = a.ACCOUNT_ID where a.ACCOUNT_ID <= 10000866
   ) b on b.ACCOUNT_COUNTRY = c.COUNTRY
```",smartshark_2_2,2232,kylin,"""[0.1433778554201126, 0.8566221594810486]"""
1745,230004,Fix NPE when updating metrics during Spark CubingJob,"When using spark engine, there is no 'Build Cube In-Mem' step task,
'getTaskByName(ExecutableConstants.STEP_NAME_BUILD_IN_MEM_CUBE).getDuration()' will get NullPointerException

error log:

java.lang.NullPointerException
 at org.apache.kylin.engine.mr.CubingJob.updateMetrics(CubingJob.java:284)
 at org.apache.kylin.engine.mr.CubingJob.onStatusChange(CubingJob.java:272)
 at org.apache.kylin.job.execution.DefaultChainedExecutable.onExecuteFinished(DefaultChainedExecutable.java:132)
 at org.apache.kylin.engine.mr.CubingJob.onExecuteFinished(CubingJob.java:266)
 at org.apache.kylin.job.execution.AbstractExecutable.onExecuteFinishedWithRetry(AbstractExecutable.java:98)
 at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:175)
 at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:300)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
 at java.lang.Thread.run(Thread.java:745)
2018-01-31 18:15:20,443 ERROR [pool-11-thread-10] threadpool.DefaultScheduler:302 : ExecuteException job:693e2cd6-c7e7-4ed9-b1d6-e7ad86f5c501
org.apache.kylin.job.exception.ExecuteException: java.lang.NullPointerException
 at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:179)
 at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:300)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
 at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
 at org.apache.kylin.engine.mr.CubingJob.updateMetrics(CubingJob.java:284)
 at org.apache.kylin.engine.mr.CubingJob.onStatusChange(CubingJob.java:272)
 at org.apache.kylin.job.execution.DefaultChainedExecutable.onExecuteFinished(DefaultChainedExecutable.java:132)
 at org.apache.kylin.engine.mr.CubingJob.onExecuteFinished(CubingJob.java:266)
 at org.apache.kylin.job.execution.AbstractExecutable.onExecuteFinishedWithRetry(AbstractExecutable.java:98)
 at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:175)
 ... 4 more",smartshark_2_2,806,kylin,"""[0.07241377234458923, 0.9275861978530884]"""
1746,230005,Filter of NOT IN (...) can cause incorrect result or NPE,"The version I use is 1.0.

      Recently I updated to 1.2. The problent is like following:

Case1:*******************************************************************

          When I use ânot inâ in my sql, the resultset is less than my correct data. Like following:

         When the sql I use not including a ânot inâ is :

         SELECT ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" AS ""H_CODE"",

       COUNT(1) AS ""sum_Number_of_Records_ok"",

       SUM(""KYLIN_VIEW_TVAD_SUMMARY"".""OCCU"") AS ""sum_OCCU_ok""

  FROM ""BD_WAREHOUSE"".""KYLIN_VIEW_TVAD_SUMMARY"" ""KYLIN_VIEW_TVAD_SUMMARY""

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_CITY_V_KYLIN"" ""KYLIN_TV_DIM_CITY_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""CITY"" =

       ""KYLIN_TV_DIM_CITY_V_KYLIN"".""CITY"")

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"" ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""DT"" =

       ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"".""DT1"")

GROUP BY ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE""

The result is like following:

(null)         206735     436180

other        169103517       480104320

æ      2597081            3161884

A       42464098         148884448

B       31945514         121964156

O      2787895            7265948

K       37694048         127243245

Whiel when I use a sql having ânot inâ, like following:

SELECT ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" AS ""H_CODE"",

       COUNT(1) AS ""sum_Number_of_Records_ok"",

       SUM(""KYLIN_VIEW_TVAD_SUMMARY"".""OCCU"") AS ""sum_OCCU_ok""

  FROM ""BD_WAREHOUSE"".""KYLIN_VIEW_TVAD_SUMMARY"" ""KYLIN_VIEW_TVAD_SUMMARY""

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_CITY_V_KYLIN"" ""KYLIN_TV_DIM_CITY_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""CITY"" =

       ""KYLIN_TV_DIM_CITY_V_KYLIN"".""CITY"")

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"" ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""DT"" =

       ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"".""DT1"")

 WHERE (""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" NOT IN ('other'))

 GROUP BY ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE""

The result I got is, there are only 3 lines, and the results are all less than the true value:

æ      323911     646111

A       7463670  21674399

K       15248274         54972183

Case2:********************************************************

         Sql1: select channel_id, sum(occu) from kylin_view_tvad_summary where channel_id not in (2, 3) group by channel_id.

         Here, 2 is a member of channel_id in kylin_view_tvad_summary, while 3 is not a member of kylin_view_tvad_summary, there will be a null pointer exception like following:

         java.sql.SQLException: error while executing SQL ""select channel_id, sum(occu) from kylin_view_tvad_summary where channel_id not in (2, 3) group by channel_id

LIMIT 50000"": null

         at org.apache.calcite.avatica.Helper.createException(Helper.java:41)

         at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:112)

         at org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:130)

         at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:354)

         at org.apache.kylin.rest.service.QueryService.queryWithSqlMassage(QueryService.java:268)

         at org.apache.kylin.rest.service.QueryService.query(QueryService.java:114)

         at org.apache.kylin.rest.service.QueryService$$FastClassByCGLIB$$4957273f.invoke(<generated>)

         at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)

        at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618)

         at org.apache.kylin.rest.service.QueryService$$EnhancerByCGLIB$$3a29d57a.query(<generated>)

         at org.apache.kylin.rest.controller.QueryController.doQueryWithCache(QueryController.java:178)

         at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:85)

         at sun.reflect.GeneratedMethodAccessor153.invoke(Unknown Source)

         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

         at java.lang.reflect.Method.invoke(Method.java:606)

         at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213)

         at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126)

         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96)

         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617)

         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578)

         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80)

         at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923)

         at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852)

         at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882)

         at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789)

         at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)

         at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)

         at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)

         at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter.doFilter(DefaultLoginPageGeneratingFilter.java:91)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:183)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)

         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)

         at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)

         at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.kylin.rest.filter.KylinApiFilter.doFilterInternal(KylinApiFilter.java:66)

         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:76)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:195)

         at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)

         at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)

         at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)

         at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)

         at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)

         at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)

         at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)

         at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)

         at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)

         at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)

         at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)

         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)

         at java.lang.Thread.run(Thread.java:744)

Caused by: java.lang.NullPointerException

         at Baz$1$1.moveNext(Unknown Source)

         at org.apache.calcite.linq4j.EnumerableDefaults.groupBy_(EnumerableDefaults.java:737)

         at org.apache.calcite.linq4j.EnumerableDefaults.groupBy(EnumerableDefaults.java:677)

         at org.apache.calcite.linq4j.DefaultEnumerable.groupBy(DefaultEnumerable.java:301)

         at Baz.bind(Unknown Source)

         at org.apache.calcite.jdbc.CalcitePrepare$CalciteSignature.enumerable(CalcitePrepare.java:281)

         at org.apache.calcite.jdbc.CalciteConnectionImpl.enumerable(CalciteConnectionImpl.java:235)

         at org.apache.calcite.jdbc.CalciteMetaImpl.createIterable(CalciteMetaImpl.java:533)

         at org.apache.calcite.avatica.AvaticaResultSet.execute(AvaticaResultSet.java:184)

         at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:63)

         at org.apache.calcite.jdbc.CalciteResultSet.execute(CalciteResultSet.java:42)

         at org.apache.calcite.avatica.AvaticaConnection$1.execute(AvaticaConnection.java:473)

         at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:566)

         at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:477)

         at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:109)

         ... 83 more

[http-bio-7070-exec-3]:[2015-12-25 09:45:26,486][INFO][org.apache.kylin.rest.service.QueryService.logQuery(QueryService.java:242)] -

==========================[QUERY]===============================

SQL: select channel_id, sum(occu) from kylin_view_tvad_summary where channel_id not in (2, 3) group by channel_id

User: ADMIN

Success: false

Duration: 0.0

Project: test_tv_album

Realization Names: [cube_test_01]

Cuboid Ids: [2]

Total scan count: 0

Result row count: 0

Accept Partial: true

Is Partial Result: false

Hit Cache: false

Message: null

while executing SQL: ""select channel_id, sum(occu) from kylin_view_tvad_summary where channel_id not in (2, 3) group by channel_id LIMIT 50000""

==========================[QUERY]===============================



[http-bio-7070-exec-3]:[2015-12-25 09:45:26,487][ERROR][org.apache.kylin.rest.controller.BasicController.handleError(BasicController.java:46)] -

org.apache.kylin.rest.exception.InternalErrorException: null

while executing SQL: ""select channel_id, sum(occu) from kylin_view_tvad_summary where channel_id not in (2, 3) group by channel_id LIMIT 50000""

         at org.apache.kylin.rest.controller.QueryController.doQueryWithCache(QueryController.java:205)

         at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:85)

         at sun.reflect.GeneratedMethodAccessor153.invoke(Unknown Source)

         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

         at java.lang.reflect.Method.invoke(Method.java:606)

         at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213)

         at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126)

         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96)

         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617)

         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578)

         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80)

         at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923)

         at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852)

         at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882)

         at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789)

         at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)

         at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)

         at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)

         at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter.doFilter(DefaultLoginPageGeneratingFilter.java:91)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:183)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)

         at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)

         at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)

         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)

         at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)

         at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.kylin.rest.filter.KylinApiFilter.doFilterInternal(KylinApiFilter.java:66)

         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:76)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:195)

         at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266)

         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)

         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)

         at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)

         at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)

         at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)

         at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)

         at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)

         at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)

         at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)

         at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)

         at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)

         at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)

         at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)

         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)

         at java.lang.Thread.run(Thread.java:744)

Case2:

          When I use ânot inâ in my sql, the resultset is less than my correct data. Like following:

         When the sql I use not including a ânot inâ is :

         SELECT ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" AS ""H_CODE"",

       COUNT(1) AS ""sum_Number_of_Records_ok"",

       SUM(""KYLIN_VIEW_TVAD_SUMMARY"".""OCCU"") AS ""sum_OCCU_ok""

  FROM ""BD_WAREHOUSE"".""KYLIN_VIEW_TVAD_SUMMARY"" ""KYLIN_VIEW_TVAD_SUMMARY""

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_CITY_V_KYLIN"" ""KYLIN_TV_DIM_CITY_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""CITY"" =

       ""KYLIN_TV_DIM_CITY_V_KYLIN"".""CITY"")

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"" ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""DT"" =

       ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"".""DT1"")

GROUP BY ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE""

The result is like following:

(null)         206735     436180

other        169103517       480104320

æ      2597081            3161884

A       42464098         148884448

B       31945514         121964156

O      2787895            7265948

K       37694048         127243245

Whiel when I use a sql having ânot inâ, like following:

SELECT ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" AS ""H_CODE"",

       COUNT(1) AS ""sum_Number_of_Records_ok"",

       SUM(""KYLIN_VIEW_TVAD_SUMMARY"".""OCCU"") AS ""sum_OCCU_ok""

  FROM ""BD_WAREHOUSE"".""KYLIN_VIEW_TVAD_SUMMARY"" ""KYLIN_VIEW_TVAD_SUMMARY""

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_CITY_V_KYLIN"" ""KYLIN_TV_DIM_CITY_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""CITY"" =

       ""KYLIN_TV_DIM_CITY_V_KYLIN"".""CITY"")

  LEFT JOIN ""BD_WAREHOUSE"".""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"" ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN""

    ON (""KYLIN_VIEW_TVAD_SUMMARY"".""DT"" =

       ""KYLIN_TV_DIM_DATE_CUBE_V_KYLIN"".""DT1"")

 WHERE (""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE"" NOT IN ('other'))

 GROUP BY ""KYLIN_TV_DIM_CITY_V_KYLIN"".""H_CODE""

The result I got is:

æ      323911     646111

A       7463670  21674399

K       15248274         54972183",smartshark_2_2,2344,kylin,"""[0.06963901966810226, 0.9303609728813171]"""
1747,230006,cube model will be overridden while creating a new cube with the same name,"Hi, I find a Bug like this in kylin-1.0:
I build a cube named TEST successfully, and then build another cube named
TEST too, those two cube has different fact table, the second cube will be
error becacuse ""The cube named TEST already exists"", But the model of first
cube will be overridden.

this is because kylin will save or update cube model before save a cube.


--reported by yufeng",smartshark_2_2,2449,kylin,"""[0.10603015869855881, 0.8939698338508606]"""
1748,230007,Fix regular expression bug in SQL comments,"Hi,all.
Recently,I was testing query function of kylin,
sometimes I just comment with /**/ instead of delete the sql,cause I need to query and compare again.
And I was confused that the results says it was ""No Support Sql"",but it can query success without comments.
For example,

{code:java}
/*
select count(*) from kylin_sales;
*/
select * from kylin_sales;
{code}

So I view the code and find the commentPatterns of  /\**/  was 
{code:java}
/\\*[^\\*/]*
{code}
,clearly it was wrong.
The regular expression of [abc] means any character in abc,such as a or b.
So the [^\\*/] means that * or / can't appear,
But under this circumstances the */ need to be as a string not separated character.
the */ can't appear not * or / can't appear.
I rewrite the regular expression,
{code:java}
/\\*[\\s\\S]*?\\*/
{code}
if you think it's necessary to change the old code,please review and replace it.
Thank for you time.",smartshark_2_2,1076,kylin,"""[0.1061100959777832, 0.8938899040222168]"""
1749,230008,Rename 0.7-staging and 0.8 branch,"As we're planning to release the latest 0.7 branch as Kylin 1.0, previous branch naming conventions will be outdated. 

Previously we had:

*0.7-staging*: dev branch for 0.7 versions, this branch spawns releases like 0.7.1, 0.7.2, etc.
*0.8* : dev branch for 0.8 versions, 0.8 is like the next generation Kylin (with streaming, spark support), it has fundamental difference with 0.7 version, which means any changes on 0.7 will not be merged to 0.8 anymore. So if your patch affects both of the branch, you should make patches for both branch.
*master*: always point to the latest stable release(stable, but not up to date)

I suggest that:
* rename 0.7-staging to 1.x-staging, 1.x-staging will spawn releases like 1.0,1.1, etc.
* rename 0.8-staging to 2.x-staging,  2.x-staging will spawn releases like 2.0, 2.1, etc.

Please leave comments if you have uncommitted changes or you have other suggestions. If possible let's make the change in this week.",smartshark_2_2,626,kylin,"""[0.9984299540519714, 0.0015700504882261157]"""
1750,230009,Kylin should throw error if HIVE_CONF dir cannot be found,Use should manually export HIVE_CONF if find-hivedependency.sh cannot find this dir.,smartshark_2_2,2448,kylin,"""[0.8997602462768555, 0.10023968666791916]"""
1751,230010,Add tool for cleanup Kylin metadata storage,"Kylin persists some resources like dictionary, lookup table snapshots in the metadata store, and will not cleanup them so far; As time goes on, some resources became usless (as the cube segment be dropped or merged) but still take space there; Need a tool to cleanup them, just like the StorageCleanupJob;",smartshark_2_2,596,kylin,"""[0.9984948635101318, 0.0015051356749609113]"""
1752,230011,Wrong logging of JobID in MapReduceExecutable.java,"Hello,

during debugging of an issue where MR Build Job (Step 2) errors - even the MR job is SUCCESS - I found out strange entry in the Kylin log:

2016-04-30 01:40:31,358 INFO  [pool-2-thread-1] execution.AbstractExecutable:107 : mr_job_id:null

After looking at the source code of Kylin 1.5.1 I found that there is probably an code-error in MapReduceExecutable.java on line 107:

logger.info(""mr_job_id:"" + extra.get(ExecutableConstants.MR_JOB_ID + "" resumed""));

I believe the correct logging logic is suposed to be:

logger.info(""mr_job_id:"" + extra.get(ExecutableConstants.MR_JOB_ID) + "" resumed"");

",smartshark_2_2,2819,kylin,"""[0.8987640738487244, 0.10123590379953384]"""
1753,230012,Changed the Pop-up box when no project selected,"I noticed that the Pop-up box was like 01.png when there is no project selected after you click the 'Calculate Cardinality',it reminds us we should choose project first after we input the tablename and click the 'Calculate Cardinality'.
And it keeps 'please wait' status on my environment after input and click like 02.png.
It's unproperly.We should ingore this step,point out that there is no project selected directly after we click the 'Calculate Cardinality',just like 03.png.",smartshark_2_2,1169,kylin,"""[0.8736635446548462, 0.12633651494979858]"""
1754,230013,add authentication for CubeMigrationCLI,"CubeMigrationCLI will consume Rest API '/api/admin/config', but it's not authenticated, need to add security check for the too.",smartshark_2_2,2206,kylin,"""[0.9977322816848755, 0.002267714822664857]"""
1755,230014,add snappy compression support,add snappy compression support in kylin,smartshark_2_2,625,kylin,"""[0.9970859885215759, 0.0029140703845769167]"""
1756,230015,"Deprecate the ""Capacity"" field from DataModel","Kylin used to use the ""Capacity"" on model as the hint to build the cube; While as only three options (SMALL, MEDIUM, LARGE), it couldn't give much flexibility, and may confuse user.

As today kylin allows cube level to overwrite the config in kylin.properties, many parameters can be specified on cube, that would give user much flexibility; So the ""Capacity"" concept is redundant, can sunset it.",smartshark_2_2,3013,kylin,"""[0.9984867572784424, 0.0015131767140701413]"""
1757,230016,kylin should add performance log,"add  performance log interface ,help us find the Performance bottlenecks to easy",smartshark_2_2,2306,kylin,"""[0.9959592223167419, 0.004040821455419064]"""
1758,230017,Ineffective comparison in BooleanDimEnc#equals(),"{code}
        return fixedLen == that.fixedLen;
{code}
fixedLen is static. true would always be returned.",smartshark_2_2,3286,kylin,"""[0.9420833587646484, 0.05791668966412544]"""
1759,230018,Backup metadata before upgrade to new version,"Always backup existing metadata when user upgrade or install new version.

",smartshark_2_2,398,kylin,"""[0.9982982277870178, 0.0017017574282363057]"""
1760,230019,IN condition will convert to subquery join when its elements number exceeds 20,"In calcite 1.11 which is used in latest master branch, SqlToRelConverter.getInSubqueryThreshold() is marked as deprecated, and its overwrite point won't take effect any more.

",smartshark_2_2,1132,kylin,"""[0.19919836521148682, 0.8008016347885132]"""
1761,230020,streaming cli support third-party streammessage parser,"different streaming cases might have different parsers for stream message, for now we adopt plugin jars to support it.",smartshark_2_2,2645,kylin,"""[0.9982129335403442, 0.0017870466690510511]"""
1762,230021,Move hackNoGroupByAggregation to cube-based storage implementations,as it only makes sense for cube-based realizations,smartshark_2_2,3116,kylin,"""[0.9980385899543762, 0.0019614887423813343]"""
1763,230022,Change the query cache expiration strategy by signature checking,"Currently to invalid query cache, {{CacheService}} will either invoke {{cleanDataCache}} or {{cleanAllDataCache}}. Both methods will clear all of the query cache, which is very inefficient. In eBay PROD environment, there's around 400 cubing jobs per day, which means the query cache will be cleared very 4 minutes. Then we introduced a signature based cache invalidation strategy. The basic idea is as follows:
* Add a signature for {{SQLResponse}}, here we choose the cube last build time
* When fetch {{SQLResponse}} for cache, first check whether the signature is consistent. If not, this cached value is overdue and will be invalidate.",smartshark_2_2,745,kylin,"""[0.9970502853393555, 0.0029496452771127224]"""
1764,230023,Modify tools that related to Acl,"Many tools, such as MigrationTooll, StorageCleanUpJob need to read acl records, and they need to be modified using the new Resource store API instead of HBase API",smartshark_2_2,2116,kylin,"""[0.9984630346298218, 0.0015370160108432174]"""
1765,230024,use column name as default dimension name when auto generate dimension for lookup table,"now we use ""{tableName}_derived"" as dimension name, it better to use column name as default dimension name when auto generate dimension for lookup table",smartshark_2_2,3284,kylin,"""[0.9969931840896606, 0.0030067688785493374]"""
1766,230025,Some improvements for lookup table - build change,build change for new lookup table,smartshark_2_2,1644,kylin,"""[0.9976049661636353, 0.002394992858171463]"""
1767,230026,"Support usage of schema name ""default"" in SQL","Calcite will treat ""DEFAULT"" as internal keyword, while ""DEFAULT"" is also used as HIVE's default schema name. We need to escape such case of usage by quote it.",smartshark_2_2,1721,kylin,"""[0.9943141341209412, 0.005685809534043074]"""
1768,230027,Revert KYLIN-2349 and KYLIN-2353,"In KYLIN-2349 and KYLIN-2353, we changed the storage format of BitmapCounter for better performance. In the new format, cardinality and serialized size are recorded in the header part. This enables us to retrieve those information without deserialize the whole data.

However, cardinality and serialized size can be quickly calculated just from the header of [roaring format|https://github.com/RoaringBitmap/RoaringFormatSpec/]. Performance tests show that we could achieve the same performance gain without the format change. The benefits are
* there is no need for user to rebuild existing cube to get better performance
* there is no need for developer to maintain two formats and deal with compatibility issues",smartshark_2_2,1241,kylin,"""[0.9969670176506042, 0.0030329504515975714]"""
1769,230028,Reducers build dictionaries locally,"In KYLIN-1851, we reduce the peek memory usage of the dictionary-building procedure by splitting a single Trie tree structure to Trie forest. But there still exist a bottleneck that all the dictionaries are built in Kylin client. In this issue, we want to use multi reducers to build different dictionaries locally and concurrentlyï¼which can further reduce the peek memory usage as well as speed up the dictionary-building procedure.",smartshark_2_2,1248,kylin,"""[0.9979554414749146, 0.00204458343796432]"""
1770,230029,"Merge ""Build"" and ""Refresh"" in one button","There are two button for cube build:
1. Build: build cube from beginning and incremental load. User could specify start and end date. 
2. Refresh: re-build cube with current date range

![screen shot 2014-10-30 at 3 05 10 pm|https://cloud.githubusercontent.com/assets/1104017/4839735/ace4441c-6004-11e4-8eb9-34a89ad8cb11.png)

Using one button to simplify and reduce confuse for users to build cube:
1. Use ""Build Cube"" as menu
2. Popup dialog for user to specify start (if appliable] and end date, 
3. Fulfill that values from existing cube metadata and instance



---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/32
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: v0.6.5 Release
Assignee: [janzhongi|https://github.com/janzhongi]
Created at: Thu Oct 30 15:29:11 CST 2014
State: open
",smartshark_2_2,1817,kylin,"""[0.9985664486885071, 0.0014335943851619959]"""
1771,230030,Metadata import / export tool (GUI),"Import and export tool for Kylin relative resources settings 

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/267
Created by: [lukehan|https://github.com/lukehan]
Labels: 
Milestone: Backlog
Created at: Fri Dec 26 11:22:09 CST 2014
State: open
",smartshark_2_2,1014,kylin,"""[0.9984863996505737, 0.001513552269898355]"""
1772,230031,Enable SonarCloud for Code Analysis,[https://sonarcloud.io|https://sonarcloud.io/]Â is one of the most popular Code Analysis online service. It could integrate with Travis very well. Developer could check the code coverage/bugs/vulnerabilities more easily.Â ,smartshark_2_2,949,kylin,"""[0.9981433153152466, 0.001856661168858409]"""
1773,230032,auto generate aggregation group ,"when user create a cube, need to generate a default optimized aggregation group for user.this is ticket is for tracking the update on this.",smartshark_2_2,2601,kylin,"""[0.9984858632087708, 0.0015141236362978816]"""
1774,230033,Enhance Project Level ACL,"We are planning on enhancing the project level ACL, the plan as below:
There are five types of predefined user access: System Admin, Project Admin, Management, Operation and Query. 
When syncing an LDAP new user, admin has the option to set the new user as system admin. The other user access, however, need to be set at project level, that means other than system admin which is applied globally on an instance, the other four user access are granted on project by project basis. So that one user, say johndoe, can be project admin in Project A and management access in Project B. 

User role modeler and analyst that are currently set when syncing users, will be retired. Project level access can be assigned to user, but not on role any more. 
Cube level ACL will be retired too, user's access control on a cube will be inherited from user's project level ACL. 

 The expected access level for those 5 types of user access are defined as attached.

!screenshot-1.png!",smartshark_2_2,2313,kylin,"""[0.9984338879585266, 0.001566092367284]"""
1775,230034,support hbase table prefix configurable,"some times we need deploy two kylin env based on same hbase, I want to change hbase table name prefix based two reasons:
1ãdifferent kylin env will generate the same table name
2ãwhile clean invalid htable for one env will cause delete all tables belong to another env.

different kylin env use different namespace is acceptable either.",smartshark_2_2,2001,kylin,"""[0.9986338019371033, 0.0013661300763487816]"""
1776,230035,Tomcat Security Vulnerability Alert. The version of the tomcat for kylin should upgrade to 7.0.85.,"

[SECURITY] CVE-2018-1305 Security constraint annotations applied too late

CVE-2018-1305 Security constraint annotations applied too late

Severity: High 

Versions Affected: Apache Tomcat 9.0.0.M1 to 9.0.4 Apache Tomcat 8.5.0 to 8.5.27 Apache Tomcat 8.0.0.RC1 to 8.0.49 Apache Tomcat 7.0.0 to 7.0.84

Description: Security constraints defined by annotations of Servlets were only applied once a Servlet had been loaded. Because security constraints defined in this way apply to the URL pattern and any URLs below that point, it was possible - depending on the order Servlets were loaded - for some security constraints not to be applied. This could have exposed resources to users who were not authorised to access them.

Mitigation: Users of the affected versions should apply one of the following mitigations. Upgrade to: - Apache Tomcat 9.0.5 or later - Apache Tomcat 8.5.28 or later - Apache Tomcat 8.0.50 or later - Apache Tomcat 7.0.85 or later

References:https://lists.apache.org/thread.html/d3354bb0a4eda4acc0a66f3eb24a213fdb75d12c7d16060b23e65781@%3Cannounce.tomcat.apache.org%3E
",smartshark_2_2,1053,kylin,"""[0.9842846393585205, 0.01571534387767315]"""
1777,230036,Check model/cube name is duplicated when creating model/cube,Add a API that check the model/cube name is duplicated,smartshark_2_2,2093,kylin,"""[0.9983969330787659, 0.0016030976548790932]"""
1778,230037,Kylin JDBC driver should not assume Kylin server listen on either 80 or 443,"Kylin JDBC driver assumes the server is working on 80 or 443 port; When user create a JDBC connection, it will check whether user specify the ""ssl"" property with default value false; if false, it will connect to server with http on port 80; if ssl=true, it will use https to connect the server on 443; Kylin should not make such assumption.",smartshark_2_2,727,kylin,"""[0.8583974838256836, 0.1416025161743164]"""
1779,230038,Make ERROR_RECORD_LOG_THRESHOLD configurable,"currently, the {{BatchConstants.ERROR_RECORD_LOG_THRESHOLD}} is hardcode to 100.I wonder why we accept the error record. 

Normally, the cubing should have zero error record.Besides, even if only have one error record, the query results will be different from Hive or Presto.

So. I think we could make the ERROR_RECORD_LOG_THRESHOLD configurable and the default value is 0.",smartshark_2_2,967,kylin,"""[0.998532772064209, 0.0014671608805656433]"""
1780,230039,HTable size displays 0 bytes if its size less than 1MB,"For cube segment whose size is less than 1MB, web UI display 0 bytes in HBASE tabpage of cube, while rest API return {.... size_kb:700 ...}.",smartshark_2_2,1582,kylin,"""[0.09739407896995544, 0.9026058912277222]"""
1781,230040,"Use deflate level 1 to enable compression ""on the fly""",It is observed that deflator for compression the endpoint results could be pretty slow (few seconds when compression tens of M). As advised by http://java-performance.info/performance-general-compression/,smartshark_2_2,2820,kylin,"""[0.9984099864959717, 0.0015899855643510818]"""
1782,230041,Release Kylin v1.1,Prepare to release Kylin v1.1,smartshark_2_2,1702,kylin,"""[0.9976871013641357, 0.0023128448519855738]"""
1783,230042,record Hive intermediate table size,"kylin only record Hive intermediate table row count, sometimes its size is also required. Record it for later reference.",smartshark_2_2,1690,kylin,"""[0.9980893731117249, 0.0019106015097349882]"""
1784,230043,bad query performance when IN clause contains a value doesn't exist in the dictionary,"For example, we had a query of 39 x values in where clause and there is one
x value not in cube,  which yielded the following result:

Duration: 60.947


Cube Names: [olap]

Total scan count: 2524898

Result row count: 39

(The log shows ""Can't translate value xxx to dictionary ID, roundingFlag 0.
Using default value \xFF"")



And we excluded the x value that is not in cube and re-run the query and got
the another result:

Duration: 2.477

Cube Names: [olap]

Total scan count: 96543

Result row count: 38",smartshark_2_2,2527,kylin,"""[0.9968827962875366, 0.003117184154689312]"""
1785,230044,Optimize HBase connection pool in Kylin side,"In last week's meetup, Yang cao shared they optimized the hbase connection pool to support more concurrent queries, and he'd like to contribute the enhancements to Kylin.

Thanks Cao!",smartshark_2_2,1864,kylin,"""[0.9982391595840454, 0.0017608811613172293]"""
1786,230045,Bind metadata version with release version,"Currently metadata does not have special property to identify its version, even though we already versions of metadata in 1.x, 2.0 and 2.x. If each file in the metadata is bound with version it will be easier for upgrading related issues.",smartshark_2_2,2587,kylin,"""[0.9984923601150513, 0.001507628709077835]"""
1787,230046,Check duplicated measure name,"The duplicated measure's name will lead to query failed, so we should check duplicated measure name.",smartshark_2_2,3113,kylin,"""[0.9170309901237488, 0.08296903222799301]"""
1788,230047,The Key of the Snapshot to the related lookup table may be not informative,"Currently the key for the snapshot stored in hbase metadata file is as follows:
ResourceStore.SNAPSHOT_RESOURCE_ROOT + ""/"" + new File(signature.getPath()).getName() + ""/"" + uuid + "".snapshot""

However, some hive tables stored in hive may organized like dirName/tableName/00, dirName/tableName/01.

Based on current setting, the key will be ResourceStore.SNAPSHOT_RESOURCE_ROOT + ""/"" + 00 + ""/"" + uuid + "".snapshot"", which is lack of the table name information.",smartshark_2_2,3145,kylin,"""[0.8358936309814453, 0.16410638391971588]"""
1789,230048,HyperLogLogPlusCounter will become inaccurate when there're billions of entries,"
        final List<HyperLogLogPlusCounter> counters = Lists.newArrayList();
        ExecutorService service = Executors.newFixedThreadPool(20);
        final CountDownLatch latch = new CountDownLatch(20);
        for (int i = 0; i < 20; i++) {

            service.submit(new Runnable() {
                @Override
                public void run() {
                    Random rand = new Random();
                    HyperLogLogPlusCounter counter = new HyperLogLogPlusCounter(14);
                    for (long j = 0; j < 500000000; j++) {
                        if (j % 1000000 == 1) {
                            System.out.println(j);
                        }
                        counter.add("""" + rand.nextLong());
                    }
                    System.out.println(""final"" + counter.getCountEstimate());
                    counters.add(counter);
                    latch.countDown();
                }
            });
        }
        latch.await();
        System.out.println(""latch done"");

        HyperLogLogPlusCounter ret = new HyperLogLogPlusCounter(14);
        for (HyperLogLogPlusCounter c : counters) {
            ret.merge(c);
        }
        System.out.println(ret.getCountEstimate());


expected output is 10B however the output can be less than 1B",smartshark_2_2,2867,kylin,"""[0.5058136582374573, 0.4941863715648651]"""
1790,230049,remove refresh in cube manipulation,"

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/107
Created by: [binmahone|https://github.com/binmahone]
Labels: enhancement, 
Assignee: [janzhongi|https://github.com/janzhongi]
Created at: Tue Nov 25 17:33:01 CST 2014
State: open
",smartshark_2_2,1591,kylin,"""[0.9969322681427002, 0.0030677334871143103]"""
1791,230050,"Refactor the ACL code about ""checkPermission"" and ""hasPermission""","According to the design of ""checkPermission"" and ""HasPermission"",

Refactoring the ACL code",smartshark_2_2,924,kylin,"""[0.9983749389648438, 0.0016250871121883392]"""
1792,230051,Cube parallel merge,Investigate to remove the restriction on cube merge: only 1 merge job is allowed for a cube at one time. This is important especially for streaming cubing.,smartshark_2_2,2379,kylin,"""[0.998497724533081, 0.0015022417064756155]"""
1793,230052,Sort cube name doesn't work well ,"Please refer to attached image.
Only happens to name column. Other columns work well.",smartshark_2_2,2660,kylin,"""[0.16106490790843964, 0.838935136795044]"""
1794,230053,Use by-layer cubing algorithm if there is memory hungry measure,"Fast-cubing (also called in-mem cubing) is not good for the measure types which cost much memory, like Distinct count and TopN. If found such measure type, use the by-layer cubing automatically.",smartshark_2_2,2693,kylin,"""[0.9984723925590515, 0.0015276232734322548]"""
1795,230054,Kylin support detail data query from fact table,"Now Kylin does not support query correct detail rows from fact table like:
select column1,column2,column3 from fact_table
The jira KYLIN-1075 add the ""SUM"" function on the measure column if defined.
But only the column number type is support.

-I change some code to support this issue:-
-Add a ""VALUE"" measure function : the same value and datatype in the input and output of this function.-

-If you want to query detail data from fact table-
-*require*:-
-1.Configure the column which not dimensions to ""VALUE"" or ""SUM"" measure.(If not configure measure function in the column will get NULL value)-
-2.The source table must has an unique value column and configure it as dimension.-

If you have the better solution please comment here.

2016-03-21:
The final solution:
Add a ""RAW"" measure function which return the list of all the value by one key.
If query detail data from fact table
*require*:
Configure the column to ""RAW"" measure.(If not configure measure function in the column and do the RAW query will get the incorrect value!)",smartshark_2_2,2719,kylin,"""[0.9387809038162231, 0.06121917441487312]"""
1796,230055,cubestatsreader support reading unfinished segments,"currently cubestatsreader only deal with READY segments, actually we have enough stats after the cubing job's first 2 or three steps",smartshark_2_2,1276,kylin,"""[0.986781895160675, 0.013218174688518047]"""
1797,230056,read topic partitions from Kafka instead of metadata,"In config json, partition number can be omitted, read it from zookeeper. Or another option is keep the configure, and validate it against zookeeper at runtime. The latter makes the mapping between partition and sharding clearer",smartshark_2_2,2603,kylin,"""[0.9983772039413452, 0.0016227646265178919]"""
1798,230057,Add 'auto.purge=true' when creating intermediate hive table or redistribute a hive table,"At kylin side, we can add auto.purge=true when creating intermediate table.

However, to make âauto.purgeâ effective for âinsert overwrite tableâ, we still need one patch for hive.
https://issues.apache.org/jira/browse/HIVE-15880",smartshark_2_2,1384,kylin,"""[0.9984355568885803, 0.0015644454397261143]"""
1799,230058,"Hive may fail to create flat table with ""GC overhead error""","in conf/kylin_hive_conf.xml, it enables ""hive.auto.convert.join.noconditionaltask"", which is the optimization about converting common join into mapjoin based on the input file size, and give a big value (300000000, 300MB) to ""hive.auto.convert.join.noconditionaltask.size"". So if the memory of the mapper isn't enough, it will cause the error.

To fix the error, user can reduce the  ""hive.auto.convert.join.noconditionaltask.size"" in conf/kylin_hive_conf.xml, and then retry the job.

",smartshark_2_2,3004,kylin,"""[0.1743106096982956, 0.8256893754005432]"""
1800,230059,Build dictionary in Hadoop cluster,"Kylin build dictionary in job engine node, usually this is okay. But if there is some high cardinality dimentions, the JVM heap couldn't fit in all distinct values, then job engine instance will crash with OOM error.

Need to enhance on this, move the dictionary building to another process or a hadoop node. Ideally only need modify ""CreateDictionaryJob.java"", move the dictionary building to a mapper task.

",smartshark_2_2,2520,kylin,"""[0.9961010217666626, 0.0038990320172160864]"""
1801,230060,Support Window Function,"Kylin didn't support window function yet. Here's a test query:
{code}
select lstg_format_name, count(*) over(partition by lstg_format_name)
from kylin_sales
{code}
The query threw a exception and here's the error log and stack trace:
{code}
Error while executing SQL ""select lstg_format_name, count(*) over(partition by lstg_format_name) from kylin_sales LIMIT 50000"": cannot translate call COUNT() OVER (PARTITION BY $t3 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
{code}
{code}
Caused by: java.lang.RuntimeException: cannot translate call COUNT() OVER (PARTITION BY $t3 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateCall(RexToLixTranslator.java:533)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate0(RexToLixTranslator.java:507)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate(RexToLixTranslator.java:219)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate0(RexToLixTranslator.java:472)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate(RexToLixTranslator.java:219)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate(RexToLixTranslator.java:214)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateList(RexToLixTranslator.java:700)
        at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateProjects(RexToLixTranslator.java:189)
        at org.apache.calcite.adapter.enumerable.EnumerableCalc.implement(EnumerableCalc.java:188)
        at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.visitChild(EnumerableRelImplementor.java:97)
        at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.visitChild(OLAPRel.java:183)
        at org.apache.calcite.adapter.enumerable.EnumerableLimit.implement(EnumerableLimit.java:106)
        at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.visitChild(EnumerableRelImplementor.java:97)
        at org.apache.kylin.query.relnode.OLAPRel$JavaImplementor.visitChild(OLAPRel.java:183)
        at org.apache.kylin.query.relnode.OLAPToEnumerableConverter.implement(OLAPToEnumerableConverter.java:108)
        at org.apache.calcite.adapter.enumerable.EnumerableRelImplementor.implementRoot(EnumerableRelImplementor.java:102)
        at org.apache.calcite.adapter.enumerable.EnumerableInterpretable.toBindable(EnumerableInterpretable.java:92)
        at org.apache.calcite.prepare.CalcitePrepareImpl$CalcitePreparingStmt.implement(CalcitePrepareImpl.java:1171)
        at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:297)
        at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:196)
        at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:721)
        at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:588)
        at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:558)
        at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:214)
        at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:573)
        at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:571)
        at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:135)
{code}",smartshark_2_2,3139,kylin,"""[0.8412446975708008, 0.15875522792339325]"""
1802,230061,Benchmark scan speed on different storage,"We want to better understand the scan speed of different storage. Namely HBase, Cassandra, Parquet, MemMappedFile etc.

To be specific, we test reading 10 millions records with single thread. Each record is 5 dimensions of type int4, and 2 measures of type long8, totally ~50 bytes if include cuboid ID and other overhead.",smartshark_2_2,2772,kylin,"""[0.9983752965927124, 0.0016246909508481622]"""
1803,230062,Avoid upgrade column in OLAPTable,"before CALCITE-845, to avoid sum(integer_typed_col) to overflow, we worked around by upgrading all integer columns (which appearing in sum measure ) to bigint type. The workaround will change the column's type without notifying users, and will easily lead to code mess. 

Now that CALCITE-845 is ready, we can use that to provide a cleaner impl",smartshark_2_2,1090,kylin,"""[0.9973276853561401, 0.0026723297778517008]"""
1804,230063,put different build job's log into different files,"currently all build job's log are messed up in a single file (kylin_job.log?), which is very unfriendly to those who want to check logs",smartshark_2_2,2346,kylin,"""[0.997986912727356, 0.0020130763296037912]"""
1805,230064,"Tableau could send ""select *"" on a big table","From 	èµµå¤©ç <zhaotianshuo@meizu.com>

you can try to add a filter on a colum of some big fact table,before you try to pick some colum value as filter, tableau will send a select * query without any limit clause to list all the column value for you to choose from.and that is where it cause this problem.

forgot to mention that we do have some dimension table with 10x million records,add a filter on that will also cause the same issue",smartshark_2_2,919,kylin,"""[0.8443574905395508, 0.15564250946044922]"""
1806,230065,Enhance BadQueryDetector to include query id,"To better correlate bad queries with the bottom physical execution information, it's better to let BadQueryDetector include query id",smartshark_2_2,834,kylin,"""[0.9982971549034119, 0.0017029113369062543]"""
1807,230066,"When modifying a model, Save after deleting a lookup table. The internal error will pop up.","In case of modifying a model which is already used by some cubes, users can delete the lookup table and do the ""Save"" operation. However, after it's done, the internal error will pop up.

In normal case, if the model is used by some cubes, we cannot modify the model without disable or drop those cubes.",smartshark_2_2,3146,kylin,"""[0.21856623888015747, 0.7814337611198425]"""
1808,230067,Out of memory in mapper when building cube in mem,"2015-04-08 03:08:56,992 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab;  Ignoring.
2015-04-08 03:08:56,996 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy;  Ignoring.
2015-04-08 03:08:56,999 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2015-04-08 03:08:57,003 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab;  Ignoring.
2015-04-08 03:08:57,004 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal;  Ignoring.
2015-04-08 03:08:57,006 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name;  Ignoring.
2015-04-08 03:08:57,012 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address;  Ignoring.
2015-04-08 03:08:57,021 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs;  Ignoring.
2015-04-08 03:08:57,022 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS;  Ignoring.
2015-04-08 03:08:57,029 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab;  Ignoring.
2015-04-08 03:08:57,034 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy;  Ignoring.
2015-04-08 03:08:57,041 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs;  Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group;  Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal;  Ignoring.
2015-04-08 03:08:57,047 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class;  Ignoring.
2015-04-08 03:08:57,048 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2015-04-08 03:08:57,050 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl;  Ignoring.
2015-04-08 03:08:57,053 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal;  Ignoring.
2015-04-08 03:08:58,279 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-04-08 03:08:58,347 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink ganglia started
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started
2015-04-08 03:08:58,570 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:
2015-04-08 03:08:58,571 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: HDFS_DELEGATION_TOKEN, Service: 10.115.206.112:8020, Ident: (HDFS_DELEGATION_TOKEN token 8430348 for b_kylin)
2015-04-08 03:08:58,650 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1427705526386_110981, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@69607702)
2015-04-08 03:08:58,928 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
2015-04-08 03:08:59,367 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /hadoop/1/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/2/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/3/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/4/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/5/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/6/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/7/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981
2015-04-08 03:08:59,567 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab;  Ignoring.
2015-04-08 03:08:59,568 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy;  Ignoring.
2015-04-08 03:08:59,569 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2015-04-08 03:08:59,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab;  Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal;  Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name;  Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.checkpoint.dir;  Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address;  Ignoring.
2015-04-08 03:08:59,576 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs;  Ignoring.
2015-04-08 03:08:59,577 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS;  Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab;  Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.name.dir;  Ignoring.
2015-04-08 03:08:59,582 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy;  Ignoring.
2015-04-08 03:08:59,585 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs;  Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group;  Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal;  Ignoring.
2015-04-08 03:08:59,587 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class;  Ignoring.
2015-04-08 03:08:59,588 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2015-04-08 03:08:59,589 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl;  Ignoring.
2015-04-08 03:08:59,591 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal;  Ignoring.
2015-04-08 03:09:00,499 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
2015-04-08 03:09:01,285 INFO [main] org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2015-04-08 03:09:02,362 INFO [main] org.apache.hadoop.mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@47e7cecf
2015-04-08 03:09:02,500 INFO [main] org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 268435452(1073741808)
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 1024
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: soft limit at 966367616
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 1073741824
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: kvstart = 268435452; length = 67108864
2015-04-08 03:09:03,717 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library
2015-04-08 03:09:03,720 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev dbd51f0fb61f5347228a7a23fe0765ac1242fcdf]
2015-04-08 03:09:03,846 INFO [main] org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2015-04-08 03:09:03,847 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.deflate]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,359 INFO [main] org.apache.hive.hcatalog.mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe with properties {name=DEFAULT.kylin_intermediate_PC_SESSION_COPY_20150322000000_20150323000000_841ce77b_81c5_4c0c_941a_6f2e309a131f, numFiles=0, field.delim=, columns.types=string,int,string,string,string,string,string,string,string,string,string,string,string,string,bigint,bigint,bigint,bigint, serialization.format=, columns=default_pc_session_tenantname,default_pc_session_tenantsite,default_pc_session_devicefamily,default_pc_session_deviceclass,default_pc_session_osfamily,default_pc_session_osversion,default_pc_session_browserfamily,default_pc_session_browserversion,default_pc_session_trafficsource,default_pc_session_continent,default_pc_session_country,default_pc_session_region,default_pc_session_city,default_pc_session_streamid,default_pc_session_bounce,default_pc_session_retvisitor,default_pc_session_serveventcnt,default_pc_session_absduration, rawDataSize=50265185482, numRows=507325307, EXTERNAL=TRUE, serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, COLUMN_STATS_ACCURATE=true, totalSize=0, serialization.null.format=\N, transient_lastDdlTime=1427989487}
2015-04-08 03:09:04,454 INFO [main] org.apache.kylin.job.hadoop.AbstractHadoopJob: The absolute path for meta dir is /hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,456 INFO [main] org.apache.kylin.common.KylinConfig: Use KYLIN_CONF=/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,476 INFO [main] org.apache.kylin.cube.CubeManager: Initializing CubeManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,480 INFO [main] org.apache.kylin.common.persistence.ResourceStore: Using metadata url /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta for resource store
2015-04-08 03:09:04,955 INFO [main] org.apache.kylin.cube.CubeDescManager: Initializing CubeDescManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,956 INFO [main] org.apache.kylin.cube.CubeDescManager: Reloading Cube Metadata from folder /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta/cube_desc
2015-04-08 03:09:05,177 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Create base cuboid 8191
2015-04-08 03:09:08,171 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 100000 records!
2015-04-08 03:09:09,776 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 200000 records!
2015-04-08 03:09:11,903 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 300000 records!
2015-04-08 03:09:13,739 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 400000 records!
2015-04-08 03:09:15,243 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 500000 records!
2015-04-08 03:09:15,389 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 500000 rows, going to compress base cuboid table
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 87680
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 10 seconds.
2015-04-08 03:09:27,213 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 600000 records!
2015-04-08 03:09:28,391 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 700000 records!
2015-04-08 03:09:29,963 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 800000 records!
2015-04-08 03:09:31,408 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 900000 records!
2015-04-08 03:09:33,124 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1000000 records!
2015-04-08 03:09:33,300 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1000000 rows, going to compress base cuboid table
2015-04-08 03:09:45,069 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 183872
2015-04-08 03:09:45,070 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 11 seconds.
2015-04-08 03:09:46,535 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1100000 records!
2015-04-08 03:09:47,942 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1200000 records!
2015-04-08 03:09:49,166 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1300000 records!
2015-04-08 03:09:50,398 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1400000 records!
2015-04-08 03:09:51,170 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Totally handled 1453555 records!
2015-04-08 03:09:51,302 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: The source data has 1453555 rows
2015-04-08 03:09:51,303 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1453555 rows, going to compress base cuboid table
2015-04-08 03:10:54,326 ERROR [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: stream build failed
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
	at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
	at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
	at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
	at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
	at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-04-08 03:10:54,329 INFO [main] org.apache.hadoop.mapred.MapTask: Starting flush of map output
2015-04-08 03:10:54,334 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2015-04-08 03:10:54,336 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.lzo_deflate]
2015-04-08 03:10:54,591 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: Failed to build cube in mapper 28
	at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:99)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
	... 8 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
	at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
	at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
	at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
	at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
	at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
	at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",smartshark_2_2,2531,kylin,"""[0.9451972842216492, 0.05480271950364113]"""
1809,230068,Faster bitmap indexes with Roaring bitmaps,"Kylin is using ConciseSet for bitmap indexing. It was found that Roaring bitmaps are often much better than ConciseSet (e.g., see experimental section in http://arxiv.org/pdf/1402.6407.pdf ). The compression is often better and the speed difference can be substantial. This is even more so with version 0.5 of Roaring.

There is a high quality Java implementation that is used by Apache Spark and Druid.io. The Druid people found that switching to Roaring bitmaps could improve real-world performance by 30% or more. 

Source code: 
https://github.com/lemire/RoaringBitmap/

Importing from Maven:

http://central.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.5.3/

<dependencies>
    <dependency>
      <groupId>org.roaringbitmap</groupId>
      <artifactId>RoaringBitmap</artifactId>
      <version>[0.5,)</version>
    </dependency>
 </dependencies>

Online API :

http://lemire.me/docs/RoaringBitmap/

JavaDoc Jar: 

http://central.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.5.3/RoaringBitmap-0.5.3-javadoc.jar



When desired, the library supports memory file mapping, so that  out-of-JVM heap memory is used instead. This can greatly improve IO issues. The library is available under the Apache license and is patent-free.

There is an extensive real-data benchmark framework which you can run for yourself to compare Roaring with competitive alternatives such as ConciseSet :

https://github.com/lemire/RoaringBitmap/tree/master/jmh

Running such a benchmark can be as simple as launching a script.

For Druid, the bitmap format was made ""pluggable"" so you can switch from one format to the other using a configuration flag. This is implemented through simple wrappers, e.g., see

https://github.com/metamx/bytebuffer-collections/tree/master/src/main/java/com/metamx/collections/bitmap

So it can be really easy to make it possible to switch the format while preserving backward compatibility if needed... 

It is probably not difficult work to integrate Roaring in Kylin (maybe a day or so of programming) and it could potentially improve performance while reducing memory storage.

Note: Roaring bitmaps were also adopted in Apache Lucene, though they have their own implementation, see 
https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps",smartshark_2_2,588,kylin,"""[0.9981247782707214, 0.001875243615359068]"""
1810,230069,Have a default measure value to save storage space,"The idea is from Alberto.

When a Cuboid is very sparse, like below:

Â 
||Dim1||Dim2||Measure||
|1|A|0|
|1|B|100|
|1|C|0|
|2|A|0|
|2|B|0|
|2|C|130|

It becomes beneficial to store only the non-zero lines and remember a default measure value for the rest.

That is to only save the below lines, plus the default value 0.
||Dim1||Dim2||Measure||
|1|B|100|
|2|C|130|

However this comes with the assumption that all combinations of dimensions exist in cuboid.

Â 

Â ",smartshark_2_2,1764,kylin,"""[0.9984109401702881, 0.0015890684444457293]"""
1811,230070,Automatically resume running jobs when job engine failover,"When Kylin's job engine failover, it will put all running job to ""ERROR"". User need manually resume such jobs to move ahead. This should be automated when we implement the auto-balance and HA.",smartshark_2_2,2180,kylin,"""[0.9985063672065735, 0.0014935929793864489]"""
1812,230071,Save cube change is very slow,"Edit a cube, made some change and then save, it takes a couple of minutes to finish; In backend there is no error message; Need investigate why it is so slow.",smartshark_2_2,594,kylin,"""[0.9840297698974609, 0.015970170497894287]"""
1813,230072,support config key and value deserializer for Kafka client,"So custom deserializer can be used to  convert value of Kafka message to json which is required by Kylin. For example, from Avro to Json.",smartshark_2_2,1023,kylin,"""[0.99842369556427, 0.0015763011761009693]"""
1814,230073,File not found Exception when processing union-all in TEZ mode,"If hive.execution.engine=TEZ and hql contains union all, it causes exception like: file not found when materializing the view or redistributing flat hive table.

Here is the reason:
http://grokbase.com/t/hive/user/162r80a2g9/anyway-to-avoid-creating-subdirectories-by-insert-with-union

i.e. ""The Tez execution of UNION is entirely parallel &
the task-ids overlaps - so the files created have to have unique names.

But the total counts for ""Map 1"" and ""Map 2"" are only available as the job
runs, so they write to different dirs.""


 ",smartshark_2_2,1078,kylin,"""[0.13795419037342072, 0.8620458841323853]"""
1815,230074,Manager large number of entries in metadata store,"Kylin saves cube metadata, table metadata as well as job history/output in a metadata store. The HBaseMetadataStore is a fault tolerant implementation which brings no extra dependencies to the system. We use it in real world deployments.

When cube or hive table is updated, the correspond entries in metadata store simply updated.(so there's no way to trace history cube definitions, anyway this is not very expected function).However Job histories and outputs are a little special, each cubing job's definition and output are saved as new entries in the metadata store. As more and more jobs accumulate, a lot of job histories will reside in the metadata store. This might harm frontend performance when user wants to query job histories.

We should tackle the problem from two perspectives:
1.Backend tool to delete/archive job history based on given conditions,e.g. ""all jobs that is older than one month and not referenced by any cube segment(each cube segment keeps track of which job created it)""
2.Frontend display enforce timestamp filter to retrieve from metadata store for efficiency. When showing job lists, for example, a ""Show last N days"" filter is enforced, where N is configurable by the user. For HBaseMetadataStore, we saved timestamp for each entry in a separate column, this is where HBase SingleColumnValueFilter can help.

We can start working this on 2.x-staging branch(as it is the latest dev branch, and is more friendly to developers), and backport it to 1.x-staging branch if necessary.",smartshark_2_2,668,kylin,"""[0.930312991142273, 0.06968702375888824]"""
1816,230075,Mondrian and Kylin integration ,"Original work from Sebastien: https://github.com/mustangore/thesis


Hi,
    This is Luke Han from Kylin team. 
    I just found your project on github: https://github.com/mustangore/thesis 

    I'm really interesting about the project's current status and would like to know do you have interesting to contribute this to Kylin community and make it more stable, flexible for Pentaho and other BI tools?
    Please feel free to let me know your idea.
    Thank you very much.

Best Regards!
---------------------

Luke Han

SÃ©bastien Jelsch
22:35 (17å°æ¶å)

åéè³ æ 
Hi Luke,

my name is Sebastien Jelsch, Iâm currently working as a student at the German company inovex. As a part of my thesis at the University of Karlsruhe (Germany) I have get the task to extend a horizontal scalable OLAP engine with an analysis UI. For this I have integrated Kylin into Pentaho Business Analytics Platform.

With Saiku as an OLAP UI on top of Kylin OLAP, I am able to generate MDX statements via dragân'drop. Mondrian converts this generated MDX statement in SQL and sends it to Kylin using the existing Kylin JDBC driver. The first results are very promising. Currently, I study and check the advantages and disadvantages, what problems arise and so onâ¦

But I did not have to make any changes to Kylin. The integration of Kylin into Pentaho Business Analytics Platform was possible with some changes to Kettle-Core and Mondrian (add KylinDialect and Patch for generate explicit joins instead of implicit joins. For this Iâve also created a ticket: https://issues.apache.org/jira/browse/KYLIN-611 ). Iâm going to create a pull request for my changes in Mondrian project soon.

As I did not have to make any changes in Kylin, Iâm not sure what I can contribute to Kylin. But I would like to help, since this project is very interesting and has a lot of potential from my point of view. Here in Germany, I have submitted a presentation at the âdata2dayâ conference. I would like to give a talk about Kylin. I hope, my proposal will be accepted soon.

Currently Iâm also working on an integration of Mondrian in Kylin, so Kylin can accept MDX statements via REST API. Iâm still at the beginning of my work, but I was able to achieve small successes, see my picture attached (you can find my first patch in the kylin folder on my github project).

Please let me know if you have an idea how I can help you and the Kylin team.

Thank you for your interest.

With best regards,
SÃ©bastien Jelsch",smartshark_2_2,3176,kylin,"""[0.9982898831367493, 0.0017100563272833824]"""
1817,230076,Improvement on subqueries: allow grouping by columns from subquery,"{code:sql}

select test_kylin_fact.lstg_format_name, xxx.week_beg_dt , sum(test_kylin_fact.price) as GMV 
 , count(*) as TRANS_CNT 
 from  

 test_kylin_fact

 inner JOIN test_category_groupings
 ON test_kylin_fact.leaf_categ_id = test_category_groupings.leaf_categ_id AND test_kylin_fact.lstg_site_id = test_category_groupings.site_id 


 inner JOIN (select cal_dt,week_beg_dt from edw.test_cal_dt  where week_beg_dt >= DATE '2010-02-10'  ) xxx
 ON test_kylin_fact.cal_dt = xxx.cal_dt 


 where test_category_groupings.meta_categ_name  <> 'Baby'
 group by test_kylin_fact.lstg_format_name, xxx.week_beg_dt 
{code}

will fail due to groupby  xxx.week_beg_dt,  because week_beg_dt does not necessarily appear in the cube",smartshark_2_2,1842,kylin,"""[0.998180627822876, 0.0018193896394222975]"""
1818,230077,Cube Name displays the name of CubeDesc on UI,"In the cube list, the name displays the real name of cube.
In cube detail tabpage, the cube name displays the name of CubeDesc.",smartshark_2_2,1838,kylin,"""[0.6681327223777771, 0.3318672776222229]"""
1819,230078,Add Hybrid as a federation of Cube and Inverted-index realization,"To better leverage Cube and Inverted-index to support queries against historic (cube) and real-time (stream) dataset, a hybrid realization can introducted into Kylin; The hybrid will be federation of multiple cube or inverted-index instances. By checking the filtering conditions it can determine and forward the query to the underlying cube or inverted-index;",smartshark_2_2,2554,kylin,"""[0.9983239769935608, 0.001676043844781816]"""
1820,230079,Kylin help has duplicate items,"Kylin help has duplicate items, refer to 'help_duplicate_record.png'.
In some cases, such as opening two clients at the same time, the help of kylin client will appear duplicate items.
Through the code debug, I find the 'initWebConfigInfo' function has been called twice, but the array parameter 'Config.documents' has not been cleaned at the beginning, so after the second call, the array appears duplicate items.
I have added code of initial array in the patch, please review it, thanks!
",smartshark_2_2,835,kylin,"""[0.9243692755699158, 0.07563064992427826]"""
1821,230080,WEB-Global-Dictionary bug fix and improve,"in the 1.5.4.1 version of Kylin, the web UI for WEB-Global-Dictionary couldn't select column from measure columns and need user to input the dictionary builder class manually.",smartshark_2_2,3297,kylin,"""[0.6275323033332825, 0.37246769666671753]"""
1822,230081,Increase HDFS block size 1GB,"Increase HDFS block size 1GB or close, use ""mapred.max.split.size"" to control mapper's input size. Verify mapreduce performance retains after the change.

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/17
Created by: [liyang-gmt8|https://github.com/liyang-gmt8]
Labels: enhancement, 
Milestone: Backlog
Assignee: [abansal|https://github.com/abansal]
Created at: Fri Oct 24 15:41:26 CST 2014
State: open
",smartshark_2_2,424,kylin,"""[0.9985278844833374, 0.0014720631297677755]"""
1823,230082,Building streaming cube do not need to set  âKAFKA_HOMEâ environment variable,"You need havaÂ  KAFKA environment and set âKAFKA_HOMEâ environment variable when buildingÂ streaming cube, or else you can not submit a streaming cube building job, like pic 1ï¼

After we change the kafka dependency scope mode described in the KYLIN-3393.patch , we can build streaming cube without a kafka environment in the kylin service node successfullyï¼ like pic 2ï¼",smartshark_2_2,1664,kylin,"""[0.9650691747665405, 0.03493088483810425]"""
1824,230083,RawQueryLastHacker should group by all possible dimensions,"currently RawQueryLastHacker make the raw query group by columns existing in query (if (tupleInfo.hasColumn(col))). The approach would fail to leverage limit push down if the existing columns are not a ""prefix"" of row keys.(org.apache.kylin.storage.gtrecord.GTCubeStorageQueryBase#enableStorageLimitIfPossible)

On the other hand, a large portion of the raw queries are random queries like ""select * from fact "" or ""select * from fact inner join lookup where year =2000"" . Keeping these queries return fast is important to impress users",smartshark_2_2,1942,kylin,"""[0.48929187655448914, 0.5107081532478333]"""
1825,230084,Cannot have comments in the end of New Query textbox,"1. Navigate to http://sandbox:7070/kylin/query

2. Input following sql into textbox:
select * from test_kylin_fact
---comment

3. Click Submit button

Found Errors:
Lexical error at line 2, column 23. Encountered: <EOF> after : """"",smartshark_2_2,2435,kylin,"""[0.6871509552001953, 0.3128490447998047]"""
1826,230085,Enhance filter on high cardinality in Tableau,"When user use seller_id as filter, the current ODBC will only show the first 100K scan result. And the query will not re-run when given specific value.

There's enhancement should be offered from Tableau side to aware such high cardinality column and perform different behavior to avoid such issue.


---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/325
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: Backlog
Created at: Fri Dec 26 15:32:09 CST 2014
State: open
",smartshark_2_2,996,kylin,"""[0.9983820915222168, 0.0016178578371182084]"""
1827,230086,Enhance project level ACL,"1. User could see all project list before login (login page will list all projects)
2. User only could login to project which he has permission (based on ACL settings)
3. Access will be denied if user try to login to project which they do not have permission
4. User could only switch to project which they have permission after login (gray other options)

",smartshark_2_2,2414,kylin,"""[0.9983115196228027, 0.0016885284567251801]"""
1828,230087,Merge segments from HBase snapshots,"HBase introducesÂ theÂ MultiTableSnapshotInputFormat since 1.2Â ; Kylin can merge segments with this method, so that the cuboid files can be deleted.

Â 
 # HBASE-13356
 # Sample code:Â https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java",smartshark_2_2,1787,kylin,"""[0.9975773692131042, 0.002422704128548503]"""
1829,230088,Provide landing page,"Sample idea:


![image|https://cloud.githubusercontent.com/assets/1104017/5556171/87511dc0-8d0d-11e4-9312-f5058f8049c8.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/299
Created by: [lukehan|https://github.com/lukehan]
Labels: newfeature, 
Milestone: Backlog
Created at: Fri Dec 26 14:45:16 CST 2014
State: open
",smartshark_2_2,1009,kylin,"""[0.9986324906349182, 0.0013674604706466198]"""
1830,230089,Support Extract() on timestamp column,"for timestamp columns, tableau will generate queries like 

""SELECT SUM(""fact"".""TOTALEVENTCT"") AS ""sum_TOTALEVENTCT_ok"",    EXTRACT(YEAR FROM ""fact"".""HOUR_START"") AS ""yr_HOUR_START_ok"" FROM ""DEFAULT"".""fact"" ""fact"" GROUP BY  EXTRACT(YEAR FROM ""fact"".""HOUR_START"")""

and kylin will throw exception:

[http-bio-8080-exec-4]:[2015-06-07 22:19:16,055][ERROR][org.apache.kylin.rest.controller.QueryController.doQuery(QueryController.java:227)] - Exception when execute sql
java.sql.SQLException: error while executing SQL ""SELECT SUM(""SEO_SESSIONS_FACT"".""TOTALEVENTCT"") AS ""sum_TOTALEVENTCT_ok"",    EXTRACT(YEAR FROM ""SEO_SESSIONS_FACT"".""HOUR_START"") AS ""yr_HOUR_START_ok"" FROM ""DEFAULT"".""SEO_SESSIONS_FACT"" ""SEO_SESSIONS_FACT"" GROUP BY  EXTRACT(YEAR FROM ""SEO_SESSIONS_FACT"".""HOUR_START"")"": org.eigenbase.rex.RexCall cannot be cast to org.eigenbase.rex.RexInputRef
at net.hydromatic.avatica.Helper.createException(Helper.java:39)
at net.hydromatic.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:79)
at org.apache.kylin.rest.service.QueryService.execute(QueryService.java:349)
at org.apache.kylin.rest.service.QueryService.executeQuery(QueryService.java:267)
at org.apache.kylin.rest.service.QueryService.query(QueryService.java:111)
at org.apache.kylin.rest.service.QueryService$$FastClassByCGLIB$$4957273f.invoke(<generated>)
at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618)
at org.apache.kylin.rest.service.QueryService$$EnhancerByCGLIB$$8c175781.query(<generated>)
at org.apache.kylin.rest.controller.QueryController.doQuery(QueryController.java:207)
at org.apache.kylin.rest.controller.QueryController.query(QueryController.java:93)
at org.apache.kylin.rest.controller.QueryController$$FastClassByCGLIB$$fc039d0b.invoke(<generated>)
at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
at org.springframework.aop.framework.Cglib2AopProxy$CglibMethodInvocation.invokeJoinpoint(Cglib2AopProxy.java:689)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:150)
at com.ryantenney.metrics.spring.TimedMethodInterceptor.invoke(TimedMethodInterceptor.java:48)
at com.ryantenney.metrics.spring.TimedMethodInterceptor.invoke(TimedMethodInterceptor.java:34)
at com.ryantenney.metrics.spring.AbstractMetricMethodInterceptor.invoke(AbstractMetricMethodInterceptor.java:59)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:622)
at org.apache.kylin.rest.controller.QueryController$$EnhancerByCGLIB$$7e28189f.query(<generated>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882)
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:789)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.springframework.web.filter.AbstractRequestLoggingFilter.doFilterInternal(AbstractRequestLoggingFilter.java:193)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:76)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:97)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter.doFilter(DefaultLoginPageGeneratingFilter.java:91)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:183)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:105)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:205)
at com.thetransactioncompany.cors.CORSFilter.doFilter(CORSFilter.java:266)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)
at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ClassCastException: org.eigenbase.rex.RexCall cannot be cast to org.eigenbase.rex.RexInputRef
at org.apache.kylin.query.relnode.OLAPProjectRel.translateRexCall(OLAPProjectRel.java:181)
at org.apache.kylin.query.relnode.OLAPProjectRel.translateRexNode(OLAPProjectRel.java:144)
at org.apache.kylin.query.relnode.OLAPProjectRel.buildColumnRowType(OLAPProjectRel.java:127)
at org.apache.kylin.query.relnode.OLAPProjectRel.implementOLAP(OLAPProjectRel.java:114)
at org.apache.kylin.query.relnode.OLAPRel$OLAPImplementor.visitChild(OLAPRel.java:78)
at org.apache.kylin.query.relnode.OLAPAggregateRel.implementOLAP(OLAPAggregateRel.java:135)
at org.apache.kylin.query.relnode.OLAPRel$OLAPImplementor.visitChild(OLAPRel.java:78)
at org.apache.kylin.query.relnode.OLAPProjectRel.implementOLAP(OLAPProjectRel.java:107)
at org.apache.kylin.query.relnode.OLAPRel$OLAPImplementor.visitChild(OLAPRel.java:78)
at org.apache.kylin.query.relnode.OLAPToEnumerableConverter.implement(OLAPToEnumerableConverter.java:63)
at net.hydromatic.optiq.rules.java.EnumerableRelImplementor.implementRoot(EnumerableRelImplementor.java:64)
at net.hydromatic.optiq.prepare.OptiqPrepareImpl$OptiqPreparingStmt.implement(OptiqPrepareImpl.java:795)
at net.hydromatic.optiq.prepare.Prepare.prepareSql(Prepare.java:282)
at net.hydromatic.optiq.prepare.Prepare.prepareSql(Prepare.java:178)
at net.hydromatic.optiq.prepare.OptiqPrepareImpl.prepare2_(OptiqPrepareImpl.java:412)
at net.hydromatic.optiq.prepare.OptiqPrepareImpl.prepare_(OptiqPrepareImpl.java:318)
at net.hydromatic.optiq.prepare.OptiqPrepareImpl.prepareSql(OptiqPrepareImpl.java:287)
at net.hydromatic.optiq.jdbc.OptiqConnectionImpl.parseQuery(OptiqConnectionImpl.java:170)
at net.hydromatic.optiq.jdbc.MetaImpl.prepare(MetaImpl.java:617)
at net.hydromatic.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:76)
... 96 more",smartshark_2_2,2638,kylin,"""[0.9889503121376038, 0.011049631983041763]"""
1831,230090,Simplify Dictionary interface,"Remove byte[] related interface. Keep only

- int getIdFromValue(T value)
- int getIdFromValue(T value, int roundingFlag)
- T getValueFromId(int id)
",smartshark_2_2,1290,kylin,"""[0.998464822769165, 0.0015351979527622461]"""
1832,230091,Make kylin log configurable,Put log properties file under conf dir.,smartshark_2_2,3006,kylin,"""[0.9981465339660645, 0.0018535349518060684]"""
1833,230092,Stream cubing auto assignment and load balance,"This is a sub task of KYLIN-1117, need assign the stream cubing job to cluster for load balance and fail over.",smartshark_2_2,3207,kylin,"""[0.9979914426803589, 0.0020085969008505344]"""
1834,230093,Enabling saving incomplete cube,"When adding cubes, the cube could only be saved when it comes to the last step, and if user switch to other functions and back, it will disappear, seems the web does not caching what user has just done, it's annoying to start up again, especially when some cubes are complicated.  So suggest to caching the cube and adding save button in the process of creating the cube. ",smartshark_2_2,2194,kylin,"""[0.9984304308891296, 0.0015695377951487899]"""
1835,230094,Merge tail small MR jobs into one,"In Kylin's cube build process, the Map Reduce job will become more heavy in first half phase, then will reduce more and more in second phase, merge last jobs will bring more efficiency since most of them just taken several minutes with MB level data

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/296
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: Backlog
Created at: Fri Dec 26 14:35:58 CST 2014
State: open
",smartshark_2_2,440,kylin,"""[0.998539924621582, 0.0014600519789382815]"""
1836,230095,Some improvements for lookup table - UI part change,UI part change,smartshark_2_2,1533,kylin,"""[0.9980610013008118, 0.001938963308930397]"""
1837,230096,Reduce the size of metadata uploaded to distributed cache,"Currently, each MR job uploads all the metadata belonging to a cube to distributed cache. When the total size of metadata increases, the submission time (""MapReduce Waiting"" at Monitor UI) also increases and could become a significant problem.

We could actually optimize the amount of metadata uploaded according to the type of job, for example

* CuboidJob only needs dictionary of the building segment
* CubeHFileJob doesn't need any dictionary",smartshark_2_2,1267,kylin,"""[0.9984476566314697, 0.0015523415058851242]"""
1838,230097,Reduce MR memory usage for global dict,"currently, in {{Build Base Cuboid Data}}, if user use the global dict and the global dict size significantly larger the mapper memory size, the {{CachedTreeMap}} will load all values as much as possible and the soft references object will stick around for a while when GC, So which will make the {{Build Base Cuboid Data}}  mapper pause for a long time even could not  finish.",smartshark_2_2,1338,kylin,"""[0.9983922839164734, 0.001607672544196248]"""
1839,230098,QueryController puts entry in Cache w/o checking QueryCacheEnabled,"org.apache.kylin.rest.controller.QueryController

around line 168

private SQLResponse doQueryWithCache(SQLRequest sqlRequest)


cacheManager.getCache(SUCCESS_QUERY_CACHE).put(new Element(sqlRequest, sqlResponse));

This should be conditional on KylinConfig.getInstanceFromEnv().isQueryCacheEnabled() as well ?",smartshark_2_2,838,kylin,"""[0.17600660026073456, 0.8239933848381042]"""
1840,230099,Segment Delete,"How to delete the segment that has drop directly but not discard ?.. If I don't delete the segment, i can't merge a new one. Please....
The following error occurs when I try to delete the segmentï¼

{""code"":""999"",""data"":null,""msg"":""Cannot delete segment '20170927140000_20170928160000' as it is neither the first nor the last segment."",""stacktrace"":""org.apache.kylin.rest.exception.InternalErrorException: Cannot delete segment '20170927140000_20170928160000' as it is neither the first nor the last segment.\n\tat org.apache.kylin.rest.controller.CubeController.deleteSegment(CubeController.java:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",smartshark_2_2,2080,kylin,"""[0.7504660487174988, 0.24953396618366241]"""
1841,230100,"Make ""ModelDataGenerator.java"" serilizable","The ModelDataGenerator  was used for mockup test data. Now it can only run in single VM. If it can run in parallel, then will generate more data.",smartshark_2_2,2041,kylin,"""[0.9984209537506104, 0.0015789978206157684]"""
1842,230101,Upgrade Spark to 2.1.2,"Spark 2.1.2 is released in last month, strongly recommend all 2.1.x users to upgrade to this stable release.

Besides, Kylin should use Spark for Hadoop 2.7, instead of Hadoop 2.6, be consistent with the version in Kylin's pom.xml",smartshark_2_2,1162,kylin,"""[0.9980499744415283, 0.0019499830668792129]"""
1843,230102,Load Kafka client configuration from properties files,"The latest kafka source hardcode the connection properties(such as ""timeout.ms"" in source files, it could be better if could refactor these properties into files.",smartshark_2_2,1304,kylin,"""[0.9982841610908508, 0.0017157633556053042]"""
1844,230103,REST API for deleting segment,"To allow user have more flexibility in managing the cube segments, Kylin need a REST API to delete segment. But the deletion can only happen on head or tail segment, one segment one time; ",smartshark_2_2,2140,kylin,"""[0.9983515739440918, 0.0016484013758599758]"""
1845,230104,HyperLogLog codec performance improvement,"We have a cube with more than ten distinct count measure, and use hll15 store the value, we found it is too slow of HyperLogLogPlusCounter, there are three methods will called frequentlly: merge/writeRegisters/readRegisters.

I found in kylin-1.5.x add a parameter 'singleBucket' to store the only one bucket which can optimize base cuboid.

However, in other step of cuboid building, it will slow down. I has modify the code to speed up the speed of three operation.",smartshark_2_2,1109,kylin,"""[0.9981282353401184, 0.0018717945786193013]"""
1846,230105,"When using rest API for create a model, better to change the format of transferred parameters from string to json.","For the parameter, ""modelDescData"", it's a little tricky to define a string as the value, for it may include many control characters or double quotes """". Better transfer all the parameters as json and parse at the backend.",smartshark_2_2,2679,kylin,"""[0.9983649849891663, 0.0016349839279428124]"""
1847,230106,Introduce genetic algorithm for cube planner,"The basic idea of this algorithm is to evolve the generation of chromosome, where a chromosome is a cuboid set to prebuilt. The detailed steps are as follows:
# Initialize a generation of chromosome
# Evolve the generation by selection, crossover and mutation by N round.
# From the final generation, choose a best chromosome that is a best cuboid set to recommend.",smartshark_2_2,1784,kylin,"""[0.998354971408844, 0.0016450468683615327]"""
1848,230107,No model clone across projects,"Nowadays, data sources and tables are separated by projects in Kylin. So cloning models across projects will leads to tables not found. Should prevent users performingÂ the action.",smartshark_2_2,1363,kylin,"""[0.9667481184005737, 0.03325190767645836]"""
1849,230108,"Extract content from kylin.tgz to ""kylin"" folder","When unpack kylin.tgz, all folders will be putted in current folder without ""kylin"". 
This will overwrite same folder there the command execute.

Expected: 
    Extract content to ""kylin"" folder ",smartshark_2_2,312,kylin,"""[0.9984361529350281, 0.0015638630138710141]"""
1850,230109,cluster management,"Kylin currently manages the cluster itself, so far it works fine since there are only 2 roles for the cluster, job engine and query engine.
However when new features such as streaming is coming, deployment will be more complex.
So new cluster management should be introduced. All the tasks followed or related can be created as a sub-task",smartshark_2_2,3162,kylin,"""[0.9984531402587891, 0.0015468724304810166]"""
1851,230110,Route unsupported queries to Hive (on Spark),"When Kylin can't serve coming SQL, it will be better to route it to enabled SQL on Hadoop like SparkSQL and execute there. 

Then get result and return to client through Kylin server.

",smartshark_2_2,1970,kylin,"""[0.9984715580940247, 0.0015284772962331772]"""
1852,230111,Support Custom Aggregation Types,"Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END)  or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation ",smartshark_2_2,2718,kylin,"""[0.9985964894294739, 0.0014035109197720885]"""
1853,230112,UT 'HiveCmdBuilderTest' fail on 'testBeeline',"this is  caused by line separator match fail.
In 'testBeeline' the expected hqlStatement is hard code and the line separator is '\n', however, the line  separator of actual hqlStatement is generated by 'StringBuffer.newLine()' which uses the platform's own notion of line separator as defined by the system property <tt>line.separator</tt>. Not all platforms use the newline character ('\n') to terminate lines.

error detail as follow:
org.junit.ComparisonFailure:  <Click to see difference>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.kylin.source.hive.HiveCmdBuilderTest.testBeeline(HiveCmdBuilderTest.java:72)",smartshark_2_2,2804,kylin,"""[0.8709506988525391, 0.12904928624629974]"""
1854,230113,Add basic support classes for cube planner algorithms,"Cube planner aims at recommending cost-effective cuboids. Currently we only consider {color:#f79232}*scanned row count*{color} at {color:#f79232}*query phase*{color} for the cost. The related formula is as follows:
bq. cuboid cost = scanned row count on target cuboid * query probability

As we know the base cuboid is to be prebuilt absolutely. If only the base cuboid is prebuilt, for other cuboids, the target cuboid will be the base cuboid and the _(scanned row count)_ is supposed to be large. When another cuboid is selected to be prebuilt, for its descendant cuboids including itself, it will be their target cuboid and the _(scanned row count)_ is supposed to become smaller. Thus, this newly cuboid will bring some benefit. We employ BPUS (benefit per unit space) for cuboid selection. The related formula for the benefit of a cuboid is as follows:
bq. cuboid benefit = total reduced cuboid cost) / (cuboid row count)

Cuboid selection is based on one basic rule:
bq. {color:#f79232}*RULE 1: Cuboids with more benefit will be preferred.*{color}

For a cube, cube planner can be used in two phases.
* Phase one is for cube normal building.
To use cube planner for this phase, the cube should be empty or the building job is for refreshing the only one segment. In this phase, we regard each cuboid own the same _(query probability)_ due to lack of query statistics.
* Phase two is for cube optimization.
Currently cube optimization is manually triggered. _(query probability)_ will be considered and its related query statistics are fetched from system cubes. Based on _(query probability)_, it's possible for us to add missing cuboids without cuboid row count info. It's based on a rule, called {color:#f79232}*mandatory rule*{color}.

bq. {color:#f79232}*RULE 2: A cuboid not pre-built should be added, if it's queried frequently and the average rollup row count from its pre-built parent cuboid is large.*{color}

From above introduction, we know cube planner is based on statistics, including cuboid row count, cuboid hit frequency, etc. Class {{CuboidStats}} is introduced to provide these info for related algorithm. 

Here, we also define the interface {{CuboidRecommendAlgorithm}} for different kinds of cube planner algorithms. As we know, if there's no space limitation, to pre-build all of the cuboids will bring the most benefit. However, it's not feasible in real world. Then with space limitation, an interface is defined to recommend a set of high benefit cuboids.
{code}
List<Long> recommend(double expansionRate);
{code}
Here, the expansion rate is compared to the size of base cuboid.",smartshark_2_2,2092,kylin,"""[0.9985387325286865, 0.001461230800487101]"""
1855,230114,"Merge job should not be blocked by ""kylin.cube.max-building-segments""","Currently there is a config ""kylin.cube.max-building-segments"" (default be 10) set the max. jobs for a cube.

In a frequently build case, that is possible that have 10 segments being built concurrently. Then there is no room for the merge jobs. If the merge job is blocked, more segments will be accumulated, and then impact on the query performance.

Â 

So I suggest to disable the checking for merge jobs.

Â 

Â ",smartshark_2_2,1776,kylin,"""[0.9840021133422852, 0.015997959300875664]"""
1856,230115,TopN measure validate code refactor to make it more clear,"_Nowadays, the FunctionRule used for validating measure info, and it should call measureType.validate() code rather than add the logical in this 2 places_Â ",smartshark_2_2,1647,kylin,"""[0.9984816908836365, 0.0015182982897385955]"""
1857,230116,"""Models"" is slow to show up","There is an ""loading"" icon, after about 1 minute it shows up, this is too slow; there must be something wrong. Pls see the screenshot",smartshark_2_2,891,kylin,"""[0.9050348401069641, 0.0949651226401329]"""
1858,230117,Refactor the package structure of dictionary module,Currently package structure is too confused,smartshark_2_2,1476,kylin,"""[0.9982832670211792, 0.001716785365715623]"""
1859,230118,Optimize endpoint's response structure to suit with no-dictionary data,current endpoint response are designed when data in endpoint was assumed to be encoded by dict. This is no longer true for streaming cases. data structures like IIRow should be redesigned accordingly,smartshark_2_2,2476,kylin,"""[0.9982505440711975, 0.0017494329949840903]"""
1860,230119,Utilize error-prone to discover common coding mistakes,"http://errorprone.info/ is a tool which detects common coding mistakes.


We should incorporate into Kylin build.",smartshark_2_2,709,kylin,"""[0.9979206919670105, 0.002079298719763756]"""
1861,230120,Restructure docs and website,"1. Move all docs under website docs
2. Setup Jekyll & Markdown for all docs, blog and others.
3. How to guide for newbie to contribute docs/blog
",smartshark_2_2,453,kylin,"""[0.9969821572303772, 0.0030178274028003216]"""
1862,230121,Allow setting REPLICATION_SCOPE on newly created tables,"As far as I can tell all tables are currently created ""equal"" in `CubeHTableUtil`.

We'd like to set REPLICATION_SCOPE to 1 on newly created tables so HBase replicates the data to a second cluster.",smartshark_2_2,1795,kylin,"""[0.9984545707702637, 0.001545472303405404]"""
1863,230122,Enforce global dictionary for bitmap count distinct column,"For bitmap based count distinct column (as the data type is not int), a Global dictionary is required. But now user can use normal dictionary, which may cause incorrect result.",smartshark_2_2,1781,kylin,"""[0.9888650178909302, 0.011134968139231205]"""
1864,230123,Improve enable limit logic (exactAggregation is too strict),"from zhaotianshuo@meizu.com:

recently I got the following error while execute query on a cube which is not that big( about 400mb, 20milion record)
==================
Error while executing SQL ""select FCRASHTIME,count(1) from UXIP.EDL_FDT_OUC_UPLOAD_FILES group by FCRASH_ANALYSIS_ID,FCRASHTIME limit 1"": Scan row count exceeded threshold: 10000000, please add filter condition to narrow down backend scan range, like where clause.

I guess what  it scan were the intermediate result, but It doesn't any order by,also the result count is limit to just 1.so it could scan to find any record with those two dimension and wala.
",smartshark_2_2,3028,kylin,"""[0.983644425868988, 0.016355616971850395]"""
1865,230124,Validate [Project] before create Cube on UI,"when user choose Project as [--Select All--] on UI, it's not supposed to create Cube since Project is not selected. should add validation to stop that.",smartshark_2_2,611,kylin,"""[0.9981525540351868, 0.001847477164119482]"""
1866,230125,load-hive-conf.sh should not get the commented configuration item,load-hive-conf.sh should not get the commented configuration item in kylin_hive_conf.xml,smartshark_2_2,1360,kylin,"""[0.9778384566307068, 0.022161604836583138]"""
1867,230126,"Move ""website"" out of source code repository","The ""website"" module traces the content of kylin home page, which is not a part of kylin source code; so we should move it out of the source git repository. We have the following options:
1) create an ASF svn repository
2) create an ASF git repository
3) create a github git repository

Personally I prefer 3) as it can be easily accessed; Please give your comment.",smartshark_2_2,2378,kylin,"""[0.9968998432159424, 0.0031001579482108355]"""
1868,230127,Support offset for segment merge,"This is a request to add an offset to the Kylin segment merge so as to avoid immediate merge of segments after a 7 day / 30 day window.

Introducing a delay(offset) would help in 2 things -

a) When auto merge kicks off I have a new segment and my daily incremental segment build script will fail because they wonât find the last segment.
b) Lot of use cases where I may need to do data backfills for some of the days of the previous week, but I end up refreshing the whole merged segment instead of a day or two.",smartshark_2_2,730,kylin,"""[0.9984802603721619, 0.0015196949243545532]"""
1869,230128,Refine the process of submitting a job,"In cases hbase is very busy, it cost some time to put all of the job related metadata into hbase. The job related metadata includes *job info* and *job output info*. Currently, kylin put *job info* into hbase first, then *job output info*. If server is down and only part of *job output info* has been put into hbase during this period, kylin will fail to load job when restarts. 
To solve this issue, it's better to adjust the order by adding *job output info* first, then *job info*.",smartshark_2_2,1052,kylin,"""[0.9984252452850342, 0.0015748041914775968]"""
1870,230129,Add the dependency check when deleting a  project,"Can't drop model when I delete the project first.

1,create project1 and project2,create model_01 for project1 and model_02 for project2,just like 01.png and 02.png.
2,delete project2,then try to drop model_02,it shows like 04.png.

",smartshark_2_2,916,kylin,"""[0.97240149974823, 0.02759856916964054]"""
1871,230130,Build Cube,Error: java.io.IOException: Failed to build cube in mapper 1 at org.apache.kylin.engine.mr.steps.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:145) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:188) at org.apache.kylin.engine.mr.steps.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:143) ... 8 more Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.AbstractInMemCubeBuilder$1.run(AbstractInMemCubeBuilder.java:84) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744) Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.build(DoggedCubeBuilder.java:128) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder.build(DoggedCubeBuilder.java:75) at org.apache.kylin.cube.inmemcubing.AbstractInMemCubeBuilder$1.run(AbstractInMemCubeBuilder.java:82) ... 5 more Caused by: java.io.IOException: java.lang.IllegalStateException at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.abort(DoggedCubeBuilder.java:196) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.checkException(DoggedCubeBuilder.java:169) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$BuildOnce.build(DoggedCubeBuilder.java:101) ... 7 more Caused by: java.lang.IllegalStateException at com.google.common.base.Preconditions.checkState(Preconditions.java:129) at org.apache.kylin.common.util.MemoryBudgetController.<init>(MemoryBudgetController.java:73) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.makeMemoryBudget(InMemCubeBuilder.java:324) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.build(InMemCubeBuilder.java:174) at org.apache.kylin.cube.inmemcubing.InMemCubeBuilder.build(InMemCubeBuilder.java:137) at org.apache.kylin.cube.inmemcubing.DoggedCubeBuilder$SplitThread.run(DoggedCubeBuilder.java:284),smartshark_2_2,1562,kylin,"""[0.4960811734199524, 0.5039187669754028]"""
1872,230131,Introduce dictionary metadata,"To allow

- Special dictionary builder for specified column
- Two or more columns to share dictionary",smartshark_2_2,2977,kylin,"""[0.9966208934783936, 0.003379049012437463]"""
1873,230132,Make HTable name prefix configurable,"Current version can support kylin_metadata, kylin_metadata_acl, kylin_metadata_users be created in hbase namespace with *kylin.metadata.url* config which in kylin.properties. 
But cube htable in namespace is not support yet. So I made some changes, let cubes can be created under specified hbase namespace with config *kylin.cube.namespace* in kylin.properties.",smartshark_2_2,2068,kylin,"""[0.9985257983207703, 0.0014742074999958277]"""
1874,230133,Support to change streaming configuration,"So far after create/save a streaming table, user couldn't edit it. Change should be supported as it is very common to change some info like cluster or parser properties.",smartshark_2_2,3299,kylin,"""[0.9984936714172363, 0.0015063342871144414]"""
1875,230134,alterable timezone,"Some developers use command line through Kylin API to build a cube, so they look the time as parameters. When they send a ""curl"" command, it has to be changed the timezone from local to GMT.
Mayby it can be set according to place where developers use.",smartshark_2_2,2783,kylin,"""[0.9986031651496887, 0.0013968630228191614]"""
1876,230135,Install script not work on Cloudera sandbox,"Current install script works for HDP 2.1 Sandbox but not works for Cloudera sandbox.


---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/6
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: v2.5 Release
Assignee: [binmahone|https://github.com/binmahone]
Created at: Fri Oct 17 13:42:24 CST 2014
State: closed
",smartshark_2_2,214,kylin,"""[0.9852446913719177, 0.014755305834114552]"""
1877,230136,Toggle to switch between warning/throwing exception when there's different extended column for same host column,Per request to Richard in KYLIN-1313,smartshark_2_2,2762,kylin,"""[0.9981127977371216, 0.001887205638922751]"""
1878,230137,optimize case when in group by ,"Similar to KYLIN-2635, for clauses like:

{code}
group by case when 1 = 1 then x 1 = 2 then y else z 
{code}

kylin only need to pick up x as grouping by column.

Again, like KYLIN-2635, we'll fix it in KYLIN rather than calcite first",smartshark_2_2,2259,kylin,"""[0.9982072114944458, 0.0017928361194208264]"""
1879,230138,Replace the use of org.codehaus.jackson,"{code}
engine-mr/src/main/java/org/apache/kylin/engine/mr/common/HadoopStatusGetter.java:import org.codehaus.jackson.JsonNode;
engine-mr/src/main/java/org/apache/kylin/engine/mr/common/HadoopStatusGetter.java:import org.codehaus.jackson.map.ObjectMapper;
{code}
com.fasterxml.jackson should be used instead",smartshark_2_2,1299,kylin,"""[0.9983547329902649, 0.001645183889195323]"""
1880,230139,Passwords in kylin.properties should be enctrypted,"When integrating LDAP authentication in Kylin, people might run into the following error message. One possible reason is that Kylin actually requires passwords in kylin.properties be encrypted.

{quote}
Invalid bean definition with name 'ldapSource' defined in class path resource \[kylinSecurity.xm\]: Input length must be multiple of 16 when decrypting with padded cipher
{quote}
Related code for passwords decryption is as follows:
{code:title=PasswordPlaceholderConfigurer.java|borderStyle=solid}
    protected String resolvePlaceholder(String placeholder, Properties props) {
        if (placeholder.toLowerCase().contains(""password"")) {
            return decrypt(props.getProperty(placeholder));
        } else {
            return props.getProperty(placeholder);
        }
    }
{code}

Related discussion in mailing list: [Link|http://mail-archives.apache.org/mod_mbox/kylin-user/201602.mbox/%3CCAJxfx2ANpe0rG1Vm1R_Sdh4XZuNP3FCswXmv1_xFUVSpdOkH_A%40mail.gmail.com%3E]


In this ticket, I would propose a CLI tool for password encryption for Kylin, and add related documentation in [How to Enable Security with LDAP and SSO|http://kylin.apache.org/docs/howto/howto_ldap_and_sso.html]",smartshark_2_2,757,kylin,"""[0.8642868399620056, 0.13571321964263916]"""
1881,230140,Enable swap column of Rowkeys in Cube Designer,"Rowkey order is very important part to optimize cube, the current Cube Designer do not support change rowkey column's order as below:

![image|https://cloud.githubusercontent.com/assets/1104017/5556083/0467c6f0-8d0a-11e4-8ec1-99a236b755c5.png]

Add swap button to move down or up selected column to change the order like:

![image|https://cloud.githubusercontent.com/assets/1104017/5556084/10681f9a-8d0a-11e4-818e-59f65a4a156d.png]

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/285
Created by: [lukehan|https://github.com/lukehan]
Labels: 
Milestone: Backlog
Created at: Fri Dec 26 14:19:19 CST 2014
State: open
",smartshark_2_2,327,kylin,"""[0.9985559582710266, 0.0014440335799008608]"""
1882,230141,Search cube by name in Web UI,"In order to work with dozens of cubes, could we add a search box at ""Model"" page? Just like the one at ""Monitor"" page.",smartshark_2_2,3111,kylin,"""[0.9984026551246643, 0.0015973319532349706]"""
1883,230142,Enable partition date column to support date and hour as separate columns for increment cube build ,"There are some data table, which save date and hour at 2 columns. For this kind of source data, kylin currently doesn't support incrementally build the cube. Open this JIRA to track this feature. 

{code}
{
 ""partition_desc"": {
    ""partition_date_column"": ""DEFAULT.TEST_KYLIN_FACT.CAL_DT"",
    ""partition_time_column"": ""DEFAULT.TEST_KYLIN_FACT.CAL_HOUR"",
    ""partition_date_format"": ""yyyy-MM-dd"",
    ""partition_time_format"": ""HH:mm"",
    ""partition_date_start"": 0,
    ""partition_type"": ""APPEND"",
    ""partition_condition_builder"": ""org.apache.kylin.metadata.model.PartitionDesc$DefaultPartitionConditionBuilder""
  }
}
{code}",smartshark_2_2,2547,kylin,"""[0.9985053539276123, 0.0014947119634598494]"""
1884,230143,"Support ""Pause"" on Kylin Job","Add one action called ""Pause"" to stop current job, user could resume this job later.

![image|https://cloud.githubusercontent.com/assets/1104017/5556023/54ae27e2-8d07-11e4-8efb-a22c041243ba.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/278
Created by: [lukehan|https://github.com/lukehan]
Labels: newfeature, 
Created at: Fri Dec 26 13:59:03 CST 2014
State: open
",smartshark_2_2,256,kylin,"""[0.9984824061393738, 0.0015176190063357353]"""
1885,230144,Add batch grant API for project ACL.,"Project ACL stored in one file, and leads to adding huge ACLs inefficient.",smartshark_2_2,1623,kylin,"""[0.998336136341095, 0.0016638773959130049]"""
1886,230145,UI update for streaming build action,"for streaming cube, it's not build from GUI, each user can schedule it in their own environment,when user click build on GUI,need a tip guide user to know how to schedule streaming cube build.",smartshark_2_2,2510,kylin,"""[0.9977445602416992, 0.002255471656098962]"""
1887,230146,Should use overrideHiveConfig for LookupHiveViewMaterialization and RedistributeFlatHiveTable,"Currently,  we use KylinConfig for LookupHiveViewMaterialization and RedistributeFlatHiveTable step. We should use cubeOverrideHiveConfig.",smartshark_2_2,1858,kylin,"""[0.998563826084137, 0.0014361371286213398]"""
1888,230147,JDBC Driver is not generic to restAPI json result,"If restAPI json structure changed, JDBC driver will fail to deserialize.",smartshark_2_2,2410,kylin,"""[0.34079453349113464, 0.659205436706543]"""
1889,230148,"segment auto merge based on calendar week, month and year","Currently kylin allows users to specify some time ranges like (7days, 30days, 365days) to instruct auto merging daily segments. However, the design fails to respect the boundary of calendar week/month/year, which is often useful for real applications. For example, if a query only wants a month's data by providing a filter like `date > 2010-01-01 and data < 2010-02-01`, with current auto merging design we may end up with visiting two adjacent 30 days long segments.",smartshark_2_2,2172,kylin,"""[0.8487052917480469, 0.1512947529554367]"""
1890,230149,Add http timeout for RestClient,we should add http timeout for RestClient in distributed env.,smartshark_2_2,1923,kylin,"""[0.9982482194900513, 0.0017518294043838978]"""
1891,230150,Tools to extract all cube/hybrid/project related metadata to facilitate diagnosing/debugging/sharing,"list of extracted items for a cube:
cube desc
cube instance
data model
all related table desc, and possibly with ext table info
each segment's dict, snapshot,statistics
cube related jobs and job outputs (optional)

list of extracted items for a hybrid:
hybrid instance
for each cube in the hybrid collect ""list of extracted items for a cube""

list of extracted items for a project:
project instance
for each cube in the project collect ""list of extracted items for a cube""
for each hybrid in the project collect ""list of extracted items for a hybrid""

",smartshark_2_2,2755,kylin,"""[0.9984476566314697, 0.001552343601360917]"""
1892,230151,Simplify deployment,"Simplify deployment to have more clear content and scripts to help administrator to easy setup.

",smartshark_2_2,303,kylin,"""[0.9972034692764282, 0.0027966084890067577]"""
1893,230152,Allow user to set more columnFamily in web ,"currently, when user set dozens of precise count distinct metrics in one cube, we put all the count distinct metrics column in one columnFamily. Which result in HBase scan become slow because the one {{KeyValue}} is too big. we could    set more columnFamily to speed up the HBase scan in this scenario.",smartshark_2_2,1246,kylin,"""[0.9984506368637085, 0.001549311215057969]"""
1894,230153,"Add ""hive.auto.convert.join"" and ""hive.stats.autogather"" to kylin_hive_conf.xml","Although ""hive.auto.convert.join"" and ""hive.stats.autogather"" are ""true"" by default in Hive (see https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties); we notice if they were disabled, Kylin's cube build will be slowed down much in the first and second step. So I will add the two parameters explicitly to Kylin's conf file with value ""true"".",smartshark_2_2,1258,kylin,"""[0.998444139957428, 0.0015558662125840783]"""
1895,230154,Support async query,"Some Queries will run long time, the best way to improve user experience is support using async way for such queries, following are the basic idea:

1. User submit query to Kylin
2. Kylin verify and check such Query is validated or not
3. Return error if query is invalidate
4. Submit query to server and record Job Id (where to store query result?)
5. Return running status to user with job information
6. User will be able to do other job without pending one this query job
7. User be able to get query status and get result later
8. Notify user when query job done on web
9. User could get their result through Kylin web

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/302
Created by: [lukehan|https://github.com/lukehan]
Labels: newfeature, 
Milestone: v2.5 Release
Created at: Fri Dec 26 14:51:04 CST 2014
State: open
",smartshark_2_2,580,kylin,"""[0.998364269733429, 0.0016357158310711384]"""
1896,230155,Disable datepicker input avoid user typing issue,"When user typing in input for datepicker, which will cause date error issue.
Could we add the attribute read-only for datepicker input?",smartshark_2_2,2211,kylin,"""[0.9983745813369751, 0.0016253949142992496]"""
1897,230156,Delegate the loading of certain package (like slf4j) to tomcat's parent classloader,"currently we use hbase command to start tomcat, which then starts kylin as a web application. The default classloader that tomcats assigns to the kylin applications is WebappClassLoader, which will search local repositories before parent classloader.

the design will lead to two separate log4j logging instances in both the ""HBase space"" and ""kylin space"", the two loggers will attempt to write to the same file, which is problematic according to official documents",smartshark_2_2,3099,kylin,"""[0.9978057742118835, 0.0021941971499472857]"""
1898,230157,Fix the joda library conflicts during Kylin start on EMR 5.8+,"When run ""bin/kylin.sh start"", it reports an NoSuchMethodError:
{code:java}
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.fasterxml.jackson.datatype.joda.JodaModule]: Constructor threw exception; nested exception is java.lang.NoSuchMethodError: org.joda.time.format.DateTimeFormatter.withZoneUTC()Lorg/joda/time/format/DateTimeFormatter;
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:154)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:102)
	at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.registerWellKnownModulesIfAvailable(Jackson2ObjectMapperBuilder.java:764)
	at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.configure(Jackson2ObjectMapperBuilder.java:607)
	at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.build(Jackson2ObjectMapperBuilder.java:590)
	at org.springframework.http.converter.json.MappingJackson2HttpMessageConverter.<init>(MappingJackson2HttpMessageConverter.java:57)
	at org.springframework.http.converter.support.AllEncompassingFormHttpMessageConverter.<init>(AllEncompassingFormHttpMessageConverter.java:61)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.<init>(RequestMappingHandlerAdapter.java:182)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142)
	... 28 more
Caused by: java.lang.NoSuchMethodError: org.joda.time.format.DateTimeFormatter.withZoneUTC()Lorg/joda/time/format/DateTimeFormatter;
	at com.fasterxml.jackson.datatype.joda.ser.JodaDateSerializerBase.<clinit>(JodaDateSerializerBase.java:15)
	at com.fasterxml.jackson.datatype.joda.JodaModule.<init>(JodaModule.java:39)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142)
	... 40 more

{code}
After doing some investigation, found the DateTimeFormatter wasn't loaded from Kylin's lib, but from:
 [Loaded org.joda.time.format.DateTimeFormatter from file:/usr/lib/hbase/lib/jruby-complete-1.6.8.jar]

Then I remove that jar (this is a client node), restart Kylin again, this time the error disappeared, by changed to another:
{code:java}
mv /usr/lib/hbase/lib/jruby-complete-1.6.8.jar /usr/lib/hbase/lib/jruby-complete-1.6.8.jar_
{code}
{code:java}
Caused by: java.lang.ClassCastException: com.fasterxml.jackson.datatype.joda.JodaModule cannot be cast to com.fasterxml.jackson.databind.Module
at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.registerWellKnownModulesIfAvailable(Jackson2ObjectMapperBuilder.java:764)
at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.configure(Jackson2ObjectMapperBuilder.java:607)
at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.build(Jackson2ObjectMapperBuilder.java:590)
at org.springframework.http.converter.json.MappingJackson2HttpMessageConverter.(MappingJackson2HttpMessageConverter.java:57)
at org.springframework.http.converter.support.AllEncompassingFormHttpMessageConverter.(AllEncompassingFormHttpMessageConverter.java:61)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.(RequestMappingHandlerAdapter.java:182)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142)
... 28 more

{code}
Then I found it was caused by 
 [Loaded com.fasterxml.jackson.datatype.joda.JodaModule from file:/usr/lib/hive/lib/jackson-datatype-joda-2.4.6.jar]

Then rename that jar:
{code:java}
mv /usr/lib/hive/lib/jackson-datatype-joda-2.4.6.jar /usr/lib/hive/lib/jackson-datatype-joda-2.4.6.jar_
{code}
Restart Kylin, then it works good.

Please avoid doing this on EMR master node, as the impact to HBase/Hive is unknown. If it is on a client node (no running service), it is okay.

Â 

--- Update

Recived report that in AWS India, EMR 5.8 to start Kylin has the following error in logs/kylin.out:

Â 
{code:java}

Jan 17, 2018 4:57:47 PM org.apache.catalina.core.StandardService stopInternal
INFO: Stopping service Catalina
Jan 17, 2018 4:57:47 PM org.apache.catalina.core.ApplicationContext log
INFO: Destroying Spring FrameworkServlet 'kylin'
Jan 17, 2018 4:57:47 PM org.apache.catalina.core.ApplicationContext log
INFO: Closing Spring root WebApplicationContext
Jan 17, 2018 4:57:47 PM org.apache.catalina.core.ApplicationContext log
INFO: Shutting down log4j
Jan 17, 2018 4:57:47 PM org.apache.catalina.loader.WebappClassLoaderBase clearReferencesJdbc
WARNING: JDBC driver de-registration failed for web application [/kylin]
java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.AutoloadedDriver40
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at java.sql.DriverManager.isDriverAllowed(DriverManager.java:556)
at java.sql.DriverManager.isDriverAllowed(DriverManager.java:548)
at java.sql.DriverManager.getDrivers(DriverManager.java:446)
at org.apache.catalina.loader.JdbcLeakPrevention.clearJdbcDriverRegistrations(JdbcLeakPrevention.java:56)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.catalina.loader.WebappClassLoaderBase.clearReferencesJdbc(WebappClassLoaderBase.java:2305)
at org.apache.catalina.loader.WebappClassLoaderBase.clearReferences(WebappClassLoaderBase.java:2223)
at org.apache.catalina.loader.WebappClassLoaderBase.stop(WebappClassLoaderBase.java:2123)
at org.apache.catalina.loader.WebappLoader.stopInternal(WebappLoader.java:663)
at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:221)
at org.apache.catalina.core.StandardContext.stopInternal(StandardContext.java:5875)
at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:221)
at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1716)
at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1705)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{code}",smartshark_2_2,2280,kylin,"""[0.9294605255126953, 0.0705394297838211]"""
1899,230158,Test manually deployment with Hadoop cluster,"Test manually deployment with Hadoop cluster environment instead of sandbox.

Manually install guide: https://github.com/KylinOLAP/Kylin/wiki/Kylin-Manual-Installation-Guide

",smartshark_2_2,309,kylin,"""[0.9983939528465271, 0.0016060255002230406]"""
1900,230159,Aggregation group validation,To add more validations on aggregation group during the loading of cubes.,smartshark_2_2,2562,kylin,"""[0.9984427094459534, 0.0015573602868244052]"""
1901,230160,Better Cube Designer Look and Feel,"Sample Idea:

![image|https://cloud.githubusercontent.com/assets/1104017/5556179/d3aa0da8-8d0d-11e4-9543-d96da0933aca.png]


---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/300
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: Backlog
Created at: Fri Dec 26 14:45:55 CST 2014
State: open
",smartshark_2_2,1008,kylin,"""[0.9984990358352661, 0.0015010136412456632]"""
1902,230161,CubeSegmentScanner generated inaccurate,"My project has three segment:
20160601000000_20160602000000,
20160602000000_20160603000000,
20160603000000_20160604000000

When I used filter condition like this : day>='2016-06-01' and day<'2016-06-02'
Kylin would generated three CubeSegmentScanner, and each CubeSegmentScanner's GTScanRequest are not empty!

When I changed filter condition like this : day>='2016-06-01' and day<='2016-06-02'
Kylin would also generated three CubeSegmentScanner, but the last CubeSegmentScanner's GTScanRequest is empty!",smartshark_2_2,971,kylin,"""[0.6310835480690002, 0.36891645193099976]"""
1903,230162,Add special mr config for base cuboid step,"Refer to http://kylin.apache.org/blog/2016/08/01/count-distinct-in-kylin/, currently, if user want to enlarge MR memory for global dict, they must use kylin.engine.mr.config-override., which will enlarge the memory of  all mr job. In fact, we only need to enlarge the memory for ""Build Base Cuboid"", so we could add a special mr config for base cuboid step.",smartshark_2_2,946,kylin,"""[0.9984947443008423, 0.0015051922528073192]"""
1904,230163,JDBC Driver httpcore dependency conflict,"Report by xwhfcenter from github:
""There is a conflict in dependency of httpcore in module JDBC Driver""

",smartshark_2_2,1227,kylin,"""[0.9593269228935242, 0.040673013776540756]"""
1905,230164,Exception while describing cube,"lens-shell>describe cube --cube cube
Command failed javax.ws.rs.InternalServerErrorException: HTTP 500 No enum const class org.apache.lens.api.metastore.XColumnType.ARRAY&lt;STRING&gt;

Exception
{noformat}
08 Feb 2015 18:42:54,735 [Grizzly-worker(7)] WARN  org.glassfish.jersey.server.ServerRuntime$Responder  - WebApplicationException cause:
java.lang.IllegalArgumentException: No enum const class org.apache.lens.api.metastore.XColumnType.ARRAY<STRING>
        at java.lang.Enum.valueOf(Enum.java:214)
        at org.apache.lens.api.metastore.XColumnType.valueOf(XColumnType.java:45)
        at org.apache.lens.server.metastore.JAXBUtils.xExprColumnFromHiveExprColumn(JAXBUtils.java:255)
        at org.apache.lens.server.metastore.JAXBUtils.xCubeFromHiveCube(JAXBUtils.java:131)
        at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getCube(CubeMetastoreServiceImpl.java:215)
        at org.apache.lens.server.metastore.MetastoreResource.getCube(MetastoreResource.java:398)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:622)
        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
        at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
        at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
        at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
        at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
        at java.lang.Thread.run(Thread.java:701)

{noformat}",smartshark_2_2,153,lens,"""[0.27779722213745117, 0.7222027778625488]"""
1906,230165,TotalQueryCost logic broken ,"The totalQueryCost method in DefaultEstimatedQueryCollection is throwing following error : 

 Error in query submitter
java.lang.ClassCastException: org.apache.lens.server.api.query.cost.FactPartitionBasedQueryCost cannot be cast to org.apache.lens.server.api.query.cost.StaticQueryCost
	at org.apache.lens.server.api.query.cost.StaticQueryCost.add(StaticQueryCost.java:33) 
	at org.apache.lens.server.query.collect.DefaultEstimatedQueryCollection.getTotalQueryCost(DefaultEstimatedQueryCollection.java:171) ",smartshark_2_2,1374,lens,"""[0.07209184765815735, 0.927908182144165]"""
1907,230166,Getting expression ast to generate default expression should consider all expression contexts,"in case an expression is answered from a segmentation, the default expression can't be generated. ",smartshark_2_2,1411,lens,"""[0.635026752948761, 0.3649732172489166]"""
1908,230167,negative constants are getting rewritten incorrectly,"consider query:-
cube select -1 as col1,col2 from table1

is getting rewritten as:-
SELECT ( 1  - ) col1 , ( table1 . col2 ) FROM table1",smartshark_2_2,34,lens,"""[0.06323091685771942, 0.9367690682411194]"""
1909,230168,Lens not looking up non-timed partition columns while finding dimtables for a dimension,"DimensionX has dimtableA. dimtableA has a partition column dt which is the time dimension partition. it also has suppose a partition column ""source"". when one queries ""cube select x from DimensionX where source=xyz"", lens complains:

No dimension table has the queried columns for user_owned_apps, columns: [xyz, source]

This is because lens doesn't identify the ""source"" partition column as a column of dimtableA",smartshark_2_2,238,lens,"""[0.10227399319410324, 0.8977260589599609]"""
1910,230169,"cli doesn't support add partition, only supports add partitions now.",add partition says successfully added but doesn't add. ,smartshark_2_2,347,lens,"""[0.3875093460083008, 0.6124906539916992]"""
1911,230170,NO_FACT_UPDATE_PERIODS_FOR_GIVEN_RANGE is overriding other causes,"{""brief"":""No fact update periods for given range"",""details"":{""f1"":{""cause"":""NO_CANDIDATE_STORAGES"",""storageCauses"":{""s1"":""UNSUPPORTED""}},""f2,f3,f4"":{""cause"":""COLUMN_NOT_FOUND"",""missingColumns"":[""c1""]},""f5"":{""cause"":""NO_CANDIDATE_STORAGES"",""storageCauses"":{""s1"":""PART_COL_DOES_NOT_EXIST""}},""f3,f6,f7"":{""cause"":""COLUMN_NOT_FOUND"",""missingColumns"":[""c2""]}}} ",smartshark_2_2,155,lens,"""[0.15098367631435394, 0.8490163087844849]"""
1912,230171,Cube metastore client's conf is not loaded with db resources,"Show cubes as the first call in session fails with ClassNotFoundException, when one of tables in metastore is created with custom serde; and the resources are added through db resources.

Here is the stacktrace :
{noformat}
01 Mar 2015 10:15:48,354 [Grizzly-worker(2)] ERROR hive.log  - error in initSerDe: java.lang.ClassNotFoundException Class <customSerde> not found 
java.lang.ClassNotFoundException: Class <customSerde> not found
  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)
  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:337)
  at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:283)
  at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:276)
  at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:626)
  at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:184)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:909)
  at org.apache.lens.cube.metadata.CubeMetastoreClient.getTable(CubeMetastoreClient.java:663)
  at org.apache.lens.cube.metadata.CubeMetastoreClient.getCube(CubeMetastoreClient.java:880)
  at org.apache.lens.cube.metadata.CubeMetastoreClient.getAllCubes(CubeMetastoreClient.java:997)
  at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllCubeNames(CubeMetastoreServiceImpl.java:165)
  at org.apache.lens.server.metastore.MetastoreResource.getAllCubes(MetastoreResource.java:261)
{noformat}

Reporting this on behalf of [~arshadmatin20]. Thanks [~arshadmatin20] for finding the issue.",smartshark_2_2,313,lens,"""[0.11293434351682663, 0.8870656490325928]"""
1913,230172,Connection leak in JDBCDriver.estimate path,"JDBCDriver.prepareInternal takes a ready made connection, and does rewrite followed by prepared statement on the rewritten query. If rewrite throws an exception, the connection is not closed.",smartshark_2_2,239,lens,"""[0.07959440350532532, 0.9204056262969971]"""
1914,230173,"For virtual facts, column start and end times are not working","For virtual facts, column start and end times are not working after LENS-1480",smartshark_2_2,1450,lens,"""[0.2041177898645401, 0.7958822250366211]"""
1915,230174,Error marshalling LensAPIResult to json,"Rest apis should be able to work with both application/xml and application/json  accept headers.

But some APIs that return LensAPIResult<?> does not work with application/json accept header.

To reproduce
1) Hit the /lensapi/queryapi/queries with one more additional header 
   Accept - application/json",smartshark_2_2,820,lens,"""[0.6297635436058044, 0.37023645639419556]"""
1916,230175,Quitting shell(quitshell()) is not closing session some times.,"Quitting session through programmatically is not closing the session some times. 
First it checks whether connection to server is active or not to close the session. It is depending on isConnectionActive variable to check the connection is active or not. Right now we are setting isConnectionActive to true only  if user calls  getClient(). isConnectionActive should be set to true when user calls setClient(client). 

code snippets:
----------------
{code}
  public LensClient getClient() {
    if (lensClient == null) {
      setClient(getClientWrapper().getClient());
      isConnectionActive = true;
    }
    return lensClient;
  }
{code}


{code}
protected static synchronized void closeClientConnection() {
    if (isConnectionActive) {
      log.debug(""Request for stopping lens cli received"");
      getClientWrapper().getClient().closeConnection();
      isConnectionActive = false;
    }
  }
{code}",smartshark_2_2,1139,lens,"""[0.1543172001838684, 0.8456827402114868]"""
1917,230176,base url is not set in lens jdbc client.,"Inside  method ""JDBCUtils.parseUrl"". we are not setting base url. due to which lens jdbc connection to server is failing. ",smartshark_2_2,835,lens,"""[0.13093122839927673, 0.8690687417984009]"""
1918,230177,Columnar JDBC Rewriter in incorrectly pushing filter against wrong alias when fact columns map to a single dimension,"Columnar JDBC Rewriter in incorrectly pushing filter against wrong alias when fact columns map to a single dimension.

Ex:

select fact.time_key, time_dim.day_of_week, location_dim_a.location_name, other_location_dim.location_name, sum(fact.dollars_sold) from sales_fact fact inner join time_dim time_dim on fact.time_key = time_dim.time_key inner join location_dim location_dim_a on fact.location_key = location_dim_a.location_key inner join location_dim other_location_dim on fact.other_location_key = other_location_dim.location_key where time_dim.time_key between '2013-01-01' and '2013-01-31' and location_dim_a.location_key = 'some-loc' group by fact.time_key, location_dim_a.location_key, other_location_dim.location_key

is being rewritten to 

select ( sales_fact___fact . time_key ), ( time_dim___time_dim . day_of_week ), ( location_dim___location_dim_a . location_name ), ( location_dim___other_location_dim . location_name ), sum(alias1) from  (select sales_fact___fact.time_key, sales_fact___fact.location_key, sales_fact___fact.other_location_key,sum(( sales_fact___fact . dollars_sold )) as alias1 from sales_fact sales_fact___fact where sales_fact___fact.time_key in  (  select time_dim .time_key from time_dim where ( time_dim. time_key ) between  '2013-01-01'  and  '2013-01-31'  ) and sales_fact___fact.location_key in  (  select location_dim .location_key from location_dim where (( location_dim. location_key ) =  'some-loc' ) ) and {{sales_fact___fact.other_location_key in  (  select location_dim .location_key from location_dim where (( location_dim. location_key ) =  'some-loc' ) )}}  group by sales_fact___fact.time_key, sales_fact___fact.location_key, sales_fact___fact.other_location_key) sales_fact___fact  inner join (select time_key,day_of_week from time_dim) time_dim___time_dim on (( sales_fact___fact . time_key ) = ( time_dim___time_dim . time_key ))  inner join (select location_key,location_name from location_dim) location_dim___location_dim_a on (( sales_fact___fact . location_key ) = ( location_dim___location_dim_a . location_key ))  inner join (select location_key,location_name from location_dim) location_dim___other_location_dim on (( sales_fact___fact . other_location_key ) = ( location_dim___other_location_dim . location_key ))  where (( time_dim___time_dim . time_key ) between  '2013-01-01'  and  '2013-01-31'  and (( location_dim___location_dim_a . location_key ) =  'some-loc' )) group by ( sales_fact___fact . time_key ), ( location_dim___location_dim_a . location_key ), ( location_dim___other_location_dim . location_key )
",smartshark_2_2,696,lens,"""[0.06668596714735031, 0.9333140254020691]"""
1919,230178,tok_decimal does not exist,"{noformat}
lens-shell>query execute select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
Launching query failed cause:Driver :org.apache.lens.driver.hive.HiveDriver Cause :Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:59 Table not found 'country_carrier_map'
Driver :org.apache.lens.driver.jdbc.JDBCDriver Cause :FUNCTION unified_global_dw.tok_decimal does not exist
{noformat}

logs
{noformat}
24 Mar 2015 08:48:23,955 [QuerySubmitter] INFO  hive.ql.parse.ParseDriver  - Parsing command: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,956 [QuerySubmitter] INFO  hive.ql.parse.ParseDriver  - Parse Completed
24 Mar 2015 08:48:23,957 [estimate-31] INFO  org.apache.lens.driver.cube.RewriteUtil  - Final rewritten query for driver:org.apache.lens.driver.hive.HiveDriver@3569158f is: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,962 [estimate-31] INFO  org.apache.lens.driver.hive.HiveDriver  - Estimate: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,962 [estimate-31] INFO  org.apache.lens.driver.hive.HiveDriver  - Explain: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,963 [estimate-31] INFO  org.apache.lens.server.api.query.AbstractQueryContext  - Generated metric id: da297501-1483-4af6-a342-45012862df9c for query: EXPLAIN EXTENDED select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,963 [estimate-31] INFO  org.apache.lens.driver.hive.HiveDriver  - Hive driver query:EXPLAIN EXTENDED select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:23,963 [estimate-31] INFO  org.apache.lens.driver.hive.HiveDriver  - New thrift connection class org.apache.lens.driver.hive.RemoteThriftConnection for thread:316 for user:dataqa connection ID=12
24 Mar 2015 08:48:23,963 [estimate-31] INFO  org.apache.lens.driver.hive.RemoteThriftConnection  - HiveDriver connecting to HiveServer @ localhost:10000
24 Mar 2015 08:48:23,963 [estimate-31] INFO  org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient  - Connecting to localhost:10000
24 Mar 2015 08:48:23,962 [estimate-32] INFO  org.apache.lens.driver.cube.RewriteUtil  - Final rewritten query for driver:org.apache.lens.driver.jdbc.JDBCDriver@5ffcaad4 is: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:24,031 [Grizzly-worker(2)] INFO  org.apache.lens.server.AuthenticationFilter  - Request from user: null, path=/queryapi/queries/26d2108d-bac5-4528-a3bf-cee6cf36260e
24 Mar 2015 08:48:24,080 [estimate-31] INFO  org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient  - Connected!
24 Mar 2015 08:48:24,081 [estimate-31] INFO  org.apache.lens.driver.hive.RemoteThriftConnection  - HiveDriver connected to HiveServer @ localhost:10000
24 Mar 2015 08:48:24,096 [estimate-31] INFO  org.apache.lens.driver.hive.HiveDriver  - No hive operation available for 0e531402-5db7-4631-862e-69b7834ca19a
24 Mar 2015 08:48:24,096 [estimate-31] ERROR org.apache.lens.server.api.query.AbstractQueryContext  - Setting driver cost failed for driver org.apache.lens.driver.hive.HiveDriver@3569158f Cause: Driver :org.apache.lens.driver.hive.HiveDriver Cause :Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:59 Table not found 'country_carrier_map'
org.apache.lens.api.LensException: Error executing query
	at org.apache.lens.driver.hive.HiveDriver.execute(HiveDriver.java:475)
	at org.apache.lens.driver.hive.HiveDriver.explain(HiveDriver.java:386)
	at org.apache.lens.driver.hive.HiveDriver.estimate(HiveDriver.java:359)
	at org.apache.lens.server.api.query.AbstractQueryContext$DriverEstimateRunnable.run(AbstractQueryContext.java:208)
	at org.apache.lens.server.query.QueryExecutionServiceImpl$RewriteEstimateRunnable.run(QueryExecutionServiceImpl.java:1137)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:59 Table not found 'country_carrier_map'
	at org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.checkStatus(ThriftCLIServiceClient.java:52)
	at org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.executeStatementInternal(ThriftCLIServiceClient.java:151)
	at org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.executeStatement(ThriftCLIServiceClient.java:129)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.invokeInternal(RetryingThriftCLIServiceClient.java:301)
	at org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.invoke(RetryingThriftCLIServiceClient.java:329)
	at com.sun.proxy.$Proxy66.executeStatement(Unknown Source)
	at org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient$CLIServiceClientWrapper.executeStatement(RetryingThriftCLIServiceClient.java:111)
	at org.apache.lens.driver.hive.HiveDriver.execute(HiveDriver.java:452)
	... 10 more
24 Mar 2015 08:48:24,096 [estimate-31] ERROR org.apache.lens.server.query.QueryExecutionServiceImpl  - Estimate failed for driver org.apache.lens.driver.hive.HiveDriver@3569158f cause: Driver :org.apache.lens.driver.hive.HiveDriver Cause :Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:59 Table not found 'country_carrier_map'
24 Mar 2015 08:48:24,142 [estimate-32] INFO  hive.ql.parse.ParseDriver  - Parsing command: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:24,143 [estimate-32] INFO  hive.ql.parse.ParseDriver  - Parse Completed
24 Mar 2015 08:48:24,143 [estimate-32] INFO  hive.ql.parse.ParseDriver  - Parsing command: select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:24,144 [estimate-32] INFO  hive.ql.parse.ParseDriver  - Parse Completed
24 Mar 2015 08:48:24,145 [estimate-32] INFO  org.apache.lens.driver.jdbc.ColumnarSQLRewriter  - @@@Query not eligible for inner subquery rewrite
24 Mar 2015 08:48:24,145 [estimate-32] INFO  org.apache.lens.driver.jdbc.ColumnarSQLRewriter  - Input Query : select cast('17.29' as decimal(4,2)) from country_carrier_map limit 1
24 Mar 2015 08:48:24,145 [estimate-32] INFO  org.apache.lens.driver.jdbc.ColumnarSQLRewriter  - Rewritten Query :  select tok_decimal( '17.29' ) from country_carrier_map  limit 1
24 Mar 2015 08:48:24,192 [estimate-32] ERROR org.apache.lens.server.api.query.AbstractQueryContext  - Setting driver cost failed for driver org.apache.lens.driver.jdbc.JDBCDriver@5ffcaad4 Cause: Driver :org.apache.lens.driver.jdbc.JDBCDriver Cause :FUNCTION unified_global_dw.tok_decimal does not exist
org.apache.lens.api.LensException: java.sql.SQLWarning: FUNCTION unified_global_dw.tok_decimal does not exist
	at org.apache.lens.driver.jdbc.JDBCDriver.prepareInternal(JDBCDriver.java:757)
	at org.apache.lens.driver.jdbc.JDBCDriver.validate(JDBCDriver.java:642)
	at org.apache.lens.driver.jdbc.JDBCDriver.estimate(JDBCDriver.java:570)
	at org.apache.lens.server.api.query.AbstractQueryContext$DriverEstimateRunnable.run(AbstractQueryContext.java:208)
	at org.apache.lens.server.query.QueryExecutionServiceImpl$RewriteEstimateRunnable.run(QueryExecutionServiceImpl.java:1137)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.sql.SQLWarning: FUNCTION unified_global_dw.tok_decimal does not exist
	at com.mysql.jdbc.SQLError.convertShowWarningsToSQLWarnings(SQLError.java:730)
	at com.mysql.jdbc.SQLError.convertShowWarningsToSQLWarnings(SQLError.java:658)
	at com.mysql.jdbc.Statement.getWarnings(Statement.java:2004)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.getWarnings(NewProxyPreparedStatement.java:1597)
	at org.apache.lens.driver.jdbc.JDBCDriver.prepareInternal(JDBCDriver.java:756)
	... 10 more
24 Mar 2015 08:48:24,192 [estimate-32] ERROR org.apache.lens.server.query.QueryExecutionServiceImpl  - Estimate failed for driver org.apache.lens.driver.jdbc.JDBCDriver@5ffcaad4 cause: Driver :org.apache.lens.driver.jdbc.JDBCDriver Cause :FUNCTION unified_global_dw.tok_decimal does not exist
24 Mar 2015 08:48:24,192 [QuerySubmitter] ERROR org.apache.lens.server.query.QueryExecutionServiceImpl  - Error launching query 26d2108d-bac5-4528-a3bf-cee6cf36260e
org.apache.lens.api.LensException: Driver :org.apache.lens.driver.hive.HiveDriver Cause :Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:59 Table not found 'country_carrier_map'
Driver :org.apache.lens.driver.jdbc.JDBCDriver Cause :FUNCTION unified_global_dw.tok_decimal does not exist
	at org.apache.lens.server.query.QueryExecutionServiceImpl.rewriteAndSelect(QueryExecutionServiceImpl.java:1071)
	at org.apache.lens.server.query.QueryExecutionServiceImpl.access$800(QueryExecutionServiceImpl.java:76)
	at org.apache.lens.server.query.QueryExecutionServiceImpl$QuerySubmitter.run(QueryExecutionServiceImpl.java:471)
	at java.lang.Thread.run(Thread.java:701)
24 Mar 2015 08:48:24,258 [Grizzly-worker(3)] INFO  org.apache.lens.server.AuthenticationFilter  - Request from user: null, path=/queryapi/queries/26d2108d-bac5-4528-a3bf-cee6cf36260e

{noformat}",smartshark_2_2,435,lens,"""[0.23193208873271942, 0.7680678963661194]"""
1920,230179,Lens server not starting on hdp2.2,"{CODE}
Exception in thread ""main"" java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.s3a.S3AFileSystem could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:224)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:181)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:377)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
	at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2563)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2574)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:345)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:293)
	at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:128)
	at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:76)
	at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
	at org.apache.hive.service.cli.CLIService.init(CLIService.java:112)
	at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
	at org.apache.lens.server.LensServices.init(LensServices.java:186)
	at org.apache.lens.server.LensServer.startServices(LensServer.java:113)
	at org.apache.lens.server.LensServer.<init>(LensServer.java:73)
	at org.apache.lens.server.LensServer.main(LensServer.java:168)
Caused by: java.lang.NoClassDefFoundError: com/amazonaws/services/s3/AmazonS3
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2532)
	at java.lang.Class.getConstructor0(Class.java:2842)
	at java.lang.Class.newInstance(Class.java:345)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
	... 20 more
Caused by: java.lang.ClassNotFoundException: com.amazonaws.services.s3.AmazonS3
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 25 more
{CODE}",smartshark_2_2,510,lens,"""[0.5242518782615662, 0.4757481515407562]"""
1921,230180,UnionCandidates getting pruned when only dim attributes projected,In current code there is a bug which prunes all UnionCandidates when only dim attributes selected.,smartshark_2_2,1313,lens,"""[0.06963905692100525, 0.9303609728813171]"""
1922,230181,LENS does not emit stats to ganglia,"Currently, Lens server emits stats to console or ganglia based on the provided configuration. But currently lens server is not emitting stats to ganglia because it is not reading the configuration.
{code:title=code snippet|borderStyle=solid}
if (hiveConf.getBoolean(LensConfConstants.ENABLE_CONSOLE_METRICS, false)) {
      // ganglia reporter initialization
...
}
{code}

instead it should be 

{code:title=Fix |borderStyle=solid}
if (hiveConf.getBoolean(LensConfConstants.ENABLE_GANGLIA_METRICS, false)) {
      // ganglia reporter initialization
...
}
{code}
",smartshark_2_2,127,lens,"""[0.16354498267173767, 0.8364549875259399]"""
1923,230182,Seeing intermittent failure TestQueryEndEmailNotifier.testLaunchFailure,One such failure : https://builds.apache.org/job/PreCommit-Lens-Build/278/testReport/junit/org.apache.lens.server.query/TestQueryEndEmailNotifier/testLaunchFailure/,smartshark_2_2,997,lens,"""[0.8647869229316711, 0.1352130025625229]"""
1924,230183,Document details of how to pass timerange in cube QL,"Lens cube query language allows passing timerange at different granularities : like secondly, minutely to yearly also. The doc should be extended to include details about how to pass each granularity.

Also doc should include details on how pass relative timeranges",smartshark_2_2,113,lens,"""[0.9982872605323792, 0.0017127221217378974]"""
1925,230184,StatusPoller logs are useless,"Endless loop of the following:


{noformat}
28 Sep 2015 11:27:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:27:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:28:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:28:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:29:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:29:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:29:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:29:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:30:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:30:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:31:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:31:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:32:21 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:32:21 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:33:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:33:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:34:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:34:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:34:59 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:34:59 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:35:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:35:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:36:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:36:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:37:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:37:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:38:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:38:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:40:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:40:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:41:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:41:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:42:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:42:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:44:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:44:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:45:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:45:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:47:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:47:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:49:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:49:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:50:49 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:50:49 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:52:19 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:52:19 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:53:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:53:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:55:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:55:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:56:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:56:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:58:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:58:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:59:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 11:59:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:01:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:01:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:02:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:02:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:03:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:03:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:05:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:05:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:06:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:06:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:08:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:08:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:09:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:09:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:11:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:11:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:12:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:12:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:13:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:13:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:15:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:15:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:16:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:16:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:18:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:18:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:19:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:19:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:20:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:20:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:22:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:22:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:23:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:23:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:25:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:25:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:26:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:26:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:28:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:28:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:29:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:29:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:31:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:31:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:32:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:32:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:34:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:34:03 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:35:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:35:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:36:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:36:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:38:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:38:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:39:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:39:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:40:39 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:40:39 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:41:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:41:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:43:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:43:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:44:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:44:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:45:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:45:44 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:46:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:46:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:48:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:48:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:49:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:49:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:50:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:50:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:51:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:51:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:53:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:53:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:54:50 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:54:50 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:56:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:56:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:57:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:57:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:59:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 12:59:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:00:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:00:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:02:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:02:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:03:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:03:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:05:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:05:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:07:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:07:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:08:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:08:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:10:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:10:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:11:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:11:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:13:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:13:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:15:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:15:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:16:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:16:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:18:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:18:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:19:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:19:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:21:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:21:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:22:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:22:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:24:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:24:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:25:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:25:58 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:27:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:27:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:29:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:29:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:30:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:30:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:32:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:32:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:33:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:33:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:35:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:35:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:36:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:36:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:38:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:38:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:39:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:39:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:40:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:40:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:42:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:42:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:43:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:43:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:44:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:44:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:46:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:46:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:47:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:47:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:48:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:48:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:50:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:50:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:51:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:51:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:52:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:52:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:54:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:54:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:55:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:55:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:56:36 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:56:36 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:57:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:57:52 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:59:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 13:59:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:00:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:00:31 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:02:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:02:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:04:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:04:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:05:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:05:20 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:06:39 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:06:39 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:07:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:07:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:09:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:09:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:10:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:10:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:11:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:11:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:13:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:13:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:14:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:14:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:15:33 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:15:33 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:16:49 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:16:49 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:18:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:18:05 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:19:21 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:19:21 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:20:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:20:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:21:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:21:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:24:19 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:24:19 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:25:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:25:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:26:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:26:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:28:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:28:07 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:29:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:29:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:30:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:30:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:32:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:32:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:33:16 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:33:16 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:34:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:34:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:35:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:35:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:37:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:37:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:38:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:38:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:39:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:39:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:41:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:41:00 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:43:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:43:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:44:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:44:51 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:46:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:46:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:47:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:47:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:48:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:48:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:50:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:50:02 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:51:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:51:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:52:36 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:52:36 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:53:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:53:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:55:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:55:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:56:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:56:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:57:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:57:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:58:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:58:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:59:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 14:59:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:01:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:01:09 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:02:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:02:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:03:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:03:48 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:05:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:05:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:06:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:06:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:07:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:07:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:08:54 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:08:54 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:10:10 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:10:10 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:11:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:11:26 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:12:47 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:12:47 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:14:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:14:08 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:15:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:15:24 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:16:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:16:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:17:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:17:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:19:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:19:12 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:20:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:20:28 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:21:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:21:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:23:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:23:01 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:24:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:24:17 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:25:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:25:37 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:26:57 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:26:57 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:28:14 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:28:14 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:29:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:29:30 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:30:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:30:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:32:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:32:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:33:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:33:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:34:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:34:42 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:36:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:36:04 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:37:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:37:25 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:38:47 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:38:47 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:40:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:40:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:41:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:41:32 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:42:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:42:53 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:44:14 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:44:14 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:45:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:45:35 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:46:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:46:56 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:48:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:48:18 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:49:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:49:41 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:50:57 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:50:57 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:52:13 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:52:13 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:53:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:53:29 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:54:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:54:45 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:56:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:56:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:57:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:57:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:58:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:58:38 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:59:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 15:59:55 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:01:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:01:11 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:02:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:02:27 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:03:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:03:43 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:04:59 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:04:59 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:06:15 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:06:15 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:07:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:07:34 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:08:50 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:08:50 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:10:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:10:06 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:11:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:11:22 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:12:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:12:40 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:13:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Polling status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
28 Sep 2015 16:13:46 [6a61d6b0-66a9-4b39-8f3a-ba2a722fb737] [StatusPoller] INFO  org.apache.lens.server.query.QueryExecutionServiceImpl - Updating status for 6a61d6b0-66a9-4b39-8f3a-ba2a722fb737
{noformat}",smartshark_2_2,695,lens,"""[0.8796894550323486, 0.12031057476997375]"""
1926,230185,Support HBase Storage for Lens,"1. Explore the options to achieve this . Apache Phoenix can be one of the options. 
2. Implement a Lens Driver to support HBase using the chosen option.",smartshark_2_2,895,lens,"""[0.998264491558075, 0.0017354961019009352]"""
1927,230186,Add suport for Join chains on dimension tables,"Currently join chains are supported only on cubes . This needs to be supported on dimension tables as well for dimension only queries 

",smartshark_2_2,168,lens,"""[0.9983049631118774, 0.001694988808594644]"""
1928,230187,Applying Query Launching Constraints before allowing a query to be launched,"Lens will always accept a new query from a user and put it in a scheduling queue for processing.

Next candidate query picked up from scheduling queue for processing will be launched only if all query constraints evaluated on candidate query and launched queries allows the candidate query to be launched, otherwise the candidate query will be added to waiting queries.

When any launched query is finished, a waiting query selector will select waiting queries for rescheduling using a list of waiting queries selection policy. Every waiting query selection policy will return a list of eligible waiting queries to be rescheduled. Query Selector will calculate intersection of multiple list of eligible waiting queries and add the result of intersection to scheduling queue for reprocessing.

At initialization, Query Constraints and Waiting Query Selection Policies will be configured using configuration values.

New Query Constraints and Waiting Query Selection Policies can be added at runtime, without rebuilding and deploying lens module. Drivers should be allowed to add more Query Constraints and Waiting Queries Selection policies.

Waiting Queries will be persisted across server restarts.

Query Constraint 1: Allow a candidate query to be launched, only if, cumulative query cost of all current queries launched by the user, who submitted candidate query, is less than  a cumulative query cost ceiling for launching a new query. 

Waiting Query Selection Policy 1: Select all waiting queries of the user whose query has finished.

Query Constraint 2: Allow a candidate query to be launched, only if, current concurrent queries launched on the selected driver are less than max concurrent queries allowed on the driver. 

Waiting Query Selection Policy 2: Select all waiting queries of the driver whose query has finished.",smartshark_2_2,556,lens,"""[0.8243695497512817, 0.17563042044639587]"""
1929,230188,Findbug error in PopulateSampleMetastore,"org.apache.lens.examples.PopulateSampleMetastore.createContinuousFactData() passes a nonconstant String to an execute or addBatch method on an SQL statement [org.apache.lens.examples.PopulateSampleMetastore] At PopulateSampleMetastore.java:[line 113]

Code - 
{quote}
statement.execute(""INSERT INTO mydb_sales_aggr_continuous_fact (order_time, delivery_time, customer_id, ""
        + ""product_id, promotion_id, customer_city_id, production_city_id, delivery_city_id, unit_sales, ""
        + ""store_sales, store_cost, max_line_item_price, max_line_item_discount) values ""
        + ""('"" + nowTime + ""','"" + nowTime + ""',2,2,1,2,2,2,1,8,2,10,2)"");
{quote}",smartshark_2_2,526,lens,"""[0.8740586042404175, 0.12594133615493774]"""
1930,230189,Consistent way of receiving request data in REST APIs,"Lets take example of createCube and createFact.

createCube accepts LensSessionHandle as query param and XCube as entity.

@POST
@Path(""cubes"")
public APIResult createNewCube(@QueryParam(""sessionid"") LensSessionHandle sessionid, XCube cube) 

On the other hand, createFact consumes multi part form data and accepts LensSessionHandle and XFactTable as form data params.


@Consumes({MediaType.MULTIPART_FORM_DATA})
@POST
@Path(""/facts"")
public APIResult createFactTable(@FormDataParam(""sessionid"") LensSessionHandle sessionid, @FormDataParam(""fact"") XFactTable fact)

createFact and createCube are logically similar but still have different ways of receiving request data in REST API. 

Logically similar APIs should choose same method to receive request data.",smartshark_2_2,853,lens,"""[0.9954493641853333, 0.0045506153255701065]"""
1931,230190,Too many warnings in YAMLToStringStrategy in test cases,"Warning are like this:

{noformat}
26 Feb 2016 15:34:18 [] [226845a2-33e5-49c3-b4a0-7d3686ae327f2cfd2fae-9d45-4aa4-b486-7990ca8521c209d1436c-95d1-45de-a4b0-b0eeb0e40ae7552eec0a-5c09-4b0a-b90d-0a4d4fca87c44d61146e-d361-439d-be03-c69e96d3ae1203e5b497-a161-44c5-a408-7a9ff7f4a900d0a3eb7c-7e80-40a6-a63b-1276e3c04ecc43f0a820-2306-4e46-9fc2-c6fedbcd0b1702c0c8db-96b8-48c5-836f-9171e65f085ae221fbf0-2ffd-4107-af48-c6d15eb6a5a3aaf6f711-97fa-47e2-94bd-1c383ac657dd61be499f-6de2-431c-b7fa-c6fdd33e1abc8d654023-57b8-4e39-80aa-f872e82a993462d7ef7d-1385-4d85-b729-ea16735311a6423d0c69-780c-40d6-9c7b-17dd526ede85e6fdf664-36a5-4603-a551-80d95ef12c6c77a4a15f-9bfd-4d37-894b-8820b1b87dead5329576-8b5b-406b-aa7e-03e7aca88f0f8f36cef5-0668-4dd2-8e5e-39b32044d8eef4356de8-7a72-4b10-8420-bdfd1681e4e7af1418e0-f478-4ed0-af7f-fdde93091c0324d18e65-b486-4478-9209-ab468f9eba283b42cae6-d122-45b9-aaee-8858f9860922a15ebde7-ee6e-4bd7-8743-b87c09c15604f4050638-c0bf-4267-930c-9922082982a14efa6e9f-5e56-4ac0-8921-f8079c74ce31ad561d3e-d0ec-4ac1-8221-9b2c8e955c3c84056264-77c1-418c-90b6-e7696bf5e80c3010c07a-0e50-48f9-904c-9d317d921e8aa866f2ae-4b14-4386-bc21-9c5239e143658584ec4e-b83d-4e8c-886c-98d3c35cc926cd8d4b59-774d-4c8d-a8e6-b8d2e0f99ec7d4ebb0c0-559f-4fee-99bf-2f50b7d75790e2112faa-5a85-4612-91f7-42b12a55130ec531458d-e1be-4c95-a2b2-3e4cf745f6b7d0ff9faa-844b-4c6d-8147-969d606ab04e6073eed1-a800-4df4-a597-d018131407e274ba8d1c-d2d5-43fa-88d9-e2add2ed5f63cda96244-8e1b-4984-bf7a-9ee6c78bbca7471157e8-26db-436c-965b-3796d06eb4476df6de09-e7b6-445d-a5bf-08411e855785d437d9d7-f614-415f-86b2-93a43c86b3c06955880c-13ff-4f58-a331-4d5f1317fdba3d40c79a-edfa-4f58-895f-d18189550a2c16aad528-bc5c-4751-a1aa-2d2379acfd18ef3cbff5-2239-4d0d-84bf-fd3c2c2625f0c0c3f8e5-359a-437e-9309-2f0c94f77ac3957271e9-ede6-472f-96e3-fd55eec67f26dae6564d-2980-4549-b881-49d7adb70a32295d0244-56ee-4d11-8cf3-cab2b455be0d9177827c-a6b8-4e19-8309-e0ba047974aac5eae575-3a84-4d62-aa78-317f5e2ac02c49520156-56a6-422c-8617-a6f060b6a712main] ERROR org.apache.lens.api.jaxb.YAMLToStringStrategy - getter access failed. Going the usual way
{noformat}
",smartshark_2_2,1055,lens,"""[0.9904838800430298, 0.009516173973679543]"""
1932,230191,Docker is failing after Lens-220,"due to change in the dist paths, docker is failing.",smartshark_2_2,225,lens,"""[0.9736165404319763, 0.026383474469184875]"""
1933,230192,Adding using POJOs as another communication protocol between Lens and Drivers,"In the developer meet dated March 28, 2015, scope of addition of new drivers like elastic search was discussed. Drivers like elastic search driver may retrieve data using programmatic APIs instead of HQL from underlying storage systems. In such cases it will be an overhead on driver side to translate HQL to inputs to be given to programmatic APIs. These drivers may not have join scope as well. This jira is to discuss the scope of communicating an object with following information to drivers:

(a) List of Select
(b) From
(c) List of Where (filters)
(d) List of Group By
(e) List of Having
(f)  List of Joins (Optional)

Lens has already parsed cube query and is aware of above mentioned lists. If lens can communicate these lists to drivers instead of HQL, it will save drivers from parsing HQL.

There may be various things which we have to consider to achieve the same. This jira is to discuss the same.

Drivers may communicate via driver configuration file whether they are interested in HQL or a POJO composed of above lists and lens may call corresponding APIs on drivers.

",smartshark_2_2,657,lens,"""[0.9983687996864319, 0.0016311691142618656]"""
1934,230193,Improvement request for Lens UI,"1. With wrong credentials, it should give some message saying ""wrong username/password"". Also there is no option to edit credentials in the text box after the authentication fails.
2. Please provide a ""Log out"" tab. As every time a new page is opening with the logged in user.
3. In some case download result is not working
4. Provide an option to set params for session from UI itself
5. Refreshing/Opening a new tab from the main UI page is creating a new grill Session",smartshark_2_2,1,lens,"""[0.9983678460121155, 0.0016321346629410982]"""
1935,230194,Provide QueryAcceptors to be declared in configuration,"Creation: 
{noformat}
  private List<QueryAcceptor> queryAcceptors = new ArrayList<QueryAcceptor>();




{noformat}
Usage:


{noformat}
  private void accept(String query, Configuration conf, SubmitOp submitOp) throws LensException {
    // run through all the query acceptors, and throw Exception if any of them
    // return false
    for (QueryAcceptor acceptor : queryAcceptors) {
      String cause = """";
      String rejectionCause = acceptor.accept(query, conf, submitOp);
      if (rejectionCause != null) {
        getEventService().notifyEvent(new QueryRejected(System.currentTimeMillis(), query, rejectionCause, null));
        throw new LensException(""Query not accepted because "" + cause);
      }
    }
    getEventService().notifyEvent(new QueryAccepted(System.currentTimeMillis(), null, query, null));
  }





{noformat}



We can take from conf and populate the list. Right now it's not populated. ",smartshark_2_2,213,lens,"""[0.9980672001838684, 0.0019328127382323146]"""
1936,230195,Create a single jar for shipping with hive queries,"Currently a number of jars have to be shipped along with each query apart from user extension jars for udf, serde etc. This has two side effects:

1. Number of ClassLoader on the hive server increases (potentially causing PermGen OOM issues)
2. Number of jar localization events in the node manager when executing a task.

The proposal is to bundle these and create a single shaded jar and make it available in the binary distribution.",smartshark_2_2,753,lens,"""[0.9985431432723999, 0.0014568445039913058]"""
1937,230196,Add help for cli commands,"show dimensions <dimtable>
show dimtables <dimensions>


Currently, pressing <tab> after ""show dimensions"" doesnt show any help",smartshark_2_2,416,lens,"""[0.9982553124427795, 0.0017447060672566295]"""
1938,230197,Create symlinks to versioned jars in packaging,"We should have symlink to versioned jars in the package that we create, sothat they can be put in other process's classpaths and they shouldn't require a restart on new version deployment
",smartshark_2_2,202,lens,"""[0.9978814721107483, 0.0021184778306633234]"""
1939,230198,Query handle stickiness for JDBC queries,Implement query handle stickiness for JDBC queries such that a query lookup is redirected to the same node that submitted the query,smartshark_2_2,499,lens,"""[0.9978193044662476, 0.0021806766744703054]"""
1940,230199,Query output files should have the query name,Query output persisted files generated post execution should have the query name specified with the query. This will let users track the query outputs better.,smartshark_2_2,687,lens,"""[0.9984824061393738, 0.0015176668530330062]"""
1941,230200,Lens Client doesn't provide the option to pass query conf while submitting the query,Lens Client doesn't provide the option to pass query conf while submitting the query. Lens Server however allow the conf to posted as a form param along with other parameters. Lens Client need to be extended to pass the LensConf along.,smartshark_2_2,1342,lens,"""[0.7459267377853394, 0.25407323241233826]"""
1942,230201,Move all version management from child modules to dependency management in main pom,"Since we dont want to have modules depend on different versions of same dependency, all the dependencies specifying version should be in main pom's dependency management section. Child poms should only child specific dependency specified.

Right now following are specified in child poms. This issue is to fix them :

{noformat}
lens amareshwari.sr$ grep ""<version>"" */pom.xml |grep -v '${project.version}' |grep -v '2.0.0-SNAPSHOT'
checkstyle/pom.xml:   <version>1.0-SNAPSHOT</version>
lens-api/pom.xml:                <version>0.8.0</version>
lens-api/pom.xml:                            <version>0.6.3</version>
lens-cli/pom.xml:        <version>2.3.1</version>
lens-cli/pom.xml:        <version>2.15</version>
lens-client-dist/pom.xml:        <version>2.7</version>
lens-dist/pom.xml:        <version>2.7</version>
lens-driver-hive/pom.xml:                <version>2.15</version>
lens-driver-impala/pom.xml:		    <version>0.1</version>
lens-driver-impala/pom.xml:	 	  <version>0.9.0</version>
lens-ml-lib/pom.xml:                <version>2.15</version>
lens-ml-lib/pom.xml:                <version>2.8</version>
lens-query-lib/pom.xml:            <version>2.3</version>
lens-server/pom.xml:                <version>2.1.1</version>
lens-server/pom.xml:            <version>2.4</version>
lens-server/pom.xml:                <version>${jetty.version}</version>
lens-server/pom.xml:                <version>1.2.1</version>
lens-server/pom.xml:                <version>2.15</version>
lens-server/pom.xml:						<version>1.28</version>

{noformat}
",smartshark_2_2,41,lens,"""[0.9983797073364258, 0.0016202782280743122]"""
1943,230202,The exception thrown for no candidate fact should contain only brief error,"The exception thrown for no candidate fact table can answer should contain only brief error and detail error should be part of query status details.  
",smartshark_2_2,803,lens,"""[0.9915218353271484, 0.00847812183201313]"""
1944,230203,Check timeline present property flag instead of checking whether its present or not,"we are checking whether this property ""cube.storagetable.partition.timeline.cache.present"" exists or not, if not then we are caching the partitions and adding this property to the table. As we cant delete a property from hive table,  we cant disable caching feature. so can we please change the logic to check whether its is null or false, instead of just checking whether its present or not.",smartshark_2_2,345,lens,"""[0.9334555268287659, 0.06654451042413712]"""
1945,230204,Add Drill Driver for Lens,Explore Drill as an execution engine for Lens. Drill has a Sql Interface and a Jdbc API. ,smartshark_2_2,848,lens,"""[0.9979422688484192, 0.002057771198451519]"""
1946,230205,Add ability to view cube data model on UI with svg file,"Lens cube is linked multiple entities in the following way :
- Linked to multiple fact tables with belongs relation.
- Linked to multiple dimensions with join chains.

This ask is to add the ability to generate the svg file with relations and view the file in lens ui with svg to html or svg to png converter.",smartshark_2_2,767,lens,"""[0.9984490871429443, 0.0015508581418544054]"""
1947,230206,Lens patch-review tool,Creating a patch review tool for ease of patch creation and upload to Jira and review board,smartshark_2_2,396,lens,"""[0.9960263967514038, 0.003973585087805986]"""
1948,230207,Update release documentation ,You can refer - https://issues.apache.org/jira/browse/LENS-505,smartshark_2_2,460,lens,"""[0.9983925223350525, 0.0016074206214398146]"""
1949,230208,tools/scripts/build-docker.sh  is not working.,"Docker image creation is not working because 
inside  lens-docker/lens-test/Dockerfile 
wget http://apache.mirrors.lucidnetworks.net/spark/spark-1.3.0/spark-1.3.0-bin-hadoop2.4.tgz
in is throwing 404.
we need to update the URI.",smartshark_2_2,791,lens,"""[0.9882693290710449, 0.011730628088116646]"""
1950,230209,Update cli.apt file (i.e. not updated by previous patches),cli.apt file was not updated earlier. ,smartshark_2_2,1429,lens,"""[0.9981440305709839, 0.001855942071415484]"""
1951,230210,Fix indentation in pom files,"Right now, some sections were indented with two spaces and some of them indented with four spaces in pom files. 

",smartshark_2_2,36,lens,"""[0.9969690442085266, 0.0030310177244246006]"""
1952,230211,Cross origin resource sharing in LENS,"This is a tracking ticket to support CORS in lens. Lens currently does not have support for CORS (cross origin resource sharing). CORS is needed when the API and frontend run on different subdomains/different ports (i.e cross origin). Currently, I think the frontend service and API that the frontend uses run on port 1999 and hence this is not a problem. However, if one needs to isolate the frontend and API's to be running on different hosts, this would need to be supported. Here's some more info on CORS: http://www.staticapps.org/articles/cross-domain-requests-with-cors 

Essentially, what we need to do is to allow the following response headers to be returned by the server :

Access-Control-Allow-Origin: As described above, this needs to be either the origin of the request or *.

Access-Control-Allow-Methods: This is a comma-separated list of the HTTP methods that are allowed, for example POST, PUT, OPTIONS.

Access-Control-Allow-Headers: A comma-separated list of allowable custom request headers, for example AUTHORIZATION, X-CLIENT-ID, X-CLIENT_SECRET.

As a proof of concept, I have added a CORSResponseFilter (similar to the authentication filter) that adds the above to the response header and I find it to be resolving the problem. Ideally, we would like these response headers to be configurable as part of lens conf (lens-site.xml) with the default behaviour assuming frontend and API can both run on same host. 

Bala",smartshark_2_2,371,lens,"""[0.9981891512870789, 0.0018109013326466084]"""
1953,230212,Create documentation for Lambda architecture,"Lambda architecture in Lens can be supported easily. Lens can easily abstract the historical data and realtime data stored in different stores providing the unified single query interface.
We should document the possible architectural patterns to achieve this.",smartshark_2_2,646,lens,"""[0.9982582926750183, 0.001741754706017673]"""
1954,230213,Add how to release page,Figure out the release process in Apache and add how to release Lens page,smartshark_2_2,289,lens,"""[0.9947419166564941, 0.005258058197796345]"""
1955,230214,Add DatabaseResourceService load errors,"Right now, DatabaseResourceService continues if loading of some resources fails. We need to have a counter emitted to know the errors in DatabaseResourceService while loading resources.",smartshark_2_2,433,lens,"""[0.9975195527076721, 0.0024803655687719584]"""
1956,230215,Assert on failed queries counter will fail intermittently,"TestQueryService#testLaunchFail

{code}
assertTrue(metricsSvc.getTotalFailedQueries() >= failedQueries + 1);
{code}

This assert is asserting that a counter on totalFailedQueries should be incremented by 1, when a query fails. 

The counter is incremented in an asynchronous thread AsyncQueryStatusListener. If this assert is executed before the counter is incremented in asynchronous thread, then this assert will fail.

To reproduce this in master branch, introduce a infinite sleep before incrementing the counter at this line: https://github.com/apache/incubator-lens/blob/apache-lens-2.2.0-beta-incubating/lens-server/src/main/java/org/apache/lens/server/metrics/MetricsServiceImpl.java#L181

Run TestQueryService#testLaunchFail after this change and the above assert will fail in master branch.



",smartshark_2_2,639,lens,"""[0.8506842255592346, 0.14931578934192657]"""
1957,230216,Verify 2.1 issues,All the issues to be verified - https://issues.apache.org/jira/issues/?jql=project%20%3D%20LENS%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%202.1%20ORDER%20BY%20priority%20DESC,smartshark_2_2,353,lens,"""[0.9979835748672485, 0.0020164004527032375]"""
1958,230217,Provide a way to distinguish between fields reached by multiple paths,"We need a way to distinguish fields reachable from different paths.

For example: If country field is reachable from two different attributes, for which the values are different.",smartshark_2_2,137,lens,"""[0.9984105825424194, 0.0015893611125648022]"""
1959,230218,Add examples execution to ML,"Create a utility that can read the ml params, test table etc from the conf and execute it via a script",smartshark_2_2,177,lens,"""[0.9956918358802795, 0.00430815014988184]"""
1960,230219,Installation issue,"root@ns01 logs]# cat lensserver.out.2016052809361464453394
OpenJDK 64-Bit Server VM warning: ignoring option PermSize=256m; support was removed in 8.0
OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0
Failed to instantiate [ch.qos.logback.classic.LoggerContext]
Reported exception:
javax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.SAXParserFactory cannot be created
	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:311)
	at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267)
	at javax.xml.parsers.SAXParserFactory.newInstance(SAXParserFactory.java:127)
	at ch.qos.logback.core.joran.event.SaxEventRecorder.buildSaxParser(SaxEventRecorder.java:81)
	at ch.qos.logback.core.joran.event.SaxEventRecorder.recordEvents(SaxEventRecorder.java:59)
	at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:134)
	at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:99)
	at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:49)
	at ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:77)
	at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:152)
	at org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:85)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:55)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:129)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:108)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:302)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:276)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
	at org.apache.lens.server.LensServer.<clinit>(LensServer.java:54)
Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.SAXParserFactory cannot be created
	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)
	... 17 more
Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.SAXParserFactory: Provider org.apache.xerces.jaxp.SAXParserFactoryImpl not found
	at java.util.ServiceLoader.fail(ServiceLoader.java:239)
	at java.util.ServiceLoader.access$300(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372)
	at java.util.ServiceLoader$LazyIterator.next(ServiceL",smartshark_2_2,876,lens,"""[0.8363971710205078, 0.1636028289794922]"""
1961,230220,Log  lens Session details in Query Logs,"As of now its not easy to map Lens Queries to Lens Sessions . 
It can be made trivial if the session details are logged in query logs .

This can be required in many scenarios. 
One such scenario : attribute a lens session close (say a user issued a query via CLI and then exited the CLI before query was scheduled for a run on lens server)  as a reason for  failure/cancellation of queries triggered via that session . ",smartshark_2_2,1114,lens,"""[0.9984377026557922, 0.0015623471699655056]"""
1962,230221,Validate license for new dependencies,"- Run ""git diff <last-release-tag> -- pom.xml""
Example git diff apache-lens-2.5.0-beta -- pom.xml

- Check the license for new dependencies and make sure its compatible with apache licensing scheme https://www.apache.org/legal/resolved.html

- If any of the new dependencies added  are not apache licensed, and are getting bundled in binary distribution, LICENSE and NOTICE should be updated in bin-dist-files/",smartshark_2_2,1171,lens,"""[0.9982226490974426, 0.0017773555591702461]"""
1963,230222,Lens build failing,"This seems to happen due to following test case repeating twice in TestCubeRewriter class file.

{code}
@Test
public void testTimeRangeIn() throws Exception {

{code}",smartshark_2_2,1435,lens,"""[0.9692710041999817, 0.03072899580001831]"""
1964,230223,Assertion on phase 1 rewritten query will fail intermittently,"TestQueryService#testExecuteAsync:

Below asserts will fail, if the query context is removed by QueryPurger thread before the test running test case could retrieve it from allQueries collection using queryService. Provided lens.server.max.finished.queries=0 in lens-site.xml

{code}

QueryContext ctx =    queryService.getQueryContext(lensQuery.getQueryHandle());
assertNotNull(ctx.getPhase1RewrittenQuery());
assertEquals(ctx.getPhase1RewrittenQuery(), ctx.getUserQuery()); 
   
{code}

These were added in LENS-24 Commit: https://github.com/apache/incubator-lens/commit/689c9a99b112b21e9ecba9a8461cf9672b88ba2f
",smartshark_2_2,530,lens,"""[0.9236798286437988, 0.07632017135620117]"""
1965,230224,Allow column name mapping for few/all columns in underlying storage tables,"This improvement proposed is to accept column mapping for few or all columns in underlying storage tables - which allows column to be different in underlying storage than column in fact/dimtable

For example:
Fact1 has col1

S1_Fact1 has col1
S2_Fact2 has col1_variant

S2_Fact2 can have column mapping property specified as col1:col1_variant. If S2_Fact2 becomes the eligible storage table, then query should be written with col1_variant if col1 is queried.",smartshark_2_2,704,lens,"""[0.9984606504440308, 0.0015393836656585336]"""
1966,230225,Update the javax mail version to current,Sending mail with the older version of javax mail was causing threads to stuck because of connection timeout to the mail server was not honored. Updating to the new version solves the problem.,smartshark_2_2,1327,lens,"""[0.9748407602310181, 0.025159208104014397]"""
1967,230226,Remove commons logging dependency,After moving all modules to slf4j. commons-logging dependency can be fully removed.,smartshark_2_2,547,lens,"""[0.9975722432136536, 0.0024277002085000277]"""
1968,230227,cubesegmentation commands are conflicting with cube commands,"Since cubesegmentation commands have prefix as cube, the auto completion and help for cube commands is not working.

We need to change the command word from cubesegmentation to something else.

Shall we change to just segmentation ?",smartshark_2_2,1129,lens,"""[0.9557340741157532, 0.04426593706011772]"""
1969,230228,Support flattening of columns selected through bridge-tables(many-to-many relationships),"Data warehousing concepts suggests to use bridge-tables to implement many-to-many relationships between cube & dimension or between two dimensions.

Here are few links explaining the bridge-tables
http://www.pythian.com/blog/implementing-many-to-many-relationships-in-data-warehousing/
http://www.kimballgroup.com/2012/02/design-tip-142-building-bridges/
http://stackoverflow.com/questions/2785158/star-schema-fact-1n-dimension-how
http://www.askjohnobiee.com/2013/08/how-to-bridge-tables-and-many-to-many.html

If we look at the following schema :
User :
|ID|Name|Gender|
|1|A|M|
|2|B|M|
|3|C|F|

User interests :
|UserID| Sports ID|
|1|1|
|1|2|
|2|1|
|2|2|
|2|3|

Sports :
|SportsID| Description|
|1|Football|
|2|Cricket|
|3|Basketball|

User Interests is the bridge table which is capturing the many-to-many relationship between Users and Sports.

If we have a fact as follows :
|UserId| Revenue|
|1|100|
|2|50|

If analyst is interested in analyzing with respect to user's interest, then the report would the following :

|User Interest|Revenue|
|Football|150|
|Cricket|150|
|BasketBall|50|

Though the individual rows are correct and the overall revenue is actually 150, looking at above report makes people assume that overall revenue is 350.

The feature ask here is to optionally flatten the selected fields, if fields involved are coming from bridge tables in join path. If flattening is enabled, the report would be the following :

|User Interest|Revenue|
|Football, Cricket|100|
|Football, Cricket, BasketBall|50|
",smartshark_2_2,703,lens,"""[0.9984622001647949, 0.0015377779491245747]"""
1970,230229,Create documentation for lens-ml component,Create user documentation for lens-ml.,smartshark_2_2,216,lens,"""[0.9979274272918701, 0.002072567818686366]"""
1971,230230,improve the brief error in case of no fact found error ,"So basically every fact was rejected with a reason. Some reasons are just error codes, some have additional details with them(e.g. in case of column not found, which columns were not found) Now there's an ordering between reasons. The max of reasons is picked and the brief error message is generated based on the facts that were rejected with the max reason and (if exists) any details of the reason. 

This approach is good enough but there's scope for improvement. The brief error message can also look at the smaller errors and include additional details  for the user. e.g. all facts except one say a column 'c' was not found and one fact says it's not partitioned with the time partition queried. Then final error can say that the column 'c' is not queriable with the given time dimension.

Consider this jira to be the place to put more such cases. ",smartshark_2_2,734,lens,"""[0.998441755771637, 0.0015582094201818109]"""
1972,230231,Refactor QueryExecutionServiceImpl class to make file size smaller. ,"Post this, also reduce the max file size number at https://github.com/apache/lens/blob/master/checkstyle/src/main/resources/checkstyle.xml#L65",smartshark_2_2,1180,lens,"""[0.9981943964958191, 0.0018056717235594988]"""
1973,230232,[Server] StateStore for LensScheduler,Dividing server side component into smaller tasks. This task is for creating a persistent store for lens scheduler.,smartshark_2_2,995,lens,"""[0.9981021285057068, 0.0018979315645992756]"""
1974,230233,Adding a Rewriter at HiveDriver Level,Adding a rewriter at Hive Driver level for doing rewrites specific to Hive Driver.,smartshark_2_2,579,lens,"""[0.9981938004493713, 0.0018062081653624773]"""
1975,230234,"Change default value of ""lens.query.result.parent.dir""","Currently the default value of ""lens.query.result.parent.dir"" is ""/tmp/lensreports""  which will fail in a scenario where HiveInputFormat is being used and data is in local, since it is not a fully qualified path name, hive will persist the result in hdfs while Lens result formatter will try to get the results from local.

Default value of ""lens.query.result.parent.dir"" must be changed to fully qualified path i.e. ""file:///tmp/lensreports""",smartshark_2_2,249,lens,"""[0.9956797361373901, 0.004320279695093632]"""
1976,230235,Getting Intermittent failure from org.apache.lens.cube.parse.TestCubeRewriter.testQueryWithContinuousUpdatePeriod,"*Error Message*

CANNOT_USE_TIMERANGE_WRITER[Partitions are in different update periods]

*Stacktrace*
org.apache.lens.server.api.error.LensException: CANNOT_USE_TIMERANGE_WRITER[Partitions are in different update periods]
	at org.apache.lens.cube.parse.BetweenTimeRangeWriter.getTimeRangeWhereClause(BetweenTimeRangeWriter.java:67)
	at org.apache.lens.cube.parse.StorageTableResolver.resolveFactStoragePartitions(StorageTableResolver.java:496)
	at org.apache.lens.cube.parse.StorageTableResolver.rewriteContext(StorageTableResolver.java:138)
	at org.apache.lens.cube.parse.CubeQueryRewriter.rewrite(CubeQueryRewriter.java:229)
	at org.apache.lens.cube.parse.CubeQueryRewriter.rewrite(CubeQueryRewriter.java:199)
	at org.apache.lens.cube.parse.CubeQueryRewriter.rewrite(CubeQueryRewriter.java:215)
	at org.apache.lens.cube.parse.TestQueryRewrite.rewriteCtx(TestQueryRewrite.java:92)
	at org.apache.lens.cube.parse.TestCubeRewriter.testQueryWithContinuousUpdatePeriod(TestCubeRewriter.java:97)
",smartshark_2_2,955,lens,"""[0.4852054715156555, 0.5147945284843445]"""
1977,230236,Remove all jars from source,"As per apache release process, source should not include any jars.

We have following jars in code and they should be removed :

$ find . -name *.jar
./lens-server/src/test/resources/restart-on-resource-move-test.jar
./lens-server/test-util/test-aux.jar",smartshark_2_2,84,lens,"""[0.9979275465011597, 0.002072438597679138]"""
1978,230237,Install and run doc should mention forked hive repo,"Lens depends on forked version of Apache Hive at http://github.com/Inmobi/hive.
Current install and run doc does not mention about the hive repo to be used and how to setup HIVE_HOME incase of user wants to build from source.",smartshark_2_2,96,lens,"""[0.998136043548584, 0.0018639431800693274]"""
1979,230238,Dateformat not being passed in prev and next calls of FactPartition ,Dateformat is not being passed in prev and next calls of FactPartition. This is resulting in the date format of Update period to be used for the newly created previous/next partitions.   ,smartshark_2_2,1234,lens,"""[0.20923513174057007, 0.7907648682594299]"""
1980,230239,Lens website doc fix - committer's page,"The lens committer's page (1) still shows the old way of applying patch. Bringing it in sync with the standard style as describe in contributor's page (2).

{quote}
git apply --check lens_patch.patch
git apply lens_patch.patch
{quote}
1. https://lens.incubator.apache.org/developer/commit.html#Commit
2. https://lens.incubator.apache.org/developer/contribute.html#Applying_a_patch",smartshark_2_2,524,lens,"""[0.9980965256690979, 0.0019034930737689137]"""
1981,230240,Create Partition Timeline and keep it cached and stored as storage table properties,"For each fact-storage table, for each of fact-storage table's update periods, for each of fact-storage table's time partitioning column, maintain a Timeline data structure which records all the time partitions existing for the combination in memory. This info will be cached in memory for server lifecycle. For the first time, it will be evaluated from all existing partitions and stored in table properties. For next server restarts, this will be read from the properties. all partition operations(add/delete/latest/...) will update the timeline and use it as best as they can. 



For the Timeline Data structure, the following implementations come to mind:
*  Store list of all partitions. 
*  Store first, last and in-between holes. 
*  Store as an interval tree/segment tree.

As part of this jira, I intend to provide an interface, atleast one implementation and (maybe) configurability of the implementation. ",smartshark_2_2,264,lens,"""[0.9970499277114868, 0.002950049238279462]"""
1982,230241,No candidate dim available exception should contain only brief error,As part of LENS-270 we have made no candidate fact exception to show only brief message. We need to do the same for no candidate dim. Along with this compare the errors from different drivers and show the appropriate one.,smartshark_2_2,844,lens,"""[0.996684730052948, 0.0033152669202536345]"""
1983,230242,Support configuring multiple connection pools for JDBC query executions,"The use case is something like this .

There are two types of uses for JDBC queries 
1. Human users who have tighter SLA's.
2. Apps like alerting systems built on top on lens, these have longer SLA's and are also capable of flooding the lens server with automatically generated requests in bulk.

From DB side, the DB admin also wants to segregate the resources allocated for the two different group of users. Group1 is more critical and will have more resources. The queries hitting DB under Group2 would have less resources and would be isolated from Group1 and hence would not affect the performance of Group1 queries.

",smartshark_2_2,912,lens,"""[0.9985389709472656, 0.001461087609641254]"""
1984,230243,Resolve login issues in UI,"1. With wrong credentials, it should give some message saying ""wrong username/password"". Also there is no option to edit credentials in the text box after the authentication fails.
2. Please provide a ""Log out"" tab. As every time a new page is opening with the logged in user.",smartshark_2_2,159,lens,"""[0.9539342522621155, 0.04606574401259422]"""
1985,230244,Add ability to analyze sessions,"We should be able to analyse all active sessions. 
We should have ability to look at the following information :

- Who are the users holding active sessions
- When was the last time the session is accessed
- How many pending queries are there in session and etc.

and more.
",smartshark_2_2,976,lens,"""[0.9983968138694763, 0.0016031272243708372]"""
1986,230245,Support to run spark in yarn-client mode,currently spark only runs in embedded mode. support to run in yarn-client mode.,smartshark_2_2,366,lens,"""[0.9981911778450012, 0.0018087985226884484]"""
1987,230246,"Make property names driver-agnostic for query constraints, query selection policies and driver query hook","Right now, each driver has it's own specific property name for these things and  reading of these properties followed by initialization of objects based on these is done in each driver. 
Such code is mostly same for all the drivers. So with an Abstract class being available to us, we can use that for removing code duplication. All code mentioned in the previous paragraph can be moved to AbstractLensDriver. Similarly, these property names can be unified.
The benefit will be that any new drivers getting added to Lens will already have these functionalities without having to write code for these. The functionality will be optional since AbstractLensDriver will default to null objects. Plus, these will be available as standard features and property names will be standard across drivers. 

",smartshark_2_2,885,lens,"""[0.9984147548675537, 0.0015852563083171844]"""
1988,230247,"Improve ""how to commit"" doc.","We can do the following improvements in how to commit doc.

- Remove ""contributor via committer"" in commit message, and make passing --author as mandatory with actual contributor.
- Update new committers section to do first commit wrt adding his name in pom.xml
- Update doc wrt reverting a commit.",smartshark_2_2,561,lens,"""[0.9980745315551758, 0.0019254660001024604]"""
1989,230248,Create scheduler client side apis.,This task is to put client side api for specifying schedule and trigger.,smartshark_2_2,1267,lens,"""[0.9978094696998596, 0.0021905172616243362]"""
1990,230249,Add logs in FieldValidator,Exceptions of fieldValidator don't provide any info in logs and it's difficult to debug why fields are not queryable together. ,smartshark_2_2,1349,lens,"""[0.9983528852462769, 0.0016471062554046512]"""
1991,230250,Error While building Lens Site,"1. The output of enunciate had changed after upgrading it. 
Need to update tools/scripts/generate-site-public.sh accordingly. 

2. Download links are pointing to lens 2.6. Need to change to point to 2.6.1

3. Roles need to be updated in pom.xml for two team members ",smartshark_2_2,1318,lens,"""[0.9961153268814087, 0.0038846866227686405]"""
1992,230251,Move pre-submit hook out of User Config Loader,"By separation of concerns, pre submit hook needs to be separate from user config loader. The code was made a part of user config loader because the need originated as a gap in user config loader. But the solution could have been separate. Creating this jira to address this issue.",smartshark_2_2,461,lens,"""[0.9984591007232666, 0.0015408559702336788]"""
1993,230252,Check for service to be up instead of using sleep in docker bootstrap,"Currently sleep is used to wait for services to come up before executing commands that require services. This is un predictable and leads to failures.
Instead test for service to be up.",smartshark_2_2,501,lens,"""[0.9793070554733276, 0.02069290354847908]"""
1994,230253,Disable test case testQueryAliveOnSessionClose ,"TestQueryIndependenceFromSessionClose.testQueryAliveOnSessionClose is failing in past few builds, commenting it temporarily to make the build pass. ",smartshark_2_2,1364,lens,"""[0.9973402619361877, 0.002659696852788329]"""
1995,230254,Move cube specific error message codes from InMobi's Hive code to Lens,"Cube code has been moved into Lens, but the error messages used by lens-cube still remain in the InMobi's Hive fork. This is to track and move all cube related error messages from (InMobi) Hive to Lens.",smartshark_2_2,331,lens,"""[0.9984172582626343, 0.0015826846938580275]"""
1996,230255,Test Failures on java8,"There are some test failures on lens-cube test module. Did not verify other modules yet.

Ex: TestBaseCubeQueries, TestExpressionResolver and few more tests are failing on java8

Tests are failing while comparing the expected query with rewritten query.  Rewritten query and expected query are same but the **joining of tables are not in the same order**.

In some tests, queries are hitting the fact tables form different cube than expected in the test(i.e. fact tables from multiple cubes can answer the query). I think we should either fix the tests in comparing with all the possibilities or fix the order of querying the cube tables.



",smartshark_2_2,747,lens,"""[0.9491142630577087, 0.05088576674461365]"""
1997,230256,Add Memory gatekeepers for InMemoryResultSet to prevent any OOM on lens server,"In Some cases operations on InMemoryResultSet can result is high memory usage on LensServer 

1. InMemoryResultSet.toQueryResult()  is done for a huge result
2. lens.query.prefetch.inmemory.resultset.rows = Huge Number . 

In both cases we need to have some gate keeping as these features can be accessed/configured by users ",smartshark_2_2,1278,lens,"""[0.9983957409858704, 0.0016042037168517709]"""
1998,230257,Altering fact table should alter its storage table descriptions also,LENS-236 makes storage table description to be part of their fact table definition itself. Altering fact should alter underlying storages as well.,smartshark_2_2,340,lens,"""[0.9976676106452942, 0.002332384930923581]"""
1999,230258,Nearest neighbor searchers sometimes fail to remove points,"When updating a Centroid in StreamingKMeans, the Centroid needs to be removed and its updated version added.

When removing points in a searcher that are already there, sometimes the searcher fails to return the closest point (the one being searched for) causing a RuntimeException.

This has been observed for TF-IDF vectors with SquaredEuclideanDistance and CosineDistance and FastProjectionSearch.",smartshark_2_2,360,mahout,"""[0.07287251949310303, 0.9271275401115417]"""
2000,230259,CrossFoldLearner trains in all folds if trackign key is negative,See: https://github.com/apache/mahout/pull/7,smartshark_2_2,451,mahout,"""[0.1320589929819107, 0.8679409623146057]"""
2001,230260,"LuceneTextValueEncoder doesn't properly set internal buffers, causing BufferUnderflowException","The LuceneTextValueEncoder throws an BufferUnderflowException when used.  See the code below.  The problem appears to be due to the CharBuffer not getting values, but I'm not sure yet.
{code}
@Test
  public void testLucene() throws Exception {
    LuceneTextValueEncoder enc = new LuceneTextValueEncoder(""text"");
    enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_34));
    Vector v1 = new DenseVector(200);
    enc.addToVector(""test1 and more"", v1);
    enc.flush(1, v1);
}
{code}

Here's the exception:
{quote}
java.nio.BufferUnderflowException
	at java.nio.HeapCharBuffer.get(HeapCharBuffer.java:127)
	at org.apache.mahout.vectorizer.encoders.LuceneTextValueEncoder$CharSequenceReader.read(LuceneTextValueEncoder.java:87)
	at org.apache.lucene.analysis.CharReader.read(CharReader.java:54)
	at org.apache.lucene.util.CharacterUtils$Java5CharacterUtils.fill(CharacterUtils.java:181)
	at org.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:273)
	at org.apache.mahout.common.lucene.TokenStreamIterator.computeNext(TokenStreamIterator.java:41)
	at org.apache.mahout.common.lucene.TokenStreamIterator.computeNext(TokenStreamIterator.java:30)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:141)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:136)
	at org.apache.mahout.vectorizer.encoders.TextValueEncoder.addText(TextValueEncoder.java:78)
	at org.apache.mahout.vectorizer.encoders.TextValueEncoder.addText(TextValueEncoder.java:69)
	at org.apache.mahout.vectorizer.encoders.TextValueEncoder.addToVector(TextValueEncoder.java:59)
	at org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder.addToVector(FeatureVectorEncoder.java:86)
	at org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder.addToVector(FeatureVectorEncoder.java:63)
	at org.apache.mahout.vectorizer.encoders.TextValueEncoderTest.testLucene(TextValueEncoderTest.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:62)

{quote}",smartshark_2_2,33,mahout,"""[0.05895601958036423, 0.9410439729690552]"""
2002,230261,Does bin/mahout work for binary distro?,"Mark reports that bin/mahout doesn't work out of the box from the binary distribution.

11/06/08 21:17:00 INFO mapred.JobClient: Task Id : attempt_201106061352_0066_r_000001_1, Status : FAILED
Error: java.lang.ClassNotFoundException: org.apache.lucene.analysis.TokenStream

This suggests the script isn't finding the job file, and indeed it is not in the same place in the binary distro as in the source tree. I am guessing this is the issue.",smartshark_2_2,1693,mahout,"""[0.9402779340744019, 0.05972203612327576]"""
2003,230262,Streaming KMeans fails when executed in Sequential Mode,"Streaming KMeans fails when executed in Sequential mode because it presently doesn't ignore 'logsCRCFilter' (in sequential execution).

{Code}
INFO: Starting StreamingKMeans clustering for vectors in /tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors; results are output to /tmp/mahout-work/reuters-streamingkmeans
Dec 15, 2013 4:11:27 AM org.slf4j.impl.JCLLoggerAdapter info
INFO: Finished running Mappers
Exception in thread ""main"" java.util.concurrent.ExecutionException: java.lang.IllegalStateException: file:/tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/_SUCCESS
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.runSequentially(StreamingKMeansDriver.java:436)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:417)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:239)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.main(StreamingKMeansDriver.java:492)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)
Caused by: java.lang.IllegalStateException: file:/tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/_SUCCESS
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable.iterator(SequenceFileValueIterable.java:62)
	at com.google.common.collect.Iterables$8.iterator(Iterables.java:713)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:62)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:37)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readFully(DataInputStream.java:152)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1512)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1490)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1479)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1474)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.<init>(SequenceFileValueIterator.java:56)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable.iterator(SequenceFileValueIterable.java:60)
	... 8 more

{Code}",smartshark_2_2,244,mahout,"""[0.0849515050649643, 0.9150484800338745]"""
2004,230263,LuceneIterable doesn't preserve the vector as a namedvector after normalization,"Per http://www.lucidimagination.com/search/document/713e7c8349727a29/reading_vectors_created_from_a_lucene_index#90f94a8d5bc78610, need to preserve NamedVector when normalizing",smartshark_2_2,1169,mahout,"""[0.09317309409379959, 0.906826913356781]"""
2005,230264,Iterator class within SimilarItems class always misses the first element,"In the next() function of SimilarItemsIterator class within SimilarItems class, variable 'index' is incremented before returning the actual element at that position, therefore the first element when iterating will always be missed.",smartshark_2_2,994,mahout,"""[0.09245564788579941, 0.9075443148612976]"""
2006,230265,Cannot use MahalanobisDistanceMeasure with Dirichlet clustering,When Dirichlet clustering is run with DistanceMeasureClusterDistribution and MahalanobisDistanceMeasure the configure method of the distance measure is not called. In this way the configuration data cannot be supplied to the distance measure. In addition there is a bug in the MahalanobisDistanceMeasure that does not take into account the matrix class,smartshark_2_2,1344,mahout,"""[0.1193508505821228, 0.880649209022522]"""
2007,230266,Infinite Recursion in DefaultTreeBuilder,"In some cases, building a random forest with a dataset that contains many numerical attributes, the DefaultTreeBuilder enters in an infinite recursion and throws a StackOverflowError.",smartshark_2_2,1302,mahout,"""[0.07191645354032516, 0.928083598613739]"""
2008,230267,Alpha_0 mixture parameter is not implemented correctly in Dirichlet,"I looked over the R reference code and alpha_0 is used in two places, not one as in the current implementation:

- in state initialization ""beta = rbeta(K, 1, alpha_0)"" [K is the number of models]
- during state update ""beta[k] = rbeta(1, 1 + counts[k], alpha_0 + N-counts[k])"" [N is the cardinality of the sample vector and counts corresponds to totalCounts in the implementation]

The value of beta[k] is then used in the Dirichlet distribution calculation which results in the mixture probabilities pi[i], for the iteration:

   other = 1                                     # product accumulator
   for (k in 1:K) {
     pi[k] = beta[k] * other;                    # beta_k * prod_{n<k} beta_n
     other = other * (1-beta[k])
     }

Alpha_0 determines the probability a point will go into an empty cluster, mostly during the first iteration.  During the first iteration, the total counts of all prior clusters are zero. Thus the Beta calculation that drives the Dirichlet distribution that determines the mixture probabilities degenerates to beta = rBeta(1, alpha_0). Clusters that end up with points for the next iteration will overwhelm the small constants (alpha_0, 1) and subsequent new mixture probabilities will derive from beta ~=  rBeta(count, total) which is the current implementation. All empty clusters will subsequently be driven by beta ~= rBeta(1, total) as alpha_0 is insignificant and count is 0.

The current implementation ends up using beta = rBeta(alpha_0/k, alpha_0) as initial values during all iterations because the counts are all initialized to alpha_0/k. Close but no cigar.

",smartshark_2_2,1541,mahout,"""[0.11339979618787766, 0.8866002559661865]"""
2009,230268,seqdirectory -filter argument silently ignored when run as MR,"Running ""seqdirectory"" (Sequence Files from Input Directory) from the command line and specifying a custom filter using the -filter parameter, the argument is ignored and the default ""PrefixAdditionFilter"" is used on the input. No exception is thrown.

When the same command is run with ""-xm sequential"", the filter is found and works as expected.",smartshark_2_2,606,mahout,"""[0.31701600551605225, 0.6829839944839478]"""
2010,230269,NPE when calling hasPreferenceValues or getPreferenceValues on newly created FileDataModel  ,"An NPE gets thrown when calling hasPreferenceValues() or getPreferenceValue(long userID, long itemID) on a newly created FileDataModel. This happens because these methods do not call checkLoaded() before forwarding the method call to the delegate. Other overridden methods do call checkLoaded(). Seems like a bug.",smartshark_2_2,1152,mahout,"""[0.06543507426977158, 0.9345649480819702]"""
2011,230270,Start Phase doesn't properly work in RecommenderJob,"I'm trying to run RecommenderJob and do --startPhase 2 since I have my prefs already in the right format.  Unfortunately, when I do that, I get:
{quote}
java.lang.IllegalArgumentException: Number of columns was not correctly set!
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)
	at org.apache.mahout.math.hadoop.similarity.RowSimilarityJob$SimilarityReducer.setup(RowSimilarityJob.java:296)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:648)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:256)
{quote}

This appears to be due to the fact that the numberOfUsers variable defaults to 0 and is only set when phase 1 is run.",smartshark_2_2,1699,mahout,"""[0.10468870401382446, 0.8953113555908203]"""
2012,230271,is it a bug of getPreferenceSQL?,"@AbstractJDBCDataModel.java
@public Long getPreferenceTime(long userID, long itemID) throws TasteException 
there is a prepareStatement of getPreferenceSQL, should it be getPreferenceTimeSQL? because i found you check the getPreferenceTimeSQL null and log for getPreferenceTimeSQL, and this is an function of getPreferenceTime...
i didn't check relative code, so i am not sure it's a bug or just what you want.",smartshark_2_2,347,mahout,"""[0.30631116032600403, 0.6936888098716736]"""
2013,230272,SSVd does not writing uSigma after uSigma true,SSVd does not writing uSigma file  after uSigma variable true,smartshark_2_2,214,mahout,"""[0.10856003314256668, 0.8914399743080139]"""
2014,230273,Collocation driver has long being statically cast to an int,"org.apache.mahout.vectorizer.collocations.llr.LLRReducer, which is part of the collocation driver, statically casts a long to an int.

private long ngramTotal;
...
int k11 = ngram.getFrequency(); /* a&b */
int k12 = gramFreq[0] - ngram.getFrequency(); /* a&!b */
int k21 = gramFreq[1] - ngram.getFrequency(); /* !b&a */
int k22 = (int) (ngramTotal - (gramFreq[0] + gramFreq[1] - ngram.getFrequency())); /* !a&!b */

These numbers are then fed into 

org.apache.mahout.math.stats.LogLikelihood

specifically the function below.

public static double logLikelihoodRatio(int k11, int k12, int k21, int k22) {
  // note that we have counts here, not probabilities, and that the entropy is not normalized.
  double rowEntropy = entropy(k11, k12) + entropy(k21, k22);
  double columnEntropy = entropy(k11, k21) + entropy(k12, k22);
  double matrixEntropy = entropy(k11, k12, k21, k22);
  if (rowEntropy + columnEntropy > matrixEntropy) {
    // round off error
    return 0.0;
  }
  return 2.0 * (matrixEntropy - rowEntropy - columnEntropy);
}

In short if the long ngramTotal is larger than Integer.MAX_VALUE (which will happen in large datasets), then the driver will either crash or in the case that it casts to a negative int, will continue as usual but produce no output due to error checking.",smartshark_2_2,1645,mahout,"""[0.1433730572462082, 0.8566269278526306]"""
2015,230274,AbstractJDBCDiffStorage.updateItemPref is updating the AVG incorrectly in most cases,"the JDBC version of the DiffStorage is not using a RunningAverage in the removePreference case, and ends up making incorrect calculations.
In a scenario where users are setting and removing a lot of preferences, the AVG stored in the diff table quickly diverges from the correct value because of this.

Right now, the input to updateItemPref comes from SlopeOneRecommender, and in the case of removePreference, it is *the old preference* value, not a delta. However, the code uses it as if it were a delta. Thus the calculation is off by PEER(removedpreference,userid)/count everytime a user removes a preference.

At first glance, the code should compute the old delta instead of the old preference, and use this in the updateItemPref ",smartshark_2_2,1282,mahout,"""[0.06621301174163818, 0.9337869882583618]"""
2016,230275,Not all Coocurrences provided to SimilarityReducer,"While doing some tests with the RecommenderJob, and more specifically the RowSimilarityJob, I noticed that in some cases not all cooccurences are used in the similarity calculations ( done in the SimilarityReducer class ).
A RowPair object with (RowA=1,RowB=2) isn't considered the same as (RowA=2,RowB=1). This causes problems as CoocurencesMapper sometimes emits rowpairs in the first form and sometimes in the second form thus separating the cooccurences. If I'm right, this is due to the fact that ordering of the WeightedCoocurrenceArray for one column isn't guaranteed to be the same as for another column.
The solution is very simple, either you can change the compare method of the RowPair class or you can adapt the CooccurencesMapper to enforce that RowA < RowB.

Hope I've not missed something obvious, and that this is intended behavior. If this is the case, please enlighten me :-)

Also, slightly off topic. While doing these tests, I've noticed that the predictions are all remarkably high and the RMSE on the movielens 100k dataset lies around 1,6.
A bit to high if you ask me. Are these normal values or am I doing something wrong?",smartshark_2_2,1276,mahout,"""[0.1250135600566864, 0.874986469745636]"""
2017,230276,Bug in SequentialAccessSparseVector full iteration,"The iterator for the SequentialAccessSparseVector doesn't return any items beyond the last non-zero.  This breaks some stuff pretty massively, but hopefully doesn't break much user code since iterating through all elements of a sparse vector is a relatively rare thing to do.",smartshark_2_2,558,mahout,"""[0.06024033576250076, 0.9397596716880798]"""
2018,230277,DatasetSplitter.run doesn't parseArguments before getOption so throws and exception always,"In DatasetSplitter.run it looks like getOption is being called before the parseArguments. When I do this 

Map<String,List<String>> parsedArgs = parseArguments(args);
if (parsedArgs == null) {
 return -1;
}

before any call to getOption in DatasetSplitter.run it completes correctly. Not exactly sure how this is supposed to be done, it doesn't look like the options get parsed in the super class automatically.

This will cause any invocation of splitDataset or DatasetSplitter to crash running the current trunk. 

On Dec 5, 2012, at 1:58 PM, Pat Ferrel <pat.ferrel@gmail.com> wrote:

does anyone know if mahout/examples/bin/factorize-movielens-1M.sh is still working? CLI version of splitDataset is crashing in my build (latest trunk). Even as in ""mahout splitDataset"" to get the params. Wouldn't be the first time I mucked up a build though.
",smartshark_2_2,409,mahout,"""[0.14107975363731384, 0.8589202761650085]"""
2019,230278,Null Pointer Exception running DictionaryVectorizer with ngram=2 on Reuters dataset,"java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:860)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:541)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocMapper$1.apply(CollocMapper.java:127)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocMapper$1.apply(CollocMapper.java:114)
	at org.apache.mahout.math.map.OpenObjectIntHashMap.forEachPair(OpenObjectIntHashMap.java:186)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocMapper.map(CollocMapper.java:114)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocMapper.map(CollocMapper.java:41)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:86)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.mahout.utils.nlp.collocations.llr.Gram.write(Gram.java:181)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
	at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:179)
	at org.apache.hadoop.mapred.Task$CombineOutputCollector.collect(Task.java:880)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner$OutputConverter.write(Task.java:1201)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocCombiner.reduce(CollocCombiner.java:40)
	at org.apache.mahout.utils.nlp.collocations.llr.CollocCombiner.reduce(CollocCombiner.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1222)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1265)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:686)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1173)
",smartshark_2_2,1212,mahout,"""[0.07738287001848221, 0.9226170778274536]"""
2020,230279,"JWriterVectorWriter doesn't flush, content not necessarily written to disk. ",Just needs to flush before returning the number of vectors written... ,smartshark_2_2,1429,mahout,"""[0.08588401228189468, 0.9141159653663635]"""
2021,230280,Vector.getLengthSquared() is dangerously optimized,"SparseVector and DenseVector both cache the value of lengthSquared, so that subsequent calls to it get the cached value.  Great, except the cache is never cleared - calls to set/setQuick or assign or anything, all leave the cached value unchanged.  

Mutating method calls should set lengthNorm to -1 so that the cache is cleared.

This could be a really nasty bug if hit.",smartshark_2_2,1532,mahout,"""[0.5139539837837219, 0.48604604601860046]"""
2022,230281,Power iterations -q=2 directory mess-up,"Not sure it this is related, but sounds similar. I can't run more than one power iteration, ie q=2 produces

11/09/21 11:25:46 INFO mapred.LocalJobRunner: reduce > reduce
11/09/21 11:25:46 INFO mapred.Task: Task 'attempt_local_0004_r_000000_0' done.
11/09/21 11:25:50 INFO mapred.JobClient: Cleaning up the staging area file:/tmp/hadoop-nathanhalko/mapred/staging/nathanhalko-200181280/.staging/job_local_0005
Exception in thread ""main"" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory temp/ABt-job-1 already exists
at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:134)
at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:830)
at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:791)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:791)
at org.apache.hadoop.mapreduce.Job.submit(Job.java:465)
at org.apache.mahout.math.hadoop.stochasticsvd.ABtJob.run(ABtJob.java:454)
at org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run(SSVDSolver.java:312)
at org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.run(SSVDCli.java:118)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
at org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.main(SSVDCli.java:163)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

for q=0,1 everything works fine. I am running with --overwrite and I rm -rf the temp dir before running.",smartshark_2_2,1704,mahout,"""[0.13978591561317444, 0.8602140545845032]"""
2023,230282,RowSimilarityJob should exit immediately if an invalid similarity measure specified and it would be nice to have an --overwrite option for the RowSimilarityJob CLI,"1. If an invalid Similarity Measure has been specified as input to the RowSimilarityJob, it presently throws a ClassNotFoundException but still proceeds with executing all of the subsequent tasks - VectorNormalizer, Cooccurrences Mapper and UnSymmetrify Mapper. We should exit the process early without having to invoke all of the subsequent tasks (all of them fail anyways).

2. It would be nice to have an --overwrite option for the Command line interface which would delete the temp and output paths at the beginning of RowSimilarityJob execution, similar to what's being done in seq2sparse, seqdirectory. If I run RowSimilarityJob over and over again with different similarity measures, I should not be forced to delete my temp and output paths first prior to invoking the job.",smartshark_2_2,78,mahout,"""[0.9980699419975281, 0.0019300957210361958]"""
2024,230283,Update History of Mahout Page on Website,"The Apache Mahout project has a long and rich history.  

The file 
https://github.com/apache/mahout/blob/master/website/front/community/history.md

is currently stubbed out with some headers (that are only directional).  While currently active members could fill this out with some facts, it would be really great if we could get past PMCs/ PMC Chairs/ Committers to fill in relevant portions of the ""History of the Apache Mahout Project"", preserving the history of the project with first person accounts, as opposed to something reconstructed from oral tradition. 

Contributors are welcome to leave their pieces in the comments and active members can add to the code base if this is more convenient (of course contributors are always welcome to add content directly via pull request as well). 

Thank you in advance. 
",smartshark_2_2,1901,mahout,"""[0.9983019828796387, 0.0016979679930955172]"""
2025,230284,Dependency on sjl4j-jcl should not have compile scope,"The top-level pom.xml and integration/pom.xml specify a dependency on slf4j-jcl with default (compile) scope. The point of slf4j is to allow libraries to work with any underlying logging system, leaving the choice up to the application programmer. If my application uses jcl-over-slf4j, and also uses your library which uses slf4j-jcl, initialization fails because there's a logging loop.

core/pom.xml and math/pom.xml specify slf4j-jcl with test scope only. That's fine, because dependencies with test scope aren't propagated to dependents.",smartshark_2_2,1664,mahout,"""[0.9961315393447876, 0.0038684315513819456]"""
2026,230285,Deprecate ununsed recommenders,"Deprecating some recommenders for the following reasons:

org.apache.mahout.cf.taste.impl.recommender.svd.FunkSVDFactorizer

RatingSGDFactorizer should be learning faster and has a nicer model as
it includes user/item biases


org.apache.mahout.cf.taste.impl.recommender.svd.ImplicitLinearRegressionFactorizer

Seems to be using the same model as ALSWRFactorizer, however there are
no tests and ALSWR can handle more explicit and implicit feedback


org.apache.mahout.cf.taste.impl.recommender.TreeClusteringRecommender
org.apache.mahout.cf.taste.impl.recommender.TreeClusteringRecommender2
org.apache.mahout.cf.taste.impl.recommender.knn

I don't recall anybody using those or asking about them the last years.",smartshark_2_2,534,mahout,"""[0.9984847903251648, 0.0015152444830164313]"""
2027,230286,Decision Tree Learning,"Is there any plan to implement ID3 and C4.5 algorithms in Mahout? I am an undergrad student who's working on Mahout as part of my senior year project. I plan to implement decision tree learning as part of my project and was wondering if it is a good enough idea to incorporate it into the mainstream application.

Regards.
Meher Anand",smartshark_2_2,186,mahout,"""[0.9981581568717957, 0.001841874560341239]"""
2028,230287,some of the pom.xml referencing old svn repository url,"I realized that the pom.xml in mahout/examples is referencing the old repository url.

Currently it is
 <scm>
    <connection>scm:svn:https://svn.apache.org/repos/asf/lucene/mahout/trunk/examples</connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/lucene/mahout/trunk/examples </developerConnection>
    <url>https://svn.apache.org/repos/asf/lucene/mahout/mahout-examples</url>
  </scm>

and I guess it should be changed to
 <scm>
    <connection>scm:svn:https://svn.apache.org/repos/asf/mahout/trunk/examples</connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/mahout/trunk/examples </developerConnection>
    <url>https://svn.apache.org/repos/asf/mahout/mahout-examples</url>
  </scm>

I saw similar references in 
mahout/utils/pom.xml
mahout/taste-web/pom.xml
mahout/math/pom.xml
mahout/core/pom.xml
mahout/buildtools/pom.xml
Tried searching in http://www.lucidimagination.com/search/ to see if there were previous discussion about this and couldnt find any.",smartshark_2_2,1202,mahout,"""[0.9947425127029419, 0.005257474258542061]"""
2029,230288,Another version of RecommenderJob that broadcasts the similarity matrix,"Add another version of RecommenderJob that computes the item similarities via RowSimilarityJob but assumes that the resulting similarity matrix fits into the memory of the mappers in the cluster. After the item similarity computation is done, the similarities are broadcasted via Hadoop's distributed cache and the recommendations are computed in a map-only pass over the data afterwards.

",smartshark_2_2,549,mahout,"""[0.9967593550682068, 0.0032406237442046404]"""
2030,230289,Allow to obtain Mahout version information through the Java API,It would be nice to be able to obtain the used Mahout version through the Java API. This enables users to output the version information for debugging purposes or simple to document which Mahout version is used.,smartshark_2_2,1620,mahout,"""[0.998259961605072, 0.0017400706419721246]"""
2031,230290,Remove references to develop branch,"There are references to a develop branch on the (git hub based) website, and in the PR template.

Remove these",smartshark_2_2,1904,mahout,"""[0.9982196688652039, 0.001780262216925621]"""
2032,230291,Remove mention of Spark in comment and Javadoc of math-scala when the code is generic,Remove mention of Spark in comment and Javadoc of math-scala module when the code is generic. This is because now mahout-samsara could support H2O and next Apache Flink as exec engines.,smartshark_2_2,990,mahout,"""[0.9976776242256165, 0.002322445623576641]"""
2033,230292,Migrate Canopy and KMeans Implementations to Vectors,"Canopy and KMeans clustering implementations use Float[] representations instead of the new Vector package. They need to be migrated and the Vector package may need some enhancement to support the notion of payloads. This would be a good project for somebody new to the project who wants to get involved. If somebody wants to implement this, just assign the issue to yourself and I will hold off doing it myself.",smartshark_2_2,1005,mahout,"""[0.9984109401702881, 0.0015890654176473618]"""
2034,230293,add option abbreviation error in RecommenderJob.java,"/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java

110 line
--------------
addOption(""maxPrefsPerUser"", ""mp"",
        ""Maximum number of preferences considered per user in final recommendation phase"",
        String.valueOf(UserVectorSplitterMapper.DEFAULT_MAX_PREFS_PER_USER_CONSIDERED));
    addOption(""minPrefsPerUser"", ""mp"", ""ignore users with less preferences than this in the similarity computation ""
        + ""(default: "" + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));

--------------

have the same otion abbreviation.",smartshark_2_2,31,mahout,"""[0.998598039150238, 0.0014019313966855407]"""
2035,230294,Start using newly acquired reviewboard instance!,"It's easy: go to https://reviews.apache.org/ and submit a review by clicking ""new review request"", then picking the project to be ""mahout"", base directory to be ""trunk"", and upload your patch.  Add ""mahout"" to the reviewer group of your reviewboard, pick some people you want to aim the review at (if you think you know the ""owner"" of the code area you're hitting), set the ""bug number"" in the appropriate field so that JIRA and RB will magically communicate, and don't forget to click the ""publish"" button when you're ready for people to look at it.  You can also look at your review directly for sanity checking before doing so by clicking on the ""view diff"" button.

ReviewBoard can be slow and clunky, but inline comments in the diff are A+ goodness.",smartshark_2_2,59,mahout,"""[0.9983088970184326, 0.001691079931333661]"""
2036,230295,Dense matrix view multiplication is 4x slower than non-view one,"if mxA and mxB are two in-core DenseMatrix matrices, then 

mxA(::,::) %*% mxB(::,::) takes 4x the time of mxA %*% mxB.

possibly an issue of dot products on VectorViews vs. DenseVectors.

dot product over DenseVectors seems to not to go through aggregate() cost-optimized framework. ",smartshark_2_2,1783,mahout,"""[0.9661824107170105, 0.033817581832408905]"""
2037,230296,grouplens-example erroneous path to .dat files,"GroupLensDataModel.java tries to read from ""src/example/org/apache/mahout/cf/taste/example/grouplens/ratings.dat"" whereas the file is included in taste-build.xml from ""${grouplens-location}/ratings.dat"".

The location is hard-coded in GroupLensDataModel.java and would need to be changed to conform to the current code organization.  A better solution would be not to hard-code the path.",smartshark_2_2,1403,mahout,"""[0.9738008975982666, 0.026199141517281532]"""
2038,230297,The LDA output does not include the topic-probability distribution per document (p(z|d)). It outputs only the topics and corresponding words.,"The current implementation of LDA outputs only topics and their words. Many applications need the p(z|d) values of a document to use this vector as a reduced representation of the document (dimensionality reduction of document). We need to introduce a new key which would keep track of the gamma values for each document (as obtained from the document.infer() method) and writes these to the output stream and finally, PrintLDATopics should output these values per document id. Also, outputting the probabilities of words in a topic would also provide a more meaningful output.",smartshark_2_2,1606,mahout,"""[0.9059782028198242, 0.0940217524766922]"""
2039,230298,"Performance and parallelization improvements for AB', A'B, A'A spark physical operators ","per name. 

PR https://github.com/apache/mahout/pull/135",smartshark_2_2,955,mahout,"""[0.9981271624565125, 0.0018728632712736726]"""
2040,230299,Options in Bayes TrainClassifier and TestClassifier,"Hi all,

As I was going through wikipedia example, I encountered a situation with TrainClassifier wherein some of the options with default values are actually mandatory. 
The documentation / command line help says that 
default source (--datasource) is hdfs but TrainClassifier has withRequired(true) while building the --datasource option. We are checking if the dataSourceType is hbase else set it to hdfs. so ideally withRequired should be set to false
default --classifierType is bayes but withRequired is set to true and we have code like
if (""bayes"".equalsIgnoreCase(classifierType)) {
        log.info(""Training Bayes Classifier"");
        trainNaiveBayes(inputPath, outputPath, params);
        
      } else if (""cbayes"".equalsIgnoreCase(classifierType)) {
        log.info(""Training Complementary Bayes Classifier"");
        // setup the HDFS and copy the files there, then run the trainer
        trainCNaiveBayes(inputPath, outputPath, params);
      }

which should be changed to

if (""cbayes"".equalsIgnoreCase(classifierType)) {
        log.info(""Training Complementary Bayes Classifier"");
        trainCNaiveBayes(inputPath, outputPath, params);
        
      } else  {
        log.info(""Training  Bayes Classifier"");
        // setup the HDFS and copy the files there, then run the trainer
        trainNaiveBayes(inputPath, outputPath, params);
      }

Please let me know if this looks valid and I'll submit a patch for a JIRA issue.

reg
Joe.",smartshark_2_2,1225,mahout,"""[0.97468501329422, 0.025314994156360626]"""
2041,230300,T1 and T2 Values in Canopy (& MeanShift) ,"Users are reporting that the T1 and T2 threshold values which work in sequential mode don't work as well in the mapreduce mode because both the mapper and reducer are using the same values. The effect of coalescing a number of points into a single centroid done by the mapper changes the distances enough that independent threshold values are needed in the reducer. 

Here is a patch which implements optional T3 and T4 threshold values which are only used by the canopy reducer. Convenience methods have been added for API compatibility and defaults included so that these values will default to T1 and T2. A new unit test confirms the thresholds are being set correctly.

If this works out as a positive improvement, I will make the same changes to MeanShift and commit them",smartshark_2_2,1712,mahout,"""[0.7485291957855225, 0.25147080421447754]"""
2042,230301,Remove most deprecated/unused mahout-math code,"Per a conversation on the mailing list, like Ted, I'd like to remove most of the remaining unused/deprecated code in mahout-math.",smartshark_2_2,1308,mahout,"""[0.9981984496116638, 0.0018015768146142364]"""
2043,230302,Make classes implements Serializable for Spark 1.5+,"Spark 1.5 comes with a new very efficient serializer that uses code generation.  It is twice as fast as kryo.  When using mahout, we have to set KryoSerializer because some classes aren't serializable otherwise.  

I suggest to declare Math classes as ""implements Serializable"" where needed.  For instance, to use coocurence package in spark 1.5, we had to modify AbstractMatrix, AbstractVector, DenseVector and SparseRowMatrix to make it work without Kryo.",smartshark_2_2,1961,mahout,"""[0.9984707236289978, 0.001529200584627688]"""
2044,230303,Item similarity in AbstractSimilarity always centers data,"I'm trying to use UncenteredCosineSimilarity. When calculating similarity between users in AbstractSimilarity>>itemSimilarity, there's a condition to determine whether to center the data:

double result;
if (centerData) {
  ... get result with centering
} else {
   ... get result without
}
   
In AbstractSimilarity>>itemSimilarity, there's no conditional. It *always* centers the data. Shouldn't it only center the data when centerData is true?",smartshark_2_2,1158,mahout,"""[0.471047967672348, 0.5289520621299744]"""
2045,230304,Move Hadoop related code out of CanopyClusterer,"Canopy clusterer no longer has Hadoop dependencies.

Hadoop configuration of canopy configuration now happens in CanopyConfigKeys.

See commit #1574684",smartshark_2_2,885,mahout,"""[0.998251736164093, 0.0017482833936810493]"""
2046,230305,Quickstart script for kmeans algorithm,"Contains a quickstart shell script for kmeans algorithm on the Reuters dataset as described at https://cwiki.apache.org/MAHOUT/k-means.html 

The script in JIRA is a slightly modified and cleaner version.",smartshark_2_2,1258,mahout,"""[0.9984524250030518, 0.001547589199617505]"""
2047,230306,RecommenderJob should use simple cooccurrence as default similarity measure,To make the new version of RecommenderJob more consistent with the past behavior we should use simple cooccurrence (org.apache.mahout.math.hadoop.similarity.vector.DistributedCooccurrenceVectorSimilarity) as default similarity measure when no similarity classname is supplied.,smartshark_2_2,1191,mahout,"""[0.998481810092926, 0.0015182298375293612]"""
2048,230307,Implement mapreduce version of ClusterIterator,"Right now, ClusterIterator consumes vectors only from in-memory and sequential hdfs. A mapreduce version to consume vectors needs to be implemented.",smartshark_2_2,112,mahout,"""[0.9980522394180298, 0.0019477829337120056]"""
2049,230308,Remove direct HBase dependency,"As discussed on the mailing list, seems desirable to remove the direct dependence on HBase for now. The integration only exists for the Naive Bayes Classifier, and is based on an old version. A more comprehensive strategy for integrating with data sources, such as via Gora, is viewed as a desirable goal for later. This is a step in that direction.",smartshark_2_2,1239,mahout,"""[0.9978240728378296, 0.0021759953815490007]"""
2050,230309,Make Cluster dumper read DictionaryVectorizer Dictionary format,"Make ClusterDumper read the SequenceFile Text=>IntWritable format to print the cluster with an optional commandline  flag

--dictionaryType or -dt (text|sequencefile) Defaults to text",smartshark_2_2,1456,mahout,"""[0.9985098242759705, 0.001490165712311864]"""
2051,230310,streamingkmeans doesn't properly validate estimatedNumMapClusters -km,"The value of -km isn't checked by the CLI, which means if you don't specify it, you get the rather cryptic:

{noformat}
Exception in thread ""main"" java.lang.NumberFormatException: null
	at java.lang.Integer.parseInt(Integer.java:454)
	at java.lang.Integer.parseInt(Integer.java:527)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.configureOptionsForWorkers(StreamingKMeansDriver.java:252)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:239)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.main(StreamingKMeansDriver.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)
{noformat}

Other parameters give helpful error messages when required",smartshark_2_2,663,mahout,"""[0.0997755229473114, 0.900224506855011]"""
2052,230311,Pull Writable (and anything else hadoop dependent) out of the matrix module,"Vector and Matrix extend Writable, and while that was merely poorly coupled before, it will be an actual problem now that we have a separate submodule for matrix: this module should not depend on hadoop at all, ideally.   Distributed matrix work, as well as simply Writable wrappers can go somewhere else (where?  core?  yet another submodule which depends on matrix?), but it would be really nice if we could produce an artifact which doesn't require Hadoop which has our core linear primitives.",smartshark_2_2,1473,mahout,"""[0.9984484910964966, 0.0015514695551246405]"""
2053,230312,OutOfMemoryError in LanczosState by way of SpectralKMeans,"Dan Brickley and I have been testing SpectralKMeans with a dbpedia dataset ( http://danbri.org/2012/spectral/dbpedia/ ); effectively, a graph with 4,192,499 nodes. Not surprisingly, the LanczosSolver throws an OutOfMemoryError when it attempts to instantiate a DenseMatrix of dimensions 4192499-by-4192499 (~17.5 trillion double-precision floating point values). Here's the full stack trace:

{quote}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.mahout.math.DenseMatrix.<init>(DenseMatrix.java:50)
	at org.apache.mahout.math.decomposer.lanczos.LanczosState.<init>(LanczosState.java:45)
	at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:146)
	at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:86)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.main(SpectralKMeansDriver.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{quote}

Obviously SKM needs a more sustainable and memory-efficient way of performing an eigen-decomposition of the graph laplacian. For those who are more knowledgeable in the linear systems solvers of Mahout than I, can the Lanczos parameters be tweaked to negate the requirement of a full DenseMatrix? Or should SKM move to SSVD instead?",smartshark_2_2,153,mahout,"""[0.511375904083252, 0.48862409591674805]"""
2054,230313,TrainLogistic regressed due to encoder changes.,"Isabel wrote:


I just tried running the SGD example with the following command line (adapted
from the corresponding JIRA issue):

./bin/mahout org.apache.mahout.classifier.sgd.TrainLogistic --passes 100 --rate
50 --lambda 0.001 --input examples/src/main/resources/donut.csv --features 21 --
output donut.model --target color --categories 2 --predictors x y xx xy yy a b c
--types n n

When running the code above I ran into a few NullPointerExceptions - I was able
to fix them with a few tiny changes. If not stripped they should be attached to
this mail to highlight the lines of code that caused the trouble. However I was
wondering whether I simply used the wrong command line.
",smartshark_2_2,1229,mahout,"""[0.09839600324630737, 0.9016039371490479]"""
2055,230314,Add the Tree/Forest Visualizer,"TreePrinter and ForestPrinter are made for checking the model on MAHOUT-840.
I think they are useful for checking the model and making unit testing.",smartshark_2_2,1668,mahout,"""[0.9984707236289978, 0.0015293116448447108]"""
2056,230315,Setup Travis CI for Mahout,We need to get Travis CI setup for mahout.  This involves setting up an account with Travis and configuring a {{travis.yml}} file.,smartshark_2_2,1818,mahout,"""[0.9974326491355896, 0.002567320829257369]"""
2057,230316,Implement of AVF algorithm,"This program realize a outlier detection algorithm called avf, which is kind of 
Fast Parallel Outlier Detection for Categorical Datasets using Mapreduce and introduced by this paper : 
    http://thepublicgrid.org/papers/koufakou_wcci_08.pdf
Following is an example how to run this program under haodoop:
$hadoop jar programName.jar avfDriver inputData interTempData outputData
The output data contains ordered avfValue in the first column, followed by original input data. 
",smartshark_2_2,1708,mahout,"""[0.9983565211296082, 0.0016434540739282966]"""
2058,230317,Make splitData smart enough to not consider a CSV header to be part of the data,"If you do something like:

MAHOUT_LOCAL=1 $MAHOUT_HOME/bin/mahout splitDataset  --input all.csv --output split --trainingPercentage 0.9 --probePercentage 0.1

The header row from your CSV will end up with 90% chance in your training data and 10% chance in your evaluation data.  To use a tool like trainlogistic or runlogistic the header file is needed in both.

Perhaps add an argument to splitData to duplicate the header line?",smartshark_2_2,526,mahout,"""[0.998496413230896, 0.0015036271652206779]"""
2059,230318,High Document Frequency pruning for seq2sparse,"This improvement allows to prune the words with high document frequencies from the tf and tf-idf vectors produced by seq2sparse, based on the standard deviation of the words' document frequencies and specifying which rods to be pruned in a means of times this standard deviation. One good option is 3 times the standard deviation",smartshark_2_2,1638,mahout,"""[0.9982757568359375, 0.0017242750618606806]"""
2060,230319,Add List of Flink algorithms to Mahout wiki page,Need to add list of flink-based algorithms on the Mahout wiki page,smartshark_2_2,2041,mahout,"""[0.9953418970108032, 0.004658137913793325]"""
2061,230320,Make Flink Profile,"Currently Flink module is just commented out of the pom. This is tacky. 

Flink should have profile which by default is disabled.",smartshark_2_2,1917,mahout,"""[0.9984340071678162, 0.0015660029603168368]"""
2062,230321,Pull Request Template,"Create a template for guiding new pull requests.
",smartshark_2_2,1902,mahout,"""[0.9979137778282166, 0.0020861804950982332]"""
2063,230322,Cleaning up the examples for clustering on the website,"Cleaning up the following clustering examples:

=====================================
https://mahout.apache.org/users/clustering/clustering-of-synthetic-control-data.html


Introduction

This example will demonstrate clustering of time series data, specifically control charts. [Control charts : http://en.wikipedia.org/wiki/Control_chart] are tools used to determine whether a manufacturing or business process is in a state of statistical control. Such control charts are generated / simulated repeatedly at equal time intervals. A simulated dataset is available for use in UCI machine learning repository. The data is described [here : http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data.html].

Problem Description

A time series of control charts needs to be clustered into their close knit groups. The data set we use is synthetic and is meant to resemble real world information in an anonymized format. It contains six different classes: Normal, Cyclic, Increasing trend, Decreasing trend, Upward shift, Downward shift. In this example we will use Mahout to cluster the data into corresponding class buckets. 

At the end of this example


   * You will have clustered data using mahout.
   * You will see how to analyse the clusters produced by mahout.  
   * You will have a starting point for incorporating clustering into your own software.

Setup

We need to do some initial setup before we are able to run the example. 


  1. Start out by downloading the input dataset (to be clustered) from the UCI Machine Learning Repository: http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data
  2. Make sure the data consists of 600 rows and 60 columns. The first 100 rows contains Normal data followed by 100 rows of Cyclic data and so on with a total of 6 classes.
  3. This example assumes that you have already set up Mahout/Hadoop. If you have not done so yet:
  4. 
      * Hadoop: Follow the instructions on http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleNodeSetup.html to set up Hadoop.
      * Mahout: Follow the instructions on the [Quickstart: https://mahout.apache.org/users/basics/quickstart.html] page.
  5. Make sure the Hadoop daemons are running if you are running Hadoop in distributed mode. 
  6. Create a directory on your local machine called Â« testdata Â» and place the input dataset in this directory.
  7. Run the following command to copy the input data into HDFS:

      * Create a directory called Â« testdata Â»  on HDFS: 

                         $HADOOP_HOME/bin/hadoop fs -mkdir testdata

      * Copy the directory named Â« testdata Â» from your local filesystem to HDFS: 
                         $HADOOP_HOME/bin/hadoop fs -put testdata
  8. The final setup step is to build Mahout by going to the $MAHOUT_HOME directory and running one of the following commands: 
  9. 
      * For a full build: mvn clean install
      * For a build without unit tests: mvn -DskipTests clean install
  10. You should see a build successful message once the build script has completed.
  11. Finally make sure that the examples have compiled successfully. You should find the compiled jar in the /examples/target directory under the name mahout-examples-{version}.job.jar
  12. This concludes all the setup required to run the examples.


Clustering Examples

There are examples available for three clustering algorithms:


   * Canopy Clustering: https://mahout.apache.org/users/clustering/canopy-clustering.html
   * k-Means Clustering: https://mahout.apache.org/users/clustering/k-means-clustering.html
   * Fuzzy k-Means Clustering: https://mahout.apache.org/users/clustering/fuzzy-k-means.html

Depending on the example you want to run the following command can be used:


   * Canopy Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.canopy.Job
   * k-Means Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job
   * Fuzzy k-Means Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job

The clustering output will be produced in the Â« output Â» directory on HDFS. The output should be copied to your local filesystem since it is overwritten on each run.

Use the following command to copy out the data to your local filesystem:

$HADOOP_HOME/bin/hadoop fs -get output $MAHOUT_HOME/examples

This creates an output folder inside examples directory. The output data points are in vector format. In order to read/analyze the output, you can use [clusterdump: https://mahout.apache.org/users/clustering/cluster-dumper.html] utility provided by Mahout.

The source code for these examples is located under the examples project.


=====================================
https://mahout.apache.org/users/clustering/clustering-seinfeld-episodes.html

",smartshark_2_2,902,mahout,"""[0.9984549283981323, 0.0015450137434527278]"""
2064,230323,Make Mahout's tests run in parallel,Maven now supports parallel execution of tests.  We should hook this in to Mahout.,smartshark_2_2,433,mahout,"""[0.9976262450218201, 0.0023737868759781122]"""
2065,230324,CanopyEstimator - Estimate T1/T2 for CanopyClusterer,"Hunting for T1/T2 values that make an interesting Canopy set is a singularly unsatisfying task. This class estimates T1 and T2 numbers given the original set.
",smartshark_2_2,96,mahout,"""[0.9444730281829834, 0.05552700534462929]"""
2066,230325,AbstractJob improvements.,"Per discussion : http://lucene.472066.n3.nabble.com/Re-input-now-Dmapred-input-dir-td852297.html#a852297

With the advent of the parsedArgs map returned by AbstractJob.parseArguments is there
a need to pass Option arguments around anymore? Could AbstractJob maintain
Options state in a sense?

For example, from RecommenderJob:

{code}
    Option numReccomendationsOpt = AbstractJob.buildOption(""numRecommendations"", ""n"", 
      ""Number of recommendations per user"", ""10"");
    Option usersFileOpt = AbstractJob.buildOption(""usersFile"", ""u"",
      ""File of users to recommend for"", null);
    Option booleanDataOpt = AbstractJob.buildOption(""booleanData"", ""b"",
      ""Treat input as without pref values"", Boolean.FALSE.toString());

    Map<String,String> parsedArgs = AbstractJob.parseArguments(
        args, numReccomendationsOpt, usersFileOpt, booleanDataOpt);
    if (parsedArgs == null) {
      return -1;
    }
{code}

Could be changed to something like:

{code}
buildOption(""numRecommendations"", ""n"", ""Number of recommendations per user"",
""10"");
buildOption(""usersFile"", ""u"", ""File of users to recommend for"", null);
buildOption(""booleanData"", ""b"", ""Treat input as without pref values"",
Boolean.FALSE.toString());
Map<String,String> parsedArgs = parseArguments(); 
{code}",smartshark_2_2,1144,mahout,"""[0.9982002973556519, 0.0017996475799009204]"""
2067,230326,simplify or alternative  Similarity arithmetic(AbstractDistributedVectorSimilarity) for boolean data,"For boolean data ,the prefValue  is  always 1.0f, We need simplify Similarity arithmetic

for example:
1) DistributedEuclideanDistanceVectorSimilarity 

package org.apache.mahout.math.hadoop.similarity.vector;

import org.apache.mahout.math.hadoop.similarity.Cooccurrence;

/**

    * distributed implementation of euclidean distance as vector similarity measure
      */
      public class DistributedEuclideanDistanceVectorSimilarity extends AbstractDistributedVectorSimilarity {

@Override
protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
double weightOfVectorB, int numberOfColumns) {

double n = 0.0;
double sumXYdiff2 = 0.0;

for (Cooccurrence cooccurrence : cooccurrences) { double diff = cooccurrence.getValueA() - cooccurrence.getValueB(); sumXYdiff2 += diff * diff; n++; }

return n / (1.0 + Math.sqrt(sumXYdiff2));
}

}

this one is always return n (=cooccurrence.size())
2) DistributedUncenteredCosineVectorSimilarity 
/**

    * distributed implementation of cosine similarity that does not center its data
      */
      public class DistributedUncenteredCosineVectorSimilarity extends AbstractDistributedVectorSimilarity {

@Override
protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
double weightOfVectorB, int numberOfColumns) {

int n = 0;
double sumXY = 0.0;
double sumX2 = 0.0;
double sumY2 = 0.0;

for (Cooccurrence cooccurrence : cooccurrences) { double x = cooccurrence.getValueA(); double y = cooccurrence.getValueB(); sumXY += x * y; sumX2 += x * x; sumY2 += y * y; n++; }

if (n == 0) { return Double.NaN; }
double denominator = Math.sqrt(sumX2) * Math.sqrt(sumY2);
if (denominator == 0.0) { // One or both vectors has -all- the same values; // can't really say much similarity under this measure return Double.NaN; }
return sumXY / denominator;
}

}

this one will always return 1.0
3) DistributedUncenteredZeroAssumingCosineVectorSimilarity 
If n users like ItemA, m users like ItemB,p users like both ItemA and ItemB,

DistributedUncenteredZeroAssumingCosineVectorSimilarity return p/(m*n).

it also can use for Boolean data, but we can provide a simple one , return (p*p)/(m*n),no so much computing.

",smartshark_2_2,1199,mahout,"""[0.9983890056610107, 0.0016110074939206243]"""
2068,230327,Initial Imports,"We needs initial base code. I'm going to import some implemented code, but
I think we need to discuss a bit about the directory structure.
",smartshark_2_2,1000,mahout,"""[0.9980825185775757, 0.0019174679182469845]"""
2069,230328,Update hadoop commands in example scripts,Hadoop 2.x does not like the `hadoop fs` command; change that to `hadoop dfs` to retain backwards compat with v1.,smartshark_2_2,719,mahout,"""[0.9984726309776306, 0.0015274022007361054]"""
2070,230329,minor fixes to spark-shell,"Terminal clutters up after exiting spark shell (as terminal settings are changed within spark repl).

Save and restore system terminal settings to avoid clutter.

Also minor fix of prompt styling",smartshark_2_2,882,mahout,"""[0.9827698469161987, 0.017230145633220673]"""
2071,230330,Make build-asf-email.sh HDFS aware,"In MAHOUT-798, build-asf-email.sh was added and it checks to see if certains tasks are already done in order to skip costly calculations.  The problem is, the checking is not HDFS aware, so in a Hadoop cluster, it repeats some unnecessary tasks (such as conversion to vectors)",smartshark_2_2,1628,mahout,"""[0.9983786344528198, 0.0016214054776355624]"""
2072,230331,Revisit the parallel ALS matrix factorization,"Our current code for computing a decomposition of a rating matrix with Alternating Least Squares (ALS) uses a lot of highly unefficient reduce side joins. 

The rating matrix A is decomposed into a matrix U of users x features and a matrix M of items x features. Each of these matrices is iteratively recomputed until a maximum number of iterations is reached

If we assume that U and M fit into the memory of a single mapper instance, each iteration can be implemented as single map-only job, which greatly improves the runtime of this job.

Note that in spite of these improvements this job is still rather slow as Hadoop is a poor fit for iterative algorithms. Each iteration has to be scheduled again and data is always read from and written to disk.
",smartshark_2_2,6,mahout,"""[0.9982818365097046, 0.0017181955045089126]"""
2073,230332,Automatic threading for java based mmul in the front end and the backend.,"As we know, we are still struggling with decisions which path to take for bare metal accelerations in in-core math. 

Meanwhile, a simple no-brainer improvement though is to add decision paths and apply multithreaded matrix-matrix multiplication (and maybe even others; but mmul perhaps is the most prominent beneficiary here at the moment which is both easy to do and to have a statistically significant improvement) 

So multithreaded logic addition to mmul is one path. 

Another path is automatic adjustment of multithreading. 
In front end, we probably want to utilize all cores available. 
in the backend, we can oversubscribe cores but probably doing so by more than 2x or 3x is unadvisable because of point of diminishing returns driven by growing likelihood of context switching overhead.

",smartshark_2_2,2001,mahout,"""[0.9982751607894897, 0.0017248062649741769]"""
2074,230333,ClusterDumper writes to System.out or local filesystem only (I would like to write to s3 when running on Elastic MapReduce),"At the end of a kmeans job at EMR, I like to look through the clusters.  Unfortunately ClusterDumper writes to System.out or a local file.  I added a small conditional to examine the filename to see if it starts with s3n://, and if so to open up a FileSystem to write to s3 instead of to the local filesystem so that the output file is available after the cluster is shut down.  I am creating the patch now (tests are still running from the change in MAHOUT-700), so I will add the patch to this issue shortly.",smartshark_2_2,1702,mahout,"""[0.4132857918739319, 0.5867141485214233]"""
2075,230334,FastUtil to improve speed of Sparse Matrix Operations,"Following the PR (https://github.com/apache/mahout/pull/81) for M-1640 - fastutil for Sparse Vectors, this is to implement fastutil for Sparse Matrix operations.",smartshark_2_2,1766,mahout,"""[0.998306393623352, 0.0016935549210757017]"""
2076,230335, fuzzy kmeans - all cluster with the same top terms," believe there is something wrong with fkmeans in trunk. 

I am using code from trunk (last checkout 6/30/11). To recreate is very simple:
1) change examples/bin/build-reuters.sh to use fkmeans and set -m 2
2) run build-reuters.sh
3) Dump the cluster. I'm doing: ../../bin/mahout clusterdump -dt sequencefile -s ./mahout-work/reuters-kmeans/clusters-6 -b 100 -o ./reuters-clusterdump.txt  -d ./mahout-work/reuters-out-seqdir-sparse-kmeans/dictionary.file-0

here is what the clusters look like:
SV-15898{n=34 c=[0:0.020, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.7254762602900604
		mln                                     =>  1.2510936664951733
		dlrs                                    =>  1.1340145215097008
		3                                       =>  1.0643797240793276
		pct                                     =>  1.0422760712239152
		reuter                                  =>  1.0202689935247569
		its                                     =>  0.9997771992646881
		from                                    =>  0.9903731234557381
		year                                    =>  0.8855389859684145
		vs                                      =>  0.8291746545786391
:SV-14766{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6406710289350412
		mln                                     =>  1.2174993414858022
		dlrs                                    =>  1.0937941570322955
		3                                       =>  1.0334420773050856
		pct                                     =>   0.991539915235039
		reuter                                  =>   0.990042452019326
		its                                     =>  0.9508638527143669
		from                                    =>  0.9403885495991262
		vs                                      =>   0.865437130369746
		year                                    =>  0.8463503194752994
:SV-14854{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.641260962665307
		mln                                     =>   1.217806578134094
		dlrs                                    =>  1.0941157210136143
		3                                       =>  1.0336934328877394
		pct                                     =>   0.991895013999163
		reuter                                  =>  0.9902889592990656
		its                                     =>  0.9512076670014483
		from                                    =>  0.9407384847445094
		vs                                      =>  0.8653426311034671
		year                                    =>  0.8466407590692175
:SV-14890{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6410352907185948
		mln                                     =>    1.21769021136256
		dlrs                                    =>  1.0939933408434481
		3                                       =>  1.0335977297579235
		pct                                     =>   0.991759193577722
		reuter                                  =>  0.9901951250301172
		its                                     =>  0.9510761761632947
		from                                    =>  0.9406047832581563
		vs                                      =>  0.8653814488835572
		year                                    =>  0.8465301083353372
:SV-14972{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.640981249652196
		mln                                     =>  1.2176595452829564
		dlrs                                    =>   1.093962519439548
		3                                       =>  1.0335737897463568
		pct                                     =>  0.9917266257955816
		reuter                                  =>  0.9901715950801396
		its                                     =>  0.9510446208123859
		from                                    =>  0.9405723357372776
		vs                                      =>  0.8653843699725567
		year                                    =>   0.846502466267153
:SV-15023{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6399319888551425
		mln                                     =>   1.217099157115808
		dlrs                                    =>  1.0933830369192543
		3                                       =>   1.033121271434882
		pct                                     =>   0.991094828319561
		reuter                                  =>  0.9897275313905611
		its                                     =>  0.9504327303592046
		from                                    =>  0.9399480272494183
		vs                                      =>  0.8655203514280634
		year                                    =>  0.8459804922897428
:SV-15330{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6411480082558068
		mln                                     =>   1.217746071140758
		dlrs                                    =>  1.0940532425506244
		3                                       =>  1.0336447143638317
		pct                                     =>  0.9918269975797083
		reuter                                  =>   0.990241145450359
		its                                     =>  0.9511417993006985
		from                                    =>  0.9406712099799636
		vs                                      =>  0.8653569180999117
		year                                    =>  0.8465844425179013
:SV-15403{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6493270418577013
		mln                                     =>   1.221708475489808
		dlrs                                    =>  1.0983489300320377
		3                                       =>  1.0370024996153944
		pct                                     =>  0.9967446058994232
		reuter                                  =>   0.993528974793619
		its                                     =>  0.9558988111209523
		from                                    =>  0.9454911460774864
		vs                                      =>  0.8633642497287671
		year                                    =>  0.8505083085439775
:SV-15514{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6414524586689534
		mln                                     =>  1.2179029815366167
		dlrs                                    =>   1.094218299808865
		3                                       =>   1.033773769117182
		pct                                     =>  0.9920102286561391
		reuter                                  =>  0.9903676795676004
		its                                     =>  0.9513191861395162
		from                                    =>  0.9408515920762511
		vs                                      =>   0.865304353452142
		year                                    =>  0.8467337135094862
:SV-15549{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.640632892454694
		mln                                     =>  1.2174764812983898
		dlrs                                    =>  1.0937717467869699
		3                                       =>   1.033424727632325
		pct                                     =>    0.99151691360307
		reuter                                  =>  0.9900253758026865
		its                                     =>  0.9508415534060888
		from                                    =>  0.9403654699584985
		vs                                      =>   0.865436402399392
		year                                    =>  0.8463303217162843
:SV-15616{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6402745961421197
		mln                                     =>   1.217287104215781
		dlrs                                    =>  1.0935749393200054
		3                                       =>  1.0332709291683844
		pct                                     =>  0.9913012005612369
		reuter                                  =>  0.9898744911012118
		its                                     =>  0.9506326562835085
		from                                    =>  0.9401525895225771
		vs                                      =>  0.8654873596392523
		year                                    =>  0.8461528918952358
:SV-15674{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6402335213893247
		mln                                     =>  1.2172651791725515
		dlrs                                    =>  1.0935522610806727
		3                                       =>  1.0332532137000938
		pct                                     =>   0.991276468108388
		reuter                                  =>  0.9898571070574692
		its                                     =>  0.9506087026962596
		from                                    =>  0.9401281555632803
		vs                                      =>  0.8654927058873914
		year                                    =>  0.8461324681573653
:SV-15720{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.641454220566282
		mln                                     =>  1.2179063418879368
		dlrs                                    =>  1.0942205822099829
		3                                       =>  1.0337754035575257
		pct                                     =>  0.9920113271819195
		reuter                                  =>  0.9903693325123661
		its                                     =>  0.9513202705619623
		from                                    =>  0.9408530174807668
		vs                                      =>  0.8653096216062077
		year                                    =>  0.8467355860669477
:SV-15732{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6418679366988789
		mln                                     =>   1.218118262616823
		dlrs                                    =>  1.0944441677361394
		3                                       =>  1.0339502052648608
		pct                                     =>  0.9922602967957669
		reuter                                  =>  0.9905406967751569
		its                                     =>  0.9515612774046113
		from                                    =>   0.941098001639954
		vs                                      =>   0.865235154416334
		year                                    =>  0.8469379811534101
:SV-15825{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6403540331112847
		mln                                     =>  1.2173302824011656
		dlrs                                    =>  1.0936192179118565
		3                                       =>  1.0333054698476525
		pct                                     =>  0.9913490440255205
		reuter                                  =>  0.9899084014354236
		its                                     =>  0.9506790000021428
		from                                    =>  0.9401999656754023
		vs                                      =>  0.8654787849286104
		year                                    =>  0.8461927112339609
:SV-15888{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.641852069569193
		mln                                     =>   1.218106579705691
		dlrs                                    =>  1.0944336674208315
		3                                       =>  1.0339422184421034
		pct                                     =>  0.9922506923700831
		reuter                                  =>  0.9905327937543529
		its                                     =>   0.951551949990525
		from                                    =>  0.9410880514065464
		vs                                      =>  0.8652299423273659
		year                                    =>  0.8469287549740471
:SV-15944{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6406094746503062
		mln                                     =>  1.2174640910103491
		dlrs                                    =>  1.0937588768380255
		3                                       =>  1.0334146735611798
		pct                                     =>  0.9915028147402405
		reuter                                  =>  0.9900155118531778
		its                                     =>  0.9508279001565995
		from                                    =>  0.9403515526055797
		vs                                      =>   0.865439705916966
		year                                    =>   0.846318717539638
:SV-15952{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.641608350634413
		mln                                     =>  1.2179827157677379
		dlrs                                    =>   1.094302484756082
		3                                       =>   1.033839606583586
		pct                                     =>  0.9921040410110572
		reuter                                  =>   0.990432219413613
		its                                     =>  0.9514099986904929
		from                                    =>  0.9409438763575203
		vs                                      =>  0.8652760331837802
		year                                    =>  0.8468099163160301
:SV-15954{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6429205353451672
		mln                                     =>  1.2186434984636658
		dlrs                                    =>  1.0950054459143779
		3                                       =>  1.0343894404834142
		pct                                     =>   0.992893505149969
		reuter                                  =>  0.9909710261706427
		its                                     =>  0.9521740690117075
		from                                    =>  0.9417194634871013
		vs                                      =>  0.8650137662755684
		year                                    =>  0.8474476266423354
:SV-16007{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>  1.6401767760282457
		mln                                     =>  1.2172339691485916
		dlrs                                    =>   1.093520432998812
		3                                       =>  1.0332284013507513
		pct                                     =>  0.9912422858233993
		reuter                                  =>  0.9898327402827573
		its                                     =>  0.9505755879363272
		from                                    =>  0.9400942591120444
		vs                                      =>  0.8654979916098049
		year                                    =>  0.8461038772989482
:SV-16037{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0
	Top Terms: 
		said                                    =>   1.640610618380475
		mln                                     =>  1.2174645746382695
		dlrs                                    =>  1.0937594396319776
		3                                       =>  1.0334151203058977
		pct                                     =>  0.9915035014016228
		reuter                                  =>  0.9900159476830741
		its                                     =>  0.9508285640147016
		from                                    =>  0.9403522136131415
		vs                                      =>  0.8654392679742507
		year                                    =>   0.846319234572972
",smartshark_2_2,1671,mahout,"""[0.251761257648468, 0.748238742351532]"""
2077,230336,Syntax error when running build-reuters.sh script,"When running examples/bin/build-reuters.sh from the root of the checked out mahout source tree I get the following syntax error:

examples/bin/build-reuters.sh: 28: Syntax error: ""("" unexpected (expecting ""fi"")

This is because the script uses array initialization syntax supported by the bash shell, algorithm=( kmeans lda ), but it's shebang line uses the bourne shell: #!/bin/sh",smartshark_2_2,1343,mahout,"""[0.9338135719299316, 0.06618645042181015]"""
2078,230337,Simplify configuring and running Mahout MapReduce jobs from Java using Java bean configuration,"Most of the Mahout features require running several jobs in sequence. This can be done via the command line or using one of the driver classes.
Running and configuring a Mahout job from Java requires using either the Driver's static methods or creating a String array of parameters and pass them to the main method of the job. If we can instead configure jobs through a Java bean or factory we it will be type safe and easier to use in by DI frameworks such as Spring and Guice.

I have added a patch where I factored out a KMeans MapReduce job plus a configuration Java bean, from KMeansDriver.buildClustersMR(...)

* The KMeansMapReduceConfiguration takes care of setting up the correct values in the Hadoop Configuration object and initializes defaults. I copied the config keys from KMeansConfigKeys.
* The KMeansMapReduceJob contains the code for the actual algorithm running all iterations of KMeans and returns the KMeansMapReduceConfiguration, which contains the cluster path for the final iteration.

I like to extend this approach to other Hadoop jobs for instance the job for creating points in KMeansDriver, but I first want some feedback on this. 

One of the benefits of this approach is that it becomes easier to chain jobs. For instance we can chain Canopy to KMeans by connecting the output dir of Canopy's configuration to the input dir of the configuration of the KMeans job next in the chain. Hadoop's JobControl class can then be used to connect and execute the entire chain.

This approach can be further improved by turning the configuration bean into a factory for creating MapReduce or sequential jobs. This would probably remove some duplicated code in the KMeansDriver.",smartshark_2_2,419,mahout,"""[0.9983423948287964, 0.0016576146008446813]"""
2079,230338,FindBugs and PMD settings unrealistic ,"We get warnings on every use of a non-final method in a constructor.  This is arguably slightly risky, but I haven't ever seen a bug from this.  Similarly, we haven't had any problems with long methods (inherent in Hadoop programs, I think) nor with putting literals after variables in comparisons.

I am going to turn these warnings off to decrease noise levels.
",smartshark_2_2,839,mahout,"""[0.9819096326828003, 0.018090438097715378]"""
2080,230339,RecommenderServlet response content types for XML and JSON,"In my opinion I believe the following changes should be made:

For the writeJSON method, the content type should be changed from ""text/plain"" to ""application/json"".  It's not as 'friendly' to some browsers but is more technically correct.  See http://www.ietf.org/rfc/rfc4627.txt

For the writeXML method, if you don't put the character encoding type into the same line at the content type, then the encoding type can be ignored and decoded as ASCII (see http://annevankesteren.nl/2005/03/text-xml).  So you could change the content type from ""text/xml"" to ""text/xml; charset=utf-8"" but it's probably better to change it to ""application/xml"".  See http://www.grauw.nl/blog/entry/489
",smartshark_2_2,56,mahout,"""[0.9938374161720276, 0.006162554956972599]"""
2081,230340,Redo RecommenderEvaluator for modularity,"The RecommenderEvaluator implementation is hard-coded around the standard algorithm for comparing a recommender's performance on training v.s. test datasets.

This is a rewrite supplying other algorithms, and the ability to directly compare DataModels. It was born out of a desire to check that a new recommender made sense compared to existing recommenders, and to directly work with DataModels that encompass the entire recommendation space.
",smartshark_2_2,1351,mahout,"""[0.998383641242981, 0.001616351306438446]"""
2082,230341,SequentialAccessSparseVector function assignment is very slow and other iterator woes,"Currently when calling .assign() on a SASV with another vector and a custom function, it will iterate through it and assign every single entry while also referring it by index.

This makes the process *hugely* expensive. (on a run of BallKMeans on the 20 newsgroups data set, profiling reveals that 92% of the runtime was spent updating assigning the vectors).

Here's a prototype patch:
https://github.com/dfilimon/mahout/commit/63998d82bb750150a6ae09052dadf6c326c62d3d",smartshark_2_2,463,mahout,"""[0.9837363362312317, 0.016263654455542564]"""
2083,230342,"Remove deprecated distance(Float[], Float[]), AbstractDistanceMeasure?","HI all, just looking at the deprecation warnings I am seeing when compiling. Looks like we've deprecated DistanceMeasure.distance(Float[], Float[]). Looks like it is easy to just remove this altogether, and with it, AbstractDistanceMeasure. OK to do?",smartshark_2_2,1537,mahout,"""[0.9984791874885559, 0.00152076396625489]"""
2084,230343,Interchangeable Solvers ,"Currently all algorithms are solving 'closed' form.  

It would be good to create open form solvers and optionally invoke them. 

Two such solvers are Stochastic Gradient Descent, and Genetic Algorithms. 

An abstract solver trait, and implementations of at least these two solving mechanisms (to avoid bias in choosing what to include/not include in the Solver trait). 

Refactoring current algorithms to accept optional Solver.",smartshark_2_2,1965,mahout,"""[0.9983493089675903, 0.0016507265390828252]"""
2085,230344,Upgrade to Hadoop 0.20.0,As the title says,smartshark_2_2,1019,mahout,"""[0.9984691739082336, 0.001530857291072607]"""
2086,230345,"clusterControlDataWithCanopy, clusterControlDataWithFuzzyKMeans, clusterControlDataWithDirichle examples are looking for output in the wrong place","When executed against Hadoop 2.0.0 RC the tests fails like this:

{noformat}
$ mahout org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job
...
Caused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1319)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)
{noformat}

{noformat}
$ mahout org.apache.mahout.clustering.syntheticcontrol.dirichlet.Job
.....
Caused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1319)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)
	... 17 more
{noformat}

{noformat}
$ mahout org.apache.mahout.clustering.syntheticcontrol.canopy.Job
....
Caused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1356)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1486)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1441)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1419)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:68)
	at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)
	... 17 more
{noformat}

For more details please look at the Bigtop test failures:
 http://bigtop01.cloudera.org:8080/view/Test/job/SmokeCluster/lastCompletedBuild/testReport/org.apache.bigtop.itest.mahout.smoke/",smartshark_2_2,122,mahout,"""[0.8901928663253784, 0.10980714112520218]"""
2087,230346,Add solve() function to the Scala DSL ,We should add a solve() function to the Scala DSL with helps with solving Ax = b for in-core matrices and vectors.,smartshark_2_2,859,mahout,"""[0.9983093738555908, 0.0016906695673242211]"""
2088,230347,Bottom Up Clustering,Bottom up clustering is achieved by starting with small clusters/single points and then merging clusters recursively which are closer than a specified control constraint.,smartshark_2_2,17,mahout,"""[0.9981258511543274, 0.001874106703326106]"""
2089,230348,"MahoutDriver yields cosmetically suboptimal exception when bin/mahout runs without args, on some Hadoop versions","If you run bin/mahout without arguments, an error is correctly displayed about lack of an argument. The part that displays the error is actually within Hadoop code. In some versions of Hadoop, in the error case, it will quit the JVM with System.exit(). In others, it does not.

In the calling code in MahoutDriver, in this error case, the main() method does not actually return. So, for versions where Hadoop code doesn't immediately exit the JVM, execution continues. This yields another exception. It's pretty harmless but ugly.

Attached is a one-line fix, to return from main() in the error case, which is more correct to begin with.",smartshark_2_2,273,mahout,"""[0.8912955522537231, 0.10870441794395447]"""
2090,230349,Distinguish implemented algorithms from algorithms which may be implemented in the future in algorithms page,"In case of the description of the Mahout algorithms web page,
(https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms)
the algorithms which may be implemented in the future are easy to be confused with the already implemented algorithms,
and I think that it is difficult to recognize both intuitively.

I think that both algorithms should be distinguished more clearly.",smartshark_2_2,229,mahout,"""[0.998275876045227, 0.001724090427160263]"""
2091,230350,Canopy Clustering,"Hello,

I'm trying out Canopy clustering.
I want to know, how to determine the optimum value for the distance thresholds t1 and t2.

Thanks.",smartshark_2_2,548,mahout,"""[0.9974507689476013, 0.00254916213452816]"""
2092,230351,Random Projection using sampled values,"Random Projection implementation which follows two deterministic guarantees:
# The same data projected multiple times produces the same output
# Dense and sparse data with the same contents produce the same output

Custom class that does Random Projection based on Johnson-Lindenstrauss. This implementation uses Achlioptas's results, which allow using method other than a full-range random multiplier per sample:
* use 1 random bit to add or subtract a sample to a row sum 
* use a random value from 1/6 to add (1/6), subtract (1/6), or ignore (4 out of 6) a sample to a row sum

Custom implementations for both dense and sparse vectors are included. The sparse vector implementation assumes the active values will fit in memory.

An implementation using full-range random multipliers made by java.util.Random is included for reference/research. 

*Database-friendly random projections: Johnson-Lindenstrauss with binary coins*
_Dimitris Achlioptas_
[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.4546&rep=rep1&type=pdf]

",smartshark_2_2,65,mahout,"""[0.5348634123802185, 0.46513664722442627]"""
2093,230352,Hotspot in RecommenderJob-PartialMultiplyMapper-Reducer,"While profiling PartialMultiplyMapper-Reducer job we noticed a hotspot consuming more than 40% of the CPU time in org.apache.mahout.math.RandomAccessSparseVector.assign method for the reducer task.  We used the script provided in mahout examples for running ASF Email recommendations for profiling. The hotspot is coming from the use of Vector.plus(Vector x) method in AggregateAndRecommendReducerc class.  The pattern used is VectorA = VectorA.plus(VectorB).  In this case VectorA doesn't have to be cloned using assign method.  The attached patch addresses the hotspot by eliminating cloning in the above case for plus and times methods.  This patch while retaining functionality (verified the output with and without patch), speeds up execution time of PartialMultiplyMapper-Reducer job by more than 10X on x86 architectures.",smartshark_2_2,520,mahout,"""[0.9915796518325806, 0.008420378901064396]"""
2094,230353,Create a list of all past board reports in CMS site (or link to them),"Some of our past reports are in the mailing list archives, some in Confluence, some I don't know where. For future reference and in order for newcomers to know where we are coming from there should be one canonical go-to page for Mahout board reports.",smartshark_2_2,453,mahout,"""[0.9982549548149109, 0.0017450714949518442]"""
2095,230354,Hadoop 2 compatibility,We must ensure that all our MR code also runs on Hadoop 2. ,smartshark_2_2,669,mahout,"""[0.997962474822998, 0.0020375456660985947]"""
2096,230355,Mahalanobis Distance + Singular Value Decomposition,"This patch contains an implementation of the Mahalanobis distance + a unit test.
As explained in wikipedia (http://en.wikipedia.org/wiki/Mahalanobis_distance) ,  it is a useful way of determining similarity of an unknown sample set to a known one. It differs from Euclidean distance in that it takes into account the correlations of the data set and is scale-invariant.

Also contained in the patch:
-A port of the SingularValueDecomposition Class to the Matrix data structure + unit tests.
-An embryonic port of the matrix.linalg Algebra class to the Matrix/Vector data structure.",smartshark_2_2,1180,mahout,"""[0.9979745745658875, 0.002025397727265954]"""
2097,230356,ClusterDumper output file should be optional,"ClusterDumper output option should be optional, defaults to System.out if -o is not specified",smartshark_2_2,474,mahout,"""[0.9977700710296631, 0.0022299932315945625]"""
2098,230357,Implement ranking autoencoder on top of gradient machine,"Implement a ranking autoencoder clusterer based on top of gradient machine.
See https://docs.google.com/present/edit?id=0AQC247eq7Jp5ZGZ6NXpyOWhfMjlmM2pzdjRkZw&authkey=CNj2h98P&hl=en_US
for details",smartshark_2_2,748,mahout,"""[0.9983832836151123, 0.0016166976420208812]"""
2099,230358,Switch to Flink 1.1.0,Switch to Flink 1.1.0,smartshark_2_2,1836,mahout,"""[0.9978845715522766, 0.002115411451086402]"""
2100,230359,"hadoop job config parameter,e.g., -Dmapred.cache.archives, support in mahout wrapper","In order to specify a custom analyzer that utilizes a Japanese Morphological Analyzer ""Igo"" referring to dictionary files on HDFS for seq2sparse, I needed to pass the following job config:

mapred.cache.archives=""hdfs://localhost:9000/user/stakeda/ipadic.zip#ipadic
mapred.create.symlink=yes

This way, the IgoAnalyzer can read dictionaries from ""./ipadic"" as follows:
https://github.com/smtakeda/mahout/blob/project101210/examples/src/main/java/org/apache/mahout/analysis/IgoAnalyzer.java

Other use case is I needed to specify mapred.job.queue.name to something to get appropriate priority for running jobs in  the work environment:
https://github.com/smtakeda/mahout/blob/yahoo/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyDriver.java
...
conf.set(""mapred.job.queue.name"", ""unfunded""); 

Based on these two use cases, I would like to request/propose to add hadoop job option support, i.e., -Dmapred.cache.archives=... to mahout wrapper.

Changes are roughly expected in two ends; ""bin/mahout"" and all main functions that parse command lines. Here is a quick patch for ""bin/mahout"":

localhost ~/workspace/mahout_git/bin: git diff -r f13e517408f20f75009e05e6c72c5fbb836e3f66 mahout 
diff --git a/bin/mahout b/bin/mahout
index 774fa11..9d78ceb 100755
--- a/bin/mahout
+++ b/bin/mahout
@@ -116,6 +116,14 @@ CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
 # so that filenames w/ spaces are handled correctly in loops below
 IFS=
 
+
+# JAVA_PROPERTIES
+JAVA_PROPERTIES=
+while [ $1 ] && [ ${1:0:2} == ""-D"" ] ; do 
+    JAVA_PROPERTIES=""$1 $JAVA_PROPERTIES""
+    shift
+done
+
 if [ $IS_CORE == 0 ] 
 then
   # add release dependencies to CLASSPATH
@@ -198,7 +206,7 @@ if [ ""$HADOOP_HOME"" = """" ] || [ ""$MAHOUT_LOCAL"" != """" ] ; then
   elif [ ""$MAHOUT_LOCAL"" != """" ] ; then 
     echo ""MAHOUT_LOCAL is set, running locally""
   fi
-  exec ""$JAVA"" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath ""$CLASSPATH"" $CLASS ""$@""
+  exec ""$JAVA"" $JAVA_HEAP_MAX $JAVA_PROPERTIES $MAHOUT_OPTS -classpath ""$CLASSPATH"" $CLASS ""$@""
 else
   echo ""Running on hadoop, using HADOOP_HOME=$HADOOP_HOME""
   if [ ""$HADOOP_CONF_DIR"" = """" ] ; then
@@ -213,7 +221,7 @@ else
     exit 1
   else
   export HADOOP_CLASSPATH=$MAHOUT_CONF_DIR:${HADOOP_CLASSPATH}
-  exec ""$HADOOP_HOME/bin/hadoop"" jar $MAHOUT_JOB $CLASS ""$@""
+  exec ""$HADOOP_HOME/bin/hadoop"" jar $MAHOUT_JOB $CLASS ""$@"" $JAVA_PROPERTIES
   fi 
 fi
",smartshark_2_2,1381,mahout,"""[0.9971760511398315, 0.0028239579405635595]"""
2101,230360,Rework quickstart website,"Our quickstart website must be much more helpful.

We should describe how to add Mahout as maven dependency to a Java project and how to start working on the source code.

https://mahout.apache.org/users/basics/quickstart.html",smartshark_2_2,649,mahout,"""[0.9972564578056335, 0.0027435878291726112]"""
2102,230361,Add ClusterEvaluator capabilities to ClusterDumper,It would be nice if the ClusterDumper spit out some of our cluster evaluation metrics.,smartshark_2_2,1713,mahout,"""[0.99790358543396, 0.002096364740282297]"""
2103,230362,Create a separate collections library,"Factor collections and as little else as possible into it's own library. Delete the AIOB test that uses matrixes until someone can refactor it to use the base collections API.
",smartshark_2_2,1428,mahout,"""[0.9976664781570435, 0.002333579119294882]"""
2104,230363,Replace deprecated Closables.closeQuietly calls,"Deprecated Guava {{Closables.closeQuietly}} API has to be replaced, it's usage is a code smell, and that method is scheduled to be removed from Guava 16.0.

See [this discussion|https://code.google.com/p/guava-libraries/issues/detail?id=1118] for more info.",smartshark_2_2,508,mahout,"""[0.9982592463493347, 0.0017407051054760814]"""
2105,230364,Use traits when probing VCL,"currently we have

eg.
{code} 
      clazz = Class.forName(""org.apache.mahout.viennacl.opencl.GPUMMul$"").getField(""MODULE$"").get(null).asInstanceOf[MMBinaryFunc]
{code}

To instantiate a Solver.. It is being cast to a {{MMBinaryFunc}}

cast this to at MMulSolver Trait and change the corresponding class GPUMMul to extend this.

",smartshark_2_2,2003,mahout,"""[0.9968817234039307, 0.003118333173915744]"""
2106,230365,Nuke Colt math functions that remain untested - The Matrix edition,"We are down to very few references to Colt's matrix capabilities.

I have implemented a new eigen-solver (with tests) and updated the Lanczos code to use it.  I also have nuked all of the DoubleMatrix*d and related classes and consolidated several others.",smartshark_2_2,123,mahout,"""[0.9984928369522095, 0.0015071722446009517]"""
2107,230366,org.apache.mahout.cf.taste.hadoop.item.RecommenderJob for Boolean recommendation,"in some case there has no preference value in the input data ,the preference value is set to zero,then 

RecommenderMapper.class

 @Override
  public void map(LongWritable userID,
                  VectorWritable vectorWritable,
                  OutputCollector<LongWritable,RecommendedItemsWritable> output,
                  Reporter reporter) throws IOException {
    
    if ((usersToRecommendFor != null) && !usersToRecommendFor.contains(userID.get())) {
      return;
    }
    Vector userVector = vectorWritable.get();
    Iterator<Vector.Element> userVectorIterator = userVector.iterateNonZero();
    Vector recommendationVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1000);
    while (userVectorIterator.hasNext()) {
      Vector.Element element = userVectorIterator.next();
      int index = element.index();
      double value = element.get();     //here will get 0.0 for Boolean recommendation 
      Vector columnVector;
      try {
        columnVector = cooccurrenceColumnCache.get(new IntWritable(index));
      } catch (TasteException te) {
        if (te.getCause() instanceof IOException) {
          throw (IOException) te.getCause();
        } else {
          throw new IOException(te.getCause());
        }
      }
      if (columnVector != null) {
        columnVector.times(value).addTo(recommendationVector); //here will set all score value to zero for Boolean recommendation
      }
    }
    
    Queue<RecommendedItem> topItems = new PriorityQueue<RecommendedItem>(recommendationsPerUser + 1,
        Collections.reverseOrder());
    
    Iterator<Vector.Element> recommendationVectorIterator = recommendationVector.iterateNonZero();
    LongWritable itemID = new LongWritable();
    while (recommendationVectorIterator.hasNext()) {
      Vector.Element element = recommendationVectorIterator.next();
      int index = element.index();
      if (userVector.get(index) == 0.0) {
        if (topItems.size() < recommendationsPerUser) {
          indexItemIDMap.get(new IntWritable(index), itemID);
          topItems.add(new GenericRecommendedItem(itemID.get(), (float) element.get()));
        } else if (element.get() > topItems.peek().getValue()) {
          indexItemIDMap.get(new IntWritable(index), itemID);
          topItems.add(new GenericRecommendedItem(itemID.get(), (float) element.get()));
          topItems.poll();
        }
      }
    }
    
    List<RecommendedItem> recommendations = new ArrayList<RecommendedItem>(topItems.size());
    recommendations.addAll(topItems);
    Collections.sort(recommendations);
    output.collect(userID, new RecommendedItemsWritable(recommendations));
  }

so maybe we need a option to distinguish boolean recommendation and slope one recommendation.

in ToUserVectorReducer.class

here no need findTopNPrefsCutoff,maybe take all item.

it's just my thinking ,maybe item is used for slope one only .
:)
 
",smartshark_2_2,1120,mahout,"""[0.20732514560222626, 0.7926748991012573]"""
2108,230367,LoadEvaluationRunner and Recommender stats,"Per MAHOUT-881, it would be nice to implement a easy to use test driver (CFLoadEvaluationRunner) that can take in a data model for CF and run the LoadEvaluator multiple times and bring back stats about how long it took to run, etc.  This likely means being able to return the StatsCallable results out to the runner, but doesn't have to.",smartshark_2_2,21,mahout,"""[0.9984228610992432, 0.0015771797625347972]"""
2109,230368,DistributedRowMatrix needs times(Vector) implementation as M/R job,pretty self-explanatory.,smartshark_2_2,1526,mahout,"""[0.9980879426002502, 0.00191205064766109]"""
2110,230369,Force Mahout to be build with guava 11.0.2 as Hadoop uses 11.0.2,"Currently Hadoop supports guava 11.02, hence Mahout needs to be compatible with guava 11.0.2",smartshark_2_2,904,mahout,"""[0.9982200264930725, 0.0017799471970647573]"""
2111,230370,Spark Dependency Reduced Jar Needs Open-MP,"The spark dependency reduced jar includes:
org.apache.mahout:mahout-native-viennacl_2.10

but not
org.apache.mahout:mahout-native-viennacl-omp_2.10

In mahout/spark/src/main/assembly/dependency-reduced.xml",smartshark_2_2,1884,mahout,"""[0.9955714344978333, 0.004428594373166561]"""
2112,230371,Cleanup website on wikipedia example,"The website on the wikipedia example needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.

https://mahout.apache.org/users/classification/wikipedia-bayes-example.html",smartshark_2_2,881,mahout,"""[0.9973540306091309, 0.0026459554210305214]"""
2113,230372,Add Stack Image to the top of the front page of the  Website,"Add a variant of stack.svg - the image of the mahout stack (pg. 64 in the book) ""Above the fold"" on the site.  This image seems to help people grasp ""what mahout is"" very quickly.    ",smartshark_2_2,2002,mahout,"""[0.9979211688041687, 0.0020788097754120827]"""
2114,230373,Allow specification of dimensions of a DRM,"Currently, in many cases, a DRM must be read to compute its dimensions when a user calls nrow or ncol. This also implicitly caches the corresponding DRM.

In some cases, the user actually knows the matrix dimensions (e.g., when the matrices are synthetically generated, or when some metadata about them is known). In such cases, the user should be able to specify the dimensions upon creating the DRM and the caching should be avoided. ",smartshark_2_2,1970,mahout,"""[0.9984757304191589, 0.0015243131201714277]"""
2115,230374,Random generator objects- slight refactor,"Problems:
* The uncommons RepeatableRNG classes are the basis of RandomUtils.
** These classes cheerfully ignore setSeed.
* Some people in the project want to move off Uncommons anyway.

This patch uses the org.apache.commons.math.random.RandomGenerator classes instead of org.apache.uncommons.maths.RepeatableRNG classes.
.



",smartshark_2_2,1707,mahout,"""[0.9982960820198059, 0.0017039395170286298]"""
2116,230375,Create properties for VCL on mac ,Create a set of properties to run OMP on mac- OS X darwin and >. OpenMP is not supported directly by CLANG in mac g++ (in most versions).,smartshark_2_2,1875,mahout,"""[0.9983943104743958, 0.001605615601874888]"""
2117,230376,Fix broken links in quickstart webpage,"http://mahout.apache.org/users/basics/quickstart.html

Fix a few broken links, remove Naive Bayes Wikipedia reference",smartshark_2_2,867,mahout,"""[0.9975518584251404, 0.0024481089785695076]"""
2118,230377,GSON not a good fit for deserializing SGD models,"The problem is that GSON uses JavaCC which does LL(k) parsing.  The way that the JSON grammar is expressed causes stack recursion for each element of
a vector.

This is evil for parsing large models.

I will be changing ModelSerializer to fix this and will provide a reverse compatible mode for anybody who has been able to serialize/deserialize these models in the past.  My thought is to just make all the SGD models Writables.",smartshark_2_2,1253,mahout,"""[0.8847069144248962, 0.11529309302568436]"""
2119,230378,Duplicated DoubleFunction in mahout and mahout-collections (mahout.math package).,"DoubleFunction is duplicated in Mahout Math and Collections. There are also other things inside m.math.* package that are kept there to avoid circular dependencies... Simply removing DoubleFunction from collections is not going to work because it is needed for compilation (again, circular dependency between collections and math). I see two solutions:

1) extract the common definitions inside math.function.* into a separate module. This is a clean solution, but obviously scatters the code even further.

2) create a compilation-time, optional dependency on mahout 0.5 in collections, remove the entire mahout.math.* subpackage from collections and live with this. I don't know how Maven handles circular dependencies of this type:

collections [trunk] -> [optional, required at build time] mahout.math 0.5
mahout.math [trunk] -> collections [trunk]

it seems tricky and error-prone, but should also work.

Looking for other ideas of cleaning this mess up, of course.
Dawid",smartshark_2_2,170,mahout,"""[0.9916774034500122, 0.008322590962052345]"""
2120,230379,Vector should allow for other normalize powers than the L-2 norm,"Modify Vector to allow other normalize functions for the Vector
See http://www.lucidimagination.com/search/document/bf3a7a7a004d4191/norm_calculations",smartshark_2_2,1495,mahout,"""[0.9984773993492126, 0.0015226536197587848]"""
2121,230380,Modifying Mahout Check style to match our current coding style,"Checkstyle currently throws a lot of errors for small things like 

private static Logger log 
if(booleanVar == false) // Required for readability 

This task is created to track 
how to match the checkstyle to relax certain rules and 
changes in code for the strict ones",smartshark_2_2,1413,mahout,"""[0.9981361627578735, 0.0018637955654412508]"""
2122,230381,Remove Hadoop 1 support.,"Remove support for Hadoop 1.

1. Disable Jenkins Hadoop 1 build.
2. Remove Hadoop 1 profile from the root {{pom.xml}}.
3. Refactor any Hadoop1 specific code *outside of* the {{mr}} module.
4. Update documentation.
5. Notify user@.

Hadoop1HDFSUtils is likely the only code that this will affect.
  ",smartshark_2_2,1841,mahout,"""[0.9983508586883545, 0.0016491172136738896]"""
2123,230382,Simplify maven structure,"As per some email to the dev list, I plan to streamline and simplify the maven structure, by eliminating the 'maven' directory and letting the top-level POM be the parent, moving release packaging to a separate distribution directory.

I do not plan to post patches here.
",smartshark_2_2,1109,mahout,"""[0.9982398748397827, 0.0017601152649149299]"""
2124,230383,ParameterEnumerable,"A utility package used to 

 * configure class
 * create default configuration files
 * parse main method arguments
 * produce human readable help
 * getters and setters for value as object and string, for future generic reflection based GUI.
",smartshark_2_2,1014,mahout,"""[0.9978652596473694, 0.0021347550209611654]"""
2125,230384,Properly route all CUDAMMul calls to the correct algorithms,"{{CUDAMMul}} has been implemented and is currently routing Sparse Sparse and Dense Dense mmul calls correctly, Sparse Dense must be properly routed.. (as well as Dense Sparse).",smartshark_2_2,1984,mahout,"""[0.8594927787780762, 0.14050725102424622]"""
2126,230385,Ability to handle non numeric itemid's,Current command line option for recommendation  does not handle non numeric itemid's.  ,smartshark_2_2,910,mahout,"""[0.9979609251022339, 0.0020391091238707304]"""
2127,230386,RMS variant of RunningAverage classes,Added as classes because I don't know if this is interesting.,smartshark_2_2,1319,mahout,"""[0.9964854717254639, 0.0035145680885761976]"""
2128,230387,"When building on linux, haswell properties are not working.","got a failure when building on linux with haswell.properties:

{code}
Warning: Could not load platform properties for class org.apache.mahout.viennacl.openmp.OMPMMul$
Generating /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp
Compiling /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/linux-haswell/libjniViennaCL.so
g++ -I/usr/include/viennacl -I/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.121.x86_64/include -I/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.121.x86_64/include/linux /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp -msse3 -ffast-math -fopenmp -fpermissive -Wl,-rpath,$ORIGIN/ -Wl,-z,noexecstack -Wl,-Bsymbolic -march=haswell -m64 -Wall -O3 -fPIC -shared -s -o libjniViennaCL.so -lOpenCL
/vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp:1:0: error: bad value (haswell) for -march= switch
 // Generated by JavaCPP version 1.2.4: DO NOT EDIT THIS FILE
 ^
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 14.721 s

{code}

Need to fix",smartshark_2_2,2008,mahout,"""[0.9200601577758789, 0.07993987202644348]"""
2129,230388,cbind() operator for Scala DRMs,"Another R-like operator, cbind (stitching two matrices together). Seems to come up now and then. 

Just like with elementwise operations, and, perhaps some other, it will have two physical implementation paths, one is zip for identically distributed operators, and another one is full join in case they are not.",smartshark_2_2,776,mahout,"""[0.9984885454177856, 0.001511430717073381]"""
2130,230389,Parallel SGD matrix factorizer for SVDrecommender,"a parallel factorizer based on MAHOUT-1089 may achieve better performance on multicore processor.

existing code is single-thread and perhaps may still be outperformed by the default ALS-WR.

In addition, its hardcoded online-to-batch-conversion prevents it to be used by an online recommender. An online SGD implementation may help build high-performance online recommender as a replacement of the outdated slope-one.

The new factorizer can implement either DSGD (http://www.mpi-inf.mpg.de/~rgemulla/publications/gemulla11dsgd.pdf) or hogwild! (www.cs.wisc.edu/~brecht/papers/hogwildTR.pdf).

Related discussion has been carried on for a while but remain inconclusive:
http://web.archiveorange.com/archive/v/z6zxQUSahofuPKEzZkzl",smartshark_2_2,566,mahout,"""[0.9982532858848572, 0.001746674533933401]"""
2131,230390,Self Organizing Map,"Implementation of  the Kohonen's Self organizing map algorithm.

Execution: run the SOMViewer .
takes 300 iteration.

- The algo is too slow because of:
	GUI: the current one is a temporary one, but should be replaced by ""prefused library"" as suggested by Ted.
	Self-Organizing Maps: Batch Algorithm is faster than the sequentiel one that I am currently 	using.

- Documentation needs to be completed. ",smartshark_2_2,1377,mahout,"""[0.9984310269355774, 0.0015689348801970482]"""
2132,230391,Set the default Parallelism for Flink execution in FlinkDistributedContext ,Remove the option to set the degree of parallelism at individual operators and set it once in FlinkDistributedContext.  ,smartshark_2_2,1796,mahout,"""[0.9986070990562439, 0.0013929057167842984]"""
2133,230392,Script to auto-generate and view the Mahout website on a local machine ,"Attached with this ticket is a script that creates a simple setup for editing Mahout Website on a local machine.

It is useful in the sense that, we can edit the source and the changes are automatically reflected in the generated site. All we need to do is refresh the browser. No further steps required.

So now one can review the website changes ( the complete website ), on a developer's machine.
",smartshark_2_2,655,mahout,"""[0.9983513355255127, 0.001648646779358387]"""
2134,230393,IDRescorers should also be able to filter candidate items,"Currently when a recommendation for a particular user is computed, at first the candidate items are determined via a CandidateItemsStrategy, then predicitions for all these items are computed and afterwards the IDRescorer can filter the items that might be recommended.

For performance reasons, it should also be possible to apply custom filtering before the estimation is done. This cannot be done in the CandidateItemsStrategy, as the filtering logic might depend on some request/or user-specific criteria that are not known in the recommender application.

It would be possible to implement such a logic in IDRescorer.isFiltered(), this is however bad for performance reasons, as the very costly estimation has already happen then.

I propose to add a method to IDRescorer that can filter the candidate items right before estimation starts.

",smartshark_2_2,475,mahout,"""[0.9985004663467407, 0.001499517704360187]"""
2135,230394,"I can't run the code in ãMahout In Actionã under Mahout 0.7,what's happening ?","I'm reading the book ãMahout In Actionã and i'd like to try the code myself.

But i found lots of the code can't run(Most of the issues are method can't be found).

1ãRandomSeedGenerator.chooseRandomPoints(pointVectors, k); chooseRandomPoints can't be found.

2ãnew Cluster(v, clusterId++, measure); the constructor can't be found.

there are many of these kind of issues.


Can anyone tell me how to find those methods ?",smartshark_2_2,462,mahout,"""[0.9540817141532898, 0.045918308198451996]"""
2136,230395,Please help me im stuck on using 20 newsgroups example on Windows,"Hello there, I've been using hadoop & mahout on my windows OS and I started the hadoop cluster before starting the mahout in order to use the cluster for it, then, I did start the mahout to test the 20newsgroups example but it throws an exception as not a valid DFS filename as show below in details from the beginning :

Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\Admin>cd\

C:\>cd mahout

C:\mahout>cd examples

C:\mahout\examples>cd bin

C:\mahout\examples\bin>classify-20newsgroups.sh
Welcome to Git (version 1.9.4-preview20140815)


Run 'git help git' to display the help index.
Run 'git help <command>' to display help for specific commands.
Please select a number to choose the corresponding task to run
1. cnaivebayes
2. naivebayes
3. sgd
4. clean -- cleans up the work area in /tmp/mahout-work-
Enter your choice : 2
ok. You chose 2 and we'll use naivebayes
creating work directory at /tmp/mahout-work-
+ echo 'Preparing 20newsgroups data'
Preparing 20newsgroups data
+ rm -rf /tmp/mahout-work-/20news-all
+ mkdir /tmp/mahout-work-/20news-all
+ cp -R /tmp/mahout-work-/20news-bydate/20news-bydate-test/alt.atheism /tmp/maho
ut-work-/20news-bydate/20news-bydate-test/comp.graphics /tmp/mahout-work-/20news
-bydate/20news-bydate-test/comp.os.ms-windows.misc /tmp/mahout-work-/20news-byda
te/20news-bydate-test/comp.sys.ibm.pc.hardware /tmp/mahout-work-/20news-bydate/2
0news-bydate-test/comp.sys.mac.hardware /tmp/mahout-work-/20news-bydate/20news-b
ydate-test/comp.windows.x /tmp/mahout-work-/20news-bydate/20news-bydate-test/mis
c.forsale /tmp/mahout-work-/20news-bydate/20news-bydate-test/rec.autos /tmp/maho
ut-work-/20news-bydate/20news-bydate-test/rec.motorcycles /tmp/mahout-work-/20ne
ws-bydate/20news-bydate-test/rec.sport.baseball /tmp/mahout-work-/20news-bydate/
20news-bydate-test/rec.sport.hockey /tmp/mahout-work-/20news-bydate/20news-bydat
e-test/sci.crypt /tmp/mahout-work-/20news-bydate/20news-bydate-test/sci.electron
ics /tmp/mahout-work-/20news-bydate/20news-bydate-test/sci.med /tmp/mahout-work-
/20news-bydate/20news-bydate-test/sci.space /tmp/mahout-work-/20news-bydate/20ne
ws-bydate-test/soc.religion.christian /tmp/mahout-work-/20news-bydate/20news-byd
ate-test/talk.politics.guns /tmp/mahout-work-/20news-bydate/20news-bydate-test/t
alk.politics.mideast /tmp/mahout-work-/20news-bydate/20news-bydate-test/talk.pol
itics.misc /tmp/mahout-work-/20news-bydate/20news-bydate-test/talk.religion.misc
 /tmp/mahout-work-/20news-bydate/20news-bydate-train/alt.atheism /tmp/mahout-wor
k-/20news-bydate/20news-bydate-train/comp.graphics /tmp/mahout-work-/20news-byda
te/20news-bydate-train/comp.os.ms-windows.misc /tmp/mahout-work-/20news-bydate/2
0news-bydate-train/comp.sys.ibm.pc.hardware /tmp/mahout-work-/20news-bydate/20ne
ws-bydate-train/comp.sys.mac.hardware /tmp/mahout-work-/20news-bydate/20news-byd
ate-train/comp.windows.x /tmp/mahout-work-/20news-bydate/20news-bydate-train/mis
c.forsale /tmp/mahout-work-/20news-bydate/20news-bydate-train/rec.autos /tmp/mah
out-work-/20news-bydate/20news-bydate-train/rec.motorcycles /tmp/mahout-work-/20
news-bydate/20news-bydate-train/rec.sport.baseball /tmp/mahout-work-/20news-byda
te/20news-bydate-train/rec.sport.hockey /tmp/mahout-work-/20news-bydate/20news-b
ydate-train/sci.crypt /tmp/mahout-work-/20news-bydate/20news-bydate-train/sci.el
ectronics /tmp/mahout-work-/20news-bydate/20news-bydate-train/sci.med /tmp/mahou
t-work-/20news-bydate/20news-bydate-train/sci.space /tmp/mahout-work-/20news-byd
ate/20news-bydate-train/soc.religion.christian /tmp/mahout-work-/20news-bydate/2
0news-bydate-train/talk.politics.guns /tmp/mahout-work-/20news-bydate/20news-byd
ate-train/talk.politics.mideast /tmp/mahout-work-/20news-bydate/20news-bydate-tr
ain/talk.politics.misc /tmp/mahout-work-/20news-bydate/20news-bydate-train/talk.
religion.misc /tmp/mahout-work-/20news-all
+ '[' 'C:\hadp' '!=' '' ']'
+ '[' '' == '' ']'
+ echo 'Copying 20newsgroups data to HDFS'
Copying 20newsgroups data to HDFS
+ set +e
+ 'C:\hadp/bin/hadoop' dfs -rmr /tmp/mahout-work-/20news-all
/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
rmr: DEPRECATED: Please use 'rm -r' instead.
-rmr: Pathname /C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all from h
dfs://localhost:9000/C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all i
s not a valid DFS filename.
Usage: hadoop fs [generic options] -rmr
+ set -e
+ 'C:\hadp/bin/hadoop' dfs -put /tmp/mahout-work-/20news-all /tmp/mahout-work-/2
0news-all
/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
-put: Pathname /C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all from h
dfs://localhost:9000/C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all i
s not a valid DFS filename.
Usage: hadoop fs [generic options] -put [-f] [-p] <localsrc> ... <dst>
+ echo 'Creating sequence files from 20newsgroups data'
Creating sequence files from 20newsgroups data
+ ./bin/mahout seqdirectory -i /tmp/mahout-work-/20news-all -o /tmp/mahout-work-
/20news-seq -ow
/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
Running on hadoop, using \hadp/bin/hadoop and HADOOP_CONF_DIR=
MAHOUT-JOB: /c/mahout/examples/target/mahout-examples-0.9-job.jar
/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory
14/12/09 21:48:57 INFO common.AbstractJob: Command line arguments: {--charset=[U
TF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.
mahout.text.PrefixAdditionFilter], --input=[C:/Users/Admin/AppData/Local/Temp/ma
hout-work-/20news-all], --keyPrefix=[], --method=[mapreduce], --output=[C:/Users
/Admin/AppData/Local/Temp/mahout-work-/20news-seq], --overwrite=null, --startPha
se=[0], --tempDir=[temp]}
Exception in thread ""main"" java.lang.IllegalArgumentException: Pathname /C:/User
s/Admin/AppData/Local/Temp/mahout-work-/20news-seq from C:/Users/Admin/AppData/L
ocal/Temp/mahout-work-/20news-seq is not a valid DFS filename.
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedF
ileSystem.java:187)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFi
leSystem.java:101)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFil
eSystem.java:1068)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFil
eSystem.java:1064)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkRes
olver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(Distribute
dFileSystem.java:1064)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1398)
        at org.apache.mahout.common.HadoopUtil.delete(HadoopUtil.java:192)
        at org.apache.mahout.common.HadoopUtil.delete(HadoopUtil.java:200)
        at org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFr
omDirectory.java:84)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesF
romDirectory.java:65)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(Progra
mDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:153)
        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)

C:\mahout\examples\bin>

Please help me I'm new to the big data tools and I need this issue resolved as soon as possible.

Thank you,,,",smartshark_2_2,691,mahout,"""[0.9591968655586243, 0.04080318287014961]"""
2137,230396,Finish iterator overhaul by using Google Guava,"To complete this truly interesting overhaul of the use of Iterator -- I rightly guessed that Guava already has some of these common classes and more. Indeed. I can see easily another 1,000 lines of code of savings by adapting to use their iterator primitives. I'm in progress now and have removed 600 lines so far. I think this will go a long way to avoiding subtle and various bugs in the iterator implementations to date. I can definitley get it in before 0.5.",smartshark_2_2,634,mahout,"""[0.9982802867889404, 0.001719772582873702]"""
2138,230397,Use faster shallowCopy for dense matices in blockify drm/package.blockify(..),"In {{sparkbindings.drm/package.blockify(...)}}, after testing the density of an incoming block, use {{DenseMatrix(blockAsArrayOfDoubles, true)}} to shallow copy the backing vector array into the {{DenseMatrix}}.  ",smartshark_2_2,1966,mahout,"""[0.9983100891113281, 0.0016899356851354241]"""
2139,230398,Adding footer note to command line utility,"Hi all,

Since ClusterDumper doesnt seem to have elaborate documentation, just created a page https://cwiki.apache.org/confluence/display/MAHOUT/Cluster+Dumper
While playing around with clusterdump utility, I learned that it can be run on hadoop or as a standalone java program.
As most of you are aware, when executed on hadoop, the seqFileDir and pointsDir should be the HDFS location else the local system path location. Since some of the clustering related wiki pages specified that we can get the output from HDFS and then run clusterdump, I was assuming that the clusterdump would always read data from local FS.

I am not sure if newbies would have this same thought process.. So I was thinking if we'd need to make this explicit by changing the help list of clusterdump
Currently ClusterDumper.java has 
 addOption(SEQ_FILE_DIR_OPTION, ""s"", ""The directory containing Sequence Files for the Clusters"", true);
Should we specify something like
 addOption(SEQ_FILE_DIR_OPTION, ""s"", ""The directory (HDFS if using Hadoop / Local filesystem if on standalone mode) containing Sequence Files for the Clusters"", true);
and so on..
The problem with this approach is itz repetitive in that we'd need to change in quite a few places.. (I believe vectordump also follows the same principle)

or 

should we modify CommandLineUtil to have a generic message in the help specifying the fact that while running hadoop, the directories should reference HDFS location else local FS.
How about adding it to the footer like 
formatter.setFooter(""Specify HDFS directories while running hadoop; else specify local File System directories"");
formatter.printFooter();

Appreciate your feedbacks / thots.

thanks
Joe.

from	Jeff Eastman <jdog@windwardsolutions.com>
reply-to	dev@mahout.apache.org
to	dev@mahout.apache.org
date	Fri, Sep 3, 2010 at 2:45 PM
subject	Re: ClusterDumper - Hadoop or standalone ?
mailed-by	mahout.apache.org
hide details Sep 3 (12 days ago)
- Show quoted text -
+1 to generic message approach
",smartshark_2_2,1222,mahout,"""[0.9985768795013428, 0.0014230911619961262]"""
2140,230399,Make the retrieval of candidate items for the most-similar-items computation customizable,"When retrieving the initial set of possibly similar items in GenericItemBasedRecommender.doMostSimilarItems(...) only co-occurring items are selected. We need an exchangable strategy (similar to the existing CandidateItemsStrategy) in order to make this behavior customizable.
",smartshark_2_2,1336,mahout,"""[0.9985010623931885, 0.0014989180490374565]"""
2141,230400,Proposal for high performance primitive collections.,"A proposal for template-driven collections library (lists, sets, maps, deques), with specializations for Java primitive types to save memory and increase performance. The ""templates"" are regular Java classes written with generics and certain ""intrinsics"", that is blocks replaceable by a regexp-preprocessor. This lets one write the code once, immediately test it (tests are also templates) and generate primitive versions from a single source.

An additional interesting part is the benchmarking subsystem written on top of JUnit ;)

There are major differences from the Java Collections API, most notably no interfaces and interface-compatible views over sub-collections or key/value sets. These classes also expose their internal implementation (buffers, addressing, etc.) so that the code can be optimized for a particular use case.
These motivations are further discussed here, together with an API overview.

http://www.carrot-search.com/download/hppc/index.html

I am curious what you think about it. If folks like it, Carrot Search will donate the code to Mahout (or Apache Commons-?) and will maintain it (because we plan to use it in our internal projects anyway).

",smartshark_2_2,1362,mahout,"""[0.998241662979126, 0.001758391852490604]"""
2142,230401,Clarify some of the messages in Preconditions.checkArgument,"In experimenting with things, I was getting some errors from RowSimilarityJob, that in looking at the source I realized were a little incomplete as to what the true issue was.  In this case, they were of the form:

Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"");

Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's ""incorrect"", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.

A quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.",smartshark_2_2,311,mahout,"""[0.9971240162849426, 0.002875964855775237]"""
2143,230402,"To test hudson, I just made a 'Patch Available' issue.","To test hudson, I just made a 'Patch Available' issue.",smartshark_2_2,1437,mahout,"""[0.9866483807563782, 0.013351579196751118]"""
2144,230403,Driver for cvb0_local does not warn about missing maxIterations command line parameter,"The driver for cvb0_local does not seem to verify whether the caller has specified the required maxIterations command line parameter. This results in an exception much further down which pretty much requires looking at the source to discover the source of the error.

Exception in thread ""main"" java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
	at org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.main2(InMemoryCollapsedVariationalBayes0.java:374)
	at org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.run(InMemoryCollapsedVariationalBayes0.java:521)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.main(InMemoryCollapsedVariationalBayes0.java:525)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)",smartshark_2_2,352,mahout,"""[0.7724619507789612, 0.22753800451755524]"""
2145,230404,Limit the number of similar items per item in the ItemSimilarityJob,"In order to keep the item-similarity-matrix sparse, it would be a useful improvement to add an option like ""maxSimilaritiesPerItem"" to o.a.m.cf.taste.hadoop.similarity.item.ItemSimilarityJob, which would make it try to cap the number of similar items per item.

However as we store each similarity pair only once it could happen that there are more than ""maxSimilaritiesPerItem"" similar items for a single item as we can't drop some of the pairs because the other item in the pair might have too little similarities otherwise.

A default value of 100 co-occurrences (similarities) will be used because this is already the default in the distributed recommender.",smartshark_2_2,1146,mahout,"""[0.9983994364738464, 0.0016005070647224784]"""
2146,230405,"Point formatting and parsing improved (StringBuilder, no need for trailing comma).","Added test case to point class, improved parsing (no need to recompile the pattern all over again) and concatenation of points (stringbuilder used internally).",smartshark_2_2,1525,mahout,"""[0.9979707598686218, 0.0020291930995881557]"""
2147,230406,Unify Vector Writables,"Per the mailing list thread, creating an issue to track patches and discussion of unifying vector writables. The essence of my attempt will be attached.",smartshark_2_2,1130,mahout,"""[0.9984092116355896, 0.0015908351633697748]"""
2148,230407,InMemoryCollapsedVariationalBayes0 should ignore _SUCCESS files,InMemoryCollapsedVariationalBayes0 should ignore _SUCCESS files when reading in the tf-vectors,smartshark_2_2,1625,mahout,"""[0.8943974375724792, 0.10560259968042374]"""
2149,230408,Propagation of Updates in DF,"Given data frame :

C = A + B

If some cells in A or B are updated:
1) calculate C using the latest values in A and B when C is accessed (pull)
2) when A or B is updated then propagate changes to C (push)

You can use different operator for = in the above cases  ",smartshark_2_2,978,mahout,"""[0.9434862732887268, 0.056513696908950806]"""
2150,230409,LDADriver not being initialised properly,"all the clustering drivers under o.a.m.clustering, eg  CanopyDriver, DirichletDriver, FuzzyKMeansDriver  etc
are bootstrapped in main() with ToolRunner

for some reason o.a.m.clustering.lda.LDADriver wasn't being boostrapped like this so job conf was null

found while trying to run LDADriver as part of examples/bin/build-reuters.sh",smartshark_2_2,1231,mahout,"""[0.4343063235282898, 0.5656936764717102]"""
2151,230410,TestNaiveBayesDriver fails in sequential mode,"As reported by Chandler Burgess, testnb fails in sequential mode with exception:

Exception in thread ""main"" java.io.FileNotFoundException: /tmp/mahout-work-andy/20news-train-vectors (Is a directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:120)
	at org.apache.hadoop.fs.RawLocalFileSystem$TrackingFileInputStream.<init>(RawLocalFileSystem.java:71)
{...}	at org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.run(TestNaiveBayesDriver.java:99)
{...}",smartshark_2_2,834,mahout,"""[0.4547934830188751, 0.5452065467834473]"""
2152,230411,"Standardize on java.util.logging, Commons Logging, log4j?","I see the log4j and Commons Logging .jars in the lib/ directory. log4j isn't used; Commons Logging is used in one class (Parametered). My code just used java.util.logging directly.

I figure we should standardize on one approach to logging. I personally think they're all just about the same; the only real best practice is using one system.

I have always just used java.util.logging since it is built into Java 1.4+. Commons Logging offers an extra layer of abstraction and lets you switch between java.util.logging and log4j underneath. That's cool, but I've not found it compelling enough to want to add another layer and another .jar file.

But, I guess log4j is present because hadoop uses it directly? The .jar seems to have a dependency on it.

In that case maybe we are better off using Commons Logging to let us integrate with log4j logging that Hadoop uses, and leave open the possibility of other callers using java.util.logging underneath.

If that's cool I can switch my code to use Commons Logging.",smartshark_2_2,1517,mahout,"""[0.9984052777290344, 0.0015947921201586723]"""
2153,230412,Establish procedure for publishing new website,Figure out how the new website will be published and write up how-to doc,smartshark_2_2,1948,mahout,"""[0.9948888421058655, 0.005111143924295902]"""
2154,230413,Improve Lanczos to handle extremely large feature sets (without hashing),"DistributedLanczosSolver currently keeps all Lanczos vectors in memory on the driver (client) computer while Hadoop is iterating.  The memory requirements of this is (desiredRank) * (numColumnsOfInput) * 8bytes, which for desiredRank = a few hundred, starts to cap out usefulness at some-small-number * millions of columns for most commodity hardware.

The solution (without doing stochastic decomposition) is to persist the Lanczos basis to disk, except for the most recent two vectors.  Some care must be taken in the ""orthogonalizeAgainstBasis()"" method call, which uses the entire basis.  This part would be slower this way.",smartshark_2_2,107,mahout,"""[0.9983136653900146, 0.0016863774508237839]"""
2155,230414,Fix Create Mahout Context after VCL Merge,"While the VCL bindings were under development I commented out a large section of {{getMahoutContext()}} in {{SparkBindings/package.scala}}

This code has been changed slightly so need to make sure it is replaced and working after rebasing and merging VCL. 

I.e.:

{code}
   // context specific jars
        val mcjars = findMahoutContextJars(closeables)

        if (log.isDebugEnabled) {
          log.debug(""Mahout jars:"")
          mcjars.foreach(j => log.debug(j))
        }

        sparkConf.setJars(jars = mcjars.toSeq ++ customJars)
        if (!(customJars.size > 0)) sparkConf.setJars(customJars.toSeq)

      } else {
        // In local mode we don't care about jars, do we?
        sparkConf.setJars(customJars.toSeq

{code}",smartshark_2_2,2005,mahout,"""[0.7146564722061157, 0.2853434681892395]"""
2156,230415,Automatic probing of in-core and back-end solvers,"In general, as we potentially expand the collection of in-core and distributed solvers relying on particular sw/hw capabilities installed (lib blas, viennacl, cuda), it would be nice to have automatic and centralized capability probing  and some sort of registry/framework that enumerates enabled features.
",smartshark_2_2,1998,mahout,"""[0.9983287453651428, 0.0016712680226191878]"""
2157,230416,Streaming KMeans should write centroid output to a 'part-r-xxxx' file when executed in sequential mode,Streaming KMeans presently creates the centroid output in the output file specified when executed in sequential mode. This is inconsistent with the behavior from the MapReduce version. ,smartshark_2_2,281,mahout,"""[0.9757735133171082, 0.024226438254117966]"""
2158,230417,Clusterdumper - Get rid of map based implementation,"Current implementation of ClusterDumper puts clusters and related vectors in map. This generally results in OOM.

Since ClusterOutputProcessor is availabale now. The ClusterDumper will at first process the clusteredPoints, and then write down the clusters to a local file. 

The inability to properly read the clustering output due to ClusterDumper facing OOM is seen too often in the mailing list. This improvement will fix that problem.",smartshark_2_2,537,mahout,"""[0.8492605090141296, 0.15073947608470917]"""
2159,230418,Unify ranking of boolean recommendations in distributed and non-distributed recommenders,"When using a weighted sum for preference estimation on boolean data, the predicted preferences can only be 1 or NaN which is mathematically correct but not very useful for ranking them. The distributed recommender should therefore adapt the behavior of GenericBooleanPrefItemBasedRecommender in that case: use the sums of similarities to rank the recommended items.",smartshark_2_2,1274,mahout,"""[0.9982525706291199, 0.0017474681371822953]"""
2160,230419,CachingUserSimilarity and CachingItemSimilarity have wrong (far to small) default maxSizes,"I am currently tuning my recommender discussed here: http://thread.gmane.org/gmane.comp.apache.mahout.user/10433.

As a first step I wrapped my LogLikelihoodSimilarity with an CachingUserSimilarity. I used Java Visual VM to profile the calls. I recognized that I didn't get any performance benefits. So I had a look into the code.

Actually line 47 this(similarity, dataModel.getNumItems()); in CachingUserSimilarity.java is wrong. If we want to cache all item similarities we need a cache with (dataModel.getNumItems()*(dataModel.getNumItems()-1))/2 possible entries.

I am now doing this in the constructor. I attached a patch to adjust this in the trunk build.",smartshark_2_2,73,mahout,"""[0.6779894828796387, 0.3220105469226837]"""
2161,230420,Make Wikipedia Example Classifier more generic,"It would be nice if the Wikipedia classifier example was a bit more generic instead of taking just countries.  For example, one could classify based on other types of categories, such as things like ""subjects"", i.e. History, Math, Science or other things.",smartshark_2_2,1046,mahout,"""[0.997619092464447, 0.002380834659561515]"""
2162,230421,VectorWritable,Hadoop serialization,smartshark_2_2,1008,mahout,"""[0.9976453185081482, 0.002354652853682637]"""
2163,230422,Ignore the line when input text file contains irregular entry,"RecommenderJob with usersFile/itemsFile which contains newline at end of file is failed.

{code}
java.lang.NumberFormatException: For input string: """"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Long.parseLong(Long.java:431)
	at java.lang.Long.parseLong(Long.java:468)
	at org.apache.mahout.cf.taste.hadoop.item.UserVectorSplitterMapper.setup(UserVectorSplitterMapper.java:61)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:629)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:310)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
{code}

I think lines which cause parse error should be ignored.",smartshark_2_2,13,mahout,"""[0.21356146037578583, 0.786438524723053]"""
2164,230423,Conjugate gradient assumes best case scenario for convergence,"The conjugate gradient solve that we have assumes that it will converge within the theoretical n steps where n is the size of the input.  In fact, convergence may not happen within exactly that number of steps due to numerical issues so we should allow for a few extra iterations.

",smartshark_2_2,846,mahout,"""[0.9433419704437256, 0.05665803328156471]"""
2165,230424,Misleading JavaDoc comment in FPGrowth,"The JavaDoc for org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth.generateTopKFrequentPatterns(...) states that if the parameter returnableFeatures is null then patterns for every frequent item are generated. 

But this works only if you give the method an empty set, submitting null results in a NullPointerException. Either change the comment or do a null check here.",smartshark_2_2,625,mahout,"""[0.9915448427200317, 0.008455160073935986]"""
2166,230425,Framework: Job notification thread does not handle notification service interruptions properly,"When a job notification fails because of a ServiceInterruption, the notification is retried indefinitely.  This is incorrect; the notification retry should at the very least obey the ServiceInterruption specification pertaining to number of retries, retry interval, and failure mode.",smartshark_2_2,1154,manifoldcf,"""[0.14454205334186554, 0.8554579615592957]"""
2167,230426,Zookeeper example is not working - ZooKeeper anonymous services don't seem to work,"This exception gets repeatedly dumped in the log when you try to run a job:

{code}
ERROR 2013-12-11 08:13:10,692 (Job notification thread) - Exception tossed: Serv
ice '_ANON_0' of type '_OUTPUTCONNECTORPOOL_null' is already active
org.apache.manifoldcf.core.interfaces.ManifoldCFException: Service '_ANON_0' of
type '_OUTPUTCONNECTORPOOL_null' is already active
        at org.apache.manifoldcf.core.lockmanager.ZooKeeperLockManager.registerS
erviceBeginServiceActivity(ZooKeeperLockManager.java:157)
        at org.apache.manifoldcf.core.lockmanager.ZooKeeperLockManager.registerS
erviceBeginServiceActivity(ZooKeeperLockManager.java:119)
        at org.apache.manifoldcf.core.connectorpool.ConnectorPool$Pool.<init>(Co
nnectorPool.java:418)
        at org.apache.manifoldcf.core.connectorpool.ConnectorPool.grab(Connector
Pool.java:243)
        at org.apache.manifoldcf.agents.outputconnectorpool.OutputConnectorPool.
grab(OutputConnectorPool.java:83)
        at org.apache.manifoldcf.crawler.system.JobNotificationThread.run(JobNot
ificationThread.java:113)
{code}
",smartshark_2_2,1050,manifoldcf,"""[0.6508889198303223, 0.34911105036735535]"""
2168,230427,SharePoint connector does not work with claims based authentication properly,"When the SharePoint Connector is used against a SharePoint claimspace instance, it fails in the following ways:

(1) The MCPermissions.asmx plugin is unable to write to the log.  ""EventLog.XXX"" is not allowed, apparently, under this configuration option.
(2) It is needing to write to the log, which indicates there is some hidden exception taking place that we aren't seeing.
(3) When this fails, we're getting bad data returned from the list method, which causes ArrayIndexOutOfBoundsException's being thrown in the relative path manipulation code, due to the fact that the library/list name is not at the front of the relative path, e.g.:

{code}
FATAL 2013-07-17 19:24:57,927 (Worker thread '46') - Error tossed: String index out of range: 19
java.lang.StringIndexOutOfBoundsException: String index out of range: 19
    at java.lang.String.substring(String.java:1955)
    at org.apache.manifoldcf.crawler.connectors.sharepoint.SharePointRepository$FileStream.addFile(SharePointRepository.java:1890)
    at org.apache.manifoldcf.crawler.connectors.sharepoint.SPSProxyHelper.getChildren(SPSProxyHelper.java:655)
    at org.apache.manifoldcf.crawler.connectors.sharepoint.SharePointRepository.processDocuments(SharePointRepository.java:1411)
    at org.apache.manifoldcf.crawler.connectors.BaseRepositoryConnector.processDocuments(BaseRepositoryConnector.java:423)
    at org.apache.manifoldcf.crawler.system.WorkerThread.run(WorkerThread.java:559)
{code}

(Regardless of the full resolution of the problem, we should definitely harden the connector against this kind of issue.)


",smartshark_2_2,858,manifoldcf,"""[0.07833363860845566, 0.9216663241386414]"""
2169,230428,Header differences from ManifoldCF 1.0.1 cause some crawls to fail,"Certain sites that were crawlable before with MCF 1.0.1 fail with trunk.  Analysis shows that the differences are due to the Host header (which includes an explicit port in trunk but not in MCF 1.0.1) and an ""Accept: */*"" header in MCF 1.0.1 that is not present in trunk.
",smartshark_2_2,565,manifoldcf,"""[0.19723935425281525, 0.802760660648346]"""
2170,230429,SharePoint 2010 claims-based authorization fails for AD groups,"It looks like, at least in some cases, in SharePoint 2010 it is not SharePoint groups that correspond to AD groups, but rather SharePoint *users* that correspond to AD groups.  For example:

{code}
<soap:Envelope xmlns:soap=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"">
   <soap:Body>
      <GetUserCollectionFromGroupResponse xmlns=""http://schemas.microsoft.com/sharepoint/soap/directory/"">
         <GetUserCollectionFromGroupResult>
            <GetUserCollectionFromGroup>
               <Users>
                  <User ID=""3620"" Sid="""" Name=""Axxx Dxxx"" LoginName=""i:0#.w|domain\dxxx"" Email=""..."" Notes="""" IsSiteAdmin=""False"" IsDomainGroup=""False"" Flags=""0""/>
                  <User ID=""1199"" Sid="""" Name=""itstrain"" LoginName=""i:0#.w|domain\itstrain"" Email=""..."" Notes="""" IsSiteAdmin=""False"" IsDomainGroup=""False"" Flags=""0""/>
                  <User ID=""2871"" Sid="""" Name=""Law Library helpdesk account"" LoginName=""i:0#.w|domain\reflaw"" Email=""..."" Notes="""" IsSiteAdmin=""False"" IsDomainGroup=""False"" Flags=""0""/>
                  <User ID=""5135"" Sid="""" Name=""Library Desk - GP"" LoginName=""i:0#.w|domain\lib-deskgp"" Email="""" Notes="""" IsSiteAdmin=""False"" IsDomainGroup=""False"" Flags=""0""/>
                  <User ID=""5899"" Sid="""" Name=""DOMAIN\$0kjf00-gcsje70g79fm"" LoginName=""c:0+.w|s-1-5-21-3052554794-3770484871-3874881240-511616"" Email="""" Notes="""" IsSiteAdmin=""False"" IsDomainGroup=""True"" Flags=""0""/>
               </Users>
            </GetUserCollectionFromGroup>
         </GetUserCollectionFromGroupResult>
      </GetUserCollectionFromGroupResponse>
   </soap:Body>
</soap:Envelope>
{code}

We therefore need to look at child users of groups to come up with the right tokens.  Furthermore, the SharePoint/AD authority should always generate user tokens, not group tokens.

",smartshark_2_2,1168,manifoldcf,"""[0.08321210741996765, 0.91678786277771]"""
2171,230430,Hopcount logic fails to notice when the max number of hops is increased between crawls,"When you do something like the following:

(1) Set the max hops for a job relatively low
(2) Crawl
(3) Increase the max hops
(4) Crawl again

... the documents that are labeled with the state ""Hop count exceeded"" at the end of the first crawl are never touched again.  This is because there are no additional links added to the intrinsiclink table during the second crawl, and thus the method reactivateHopcountRemovedRecords() is never called, leaving the documents in an incorrect state.",smartshark_2_2,861,manifoldcf,"""[0.0648806244134903, 0.9351193904876709]"""
2172,230431,"Under some conditions, documents are left in PENDINGPURGATORY without a docpriority set","Under some conditions, documents are left in PENDINGPURGATORY without a docpriority set.  The conditions appear to be when the java-agents process is cycled.
",smartshark_2_2,960,manifoldcf,"""[0.0951668918132782, 0.9048330783843994]"""
2173,230432,Multiple To and From Fields are Ignored,Only the last one is considered and remains are ignored when there are multiple To and From fields at e-mails.,smartshark_2_2,795,manifoldcf,"""[0.13324347138404846, 0.8667564988136292]"""
2174,230433,File system crawl with HSQLDB aborts with a constraint error,"While running two jobs with overlapping files with HSQLDB, I got this error on the second job that aborted it:

Error: integrity constraint violation: unique constraint or index violation; SYS_PK_10041 table: INGESTSTATUS

The complete exception is here:

ERROR 2011-08-31 21:07:06,029 (Worker thread '34') - Exception tossed: integrity constraint violation: unique constraint or index violation; SYS_PK_10041 table: INGESTSTATUS
org.apache.manifoldcf.core.interfaces.ManifoldCFException: integrity constraint violation: unique constraint or index violation; SYS_PK_10041 table: INGESTSTATUS
	at org.apache.manifoldcf.core.database.DBInterfaceHSQLDB.reinterpretException(DBInterfaceHSQLDB.java:587)
	at org.apache.manifoldcf.core.database.DBInterfaceHSQLDB.performModification(DBInterfaceHSQLDB.java:607)
	at org.apache.manifoldcf.core.database.DBInterfaceHSQLDB.performUpdate(DBInterfaceHSQLDB.java:242)
	at org.apache.manifoldcf.core.database.BaseTable.performUpdate(BaseTable.java:88)
	at org.apache.manifoldcf.agents.incrementalingest.IncrementalIngester.updateRowIds(IncrementalIngester.java:628)
	at org.apache.manifoldcf.agents.incrementalingest.IncrementalIngester.documentCheckMultiple(IncrementalIngester.java:588)
	at org.apache.manifoldcf.crawler.system.WorkerThread.run(WorkerThread.java:653)
Caused by: java.sql.SQLException: integrity constraint violation: unique constraint or index violation; SYS_PK_10041 table: INGESTSTATUS
	at org.hsqldb.jdbc.Util.sqlException(Util.java:255)
	at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(JDBCPreparedStatement.java:4659)
	at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(JDBCPreparedStatement.java:311)
	at org.apache.manifoldcf.core.database.Database.execute(Database.java:606)
	at org.apache.manifoldcf.core.database.Database$ExecuteQueryThread.run(Database.java:421)
Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; SYS_PK_10041 table: INGESTSTATUS
	at org.hsqldb.error.Error.error(Error.java:134)
	at org.hsqldb.Constraint.getException(Constraint.java:914)
	at org.hsqldb.index.IndexAVL.insert(IndexAVL.java:731)
	at org.hsqldb.persist.RowStoreAVL.indexRow(RowStoreAVL.java:171)
	at org.hsqldb.persist.RowStoreAVLDisk.indexRow(RowStoreAVLDisk.java:169)
	at org.hsqldb.TransactionManagerMVCC.addInsertAction(TransactionManagerMVCC.java:401)
	at org.hsqldb.Session.addInsertAction(Session.java:434)
	at org.hsqldb.Table.insertSingleRow(Table.java:2553)
	at org.hsqldb.StatementDML.update(StatementDML.java:1032)
	at org.hsqldb.StatementDML.executeUpdateStatement(StatementDML.java:541)
	at org.hsqldb.StatementDML.getResult(StatementDML.java:196)
	at org.hsqldb.StatementDMQL.execute(StatementDMQL.java:190)
	at org.hsqldb.Session.executeCompiledStatement(Session.java:1340)
	at org.hsqldb.Session.execute(Session.java:993)
	at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(JDBCPreparedStatement.java:4651)



",smartshark_2_2,207,manifoldcf,"""[0.11019441485404968, 0.8898055553436279]"""
2175,230434,IO exception: No FileSystem for scheme: hdfs,"Exception occurs in HDFS Connector.
Connection status:	Connection temporarily failed: IO exception: No FileSystem for scheme: hdfs
",smartshark_2_2,1139,manifoldcf,"""[0.17582134902477264, 0.8241786360740662]"""
2176,230435,Redirection fails on SharePoint connector,"The SharePoint connector uses httpclient's DefaultRedirectStrategy, but uses POST operations, so this has no effect.  It needs to use LaxRedirectStrategy instead.
",smartshark_2_2,914,manifoldcf,"""[0.09855490177869797, 0.9014450907707214]"""
2177,230436,Not all SharePoint Metadata Fields are returned - 2,"Hello Karl,

This is a follow up ticket for [1284|https://issues.apache.org/jira/browse/CONNECTORS-1284].
There is still a problem with getting meta-data from sharepoint lists.

E.g. I'm missing the ""title"" and ""description"" fields in the result documents.

Let's take the ""title"" field from a list document:
{code:xml|title=DEBUG: SharePoint: getFieldList xml response:}
...
    <ns1:Field ID=""{fa564e0f-0c70-4ab9-b863-0177e6ddd247}"" Type=""Text"" Name=""Title""
    DisplayName=""Task Name"" Required=""TRUE"" SourceID=""http://schemas.microsoft.com/sharepoint/v3""
    StaticName=""Title"" FromBaseType=""TRUE"" Sealed=""TRUE"" ColName=""nvarchar1"" />
...
{code}

The field {{Name}} is the internal (technical) field name, and {{DisplayName}} is the frontend (user-friendly) name. 
The connector maps {{Name}} to {{DisplayName}} when it is preparing the request:
{code:java|title=SharePointRepository.java}
            for (String field : fieldNames.keySet())
            {
              String value = fieldNames.get(field);
              fields[j++] = (value==null)?field:value;
{code}

Changing the last two lines to:
{code:java}
              fields[j++] = field;
{code}
solves the problem.
I doubt, the {{DisplayName}} should be used at all, as it can contain non-ascii chars, e.g.:
{code:xml}
...
<ns1:Field ID=""{fa564e0f-0c70-4ab9-b863-0177e6ddd247}"" Type=""Text"" Name=""Title"" DisplayName=""&#xDC;berschrift"" ...
{code}

Currently, only fields with the same Name/DisplayName values can be indexed.
Could you please look into this?
Thank you!
",smartshark_2_2,699,manifoldcf,"""[0.2035219520330429, 0.7964780330657959]"""
2178,230437,No way to reset scan time for documents; restarting a job does not always do it,"When a job is restarted, all outstanding document scheduling should be reset.  Unfortunately, that is not the case now; documents still adhere to formerly determined schedules despite changes to those parameters.
",smartshark_2_2,1054,manifoldcf,"""[0.6302226781845093, 0.36977729201316833]"""
2179,230438,SharePoint connector does not authenticate properly against some SharePoint instances,"The SharePoint connector does not always authenticate against the SharePoint instance.  The problem may be a bug in commons-httpclient, and a corresponding problem in httpcomponents/httpclient.  cURL authenticates just fine, and even when all headers are changed to be identical to what MCF is sending, it continues to authenticate properly.
",smartshark_2_2,529,manifoldcf,"""[0.11603997647762299, 0.8839600086212158]"""
2180,230439,Insert Here button doesn't work,"In Paths Tab of Job, ""Insert Here"" button doesn't work.",smartshark_2_2,512,manifoldcf,"""[0.24325144290924072, 0.7567485570907593]"""
2181,230440,ElasticSearch connector and OpenSearchServer connector both exclude all null extensions,"Both connectors skip blank lines in the extension list, and thus prohibit URLs that have no extension.",smartshark_2_2,669,manifoldcf,"""[0.19566284120082855, 0.804337203502655]"""
2182,230441,"Pipeline does not handle ""no document"" case properly","The incremental ingester and its pipeline code, and indeed transformation connectors themselves, do not handle the ""no document"" case properly.

What should happen is that whenever a document is not retransmitted into a branch of the pipeline, the document version should nevertheless be recorded in the ingeststatus table for the corresponding output.  But transformation connectors can interfere with that transmission if they decide simply to not index an incoming document.  They may also receive a null RepositoryDocument into the transformation method, and will be unequipped to handle that.

Instead, I propose the following:
(1) An explicit IIncrementalIngester method needs to be invented for handling this case
(2) The activity class used for transformation methods should have a ""noDocument()"" method
(3) IncrementalIngester needs to propagate ""noDocument"" method calls down the pipeline without calling transformations on them

",smartshark_2_2,1266,manifoldcf,"""[0.07834504544734955, 0.9216548800468445]"""
2183,230442,Handling of pool closing is not resilient against multiple closes,"When you close a connectorpool, it doesn't properly notice if it's already been closed.  This is an issue when running under jetty, because multiple wars share the same pool, and each one tries to close it.
",smartshark_2_2,1317,manifoldcf,"""[0.10922105610370636, 0.8907789587974548]"""
2184,230443,Cannot add Livelink folder to job,"There was a bug in LivelinkConnector.java since version 1.8, when I try to add a Livelink folder in Path tab, it was breaking and today I found that this was because, you missed seqPrefix+ in line no 3265, Please consider changing this in the next release.
Now since, I became more familiar with ManifoldCF architecture, I may be try to find more bug in livelink connector here on.

https://github.com/apache/manifoldcf/pull/3/files",smartshark_2_2,1472,manifoldcf,"""[0.12588399648666382, 0.8741160035133362]"""
2185,230444,"Livelink connector: Metadata ""path"" attribute name is lost by UI","Fill in the path name, and save the job.  The path name is lost and doesn't show up on the view page.
",smartshark_2_2,728,manifoldcf,"""[0.30959153175354004, 0.69040846824646]"""
2186,230445,Correct click wheel on the web interface,"Hello,
If you use on the click wheel on the link on the ManifoldCF interface to open a new tab, you lost the CSS.

Thanks",smartshark_2_2,1093,manifoldcf,"""[0.4903976023197174, 0.509602427482605]"""
2187,230446,JDBC connector can leak connection handles,"The JDBC connector seems to be able to leak JDBC connection handles under some conditions.
",smartshark_2_2,576,manifoldcf,"""[0.08696027845144272, 0.9130396842956543]"""
2188,230447,JobResetThread not working properly for multiple Agents processes,"JobResetThread seems to expect that it is the only thread moving documents from the various states it deals with to the particular result states it targets.  Unfortunately, this is not true in multi-agents setups.  Either a write lock needs to go around jobManager.finishJobStops(), or jobManager.finishJobStops() needs to create a database transaction.",smartshark_2_2,1165,manifoldcf,"""[0.07257477939128876, 0.9274252653121948]"""
2189,230448,Error during CMIS ingestion of nodes without binary content,"In case of some nodes in the CMIS repository don't have binary stream, the connector returns:
{code}
FATAL 2012-09-13 10:05:14,961 (Worker thread '1') - Error tossed: null
java.lang.NullPointerException
	at org.apache.manifoldcf.crawler.connectors.cmis.CmisRepositoryConnector.processDocuments(CmisRepositoryConnector.java:1070)
	at org.apache.manifoldcf.crawler.connectors.BaseRepositoryConnector.processDocuments(BaseRepositoryConnector.java:423)
	at org.apache.manifoldcf.crawler.system.WorkerThread.run(WorkerThread.java:551)
{code}",smartshark_2_2,484,manifoldcf,"""[0.06254882365465164, 0.9374511241912842]"""
2190,230449,SolrCloud connector Unhandled SolrServerException: java.lang.NullPointerException,"I successful configure Solr standalone server as output connector but when I change to SolrCloud (and configure zookeeper) there is an error:

[Thread-2025] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zoo1.dev.kdm.wcss.pl:2181,solr-1.dev.kdm.wcss.pl:2181,solr-2.dev.kdm.wcss.pl:2181 sessionTimeout=60000 watcher=org.apache.solr.common.cloud.ConnectionManager@4499dfc
[Thread-2025] INFO org.apache.solr.common.cloud.ConnectionManager - Waiting for client to connect to ZooKeeper
[Thread-2025-SendThread(solr-2.foo.org:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server solr-2.dev.kdm.wcss.pl/10.26.26.19:2181. Will not attempt to authenticate using SASL (unknown error)
[Thread-2025-SendThread(solr-2.foo.org:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established to solr-2.foo.org/10.26.26.19:2181, initiating session
[Thread-2025-SendThread(solr-2.foo.org:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server solr-2.dev.kdm.wcss.pl/10.26.26.19:2181, sessionid = 0x347ee653963002d, negotiated timeout = 40000
[Thread-2025-EventThread] INFO org.apache.solr.common.cloud.ConnectionManager - Watcher org.apache.solr.common.cloud.ConnectionManager@4499dfc name:ZooKeeperConnection Watcher:zoo1.foo.org:2181,solr-1.foo.org:2181,solr-2.foo.org:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None
[Thread-2025] INFO org.apache.solr.common.cloud.ConnectionManager - Client is connected to ZooKeeper
[Thread-2025] INFO org.apache.solr.common.cloud.ZkStateReader - Updating cluster state from ZooKeeper... 
org.apache.manifoldcf.core.interfaces.ManifoldCFException: Unhandled SolrServerException: java.lang.NullPointerException
        at org.apache.manifoldcf.agents.output.solr.HttpPoster.handleSolrServerException(HttpPoster.java:351)
        at org.apache.manifoldcf.agents.output.solr.HttpPoster.checkPost(HttpPoster.java:636)
        at org.apache.manifoldcf.agents.output.solr.SolrConnector.check(SolrConnector.java:433)
        at org.apache.jsp.viewoutput_jsp._jspService(viewoutput_jsp.java:305)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:432)
        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:390)
        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:334)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
        at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java:749)
        at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:487)
        at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:412)
        at org.apache.catalina.core.ApplicationDispatcher.forward(ApplicationDispatcher.java:339)
        at org.apache.jasper.runtime.PageContextImpl.doForward(PageContextImpl.java:746)
        at org.apache.jasper.runtime.PageContextImpl.forward(PageContextImpl.java:716)
        at org.apache.jsp.execute_jsp._jspService(execute_jsp.java:1028)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:432)
        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:390)
        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:334)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)
        at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)
        at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:931)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
        at org.apache.catalina.valves.RequestFilterValve.process(RequestFilterValve.java:305)
        at org.apache.catalina.valves.RemoteAddrValve.invoke(RemoteAddrValve.java:83)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)
        at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
        at org.apache.tomcat.util.net.AprEndpoint$SocketProcessor.run(AprEndpoint.java:1822)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
",smartshark_2_2,1304,manifoldcf,"""[0.08279518038034439, 0.917204737663269]"""
2191,230450,Possible leakage of database connection handles,"Reports of a leak of connection handles, shown because ManifoldCF winds up waiting to get a connection, and failing.

The database in question so far always seems to be PostgreSQL, FWIW, and it has been proven that there is a connection handle leak, so that all the crawler threads are eventually waiting for non-existent connection handles.

",smartshark_2_2,592,manifoldcf,"""[0.09599491953849792, 0.9040050506591797]"""
2192,230451,Solr connector gets socket timeouts on slow documents,"The Solr connector fails on some documents with the following exception.

{code}
                ERROR 2013-01-11 11:13:59,372 (Worker thread '36') - Exception tossed: Repeated service interruptions - failure processing document: Software caused connection abort: recv failed
org.apache.manifoldcf.core.interfaces.ManifoldCFException: Repeated service interruptions - failure processing document: Software caused connection abort: recv failed
                at org.apache.manifoldcf.crawler.system.WorkerThread.run(WorkerThread.java:585)
Caused by: java.net.SocketException: Software caused connection abort: recv failed
                at java.net.SocketInputStream.socketRead0(Native Method)
                at java.net.SocketInputStream.read(Unknown Source)
                at java.net.SocketInputStream.read(Unknown Source)
                at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:166)
                at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:90)
                at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:281)
                at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:92)
                at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:61)
                at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:254)
                at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:289)
                at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:252)
                at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:191)
                at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:300)
                at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:127)
                at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:716)
                at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:521)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)
                at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:352)
                at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)
                at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)
                at org.apache.manifoldcf.agents.output.solr.HttpPoster$IngestThread.run(HttpPoster.java:742)
{code}
",smartshark_2_2,572,manifoldcf,"""[0.09252681583166122, 0.9074732065200806]"""
2193,230452,Solr connector: changing the collection name does not force reindexing,"The collection name is not included in the output version string, so changing it does not cause reindexing to be automatic.
",smartshark_2_2,1492,manifoldcf,"""[0.22368556261062622, 0.776314377784729]"""
2194,230453,Creating database in PostgreSQL does not work,"When you try to create a PostgreSQL database, you get back the following error:

Exception: Database exception: Exception doing query: ERROR: syntax error at or near ""$1""
org.apache.manifoldcf.core.interfaces.ManifoldCFException: Database exception: Exception doing query: ERROR: syntax error at or near ""$1""
	at org.apache.manifoldcf.core.database.Database.executeViaThread(Database.java:461)
	at org.apache.manifoldcf.core.database.Database.executeUncachedQuery(Database.java:505)
	at org.apache.manifoldcf.core.database.Database$QueryCacheExecutor.create(Database.java:1131)
	at org.apache.manifoldcf.core.cachemanager.CacheManager.findObjectsAndExecute(CacheManager.java:144)
	at org.apache.manifoldcf.core.database.Database.executeQuery(Database.java:168)
	at org.apache.manifoldcf.core.database.DBInterfacePostgreSQL.createUserAndDatabase(DBInterfacePostgreSQL.java:536)
	at org.apache.manifoldcf.core.system.ManifoldCF.createSystemDatabase(ManifoldCF.java:656)
	at org.apache.manifoldcf.jettyrunner.ManifoldCFJettyRunner.main(ManifoldCFJettyRunner.java:202)
Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""$1""
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:1548)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1316)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:191)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:452)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:351)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:305)
	at org.apache.manifoldcf.core.database.Database.execute(Database.java:606)
	at org.apache.manifoldcf.core.database.Database$ExecuteQueryThread.run(Database.java:421)

",smartshark_2_2,103,manifoldcf,"""[0.123541459441185, 0.8764585852622986]"""
2195,230454,ElasticSearch delete function always returns ERROR status on deletion,"When viewed in the Simple History, all deleted documents from ElasticSearch return ERROR with no description.
",smartshark_2_2,600,manifoldcf,"""[0.15197205543518066, 0.8480280041694641]"""
2196,230455,Index out of bounds exception while crawling web documents,"FATAL 2012-09-28 11:42:32,112 (Worker thread '29') - Error tossed: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
",smartshark_2_2,503,manifoldcf,"""[0.07087703794240952, 0.9291229844093323]"""
2197,230456,ReprioritizationTracker should be implemented using the Interface/Factory paradigm,"The ReprioritizationTracker does not need to be instantiated more than once per thread.  Using the interface/factory paradigm instead would make the most sense.
",smartshark_2_2,1041,manifoldcf,"""[0.9984439015388489, 0.0015561360633000731]"""
2198,230457,Add mode to use null content if chromed content not found to the RSS connector,I have a public RSS feed on an intranet that lists important bookmarks. The list has many external links in it. So ManifoldCF would need to know when to use the company's proxy to index the external links.,smartshark_2_2,563,manifoldcf,"""[0.9984920024871826, 0.0015079458244144917]"""
2199,230458,"We need weekly, or manual, ManifoldCF project site builds and publishes","We need weekly, or manual, ManifoldCF project site builds and publishes.  See CONNECTORS-88 for a discussion of the process we'd need.",smartshark_2_2,1329,manifoldcf,"""[0.9983229041099548, 0.0016771237133070827]"""
2200,230459,add username mappers functionality,"If we deal with many auth sources there will be for sure need to map provided username to user id specific for particular auth source. As I remember - in book there is a note that this is the auth connector responsibility, but it means that in different scenarios we need different connectors (for example: in one scenario provided username is e-mail, in other - just login. You cannot use same auth connector in both cases).

we should be able to configure ""username mapper"" for auth connector which may be simple regexp mapper or custom class providing mapping interface with more complex logic.",smartshark_2_2,832,manifoldcf,"""[0.9985049962997437, 0.001495069358497858]"""
2201,230460,Work on removing HttpClient 4.3 deprecation warnings,"HttpClient 4.3 deprecates pretty much the entire way you used to build httpclient instances; fixing this requires a lot of work in ManifoldCF.

",smartshark_2_2,1192,manifoldcf,"""[0.9983184337615967, 0.001681632362306118]"""
2202,230461,"Add hooks to output connectors for receiving event notifications, specifically job start, job end, etc.","Currently there is no logic that informs an output connection of a job start, end, deletion, or other activity.  While this would seem to have little to do with an output connector, this feature has been requested by Jack Krupansky as a potential way of deciding when to tell Solr to commit documents, rather than leave it up to Solr's configuration.
",smartshark_2_2,58,manifoldcf,"""[0.9983999133110046, 0.0016001071780920029]"""
2203,230462,Testing hierarchy needs work,"The testing hierarchy is in need of some work.  Specifically:
- Load tests should be pulled back into the ""tests"" directory, if possible, and given a suffix (LT?) which allows Maven to skip them by default.  This will simplify the ant build.xml too.
- Final ant targets (test-dr, load-dr, UI, etc.) need to be regularized.  They all should begin with ""test"", for one thing.  I don't yet have a proposal though.
- There's a lot of code duplication in the hierarchy and across the different database base classes.  For example, methods that talk with the API appear in the base class for each database, and could be shared if there was such a thing as multiple inheritance.  Since there isn't we could instead pull these into their own support class, like HTMLTester does for the UI stuff.
",smartshark_2_2,297,manifoldcf,"""[0.998237133026123, 0.0017629115609452128]"""
2204,230463,Update WebCrawler Connector HTML String to Velocity Template,"Hi,

I am currently in the process of converting the stringified HTML to Velocity template for WebCrawler connector. This is the first attempt in updating all the connectors to use velocity template.

I don't know when we are planning for the code freeze but will try to complete by next week.

Thanks,
Kishore Kumar
",smartshark_2_2,876,manifoldcf,"""[0.998529314994812, 0.0014706481015309691]"""
2205,230464,Localization data in the wrong place,"The localization data for the Crawler UI is currently put in the core jar.  It should really be moved to the ui-core jar.
",smartshark_2_2,288,manifoldcf,"""[0.9979645013809204, 0.0020355512388050556]"""
2206,230465,Initial implementation of the CMIS Output Connector,"After we have discussed about the possibility to have some output connector dedicated to migrate contents from a specific repo to another one, this is the first implementation for the CMIS protocol.

The discussion is the following:
http://mail-archives.apache.org/mod_mbox/manifoldcf-dev/201611.mbox/%3CCAEO2op-bjNv4xSTPwGN%3DV2v47Sy8d%2BwKNtd1RpV2PC85y_JAgw%40mail.gmail.com%3E

The main scenario is migrate contents from any repository type to a CMIS-compliant repo.",smartshark_2_2,889,manifoldcf,"""[0.9983816146850586, 0.0016183608677238226]"""
2207,230466,Notification implementation is clunky because it does not change the state of jobs entering notification,"The notification phase of crawling is gated by the Job Notification Thread, which does not actually change the state of the jobs undergoing notification.  This is a potential future problem because there may someday be more than one thread doing it.
",smartshark_2_2,99,manifoldcf,"""[0.8261275887489319, 0.17387239634990692]"""
2208,230467,Maven build needs to be conditionalized in order to handle proprietary connectors,"The maven build currently does not handle proprietary connectors, but should.  This involves figuring out how to conditionalize it.
",smartshark_2_2,1325,manifoldcf,"""[0.9982036352157593, 0.0017963629215955734]"""
2209,230468,"Add CLI options to pipeline modules, e.g. allow Tika to export TEXT, not BASE64","Would love to have Tika spout TEXT, not BASE64.",smartshark_2_2,887,manifoldcf,"""[0.9983141422271729, 0.0016858676681295037]"""
2210,230469,We need a way of selecting documents based on parent folder,"We need a way of selecting documents via parent folder.
",smartshark_2_2,105,manifoldcf,"""[0.9982731342315674, 0.0017268978990614414]"""
2211,230470,FileConnector should skip access denied files,"There are permission controlled files that are denied to access on Windows, 
then FileConnector throws IO Exception ""ã¢ã¯ã»ã¹ãæå¦ããã¾ãã"" (Access denied) 
and this job has stop (not normally end).
I'd like to skip those files during crawling.",smartshark_2_2,857,manifoldcf,"""[0.917320191860199, 0.08267979323863983]"""
2212,230471,"Solr output connector option to commit at end of job, by default","By default, Solr will eventually commit documents that have been submitted to the Solr Cell interface, but the time lag can confuse and annoy people. Although commit strategy is a difficult issue in general, an option in LCF to automatically commit at the end of a job, by default, would eliminate a lot of potential confusion and generally be close to what the user needs.

The desired feature is that there be an option to commit for each job that uses the Solr output connector. This option would default to ""on"" (or a different setting based on some global configuration setting), but the user may turn it off if commit is only desired upon completion of some jobs.",smartshark_2_2,70,manifoldcf,"""[0.9984986782073975, 0.00150134670548141]"""
2213,230472,"When SolrJ 4.7.0 is available, switch to HttpClient 4.3.x","Several fixes are available in httpclient 4.3.x that are not available in httpclient 4.2.x, but since SolrJ is not going to 4.3.x until the 4.7.0 release, we've got to wait for Solr to catch up.
",smartshark_2_2,1182,manifoldcf,"""[0.9985270500183105, 0.0014728751266375184]"""
2214,230473,Tests and test infrastructure needed for Documentum connector,"We need tests and testing infrastructure that will allow the Documentum connector to be tested.
",smartshark_2_2,855,manifoldcf,"""[0.9976123571395874, 0.002387682907283306]"""
2215,230474,Break up jira connector url into components in the configuration and the UI,"All of our connectors allow URLs to be specified as components, e.g protocol, host, port, path.  This seems like extra work but it prevents problems of people entering illegal URLs in the UI.",smartshark_2_2,837,manifoldcf,"""[0.9984404444694519, 0.0015595610020682216]"""
2216,230475,Update all pom.xml files on trunk to specify a version of 0.6,"The pom.xml files must all be updated for the new version, 0.6.
",smartshark_2_2,390,manifoldcf,"""[0.9978679418563843, 0.0021320893429219723]"""
2217,230476,Maven build broken on SharePoint Connector,"Trying to execute the maven build, it returns the following exception:
{code}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 23.103s
[INFO] Finished at: Mon Sep 24 15:53:10 CEST 2012
[INFO] Final Memory: 9M/486M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:axistools-maven-plugin:1.4:wsdl2java (default) on project mcf-sharepoint-connector: Execution default of goal org.codehaus.mojo:axistools-maven-plugin:1.4:wsdl2java failed: A required class was missing while executing org.codehaus.mojo:axistools-maven-plugin:1.4:wsdl2java: org/apache/axis/wsdl/WSDL2Java
[ERROR] -----------------------------------------------------
[ERROR] realm =    plugin>org.codehaus.mojo:axistools-maven-plugin:1.4
[ERROR] strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy
[ERROR] urls[0] = file:/Users/piergiorgiolucidi/.m2/repository/org/codehaus/mojo/axistools-maven-plugin/1.4/axistools-maven-plugin-1.4.jar
[ERROR] urls[1] = file:/Users/piergiorgiolucidi/.m2/repository/org/codehaus/plexus/plexus-compiler-api/1.5.3/plexus-compiler-api-1.5.3.jar
[ERROR] urls[2] = file:/Users/piergiorgiolucidi/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jar
[ERROR] urls[3] = file:/Users/piergiorgiolucidi/.m2/repository/org/codehaus/plexus/plexus-utils/1.5.6/plexus-utils-1.5.6.jar
[ERROR] urls[4] = file:/Users/piergiorgiolucidi/.m2/repository/axis/axis/1.4/axis-1.4.jar
[ERROR] Number of foreign imports: 1
[ERROR] import: Entry[import  from realm ClassRealm[maven.api, parent: null]]
[ERROR] 
[ERROR] -----------------------------------------------------: org.apache.axis.wsdl.WSDL2Java
{code}",smartshark_2_2,497,manifoldcf,"""[0.9656482934951782, 0.03435174748301506]"""
2218,230477,Update Solr-3.x and Solr-4.x plugin versions included with MCF,"Update Solr-3.x and Solr-4.x plugin versions included with MCF.
",smartshark_2_2,1184,manifoldcf,"""[0.9982717037200928, 0.001728322240523994]"""
2219,230478,All messages in Filesystem connector should be extracted out to properties file,"To fully support I18N, all messages should be extracted out to properties file.",smartshark_2_2,310,manifoldcf,"""[0.9976043105125427, 0.002395733492448926]"""
2220,230479,"Default database name, user name, and password have MetaCarta branding, and should be changed","The default database name, database user name, and database password are specific to MetaCarta and should be changed to something more appropriate to Apache.
",smartshark_2_2,4,manifoldcf,"""[0.9981932044029236, 0.001806753920391202]"""
2221,230480,New UI doesn't handle connector javascript form checking properly,"When a connector has javascript that checks for Configuration form legality via the checkConfig() method, switching tabs causes the UI to get confused.  Sequence to reproduce:

(1) Create or edit a connector configuration with a checkConfig() method
(2) Click on a tab that has some checking
(3) Modify a field so that it is illegal
(4) Click on another tab

- The proper error message is indeed displayed
- The user clicks ""OK"" to cancel the alert
- The proper tab continues to be displayed
- The *selected* tab changes, and it *should not*

If you then click ""Save"", you will be incorrectly told that there is another connection of the same name.

You can try this easily by creating a SharePoint connection, going to the Server tab, and supplying a non-integer for the port number. Then click on the Throttling tab and see what happens.
",smartshark_2_2,964,manifoldcf,"""[0.10326524078845978, 0.8967347145080566]"""
2222,230481,LockClean should support relative path,"About a synchronization directory, 
LockManager supports both absolute and relative path.
But LockClean supports absolute path only.
Directory support of LockClean should be the same as LockManager.",smartshark_2_2,155,manifoldcf,"""[0.9978340268135071, 0.002165983896702528]"""
2223,230482,Velocity params in Open Search Server Connector,I see opensearchserver is the last connector that uses the custom script in the html pages instead of velocity. I updated it with velocity as for the other projects.,smartshark_2_2,369,manifoldcf,"""[0.7052145600318909, 0.2947854697704315]"""
2224,230483,Update Livelink Connector HTML String to Velocity Template,"Hi [~kwright@metacarta.com]

I would like to contribute to Livelink connector by replacing all stringified html into velocity templates, if you think that is possible.",smartshark_2_2,1486,manifoldcf,"""[0.9983737468719482, 0.0016262418357655406]"""
2225,230484,It is possible to create a new connection of the same name as an existing one,"If you try to create a new connection that has the same name as an existing connection, the connection saves fine, but completely replaces the original connection.  This is bad for many reasons, but one of them is that the type of the connection might change out from under a job, causing carnage of a major sort.
",smartshark_2_2,65,manifoldcf,"""[0.2952564060688019, 0.7047435641288757]"""
2226,230485,web crawler feed into solr with all html tag removed,"All html tags are removed when manifoldCF feeds the content into Solr

I am new for Solr. I use manifoldcf crawling webpages and send the content into solr for indexing. I found that all the html tags are removed when I get query result from solr. I am not sure if manifoldcf removed them before sending to solr or solr removed them. 

p.s. As I could not find a way to send email to the user list, so I open a ticket here.

Appreciate any suggestions/comments.

Gene",smartshark_2_2,513,manifoldcf,"""[0.30846741795539856, 0.691532552242279]"""
2227,230486,SharePoint connector on SP2010 throws exception when there are too many documents in a library,"When there are more than the document list limit set by the administrator, no documents for the library are crawled. Instead the following exception is thrown:

{code}
DEBUG 2012-07-16 23:58:04,036 (Worker thread '19') - Mapping Exception to AxisFault
AxisFault
 faultCode: {http://schemas.xmlsoap.org/soap/envelope/}Server
 faultSubcode: 
 faultString: Exception of type 'Microsoft.SharePoint.SoapServer.SoapServerException' was thrown.
 faultActor: 
 faultNode: 
 faultDetail: 
	{http://schemas.microsoft.com/sharepoint/soap/}errorstring:The attempted operation is prohibited because it exceeds the list view threshold enforced by the administrator.
	{http://schemas.microsoft.com/sharepoint/soap/}errorcode:0x80070024

Exception of type 'Microsoft.SharePoint.SoapServer.SoapServerException' was thrown.
	at org.apache.axis.message.SOAPFaultBuilder.createFault(SOAPFaultBuilder.java:222)
	at org.apache.axis.message.SOAPFaultBuilder.endElement(SOAPFaultBuilder.java:129)
	at org.apache.axis.encoding.DeserializationContext.endElement(DeserializationContext.java:1087)
	at org.apache.xerces.parsers.AbstractSAXParser.endElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanEndElement(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.parse(Unknown Source)
	at org.apache.axis.encoding.DeserializationContext.parse(DeserializationContext.java:227)
	at org.apache.axis.SOAPPart.getAsSOAPEnvelope(SOAPPart.java:696)
	at org.apache.axis.Message.getSOAPEnvelope(Message.java:435)
	at org.apache.axis.handlers.soap.MustUnderstandChecker.invoke(MustUnderstandChecker.java:62)
	at org.apache.axis.client.AxisClient.invoke(AxisClient.java:206)
	at org.apache.axis.client.Call.invokeEngine(Call.java:2784)
	at org.apache.axis.client.Call.invoke(Call.java:2767)
	at org.apache.axis.client.Call.invoke(Call.java:2443)
	at org.apache.axis.client.Call.invoke(Call.java:2366)
	at org.apache.axis.client.Call.invoke(Call.java:1812)
	at com.microsoft.schemas.sharepoint.soap.ListsSoapStub.getListItems(ListsSoapStub.java:1841)
	at org.apache.manifoldcf.crawler.connectors.sharepoint.SPSProxyHelper.getDocuments(SPSProxyHelper.java:629)
	at org.apache.manifoldcf.crawler.connectors.sharepoint.SharePointRepository.processDocuments(SharePointRepository.java:909)
	at org.apache.manifoldcf.crawler.connectors.BaseRepositoryConnector.processDocuments(BaseRepositoryConnector.java:423)
	at org.apache.manifoldcf.crawler.system.WorkerThread.run(WorkerThread.java:561)
DEBUG 
{code}
",smartshark_2_2,478,manifoldcf,"""[0.08995740115642548, 0.9100425839424133]"""
2228,230487,Alfresco Connector has unresolvable dependencies in POM,"The Alfresco connector's POM has dependencies for org.opensaml:opensaml:1.0.1 and xml-security:xmlsec:1.4.1
These artifacts are not in the Maven Central Repository nor the Alfresco public repository (alfresco-public from the POM).
The versions org.opensaml:opensaml:1.1 and xml-security:xmlsec:1.4.2 are available. 
Can't we sue these versions instead. I don't have an Alfresco installation at hand. unfortunately I cannot test if it works. It compiles without problems though.",smartshark_2_2,294,manifoldcf,"""[0.9848660230636597, 0.015134025365114212]"""
2229,230488,Add support for SharePoint 2013,"SharePoint 2013 has RTM'd, so it is time to start figuring out how to support it.
",smartshark_2_2,1135,manifoldcf,"""[0.997925877571106, 0.0020741564221680164]"""
2230,230489,UpdateProcessor SolrCloud and ManifoldCF,"Hello,
[Link to Apache mail archive|http://mail-archives.apache.org/mod_mbox/manifoldcf-user/201804.mbox/%3C079e01d3d7da%24807b8f60%248172ae20%24%40citya.com%3E]

When we used Argument option in ManifoldCF for SolrCloud, ManifoldCF add they arguments on the POST request and not on the url parameters. So, for add a (pre)processor or a post-processor with the url, it's not possible.

[SolrConfig updateRequestProcessorChain|https://lucene.apache.org/solr/guide/6_6/config-api.html#ConfigAPI-Whatabout_updateRequestProcessorChain_]

[call UpdateRequestProcessors|https://lucene.apache.org/solr/guide/6_6/update-request-processors.html#UpdateRequestProcessors-Processor_Post-ProcessorRequestParameters]

[Conf image|https://image.ibb.co/cZC8bn/jira_update_processor.png]
Solr response:
org.apache.solr.common.SolrException: ERROR: [doc=file://///srvics01/ways_holding/gestion_ged/gerance/3573/201102081135_ENVOIDEVISPP.doc] unknown field 'processor'",smartshark_2_2,949,manifoldcf,"""[0.8345271944999695, 0.16547280550003052]"""
2231,230490,Automatic HttpClient retries should be disabled everywhere,"HttpClient retries are both unnecessary, and also problematic when they occur on output.  Disabling them everywhere as a general policy seems like a good idea.",smartshark_2_2,569,manifoldcf,"""[0.9981471300125122, 0.0018528539221733809]"""
2232,230491,"It would be better if the lib distribution contained jars without being under ""lib""","Since the lib distribution should contain only what's under lib, we should just ship the contents of lib, without insisting it's in a folder 'lib' too.
",smartshark_2_2,402,manifoldcf,"""[0.997740626335144, 0.002259371802210808]"""
2233,230492,Add mimetype and/or filename support to Alfresco connector,"Add mimetype and/or filename support to Alfresco connector.  The appropriate methods are:

RepositoryDocument.setFileName(...)
and
RepositoryDocument.setMimeType(...)
",smartshark_2_2,617,manifoldcf,"""[0.9984536170959473, 0.0015464136376976967]"""
2234,230493,switch to maven build for solr3x plugin,Current ant build downloads solr distribution which is not required.,smartshark_2_2,1191,manifoldcf,"""[0.9979972243309021, 0.0020027784630656242]"""
2235,230494,Build needs some refactoring on how to handle integration tasks,"The integration directories for connectors no longer contain the integration binaries.  The binaries are copied twice, once to the integration area, and then to the dist/integration area.  A single copy would make life much easier.
",smartshark_2_2,397,manifoldcf,"""[0.9972054362297058, 0.0027945078909397125]"""
2236,230495,Missing Japanese localization for new crawler-UI internationalizations,"The resources under framework/ui-core/src/main/native2ascii/org/apache/manifoldcf/ui/i18n are missing some Japanese localizations.

",smartshark_2_2,585,manifoldcf,"""[0.9976831674575806, 0.002316761063411832]"""
2237,230496,Windows Share connector: SmbException tossed: 0xC0000205,"Windows share jobs stop when encountering an [Insufficient server resources exist to complete the request|https://msdn.microsoft.com/en-us/library/cc704588.aspx] server reply (0xC0000205 - STATUS_INSUFF_SERVER_RESOURCES).
Is it possible to catch that exception as well?
Thank you!",smartshark_2_2,683,manifoldcf,"""[0.36328691244125366, 0.6367130875587463]"""
2238,230497,Japanese translation needed for LDAP connector documentation,"I've added a section for the LDAP authority to en_US/end-user-documentation.xml.  We need a similar section in Japanese added to ja_JP/end-user-documentation.xml.
",smartshark_2_2,492,manifoldcf,"""[0.9974480867385864, 0.002551940269768238]"""
2239,230498,"Livelink connector should not use Date.toString(), but rather Date.getTime()","Date.toString() produces an unparseable date string; we don't want to index that.
",smartshark_2_2,1342,manifoldcf,"""[0.8209829926490784, 0.17901699244976044]"""
2240,230499,Add support for reports to API,"The API does not currently have implemented support for any ManifoldCF reports.  Add this functionality.
",smartshark_2_2,611,manifoldcf,"""[0.9980493783950806, 0.0019505928503349423]"""
2241,230500,"Proposal for initial two releases of LCF, including packaged product and full API","Currently, LCF has a relatively high-bar for evaluation and use, requiring developer expertise. Also, although LCF has a comprehensive UI, it is not currently packaged for use as a crawling engine for advanced applications.

A small set of individual feature requests are needed to address these issues. They are summarized briefly to show how they fit together for two initial releases of LCF, but will be broken out into individual LCF Jira issues.

Goals:

1. LCF as a standalone, downloadable, usable-out-of-the-box product (much as Solr is today)
2. LCF as a toolkit for developers needing customized crawling and repository access
3. An API-based crawling engine that can be integrated with applications (as Aperture is today)

Larger goals:

1. Make it very easy for users to evaluate LCF.
2. Make it very easy for developers to customize LCF.
3. Make it very easy for appplications to fully manage and control LCF in operation.

Two phases:

1) Standalone, packaged app that is super-easy to evaluate and deploy. Call it LCF 0.5.
2) API-based crawling engine for applications for which the UI might not be appropriate. Call it LCF 1.0.


Phase 1
-------

LCF 0.5 right out of the box would interface loosely with Solr 1.4 or later.
It would contain roughly the features that are currently in place or currently underway, plus a little more.

Specifically, LCF 0.5 would contain these additional capabilities:

1. Plug-in architecture for connectors (CONNECTORS-40 - DONE)
2. Packaged app ready to run with embedded Jetty app server (CONNECTORS-59)
3. Bundled with database - PostgreSQL or derby - ready to run without additional manual setup (CONNECTORS-55)
4. Mini-API to initially configure default connections and ""example"" jobs for file system and web crawl (CONNECTORS-58)
5. Agent process started automatically (CONNECTORS-60)
6. Solr output connector option to commit at end of job, by default (CONNECTORS-57)

Installation and basic evaluation of LCF would be essentially as simple as Solr is today. The example
connections and jobs would permit the user to initiate example crawls of a file system example
directory and an example web on the LCF web site with just a couple of clicks (as opposed to the
detailed manual setup required today to create repository and output connections and jobs.

It is worth considering whether the SharePoint connector could also be included as part of the default package.

Users could then add additional connectors and repositories and jobs as desired.

Timeframe for release? Level of effort?

Phase 2
-------

The essence of Phase 2 is that LCF would be split to allow direct, full API access to LCF as a
crawling ""engine"", in additional to the full LCF UI. Call this LCF 1.0.

Specifically, LCF 1.0 would contain these additional capabilities:

1. Full API for LCF as a crawling engine (CONNECTORS-56)
2. LCF can be bundled within an app (CONNECTORS-61)
3. LCF event and activity notification for full control by an application (CONNECTORS-41)

Overall, LCF will offer roughly the same crawling capabilities as with LCF 0.5, plus whatever bug
fixes and minor enhancements might also be added.

Timeframe for release? Level of effort?

-------------------------

Issues:

- Can we package PostgreSQL with LCF so LCF can set it up?
  - Or do we need Derby for that purpose?
- Managing multiple processes (UI, database, agent, app processes)
- What exactly would the API look like? (URL, XML, JSON, YAML?)
",smartshark_2_2,46,manifoldcf,"""[0.9983331561088562, 0.0016668281750753522]"""
2242,230501,"External HSQLDB fails with exceptions on Ubuntu and Mac OS, but runs fine on Windows","When you run the multi-process example on Windows, there are no errors starting the database and running initialize.bat.  But when you run the same thing on Ubuntu, you get these on the server:

{code}
[Server@732a54f9]: To close normally, connect and execute SHUTDOWN SQL
[Server@732a54f9]: From command line, use [Ctrl]+[C] to abort abruptly
[Server@732a54f9]: [Thread[HSQLDB Connection @210a6ae2,5,HSQLDB Connections @732a54f9]]: database alias=xdb does not exist
[Server@732a54f9]: [Thread[HSQLDB Connection @4bbd7848,5,HSQLDB Connections @732a54f9]]: database alias=xdb does not exist
[Server@732a54f9]: [Thread[HSQLDB Connection @351e1e67,5,HSQLDB Connections @732a54f9]]: database alias=xdb does not exist
[Server@732a54f9]: [Thread[HSQLDB Connection @46b8c8e6,5,HSQLDB Connections @732a54f9]]: database alias=xdb does not exist
{code}

... and these on the client:

{code}
org.apache.manifoldcf.core.interfaces.ManifoldCFException: Error getting connection: General error: database alias does not exist
	at org.apache.manifoldcf.core.database.ConnectionFactory.getConnection(ConnectionFactory.java:100)
	at org.apache.manifoldcf.core.database.Database.executeUncachedQuery(Database.java:722)
	at org.apache.manifoldcf.core.database.Database$QueryCacheExecutor.create(Database.java:1394)
	at org.apache.manifoldcf.core.cachemanager.CacheManager.findObjectsAndExecute(CacheManager.java:144)
	at org.apache.manifoldcf.core.database.Database.executeQuery(Database.java:186)
	at org.apache.manifoldcf.core.database.DBInterfaceHSQLDB.createUserAndDatabase(DBInterfaceHSQLDB.java:591)
	at org.apache.manifoldcf.core.system.ManifoldCF.createSystemDatabase(ManifoldCF.java:699)
	at org.apache.manifoldcf.crawler.system.ManifoldCF.createSystemDatabase(ManifoldCF.java:123)
	at org.apache.manifoldcf.crawler.InitializeAndRegister.doExecute(InitializeAndRegister.java:37)
	at org.apache.manifoldcf.crawler.InitializeAndRegister.main(InitializeAndRegister.java:60)
Caused by: java.sql.SQLException: General error: database alias does not exist
	at org.hsqldb.jdbc.Util.sqlException(Util.java:418)
	at org.hsqldb.jdbc.Util.sqlException(Util.java:113)
	at org.hsqldb.jdbc.JDBCConnection.<init>(JDBCConnection.java:3604)
	at org.hsqldb.jdbc.JDBCDriver.getConnection(JDBCDriver.java:312)
	at org.hsqldb.jdbc.JDBCDriver.connect(JDBCDriver.java:260)
	at java.sql.DriverManager.getConnection(DriverManager.java:620)
	at java.sql.DriverManager.getConnection(DriverManager.java:200)
	at org.apache.manifoldcf.core.jdbcpool.ConnectionPool.getConnection(ConnectionPool.java:94)
	at org.apache.manifoldcf.core.database.ConnectionFactory.getConnectionWithRetries(ConnectionFactory.java:125)
	at org.apache.manifoldcf.core.database.ConnectionFactory.getConnection(ConnectionFactory.java:96)
	... 9 more
Caused by: org.hsqldb.HsqlException: General error: database alias does not exist
	at org.hsqldb.error.Error.error(Error.java:285)
	at org.hsqldb.ClientConnection.<init>(ClientConnection.java:147)
	at org.hsqldb.jdbc.JDBCConnection.<init>(JDBCConnection.java:3588)
	... 16 more
{code}
",smartshark_2_2,465,manifoldcf,"""[0.596011757850647, 0.403988242149353]"""
2243,230502,Provide a script for OAuth registration for GoogleDrive connector?,"The GoogleDrive connector requires a context token.  In order to obtain that context token, it is necessary to register the application against a central server somewhere (which I believe is covered in the GoogleDrive end-user documentation).  It may be helpful to provide a script which does this instead; would make life potentially easier and could be extended for SharePoint 2013 and others.",smartshark_2_2,891,manifoldcf,"""[0.9984534978866577, 0.0015464548487216234]"""
2244,230503,GSOC: MongoDB Output Connector,"This is a projectÂ idea for [Google Summer of Code|https://summerofcode.withgoogle.com/] (GSOC).

To discuss this or other ideas with your potential mentor from the ApacheÂ ManifoldCF project, sign up and post to the dev@manifoldcf.apache.org list, including ""[GSOC]"" in the subject. You may also comment on this Jira issue if you have created an account.Â 

We would like to extend the Content Migration capabilities adding MongoDBÂ / GridFS as a new output connector for importing contents from one or more repositoriesÂ supportedÂ by ManifoldCF. In this way we will help developers on migrating contents from different data sources on MongoDB.

You will be involved in theÂ developmentÂ of the following tasks, you will learn how to:
 * Write the connector implementation
 * Implement unit tests
 * Build all the integration testsÂ for testing the connector inside the framework
 * WriteÂ the documentation for this connector

We have a complete documentation on how to implement an Output Connector:

[https://manifoldcf.apache.org/release/release-2.9.1/en_US/writing-output-connectors.html]

Take a look also at our book to understand better the framework and how to implement connectors:

[https://github.com/DaddyWri/manifoldcfinaction/tree/master/pdfs]

Â 

Prospective GSOC mentor: [piergiorgio@apache.org|mailto:piergiorgio@apache.org]",smartshark_2_2,973,manifoldcf,"""[0.9982869029045105, 0.0017130866181105375]"""
2245,230504,Ldap authentication for the administration console,"Hi Guys,

Could more options be added for authenticating to the MCF administration ui?

The username / password can be set in the properties.xml but it would make it easier for enterprise integration if ldap was supported.

Thanks,

Colin",smartshark_2_2,895,manifoldcf,"""[0.9982064962387085, 0.0017935094656422734]"""
2246,230505,Refactor LDAPAuthority Class Server Protocols,LDAPAuthority class is not readable and server protocols should be organized.,smartshark_2_2,713,manifoldcf,"""[0.9981245398521423, 0.0018754631746560335]"""
2247,230506,Upgrade OpenCMIS to the latest 1.0.0 version,"We should update the OpenCMIS library to the latest release 1.0.0.

Here all the improvements and bugfixes starting from our current version 0.13.0:

OpenCMIS 0.14.0:
https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310936&version=12331954

OpenCMIS 1.0.0:
https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310936&version=12315135
",smartshark_2_2,926,manifoldcf,"""[0.9983412027359009, 0.0016588270664215088]"""
2248,230507,API security parameter documentation lists incorrect property names for API security,"1) Set the api login parameters:

<property name=""org.apache.manifoldcf.login.apiname"" value=""test"" />
<property name=""org.apache.manifoldcf.login.apipassword"" value=""test"" />

2) Start MCF
3) From script engine directory, run:

{code}
run-script.bat file-crawl-example.mcf <some_directory>
{code}

Output is as follows:

{code}
Login successful
Output connection created (or already exists)
Repository connection created (or already exists)
Job created
The job id is 1435524529516
...
{code}

Clearly the api credentials were not honored.
 
",smartshark_2_2,1529,manifoldcf,"""[0.9673629999160767, 0.03263701871037483]"""
2249,230508,Minor errors in script engine documentation,"There are a number of places where ""<"" and "">"" are used instead of ""<<"" and "">>"".
",smartshark_2_2,372,manifoldcf,"""[0.9929445385932922, 0.007055398542433977]"""
2250,230509,Field names are URL encoded,"The field names provided by some repositories such as Alfresco are based on an URI similar to:
{code}
{http://www.alfresco.org/model/system}store_identifier
{code}

But in Solr we found the following field name:
{code}
http_3a_2f_2fwww_alfresco_org_2fmodel_2fsystem_2f1_0_7dstore_identifier
{code}

The code involved in the Solr connector is the following:

{code}
protected static String preEncode(String fieldName)
  {
      return URLEncoder.encode(fieldName);
  }
{code}

Probably we should try to solve it removing the preEncode invocation.",smartshark_2_2,1313,manifoldcf,"""[0.4811403453350067, 0.5188596844673157]"""
2251,230510,start-webapps.bat/sh should have a -D switch pointing it at properties.xml,"The start-webapps script invokes ManifoldCFJettyRunner, which looks for properties.xml values.  Unfortunately, it does not point at the right properties.xml via a -D switch, so whatever properties.xml it finds is likely to be the wrong one.
",smartshark_2_2,1133,manifoldcf,"""[0.9918357729911804, 0.008164205588400364]"""
2252,230511,"When MCF software changes, it is sometimes necessary to reseed from the beginning of time, so a button would help","When we change how a connector works, under some crawling models it is sometimes necessary to reset the last seeding time for a job to the beginning of time.  While it is possible to fake MCF out by editing the job and changing the job specification information, this is clunky and difficult to describe to people. Consider introducing a dedicated button on the job viewing page that does this with a click to address this.",smartshark_2_2,1169,manifoldcf,"""[0.9983918070793152, 0.0016081832582131028]"""
2253,230512,Document new properties related to UI/API authentication/authorization via LDAP,Need to document how to specify the proper IAuth implementation and how to configure the LDAP IAuth implementation.,smartshark_2_2,1532,manifoldcf,"""[0.9984880685806274, 0.0015119072049856186]"""
2254,230513,All patched jars must be built within ManifoldCF release process,"Downloading already-built patched jars is not permissible within Apache any longer.  The fix to this will entail downloading source and patching that as part of the build process.  In order for the build process to succeed on Windows, this currently means that the sources must be checked out using svn.

The following changes will therefore need to occur under this ticket:

(1) Apache binaries that are available as releases but which are not in Maven should be downloaded and unpacked from http://archive.apache.org/dist.  This will include the solr and sharepoint mcf plugins.

(2) The ""download-dependencies"" target must be split into ""download-dependencies"" and ""download-proprietary-dependencies"".  The ""download-dependencies"" target will become responsible for downloading, patching, and building the non-proprietary packages.  Packages that will need to be checked out by this target from svn include: httpclient 3.1, xerces 2.9.1, hsqldb 2.2.8, and jetty 6.1.26.

(3) A new release artifact, XXX-dep.zip/tar.gz, will be produced which will contain all the dependency binaries.

(4) Any non-sourceable dependency binaries we find must be replaced with a sourced equivalent, or code for it developed from scratch.

",smartshark_2_2,398,manifoldcf,"""[0.996859073638916, 0.0031409349758177996]"""
2255,230514,All output connectors should be updated so they can coexist with each other in the UI,Our output connectors cannot currently be used together in the same job because they haven't yet adopted the sequence number paradigm for their javascript methods and tabs.  We need to update this so there are no collisions and they can be used together.,smartshark_2_2,1252,manifoldcf,"""[0.9985034465789795, 0.0014965248992666602]"""
2256,230515,Send metadata to Solr using multipart request,"In Solrj 4.3, setUserMultipartRequest is exposed.  Rather then encoding the metadata in the URL string, we should send it in the request body",smartshark_2_2,814,manifoldcf,"""[0.9973304271697998, 0.0026695735286921263]"""
2257,230516,Forced solr attributes in job specification and/or configuration,"Would be nice if there was a globally managed tab (like ""connection"" or ""scheduling"") for job specification (or configuration) allowing to force some solr attributes. It could look and work similar to ""Solr Field Mapping"" allowing to specify name=value associations.

I am thinking about such case:
Index all documents from repository X, and set then ""source"" attribute to ""repository X"". Then I could filter results to those that came from specified source. But I think there can be other possibilities, like: index all documents from windows share and set them field ""client"" to ""Client X"", because all documents there are associated with one client and I would like to have filters, facets on such field (and I cannot fetch such value from documents because people never set meta tags...).

Real life: I have three document sources: Samba share with some project documents, internal wiki system, mantis bug tracker. I would like to query Solr for ""all documents from wiki, which contain phrase XXXX"".",smartshark_2_2,584,manifoldcf,"""[0.9984623193740845, 0.0015377282397821546]"""
2258,230517,"ElasticSearch now returns 201 http codes on success, not just 200 codes","Elastic Search now returns httpcode 201 for successful indexing events.  The connector needs to accept these.
",smartshark_2_2,598,manifoldcf,"""[0.2956310510635376, 0.7043690085411072]"""
2259,230518,Connector for Sharepoint Office 365,"Do ManifoldCf supports crawling from Sharepoint Office 365 as well?

Please suggest.",smartshark_2_2,1075,manifoldcf,"""[0.9981498718261719, 0.0018502086168155074]"""
2260,230519,Upgrade to Java 1.8,We should update source and target dependencies to Java 1.8 and fix any problems related to Java 1.8 upgrade.,smartshark_2_2,793,manifoldcf,"""[0.9979508519172668, 0.002049220958724618]"""
2261,230520,Go to Solrj 4.10.0 and try to get rid of deprecation warnings by using newer versions of Modified classes,Go to Solrj 4.10.0 and try to get rid of deprecation warnings by using newer versions of Modified classes,smartshark_2_2,1357,manifoldcf,"""[0.9972851276397705, 0.002714857691898942]"""
2262,230521,Documents crawled using manifoldcf 1.6 or earlier are needlessly recrawled after upgrade to 1.7 or later,"After upgrading to mcf 1.7 or later, pre-existing documents are recrawled and re-indexed even if they have not changed in any way since their last pre-upgrade crawl. The impact can be significant for large manifold deployments with millions+ static documents.

There appear to be three contributing factors:
1. The empty transformation version of a legacy document is different from the initial value of ""0+0!"" - in PipelineObjectWithVersions#buildAddPipeline and IncrementalIngester#checkFetchDocument
2. Incorrect comparison of output versions in PipelineObjectWithVersions#buildAddPipeline where oldOutputVersion is compared to a VersionContext object instead of the version string, which can be obtained by calling VersionContext#getVersionString - if IPipelineSpecification#getStageDescriptionString continues to return a VersionContext object, a rename of the method could be useful
3. In PipelineObjectWithVersions#buildAddPipeline, a null value for newAuthorityNameString is not treated the same as an empty string (like it is in other methods)
",smartshark_2_2,1469,manifoldcf,"""[0.9813874959945679, 0.01861243136227131]"""
2263,230522,"Derby stalls, does not perform well, on multi-threaded tests","Derby has been problematic for a while.  On one particular test it is easy to see it without fail: ""ant run-rss-tests-derby"". I've opened a ticket for the Derby project to track the problem, but there appears to be little interest in addressing it.  DERBY-6011.
",smartshark_2_2,636,manifoldcf,"""[0.8309926390647888, 0.16900737583637238]"""
2264,230523,Dropbox connector needs documentation,"The dropbox connector needs end-user documentation.
",smartshark_2_2,648,manifoldcf,"""[0.9981380701065063, 0.0018619089387357235]"""
2265,230524,"Update Japanese document of ""Included connectors""","Japanese ""Included connectors""(ã³ãã¯ã¿ä¸è¦§) needs to be updated.",smartshark_2_2,1463,manifoldcf,"""[0.9982929825782776, 0.0017069806344807148]"""
2266,230525,Binary distribution is too big,"The binary distribution is some 220MB right now.  This is way too big.

Options for cutting it down to size include:

(1) Remove source code from binary distribution
(2) Share common jars across the two examples

",smartshark_2_2,362,manifoldcf,"""[0.9972135424613953, 0.002786479191854596]"""
2267,230526,Main README.txt file is inappropriate for the binary distribution,"The main README.txt file is inappropriate for the binary distribution in that it seems to indicate you need to build it rather than just run it.
",smartshark_2_2,394,manifoldcf,"""[0.9961414933204651, 0.0038584705907851458]"""
2268,230527,Upgrade SolrJ to 4.1.0 from 4.0.0,Solr 4.1.0 had been released. I would like to follow upgrade of Solr.,smartshark_2_2,587,manifoldcf,"""[0.9980717897415161, 0.0019281399436295033]"""
2269,230528,pom.xml refers to jars not available in public repositories,"Maven's pom.xmls refers to jars that aren't available in public repositories, as maven central, apache repository, etc. This includes:
 - com.bitmechanic:jdbcpool
 - org.hsqldb:hsqldb:jar:2.2.5.6-9-2011 (at maven central only version 2.2.4 is available right now)

I think, that ManifoldCF should adopt the same approach as other Apache projects, like Tika, when all needed jars first promoted to public repositories, and only after that, they are used as dependency...",smartshark_2_2,304,manifoldcf,"""[0.9968445301055908, 0.003155540209263563]"""
2270,230529,More PostgreSQL performance improvements,"More performance improvements may be possible.
",smartshark_2_2,1387,manifoldcf,"""[0.9978793859481812, 0.0021206361707299948]"""
2271,230530,Optionally encrypt the file which the crawler configuration is exported to,"The ""org.apache.manifoldcf.crawler.ExportConfiguration"" command class is exporting passwords, for instance to the configured Solr server (Solr Output Connector). This may be a security problem if the export file is version-controlled or placed on a public server.

We should add an extra ""no password"" argument to the command class in order to skip such passwords.",smartshark_2_2,483,manifoldcf,"""[0.9962149262428284, 0.0037850909866392612]"""
2272,230531,A job is stopped by SmbException(No process is on the other end of the pipe).,"I am using ManifoldCF(from trunk build. 0.5?) for crawling Windows Share Folder for our application. When I run ManifoldCF sometimes I am getting SmbException. SmbException occur more often around crawling 70,000 files over. I try to reduce this JCIFS connection(2-5). However, SmbException will occur.

I read ManifoldCF Source Code.

SharedDriverConnector.java
 +processSMBException()

This method don't handle ""No process is on the other end of the pipe."" I want to continue the job even in the exception. Therefore, I modified the source code(SharedDriverConnector.java/Line number 1176). As a result, a job continue without stopping.

Source Code
http://hb2.sakura.ne.jp/manifoldcf/SharedDriveConnector.java

Source Code Parts
+++++++++++++++++++++
    else if(se.getMessage().indexOf(""No process is on the other end of the pipe"") != -1) {
      Logging.connectors.warn(""JCIFS: 'No process is on the other end of the pipe' response when ""+activity+"" for ""+documentIdentifier+"": retrying..."",se);
      // 'No process is on the other end of the pipe' skip the document and keep going
      throw new ServiceInterruption(""Timeout or other service interruption: ""+se.getMessage(),se,currentTime + 300000L,
        currentTime + 3 * 60 * 60000L,-1,false);
    }
+++++++++++++++++++++
",smartshark_2_2,388,manifoldcf,"""[0.17552027106285095, 0.8244797587394714]"""
2273,230532,Need a way to reset LCF when external conditions change,"When a change is made external to LCF, such as a Solr configuration change, LCF needs some way for a user to signal that that change took place.  For example, a button or link on the ""view output connection"" page might signal some undefined global change in the target of that output connection.  A similar button or link on the repository connection view page might signal a corresponding change to the underlying repository.

Clicking the button would do the following things:

(1) It would clear the current version string for all documents that passed through that connection.  This would guarantee that the documents would be reingested if and when they were processed the next time.

(2) It would reset the ""last job time"" value for all jobs affected by the connection to zero.  This would guarantee that all documents belonging to that job would be rechecked.



",smartshark_2_2,32,manifoldcf,"""[0.9969009160995483, 0.0030991595704108477]"""
2274,230533,Merido connector needs to be internationalized,Messages in Merido connector needs to be externalized to properties file.,smartshark_2_2,329,manifoldcf,"""[0.9981144666671753, 0.0018855463713407516]"""
2275,230534,Explore ways to make job start be faster in systems with lots of documents,"Job start requires all documents to be marked as needing reprioritization now.  We should consider ways in which we can reduce the need to do this as much as possible.  For example, if there are NO documents at all for a job, reprioritization is by definition unneeded.  Alternatively, coming up with a way of determining if there are any bin-level overlaps between documents made active by a job start at documents elsewhere, we could be more targeted.
",smartshark_2_2,1454,manifoldcf,"""[0.998292863368988, 0.0017071552574634552]"""
2276,230535,Fix deprecation warnings coming from commons/fileupload,"framework core seems to generate a deprecation warning for every fileupload method it uses.  Looks like somebody completely changed the API.
",smartshark_2_2,1180,manifoldcf,"""[0.9982016086578369, 0.0017984197475016117]"""
2277,230536,Write Load test for Alfresco Webscript Connector,"Based on the specs, defined in https://issues.apache.org/jira/browse/CONNECTORS-1060 , write a Load test suite that:
- Runs Alfresco in integration-test, as the existing IT test
- Spawns 2 parallel threads, one that runs Manifold jobs, the other pushing documents into Alfresco via CMIS
- Creates between 100.000 and 400.000 documents in Alfresco (should not exceed 4 hours run; it must also be explicitly invoked and kept out of automated runs)
- Outputs the throughput of insertion and crawling",smartshark_2_2,703,manifoldcf,"""[0.9985983967781067, 0.0014015628257766366]"""
2278,230537,add contributors to CHANGES.txt,"As mentioned on the connectors-dev@ list (change the format of CHANGES.txt), I propose we modify CHANGES.txt
to give credit to contributors who have given bug reports, comments, patches, etc.

I'll volunteer to go thru the mail archives and jira issues that are marked 'resolved' and upload a patch here.
",smartshark_2_2,124,manifoldcf,"""[0.9983429908752441, 0.0016569705912843347]"""
2279,230538,Wiki connector may need to support basic auth,"A user wants to use the Wiki Connector against a site that has basic authorization enabled.
",smartshark_2_2,654,manifoldcf,"""[0.995766282081604, 0.004233730025589466]"""
2280,230539,OpenSearchServer connector needs how-to-build-and-deploy section,"The how-to-build-and-deploy page needs to be updated to include the OpenSearchServer connector info.
",smartshark_2_2,187,manifoldcf,"""[0.9977013468742371, 0.0022986598778516054]"""
2281,230540,Usage and meaning of ManifoldCFException type REPOSITORY_CONNECTION_ERROR needs to be reviewed and clarified,"The ManifoldCFException type REPOSITORY_CONNECTION_ERROR seems to be treated by the framework somewhat inconsistently.  In some places it is treated as a permanent connection exception, and in others as a temporary connection exception (in lieu of a ServiceInterruption where ServiceInterruption is not possible).  Only two connectors use it (LiveLink and jCIFS), and the JCIFS case is not interesting.  So really this is currently here to support Livelink.

There are two ways forward.  The first way is to convert the Livelink connector's exception to a true ServiceInterruption, and revert REPOSITORY_CONNECTION_ERROR to its original meaning, which has now been deprecated as a result of the fact that connect() methods can no longer throw ManifoldCFExceptions at all.  The second is to continue the current Livelink-style usage, and make ALL usages consistent with that.
",smartshark_2_2,172,manifoldcf,"""[0.9807912111282349, 0.01920880191028118]"""
2282,230541,Support ISO8601 dates in RSS connector,"While RSS fields explicitly use RFC822 dates, I've been seeing feeds with ISO8601 dates instead.  So the RSS connector might as well support that format too.
",smartshark_2_2,622,manifoldcf,"""[0.9984863996505737, 0.001513515948317945]"""
2283,230542,"Jetty 7.5 does not work with JDK 8, try upgrading to jetty 8",Jetty 7.5 does not work with JDK 8; jsp's don't compile.  Try upgrading to jetty 8.,smartshark_2_2,1229,manifoldcf,"""[0.972822904586792, 0.027177108451724052]"""
2284,230543,"SolrJ in Solr Cloud mode does not use multipart post, and this is causing trouble for ManifoldCF users","SolrJ in Solr Cloud mode does not use multipart post, and this is causing trouble for ManifoldCF users.  Specifically, CloudSolrServer calls a
LBHttpSolrServer that calls a HttpSolrServer with useMultiPartPost=false.  Unfortunately that's not even our hacked HttpSolrServer, so setting multipart post to true would still not work.
",smartshark_2_2,1196,manifoldcf,"""[0.4508914351463318, 0.5491085648536682]"""
2285,230544,Use OpenSearchServer's abilities on text extraction and metadata extraction ,OpenSearchServer is able to extract text from binary documents as well as metadata. The next version (v1.6) will expose APIs for that purpose.,smartshark_2_2,1415,manifoldcf,"""[0.9984918832778931, 0.0015080694574862719]"""
2286,230545,Zookeeper hangs eventually with specified parameters,"The zookeeper parameters we deliver are missing apparently important limits on growth:

autopurge.snapRetainCount=3 : default value
autopurge.purgeInterval=1: default value

",smartshark_2_2,1298,manifoldcf,"""[0.19663645327091217, 0.803363561630249]"""
2287,230546,Crawler should follow the robots meta tag rules,"The web crawler does obey robots.txt files, but not the robots meta tag rules. If a document has the following meta tag included, the crawler just ignores and fetches it anyway:
<meta name=""robots"" content=""noindex, nofollow"" />

I would recommend that the following changes are done in order to improve the crawler if one of the ""Obey robots.txt ..."" options is set:

1. <meta name=""robots"" content=""noindex, nofollow"" />
- do not fetch the document at all

2. <meta name=""robots"" content=""noindex, follow"" />
- only follow the other links in this document

3. <meta name=""robots"" content=""index, nofollow"" />
- fetch the document, but do no follow any link in it.

4. Change most of the text that appear on the page for robots option settings to something like:
""Robots.txt usage"" => ""Robots.txt and Robots <meta> tag usage""
""Don't look at robots.txt"" => ""Ignore robots settings""
""Obey robots.txt for data caches only"" => ""Follow robots rules for data caches only""
""Obey robots.txt for all fetces"" => ""Follow robots rules for all fetches""

",smartshark_2_2,104,manifoldcf,"""[0.9695754647254944, 0.030424561351537704]"""
2288,230547,"When job deletion has started, but job is not yet deleting documents, ""Not yet run"" is displayed on status page","""Not yet run"" is displayed but the correct status should be something else.  The job's status value is ""V"", which corresponds to state DELETESTARTINGUP.
",smartshark_2_2,238,manifoldcf,"""[0.27486124634742737, 0.7251387238502502]"""
2289,230548,multiprocess-zk-example agents process won't start on standard Ubuntu release,Looks like it runs out of resources somewhere.  This is standard Ubuntu 12.04.,smartshark_2_2,1143,manifoldcf,"""[0.9543190002441406, 0.04568103700876236]"""
2290,230549,Consider adding feature to web connector to skip pages that match specified criteria,"The user wants to skip content that matches specified criteria, because some sites don't return a 404 code (for instance) but instead return 200 with a textual error message.",smartshark_2_2,1515,manifoldcf,"""[0.9984118938446045, 0.0015881230356171727]"""
2291,230550,Quotation marks in translated messages should be removed,"Quotation marks exist in some translated messages, e.g. in common_en_US.properties for the Web Connector. These should be removed and added elsewhere. An example:
editjob.ExpirationIntervalMustBeAValidIntegerOrNull=""Expiration interval must be a valid integer or null""
The quotation marks should in the example above be placed in the JSP file instead.",smartshark_2_2,391,manifoldcf,"""[0.9967315196990967, 0.0032685445621609688]"""
2292,230551,Japanese top page and concepts pages are not fully translated,There are some untranslated sentences in Japanese top page and concepts pages.,smartshark_2_2,345,manifoldcf,"""[0.9964451193809509, 0.003554859198629856]"""
2293,230552,Wiki connector should include last-modified date as part of the metadata indexed,"Wiki connector should include last-modified date as part of the metadata indexed.
",smartshark_2_2,226,manifoldcf,"""[0.9978126287460327, 0.002187297912314534]"""
2294,230553,make the thresholds of isText() input-able,"Currently the thresholds of isText() is 0.30 as default.
This is too strict value for Japanese sites because those sites don't often have ASCII characters.
As a result some sites is judged as not-text then MCF can't extract links from those documents.
I'd like to make this value input-able at Repository connection. 
There is no patch from me now.

{code:title=WebcrawlerConnector.java|borderStyle=solid}
  /** Test to see if a document is text or not.  The first n bytes are passed
  * in, and this code returns ""true"" if it thinks they represent text.  The code
  * has been lifted algorithmically from products/Sharecrawler/Fingerprinter.pas,
  * which was based on ""perldoc -f -T"".
  */
  protected static boolean isText(byte[] beginChunk, int chunkLength)
  {
    if (chunkLength == 0)
      return true;
    int i = 0;
    int count = 0;
    while (i < chunkLength)
    {
      byte x = beginChunk[i++];
      if (x == 0)
        return false;
      if (isStrange(x))
        count++;
    }
    return ((double)count)/((double)chunkLength) < 0.30;
  }

  /** Check if character is not typical ASCII. */
  protected static boolean isStrange(byte x)
  {
    return (x > 127 || x < 32) && (!isWhiteSpace(x));
  }
{code} ",smartshark_2_2,561,manifoldcf,"""[0.998244047164917, 0.0017559637781232595]"""
2295,230554,Add 'hopcountremoved' to the document status and queue status reports,"It would be great of the new hopcountremoved status showed up in the document status and queue status reports.
",smartshark_2_2,459,manifoldcf,"""[0.9985526204109192, 0.0014473563060164452]"""
2296,230555,Reorganize dist area to reduce -bin package size,"Our -bin packages now exceed 200M.  We need to do what we can to reduce their sizes in the future.

One of the biggest contributors to bloat in the bin package is the duplication of jars.  While some of this is unavoidable (we will still need the connector-lib and connector-lib-proprietary directories, for instance), we can save maybe 100M simply by having only one copy of a jar for all jars used in explicit classpaths.  The strategy of putting jars needed in a given classpath in a specific directory, for example, could be replaced by having jars reside in one place, and building class paths at compile time based on dynamic conditions.  We may need an ant plugin for this, but I think it would be well worth it.",smartshark_2_2,1142,manifoldcf,"""[0.9981561303138733, 0.001843819161877036]"""
2297,230556,Manifold web vs Oracle RAC,"Hi,
on Manifold CF 1.2 web console, it isn't possible to set a connection pool for Oracle Database connector when you configure a Output Connection, although you put the latest version of ojdbc6.jar into the Tomcat classpath.

It's needed to have the possibility to configure a connection pool for instance:
""jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=rac01)(PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=rac02)(PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=rac03)(PORT=1521))(LOAD_BALANCE=yes)(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=test)(FAILOVER_MODE=(TYPE=select)(METHOD=session)(RETRIES=3)(DELAY=10))))""

This configuration permits high availability and redundancy into the connection to Oracle RAC database.


",smartshark_2_2,1024,manifoldcf,"""[0.8238698244094849, 0.17613017559051514]"""
2298,230557,Alfresco IT test failure,"This stack trace appears when the alfresco webscript test is run:

{code}
[junit] Testcase: sanityCheck(org.apache.manifoldcf.crawler.connectors.alfrescowebscript.tests.APISanityHSQLDBIT):	Caused an ERROR
    [junit] com/ctc/wstx/stax/WstxInputFactory
    [junit] java.lang.NoClassDefFoundError: com/ctc/wstx/stax/WstxInputFactory
    [junit] 	at org.apache.chemistry.opencmis.client.bindings.spi.atompub.AtomPubParser.parse(AtomPubParser.java:99)
    [junit] 	at org.apache.chemistry.opencmis.client.bindings.spi.atompub.AbstractAtomPubService.parse(AbstractAtomPubService.java:676)
    [junit] 	at org.apache.chemistry.opencmis.client.bindings.spi.atompub.AbstractAtomPubService.getRepositoriesInternal(AbstractAtomPubService.java:874)
    [junit] 	at org.apache.chemistry.opencmis.client.bindings.spi.atompub.RepositoryServiceImpl.getRepositoryInfos(RepositoryServiceImpl.java:66)
    [junit] 	at org.apache.chemistry.opencmis.client.bindings.impl.RepositoryServiceImpl.getRepositoryInfos(RepositoryServiceImpl.java:92)
    [junit] 	at org.apache.chemistry.opencmis.client.runtime.SessionFactoryImpl.getRepositories(SessionFactoryImpl.java:120)
    [junit] 	at org.apache.chemistry.opencmis.client.runtime.SessionFactoryImpl.getRepositories(SessionFactoryImpl.java:107)
    [junit] 	at org.apache.manifoldcf.crawler.connectors.alfrescowebscript.tests.CMISUtils.getSession(CMISUtils.java:51)
    [junit] 	at org.apache.manifoldcf.crawler.connectors.alfrescowebscript.tests.CMISUtils.createDocument(CMISUtils.java:58)
    [junit] 	at org.apache.manifoldcf.crawler.connectors.alfrescowebscript.tests.APISanityHSQLDBIT.createTestArea(APISanityHSQLDBIT.java:70)
    [junit] Caused by: java.lang.ClassNotFoundException: com.ctc.wstx.stax.WstxInputFactory
    [junit] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    [junit] 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    [junit] 
{code}
",smartshark_2_2,927,manifoldcf,"""[0.917094349861145, 0.08290567994117737]"""
2299,230558,Add support for pipeline connector,"In the Amazon Search Connector, we finally found an example of an output connector that needed to do full document processing in order to work.  This ticket represents work in the framework to create a concept of ""pipeline connector"".  Pipeline connections would receive RepositoryDocument objects, and transform them to new RepositoryDocument objects.  There would be a single important method:

{code}
public void transformDocument(RepositoryDocument rd, ITransformationActivities activities) throws ServiceInterruption, ManifoldCFException;
{code}

... where ITransformationActivities would include a method that would send a RepositoryDocument object onward to either the output connection or to the next pipeline connection.

Each pipeline connection would have:
- A name
- A description
- Configuration data
- An optional prerequisite pipeline connection

Every output connection would have a new field, which is an optional prerequisite pipeline connection.

This design is based loosely on how mapping connections and authority connections interrelate.  An alternate design would involve having per-job specification information, but I think this would wind up being way too complex for very little benefit, since each pipeline connection/stage would be expected to do relatively simple/granular things, not usually involving interaction with an external system.",smartshark_2_2,1240,manifoldcf,"""[0.9985607266426086, 0.001439232612028718]"""
2300,230559,"We need to separate our site into two pieces: One for the ManifoldCF project, and one for each release","Part of the ManifoldCF site has to do with the ManifoldCF project, and part of it is germane to a given release.  We should separate these so that the project-related part builds separately, under ""site/trunk"", and references one (or more) release-related sites, under branches/XXX/site.
",smartshark_2_2,432,manifoldcf,"""[0.9980966448783875, 0.001903338241390884]"""
2301,230560,Can we add excluded URLs to the RSS connector?,"I need to exclude this URL https://c3qa.llan.ll.mit.edu/profiles/atom/profileType.do?type=default found in a RSS feed;
It's causing socket IOException time outs to occur.
Can we add the exclude URL capability from the web connector to the RSS connector?",smartshark_2_2,591,manifoldcf,"""[0.9983402490615845, 0.0016597732901573181]"""
2302,230561,Heavy contention over repository connections,"On a recent test using (a slightly customised version of) the JCIFS repository connector, I noticed the Stuffer thread spends more than 50% of its time calling out to IRepositoryConnectorPool#grab (between wait() and talking to ZK). 

I wondered if the StufferThread#run method could benefit from caching repository connections along the lines of CONNECTORS-1094 so I patched it up (see attachement) which took the time down a bit. Upon closer inspection I saw the calls to RepositoryConnectorPool#grab and RepositoryConnectorPool#releaseMultiple were constantly blocked waiting on a lock owned by the Idle cleanup thread. It turns out the Idle cleanup thread does some relatively expensive work ** at least for the JCIFS connector ** once it's acquired a lock on ConnectorPool#poolHash. It reads a config property in SharedDriveConnector#setThreadContext and that involves a trip to ZK and some  XML parsing which results in loading and processing JAR files. I subsequently monitored other threads and found that many can be impacted for prolonged periods of time as they try to acquire and release repository connections.

Caching the values of config properties almost eliminates the time the Idle cleanup thread spends under lock and is easy enough to implement for the JCIFS connector. It would be great if this could be done in a way that is more generic to prevent a slight inefficiency in the code of one connector slowing down the entire system.
",smartshark_2_2,1410,manifoldcf,"""[0.9690076112747192, 0.030992351472377777]"""
2303,230562,Request-URI Too Long,"I run email connector job and follow ""Simple History"" from UI. I see an error as follow:


{code}
Error from server at http://localhost:8983/solr/mycore: non ok status: 414, message:Request-URI Too Long
{code}
It is sent by Solr. 

Solr logs say: 
{code}
HttpParser - URI is too large >8192
{code}

and 
{code}
HttpParser - bad HTTP parsed: 414 for HttpChannelOverHttp@2b6931dd{r=0,&#8203;c=false,&#8203;a=IDLE,&#8203;uri=null} 
{code}

ManifoldCF ModifiedHttpSolrClient.java has following code:

{code}
 // It is has one stream, it is the post body, put the params in the URL
      else {
        String pstr = toQueryString(wparams, false);
        HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == request.getMethod() ?
            new HttpPost(url + pstr) : new HttpPut(url + pstr);

{code}

There is ""pstr"" field appended to the URL. ""pstr"" field have all Solr params. It contains email content. We have ""URI is too large"" error when email has large content.",smartshark_2_2,798,manifoldcf,"""[0.16971762478351593, 0.8302823901176453]"""
2304,230563,Dependencies issues,"There are several issues with the dependencies:

1) POI should be 3.13, since tika 1.12 uses that version. With POI 3.14 tika cannot parse presentation files (ppt):
{code}
FATAL 2016-05-03 10:39:16,821 (Worker thread '0') - Error tossed: org.apache.poi.xslf.usermodel.XSLFTextShape.getTextType()Lorg/apache/poi/xslf/usermodel/Placeholder;
java.lang.NoSuchMethodError: org.apache.poi.xslf.usermodel.XSLFTextShape.getTextType()Lorg/apache/poi/xslf/usermodel/Placeholder;
	at org.apache.tika.parser.microsoft.ooxml.XSLFPowerPointExtractorDecorator.extractContent(XSLFPowerPointExtractorDecorator.java:154)
	at org.apache.tika.parser.microsoft.ooxml.XSLFPowerPointExtractorDecorator.buildXHTML(XSLFPowerPointExtractorDecorator.java:88)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:110)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:112)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.manifoldcf.agents.transformation.tika.TikaParser.parse(TikaParser.java:48)
{code}

2) jcifs ""1.3.17"" is used currently. Available is ""1.3.18"".

3) Java Advanced Imaging (JAI), jbig2 format libs are not included, but required for parsing embedded images.

Thank you!",smartshark_2_2,686,manifoldcf,"""[0.9218555688858032, 0.07814449071884155]"""
2305,230564,"Update documentation regarding JDBC, MySQL and CONCAT","A little late but here is my solution to the mysql concat problem from the mailing list for the documentation. ;-)

When using the generic database connector with a mysql connection you have to be aware of the behaviour of the {{concat}} command if you combine multiple columns.

For example the following query will not work if any value is {{NULL}}:

{code:sql}
SELECT id AS $(IDCOLUMN),
CONCAT(""http://my.base.url/show.html?record="", id) AS $(URLCOLUMN),
CONCAT(name, "" "", description, "" "", what_ever) AS $(DATACOLUMN)
FROM accounts WHERE id IN $(IDLIST)
{code}

You'll have to use the {{ifnull}} operator to circumvent the issue.

{code:sql}
SELECT id AS $(IDCOLUMN),
CONCAT(""http://my.base.url/show.html?record="", id) AS $(URLCOLUMN),
CONCAT(name, "" "", IFNULL(description, """"), "" "", IFNULL(what_ever, """"))
AS $(DATACOLUMN)
FROM accounts WHERE id IN $(IDLIST)
{code}

",smartshark_2_2,1374,manifoldcf,"""[0.9963191747665405, 0.0036808692384511232]"""
2306,230565,Retry interval in crawl date is set to 0,"When i fetch and parse a feed with the feed plugin,
http://www.wachauclimbing.net/home/impressum-disclaimer/feed/
another crawl date is generated
http://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/

after fetching a second round
the dump in the crawl db still shows a retry interval with value 0.

http://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/ Version: 7
Status: 2 (db_fetched)
Fetch time: Wed Dec 02 12:48:22 CET 2009
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 0 seconds (0 days)
Score: 1.0833334
Signature: db9ab2193924cd2d0b53113a500ca604
Metadata: _pst_: success(1), lastModified=0

a check should be done in DefaultFetchSchedule (or AbstractFetchSchedule) in the
method 
setFetchSchedule

",smartshark_2_2,1632,nutch,"""[0.11731867492198944, 0.8826813101768494]"""
2307,230566,segment size is never as big as topN or crawlDB size in a distributed deployement,"I didn't reopen NUTCH-136 since it is may related to the hadoop split.
I tested this on two different deployement (with 10 ttrackers + 1 jobtracker and 9 ttracks and 1 jobtracker).
Defining map and reduce task number in a mapred-default.xml does not solve the problem. (is in nutch/conf on all boxes)
We verified that it is not  a problem of maximum urls per hosts and also not a problem of the url filter.

Looks like the first job of the Generator (Selector) already got to less entries to process. 
May be this is somehow releasted to split generation or configuration inside the distributed jobtracker since it runs in a different jvm as the jobclient.
However we was not able to find the source for this problem.

I think that should be fixed before  publishing a nutch 0.8. 


",smartshark_2_2,211,nutch,"""[0.14873731136322021, 0.8512626886367798]"""
2308,230567,parse-html increase chunk size used to detect charset,"The chunk used to detect the encoding of a document is set to 2000 bytes. Although it is definitely best practice to ""define"" the character set on top, 2000 bytes are sometimes not enough: 20 longer <link> elements pointing to javascript and css libs may ""hide"" the <meta> element containing content type and encoding. Same problem has been observed in TIKA-357 and solved by increasing the buffer size to 8 kB.",smartshark_2_2,1039,nutch,"""[0.9321778416633606, 0.06782209873199463]"""
2309,230568,Remove copying of ID and URL field in solrmapping,"Guys, the Solrindexer seems to be broken in trunk. With current solrmapping and code you'll get an exception complaining about multiple values in a non-multivalued field; the ID field which must of course be single valued. This happens because of the current mapping code and mapping config copy the url and id fields. The old 1.3 NutchDocument does not contain an ID field but in trunk it does.

I propose to change the current solrmapping configuration by simply removing:
                <field dest=""id"" source=""url""/>
                <copyField source=""url"" dest=""url""/>

If not, we need to do something about the solrmapping code.
",smartshark_2_2,2138,nutch,"""[0.4995688498020172, 0.5004311800003052]"""
2310,230569,gora mongodb mapping file error,"conf/gora-mongodb-mapping.xml


{code}
 <field name=""stmPriority"" family=""stmPriority"" type=""int32""/>
{code}
should be
 {code} 
 <field name=""stmPriority"" docfield=""stmPriority"" type=""int32""/>
{code}
 Otherwise it is throwing exception.",smartshark_2_2,1370,nutch,"""[0.2700796127319336, 0.7299203276634216]"""
2311,230570,Bug in SegmentReader causes infinite loop,"A small bug in SegmentReader.get() may lead to an infinite loop.

...
    int cnt = 0;
    do {
      try {
        Thread.sleep(5000);
      } catch (Exception e) {};
      it = threads.iterator();
      while (it.hasNext()) {
        if (((Thread)it.next()).isAlive()) cnt++;
      }
      if ((cnt > 0) && (LOG.isDebugEnabled())) {
        LOG.debug(""("" + cnt + "" to retrieve)"");
....

      }
    } while (cnt > 0); 


If cnt ever becomes non-zero, SegmentReader gets stuck in that loop.

This bug is discovered by Ilya Vishnevsky.",smartshark_2_2,357,nutch,"""[0.05864609032869339, 0.9413539171218872]"""
2312,230571,max. redirects not handled correctly: fetcher stops at max-1 redirects,"The fetcher stops following redirects one redirect before the max. redirects is reached.

The description of http.redirect.max
> The maximum number of redirects the fetcher will follow when
> trying to fetch a page. If set to negative or 0, fetcher won't immediately
> follow redirected URLs, instead it will record them for later fetching.
suggests that if set to 1 that one redirect will be followed.

I tried to crawl two documents the first redirecting by
 <meta http-equiv=""refresh"" content=""0; URL=./to/meta_refresh_target.html"">
to the second with http.redirect.max = 1
The second document is not fetched and the URL has state GONE in CrawlDb.

fetching file:/test/redirects/meta_refresh.html
redirectCount=0
-finishing thread FetcherThread, activeThreads=1
 - content redirect to file:/test/redirects/to/meta_refresh_target.html (fetching now)
 - redirect count exceeded file:/test/redirects/to/meta_refresh_target.html

The attached patch would fix this: if http.redirect.max is 1 : one redirect is followed.
Of course, this would mean there is no possibility to skip redirects at all since 0
(as well as negative values) means ""treat redirects as ordinary links"".

",smartshark_2_2,1792,nutch,"""[0.0808636024594307, 0.9191364049911499]"""
2313,230572,Ensure that all WebApp files are copied into generated artifacts for 1.X Webapp,"Right now in trunk you get a hellish message when you initiate both webapp and startserver commands from within the bin/nutch script.
This issue addresses the bug which essentially after a ton of debugging relates to a failure for all Webapp files e.g. .css, .properties, .html, etc. being included within the generated artifact.
Patch coming up ASAP.",smartshark_2_2,1015,nutch,"""[0.906197726726532, 0.09380223602056503]"""
2314,230573,MapWritable.equals() doesn't work properly,"MapWritable.equals() is sensitive to the order in which map entries have been created. E.g. this fails but it should succeed:

    MapWritable map1 = new MapWritable();
    MapWritable map2 = new MapWritable();
    map1.put(new UTF8(""key1""), new UTF8(""val1""));
    map1.put(new UTF8(""key2""), new UTF8(""val2""));
    map2.put(new UTF8(""key2""), new UTF8(""val2""));
    map2.put(new UTF8(""key1""), new UTF8(""val1""));
    assertTrue(map1.equals(map2));

Users expect that this should not be the case, i.e. this class should follow the same rules as Map.equals() (""Returns true if the given object is also a map and the two Maps represent the same mappings"").",smartshark_2_2,87,nutch,"""[0.06587516516447067, 0.9341248273849487]"""
2315,230574,ElasticSearchIndexer fails to properly delete documents,"Exception is thrown because the indexer does not properly set the type and index for delete commands. This comes from the original source so 2x may be affected as well.

{code}
ava.io.IOException
        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.makeIOException(ElasticIndexWriter.java:173)
        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.delete(ElasticIndexWriter.java:168)
        at org.apache.nutch.indexer.IndexWriters.delete(IndexWriters.java:108)
        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:52)
        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)
        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:458)
        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:500)
        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:203)
        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)
Caused by: org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: index is missing;2: type is missing;
        at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
        at org.elasticsearch.action.support.replication.ShardReplicationOperationRequest.validate(ShardReplicationOperationRequest.java:126)
        at org.elasticsearch.action.delete.DeleteRequest.validate(DeleteRequest.java:84)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:55)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
        at org.elasticsearch.client.support.AbstractClient.delete(AbstractClient.java:121)
        at org.elasticsearch.action.delete.DeleteRequestBuilder.doExecute(DeleteRequestBuilder.java:147)
        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.delete(ElasticIndexWriter.java:165)
        ... 10 more
2013-07-03 11:43:39,957 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)
        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)
        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195) 
{code}",smartshark_2_2,2366,nutch,"""[0.06203140690922737, 0.9379686117172241]"""
2316,230575,Store detected content type in crawldatum metadata,The MIME-type detected by Tika's Detect() API is never added to a Parse's ContentMetaData or ParseMetaData. Because of this bad Content-Types will end up in the documents. ,smartshark_2_2,2075,nutch,"""[0.7592236399650574, 0.24077634513378143]"""
2317,230576,file URL are filtered out by the crawler,"I tried to index file system using the file:/ protocol, which worked fine in version 0.9
The file URL are being filtered out and not fetched at all.

I investigated the code and saw that there are 2 issues:
1) One is with the class UrlValidator: when validating an URL, it check the 'authority', a combination of host and port. As it is null for file, the URL is rejected.
2) Once this check is removed, files that contain space characters (and maybe other characters to be URL encoded) are also filtered out. It maybe be because the file protocol plugin doesn't URL encode space characters and/or UrlValidator is enforce the rule to encode such character.

To workaround these issues, I just commented out UrlValidator checks and it works fine.",smartshark_2_2,332,nutch,"""[0.1271209865808487, 0.8728789687156677]"""
2318,230577,Fetching via https does not work with a proxy (patch),"Trying to fetch content from an SSL-Server using a proxy does not work due to a bug in the protocol-httpclient plugin.
The attached patch fixes this problem.

Ciao
 -Fritz

",smartshark_2_2,374,nutch,"""[0.08745823055505753, 0.9125418066978455]"""
2319,230578,Fetcher saving redirected robots.txt under redirect target URL,"NUTCH-2300 lets the Fetcher store optionally the robots.txt response (content and HTTP status). If the '.../robots.txt' is redirected, the redirected content is also stored but with the redirect source URL as key. It should use the redirect target URL instead. Otherwise one of the responses is overwritten in the segments map file.",smartshark_2_2,1333,nutch,"""[0.13742278516292572, 0.8625771999359131]"""
2320,230579,Generator throws java.io.IOException and dies on injected urls with no protocol ,"On trunk nutch, injecting URLs with no protocol (like issues.apache.org/jira/ vs. https://issues.apache.org/jira/) causes the generator to fail with an IOException:

java.net.MalformedURLException: no protocol: www.variogr.am
        at java.net.URL.<init>(URL.java:567)
        at java.net.URL.<init>(URL.java:464)
        at java.net.URL.<init>(URL.java:413)
        at org.apache.nutch.crawl.Generator$Selector.reduce(Generator.java:187)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:326)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:155)
2007-09-15 11:11:26,986 FATAL crawl.Generator - Generator: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604)
        at org.apache.nutch.crawl.Generator.generate(Generator.java:416)
        at org.apache.nutch.crawl.Generator.run(Generator.java:557)
        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)
        at org.apache.nutch.crawl.Generator.main(Generator.java:520)

To test:

# cat test/urls.txt
www.variogr.am
http://www.variogr.am/

# bin/nutch inject testcrawl/crawldb test/
(this goes fine)

# bin/nutch generate testcrawl/crawldb testcrawl/segments -topN 10
Generator: Selecting best-scoring urls due for fetch.
Generator: starting
Generator: segment: testcrawl/segments/20070915111125
Generator: filtering: true
Generator: topN: 10
Generator: jobtracker is 'local', generating exactly one partition.
Generator: java.io.IOException: Job failed!
 

This issue did not exist in earlier versions of nutch -- it would ignore the malformed URL without crashing.




",smartshark_2_2,331,nutch,"""[0.08123597502708435, 0.918764054775238]"""
2321,230580,SegmentReader broken in distributed mode,"SegmentReader -list option ignores the -no* options, causing the following exception in distributed mode:

{code}
Exception in thread ""main"" java.lang.NullPointerException
        at java.util.ComparableTimSort.sort(ComparableTimSort.java:146)
        at java.util.Arrays.sort(Arrays.java:472)
        at org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(SequenceFileOutputFormat.java:85)
        at org.apache.nutch.segment.SegmentReader.getStats(SegmentReader.java:463)
        at org.apache.nutch.segment.SegmentReader.list(SegmentReader.java:441)
        at org.apache.nutch.segment.SegmentReader.main(SegmentReader.java:587)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
{code}",smartshark_2_2,813,nutch,"""[0.06199119985103607, 0.9380087852478027]"""
2322,230581,Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.,"Hello,

I am performing a local file system crawling.
My problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.

For example, I will see the following error:

fetching file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html
org.apache.nutch.protocol.file.FileError: File Error: 404
        at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:92)
        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:535)
fetch of file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html failed with: org.apache.nutch.protocol.file.FileError: File Error: 404

I am using nutch-1.0.

Among other standard settings, I configured nutch-site.conf as follows:

<property>
  <name>plugin.includes</name>
  <value>protocol-file|protocol-http|urlfilter-regex|parse-(text|html|js|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins. In order to use HTTPS please enable
  protocol-httpclient, but be aware of possible intermittent problems with the
  underlying commons-httpclient library.
  </description>
</property>

<property>
  <name>file.content.limit</name>
  <value>-1</value>
</property>

Moreover, crawl-urlfilter.txt   looks like:

# skip http:, ftp:, & mailto: urls
-^(http|ftp|mailto):

# skip image and other suffixes we can't yet parse
-\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$

# skip URLs containing certain characters as probable queries, etc.
-[?*!@=]

# skip URLs with slash-delimited segment that repeats 3+ times, to break loops
-.*(/[^/]+)/[^/]+\1/[^/]+\1/

# accept hosts in MY.DOMAIN.NAME
#+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/

# accept everything else
+.*
~    

---

Thanks,

Michela
",smartshark_2_2,1790,nutch,"""[0.11339765042066574, 0.8866024017333984]"""
2323,230582,Plugin repository cache can lead to memory leak,"While I was trying to solve a problem I reported a while ago (see Nutch-314), I found out that actually the problem was related to the plugin cache used in class PluginRepository.java.
As  I said in Nutch-314, I think I somehow 'force' the way nutch is meant to work, since I need to frequently submit new urls and append their contents to the index; I don't (and I can't) have an urls.txt file with all urls I'm going to fetch, but I recreate it each time a new url is submitted.
Thus,  I think in the majority of times you won't have problems using nutch as-is, since the problem I found occours only if nutch is used in a way similar to the one I use.
To simplify your test I'm attaching a class that performs something similar to what I need. It fetches and index some sample urls; to avoid webmasters complaints I left the sample urls list empty, so you should modify the source code and add some urls.
Then you only have to run it and watch your memory consumption with top. In my experience I get an OutOfMemoryException after a couple of minutes, but it clearly depends on your heap settings and on the plugins you are using (I'm using 'protocol-file|protocol-http|parse-(rss|html|msword|pdf|text)|language-identifier|index-(basic|more)|query-(basic|more|site|url)|urlfilter-regex|summary-basic|scoring-opic').

The problem is bound to the PluginRepository 'singleton' instance, since it never get released. It seems that some class maintains a reference to it and this class is never released since it is cached somewhere in the configuration.

So I modified the PluginRepository's 'get' method so that it never uses the cache and always returns a new instance (you can find the patch in attachment). This way the memory consumption is always stable and I get no OOM anymore.
Clearly this is not the solution, since I guess there are many performance issues involved, but for the moment it works.
",smartshark_2_2,2221,nutch,"""[0.14228208363056183, 0.8577178716659546]"""
2324,230583,URL gone with 404 after db.fetch.interval.max stays db_unfetched in CrawlDb and is generated over and over again,"A document gone with 404 after db.fetch.interval.max (90 days) has passed
is fetched over and over again but although fetch status is fetch_gone
its status in CrawlDb keeps db_unfetched. Consequently, this document will
be generated and fetched from now on in every cycle.

To reproduce:
# create a CrawlDatum in CrawlDb which retry interval hits db.fetch.interval.max (I manipulated the shouldFetch() in AbstractFetchSchedule to achieve this)
# now this URL is fetched again
# but when updating CrawlDb with the fetch_gone the CrawlDatum is reset to db_unfetched, the retry interval is fixed to 0.9 * db.fetch.interval.max (81 days)
# this does not change with every generate-fetch-update cycle, here for two segments:
{noformat}
/tmp/testcrawl/segments/20120105161430
SegmentReader: get 'http://localhost/page_gone'
Crawl Generate::
Status: 1 (db_unfetched)
Fetch time: Thu Jan 05 16:14:21 CET 2012
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 6998400 seconds (81 days)
Metadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone

Crawl Fetch::
Status: 37 (fetch_gone)
Fetch time: Thu Jan 05 16:14:48 CET 2012
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 6998400 seconds (81 days)
Metadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone


/tmp/testcrawl/segments/20120105161631
SegmentReader: get 'http://localhost/page_gone'
Crawl Generate::
Status: 1 (db_unfetched)
Fetch time: Thu Jan 05 16:16:23 CET 2012
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 6998400 seconds (81 days)
Metadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone

Crawl Fetch::
Status: 37 (fetch_gone)
Fetch time: Thu Jan 05 16:20:05 CET 2012
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 6998400 seconds (81 days)
Metadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone
{noformat}

As far as I can see it's caused by setPageGoneSchedule() in AbstractFetchSchedule. Some pseudo-code:
{code}
setPageGoneSchedule (called from update / CrawlDbReducer.reduce):
    datum.fetchInterval = 1.5 * datum.fetchInterval // now 1.5 * 0.9 * maxInterval
    datum.fetchTime = fetchTime + datum.fetchInterval // see NUTCH-516
    if (maxInterval < datum.fetchInterval) // necessarily true
       forceRefetch()

forceRefetch:
    if (datum.fetchInterval > maxInterval) // true because it's 1.35 * maxInterval
       datum.fetchInterval = 0.9 * maxInterval
    datum.status = db_unfetched // 


shouldFetch (called from generate / Generator.map):
    if ((datum.fetchTime - curTime) > maxInterval)
       // always true if the crawler is launched in short intervals
       // (lower than 0.35 * maxInterval)
       datum.fetchTime = curTime // forces a refetch
{code}
After setPageGoneSchedule is called via update the state is db_unfetched and the retry interval 0.9 * db.fetch.interval.max (81 days). 
Although the fetch time in the CrawlDb is far in the future
{noformat}
% nutch readdb testcrawl/crawldb -url http://localhost/page_gone
URL: http://localhost/page_gone
Version: 7
Status: 1 (db_unfetched)
Fetch time: Sun May 06 05:20:05 CEST 2012
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 6998400 seconds (81 days)
Score: 1.0
Signature: null
Metadata: _pst_: notfound(14), lastModified=0: http://localhost/page_gone
{noformat}
the URL is generated again because (fetch time - current time) is larger than db.fetch.interval.max.
The retry interval (datum.fetchInterval) oscillates between 0.9 and 1.35, and the fetch time is always close to current time + 1.35 * db.fetch.interval.max.

It's possibly a side effect of NUTCH-516, and may be related to NUTCH-578",smartshark_2_2,415,nutch,"""[0.07635904848575592, 0.9236409664154053]"""
2325,230584,"jsoup-extractor structure correction, typo fixed","Several bugs faced during testing with my project have been fixed
1. Missed root tag <extractor> added in jsoup-extractor.xml like jsoup-extractor-example.xml
2. jsoup API text() used instead of ownText() to get full contents under CSS selector
3. <default> => <default-value> typo fixed
",smartshark_2_2,1341,nutch,"""[0.22185970842838287, 0.7781403064727783]"""
2326,230585,Included Solr schema.xml and solrindex-mapping.xml don't play together,"conf/solrindex-mapping.xml already fills in ""id"" field, but conf/schema.xml uses copyField to create ""id"" from ""url"". This results in multiple values of the ""id"" field which is not allowed, because id is defined as uniqueKey in the schema.",smartshark_2_2,1524,nutch,"""[0.253974586725235, 0.7460253834724426]"""
2327,230586,Fetcher throws NullPointer if redirect URL is filtered,"Inside the Fetcher class if a redirect URL is filtered, for example jessionid pages are filtered with the default URL filter, then a NullPointerException is thrown when Fetcher trys to print out that the url was skipped for being an identical url.  It is not an identical URL but a filtered url.  So what we really need is two different checks.  One for null url and one for identical url.  I have included a patch that handles this.",smartshark_2_2,90,nutch,"""[0.06168637424707413, 0.9383136034011841]"""
2328,230587,CleaningJob's reducer does not commit deleted docs. ,"In cleanup(Context context) method, ""if condition"" has logical problem.",smartshark_2_2,522,nutch,"""[0.14693701267242432, 0.8530630469322205]"""
2329,230588,Empty Summaries and Cached Pages,There is a bug where some search results do not have summaries and viewing their cached pages causes a NullPointer.  This bug is due to redirects getting stored under the new url and the getURL method of FetchedSegments getting the wrong (old) url which is stored in crawldb but has no content or parse objects.,smartshark_2_2,1760,nutch,"""[0.06677722185850143, 0.933222770690918]"""
2330,230589,Charset issues when using -addBinaryContent and -base64 options,"The bug is reproducible with these steps:

# find a site with cp1252 encoded pages like ""http://www.ilsole24ore.com/"" and characters with accents (byte representation >127, like [Ã Ã¨Ã©Ã¬Ã²Ã¹])
# start a crawl on that site indexing on Solr with options -addBinaryContent -base64
# find a document inside the newly indexed Solr collection with those accented characters
# get the base64 binary representation for said html page and decode it back to raw binary, save it

The file obtained will have invalid characters, which are neither UTF-8 nor cp1252.",smartshark_2_2,1192,nutch,"""[0.0723232850432396, 0.9276767373085022]"""
2331,230590,OpenSearch Servlet ist broken,"the last path from 31.1.2006 seems to have the servlet boken. My suggestion to fix:

--- OpenSearchServlet.java      2006-02-03 14:30:33.000000000 +0100
+++ src/java/org/apache/nutch/searcher/OpenSearchServlet.java   2006-02-01 09:29:19.000000000 +0100
@@ -59,10 +59,10 @@
   private NutchBean bean;
   private NutchConf nutchConf;
 
-  public void init(ServletConfig config) throws ServletException {
+  public void init(ServletConfig config, NutchConf nutchConf) throws ServletException {
     try {
-      this.nutchConf = new NutchConf();
       bean = NutchBean.get(config.getServletContext(), nutchConf);
+      this.nutchConf = nutchConf;
     } catch (IOException e) {
       throw new ServletException(e);
     }

Opensearch also needs Xalan. This seems not to be included anymore. ",smartshark_2_2,163,nutch,"""[0.07995179295539856, 0.9200482368469238]"""
2332,230591,SolrDedup broken,"Some Solr indices are unable to be deduped from Nutch. For unknown reasons Nutch will throw the exception below. There are no peculiarities to be found in the Solr logs, the queries are normal and seem to succeed.

{code}
java.lang.NullPointerException
        at org.apache.hadoop.io.Text.encode(Text.java:388)
        at org.apache.hadoop.io.Text.set(Text.java:178)
        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:272)
        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:243)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{code}",smartshark_2_2,1988,nutch,"""[0.05968528613448143, 0.940314769744873]"""
2333,230592,Fetcher2 - java.lang.NullPointerException when host does not exist and fetcher.threads.per.host.by.ip is set to true causes threads to finish.,"When fetcher.threads.per.host.by.ip is set to true the following exception is thrown when the host does not exist. FetchItem.create returns null when it is not able to resolve the host address when it is redirecting.

2007-12-30 15:34:42,720 WARN  fetcher.Fetcher2 - Unable to resolve: {url}  , skipping.
2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - java.lang.NullPointerException
2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:327)
2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:323)
2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetcherThread.run(Fetcher2.java:632)
2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - fetcher caught:java.lang..NullPointerException
2007-12-30 15:34:42,721 INFO  fetcher.Fetcher2 - -finishing thread FetcherThread, activeThreads=49",smartshark_2_2,309,nutch,"""[0.06593501567840576, 0.9340649843215942]"""
2334,230593,incorrect mime type detection by MoreIndexingFilter plugin,When server sends {{Content-Type}} header with optional params like {{Content-Type: text/html; charset=UTF-8}} MoreIndexingFilter returns null in {{type}} field.,smartshark_2_2,347,nutch,"""[0.07303520292043686, 0.9269648194313049]"""
2335,230594,Nutch File Dump - FileNotFoundException (Invalid Argument) Error,"Got *FileNotFoundException* while running nutch dump.

*Cause*: Character '?' in file name/extension producing the below error.

*Error Details*
java.io.FileNotFoundException: /media/PATRO/Karan/nutch_12Oct/other_gun_urls/img/99/fb/97d3980f9954b597f372d092b97eff22_27tlt_recon_1_black_g_10_handle_.jpeg? (Invalid argument)
at java.io.FileOutputStream.open(Native Method)
at java.io.FileOutputStream.(FileOutputStream.java:221)
at java.io.FileOutputStream.(FileOutputStream.java:171)
at org.apache.nutch.tools.FileDumper.dump(FileDumper.java:222)
at org.apache.nutch.tools.FileDumper.main(FileDumper.java:325)",smartshark_2_2,1004,nutch,"""[0.30411988496780396, 0.695880115032196]"""
2336,230595,URL filtering is always disabled in Generator when invoked by Crawl,"When a crawl is done using the 'bin/nutch crawl' command, no filtering is done in Generator even if 'crawl.generate.filter' is set to true in the configuration file.

The problem is that in the Generator's generate method, the following code unconditionally sets the filter value of the job to whatever is passed to it:-

{code}job.setBoolean(CRAWL_GENERATE_FILTER, filter);{code}

The code in Crawl.java always passes this as false. 

This has been fixed by exposing an overloaded generate method which takes only the 5 arguments that Crawl needs to set. This overloaded method reads the configuration and sets the filter value appropriately.",smartshark_2_2,1468,nutch,"""[0.09167276322841644, 0.9083272218704224]"""
2337,230596,HTML redirections are not followed when using parse-tika,"Html redirections using meta tags are supported in nutch. They work well when using parse-html to parse files. However, when using parse-tika, they are not detected.

This is because of https://issues.apache.org/jira/browse/TIKA-2652

Tika emits redirection meta tags as :

{code:xml}
<meta name=""refresh"" content=""0; url=http://example.com""/>
{code}

whereas org.apache.nutch.parse.tika.HTMLMetaProcessor expects meta tags having the following format :

{code:xml}
<meta http-equiv=""refresh"" content=""0; url=http://example.com"">
{code}


The bug can be reproduced with the following nutch-site.xml:
{code:xml}
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>plugin.includes</name>
        <value>protocol-http|parse-tika</value>
    </property>
    <property>
        <name>http.agent.name</name>
        <value>blah</value>
    </property>
</configuration>
{code}

fetching this url: http://www.google.com/policies/technologies/ads/

The resulting status is {code}success(1,0){code} whereas using parse-html, the resulting status is {code:html}success(1,100), args[0]=https://policies.google.com/technologies/ads, args[1]=0{code}",smartshark_2_2,2555,nutch,"""[0.22359627485275269, 0.7764037251472473]"""
2338,230597,LinkDb Fails rename doesn't create parent directories,"The LinkDb install method fails to correctly rename (move) the LinkDb working directory to the final directory if the parent directories do not exist.

For example if I am creating a linkdb by the name of crawl/linkdb the install method trys to rename the working linkdb directory (something like linkdb-20060523 in root of DFS) to crawl/linkdb/current.  But if the crawl/linkdb directory does not already exist then the rename fails and the linkdb-20060523 working directory stays in the root directory of the DFS for the user.

The attached patch adds a mkdirs command to the install method to ensure that the parent directories exist before trying to rename.",smartshark_2_2,91,nutch,"""[0.08720005303621292, 0.9127999544143677]"""
2339,230598,Injector overwrite does not always work properly,"db.injector.update works as it should but db.injector.overwrite doesn't always seem to properly overwrite the record. This issue exists for some time and we've already fixed it in our dist of Nutch.

This record just has been updated (interval).
{code}
Injector: starting at 2013-07-03 10:34:15
Injector: crawlDb: crawl/crawldb
Injector: urlDir: seeds
Injector: Converting injected urls to crawl db entries.
Injector: total number of urls rejected by filters: 0
Injector: total number of urls injected after normalization and filtering: 9
Injector: Merging injected urls into crawl db.
Injector: finished at 2013-07-03 10:34:21, elapsed: 00:00:05
URL: url
Version: 7
Status: 2 (db_fetched)
Fetch time: Fri Jul 05 12:11:44 CEST 2013
Modified time: Fri Jun 28 12:11:44 CEST 2013
Retries since fetch: 0
Retry interval: 604800 seconds (7 days)
Score: 0.0
Signature: ba29ef3e680323a6d0da74c156800e03
Metadata: Content-Type: text/html_pst_: success(1), lastModified=0

{code}

If we now overwrite the record, nothing happens. With this patch installed it overwrites the record as it should and also logs update & overwrite switches to console:

{code}
Injector: starting at 2013-07-03 10:36:30
Injector: crawlDb: crawl/crawldb
Injector: urlDir: seeds
Injector: Converting injected urls to crawl db entries.
Injector: total number of urls rejected by filters: 0
Injector: total number of urls injected after normalization and filtering: 9
Injector: Merging injected urls into crawl db.
Injector: overwrite: true
Injector: update: false
Injector: finished at 2013-07-03 10:36:36, elapsed: 00:00:05
URL: url
Version: 7
Status: 1 (db_unfetched)
Fetch time: Wed Jul 03 10:36:30 CEST 2013
Modified time: Thu Jan 01 01:00:00 CET 1970
Retries since fetch: 0
Retry interval: 14000 seconds (0 days)
Score: 1.0
Signature: null
Metadata: fixedInterval: 14000.0
{code}",smartshark_2_2,2367,nutch,"""[0.08070606738328934, 0.9192939400672913]"""
2340,230599,Indexing filter checker leaks threads,Same issue as NUTCH-2320.,smartshark_2_2,2455,nutch,"""[0.06642229110002518, 0.9335776567459106]"""
2341,230600,-force and -resume arguments being ignored in ParserJob,"From the log below there is obviously something not right here as both -resume and -force are passed to the CLI but blatantly ignored within the log output.

lewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch parse
Usage: ParserJob (<batchId> | -all) [-crawlId <id>] [-resume] [-force]
    <batchId>     - symbolic batch ID created by Generator
    -crawlId <id> - the id to prefix the schemas to operate on, 
 	 	    (default: storage.crawl.id)
    -all          - consider pages from all crawl jobs
    -resume       - resume a previous incomplete job
    -force        - force re-parsing even if a page is already parsed
lewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch parse -all -resume -force
ParserJob: starting
ParserJob: resuming:	false
ParserJob: forced reparse:	false
ParserJob: parsing all
Parsing http://www.trancearoundtheworld.com/
ParserJob: success
",smartshark_2_2,2179,nutch,"""[0.25820210576057434, 0.7417978644371033]"""
2342,230601,Release deliverable as zip,"Like Lucene, Nutch could be delivered as a .zip file so it can be used with default tools on Windows.",smartshark_2_2,1578,nutch,"""[0.9975675344467163, 0.002432540524750948]"""
2343,230602,Add method to nutch and tika system(Code written),"  public MimeType getMimeType(byte[] content){
	  return this.mimeTypes.getMimeType(content);
  }
 paste this code into nutch/src/java/org/apache/nutch/util/MimeUtil.java which will allow byte content to be used to determine file type",smartshark_2_2,1467,nutch,"""[0.9984748959541321, 0.0015250592259690166]"""
2344,230603,Generator to select on retry interval,"The generator already has a mechanism to select entries with a score larger than specified threshold but should also have a means to select entries with a retry interval lower than specified by a configuration option.

Such a feature is particulary useful when dealing with too large crawldb's where you still want a crawl to fetch rapid changing url's first.

This issue should also add the missing generate.min.score configuration to nutch-default.",smartshark_2_2,2248,nutch,"""[0.9984412789344788, 0.001558698364533484]"""
2345,230604,ParseData's contentMeta accumulates unnecessary values during parse,"After NUTCH-506, if you run parse on a segment, parseData's contentMeta accumulates metadata of every content parsed so far. This is because NUTCH-506 changed constructor to create a new metadata (before NUTCH-506, a new metadata was created for every call to readFields). It seems hadoop somehow caches Content instance so each new call to Content.readFields during ParseSegment increases size of metadata. Because of this, one can end up with *huge* parse_data directory (something like 10 times larger than content directory)


",smartshark_2_2,1454,nutch,"""[0.8775734901428223, 0.12242656201124191]"""
2346,230605,SolrIndexer to write to multiple servers.,"SolrUtils should return an array of SolrServers and read the SolrUrl as a comma delimited list of URL's using Configuration.getString(). SolrWriter should be able to handle this list of SolrServers.

This is useful if you want to send documents to multiple servers if no replication is available or if you want to send documents to multiple NOCs.

edit:
This does not replace NUTCH-1377 but complements it. With NUTCH-1377 this issue allows you to index to multiple SolrCloud clusters at the same time.",smartshark_2_2,2382,nutch,"""[0.9982637763023376, 0.0017362349899485707]"""
2347,230606,Dump via Services end point ,"Expose the ./bin/nutch dump command via the REST api. 

Please review the documentation of the api design on http://docs.apachenutchrestapi.apiary.io/# and give your feedbacks. 

Thank you all for your inputs :) ",smartshark_2_2,2645,nutch,"""[0.9984371066093445, 0.001562926801852882]"""
2348,230607,Refactor configuration end point,To better define the endpoint to create a new configuration and add a new endpoint to update a particular property value of a configuration. ,smartshark_2_2,1036,nutch,"""[0.9978882670402527, 0.0021117909345775843]"""
2349,230608,Fix [cast] javac warnings,"A typical example of this is
{code}
trunk/src/java/org/apache/nutch/crawl/CrawlDatum.java:460: warning: [cast] redundant cast to int
    [javac]         res ^= (int)(signature[i] << 24 + signature[i+1] << 16 + 
{code}
these should all be fixed by replacing with the correct implementations.",smartshark_2_2,2261,nutch,"""[0.9968860745429993, 0.0031139706261456013]"""
2350,230609,Make linkDB optional during indexing,"Having a linkDB is currently mandatory for indexing, however not all users are interested in using the anchors. The linkDB should be optional while indexing ",smartshark_2_2,1926,nutch,"""[0.9985197186470032, 0.0014803445665165782]"""
2351,230610,Omit anchor in webgraph's LinkDatum,"Anchors are stored  unchecked in the webgraph. it looks like for cosmetic reasons only. When dealing with hundreds of millions of records it takes up significant space and I/O time.

This issue should add an option to omit the anchor.
",smartshark_2_2,937,nutch,"""[0.9955549836158752, 0.004445053171366453]"""
2352,230611,Make protocol-selenium README part of plugin,"This is a simple issue which merely ports the documentation from the selenium plugin over to the source code
https://github.com/momer/nutch-selenium/blob/master/README.md",smartshark_2_2,762,nutch,"""[0.9977681636810303, 0.0022318721748888493]"""
2353,230612,Dmoz Structure Parser Tool,This is a tool that will take the dmoz structure RDF file and return a listing of the categories.  The categories return can be limited by depth or by regular expression pattern.  This tool borrows heavily from the DmozParser.,smartshark_2_2,287,nutch,"""[0.9983708262443542, 0.0016291692154482007]"""
2354,230613,nutchgora job failures should be noticed by submitter,"I stumbled upon an issue where crawling seem to go right, only to notice much later on that jobs actually failed as a whole.

This is caused because for most jobs that are submitted, Nutchgora does not check the 'succeeded' boolean that is returned. This should be done and acted upon appropriately. (Either throwing an exception or returning non-zero exit codes).",smartshark_2_2,1957,nutch,"""[0.7991645932197571, 0.20083536207675934]"""
2355,230614,Optimize UpdateDb to load less field from Store,"While running large crawl i found that updatedb run very slow, especially the Map task which loading data from store.
We can't use filter by batchId to load less url due to bug in NUTCH-1679 so we must always update the whole table.

After checking the field loaded in UpdateDbJob i found that it load many fields from store (at least 15/25 field) which make updatedb slow

I think that UpdateDbJob only need to load few field: SCORE, OUTLINKS, METADATA which is used to compute link score, distance that i think the main purpose of this job.
The other fields is used to compute url schedule to parser and fetcher, we can move code to Parser or Fetcher whithout loading much new field because many field are generated from parser. WE can also use gora filter for Fetcher or Parser so load new field is not a problem.

I also add new field SCOREMETA to WebPage to store CASH, and DISTANCE. It is currently store in METADATA. field CASH is used in OPICScoring which is used only in UpdateDB and distance is used only in Generator and Updater so move both field two new Metadata field can prevent reading METADATA at Generator and Updater, METADATA contains many data that is used only at Parser and Indexer

So with new change
UpdateDb only load SCORE, SCOREMATA (CASH, DISTANCE), OUTLINKS, MAKERS: we don't need to load big family Fetch and INLINKS.
Generator only load SCOREMETA (which is smaller than current METADATA)
",smartshark_2_2,623,nutch,"""[0.9891448616981506, 0.010855190455913544]"""
2356,230615,Eclipse shows build path errors on building Nutch,"On running ant eclipse and importing the project in eclipse, Eclipse throws build path errors for missing test packages. (geoip/test, lib-selenium/test, protocol-selenium/test)",smartshark_2_2,988,nutch,"""[0.9928834438323975, 0.007116578985005617]"""
2357,230616,Extreme Nested Tags causes StackOverflowException in DomContentUtils...Spider Trap,Some webpages have a form of a spider trap that causes a StackOverflowException in DomContentUtils by having nested tags with thousands of layers deep.  DomContentUtils when trying to get outlinks uses a recursive method to parse the html.  With this type of nesting it errors out.,smartshark_2_2,285,nutch,"""[0.07119149714708328, 0.9288084506988525]"""
2358,230617,Fetcher to log thread ID,Better logging for the fetcher.,smartshark_2_2,2561,nutch,"""[0.9982168078422546, 0.0017831942532211542]"""
2359,230618,Add pull-reqest template to github,"Github allows to add [pull request templates](https://help.github.com/articles/creating-a-pull-request-template-for-your-repository/). For contributors already familiar with github from other projects that's probably the best place to show a check list which helps us to get pull requests merged more quickly. Here's a draft:
{noformat}
Thanks for your contribution to [Apache Nutch](http://nutch.apache.org/)! Your help is appreciated!

Before opening the pull request, please verify that
* there is an open issue on the [Nutch issue tracker](https://issues.apache.org/jira/projects/NUTCH) which describes the problem or the improvement. We cannot accept pull requests without an issue because the change wouldn't be listed in the release notes.
* the issue ID (`NUTCH-XXXX`)
Â  - is referenced in the title of the pull request
Â  - and placed in front of your commit messages
* commits are squashed into a single one (or few commits for larger changes)
* Java source code follows [Nutch Eclipse Code Formatting rules](https://github.com/apache/nutch/blob/master/eclipse-codeformat.xml)
* Nutch builds and unit tests pass by running `ant clean runtime test`
{noformat}",smartshark_2_2,2546,nutch,"""[0.9983931183815002, 0.0016068837139755487]"""
2360,230619,Remove debug log from MimeUtil,"My patch for NUTCH-1991 contained debug logs on level WARN (sorry, should definitely check the patch files before uploading them :(). Needless warnings are now shown:
{noformat}
2015-05-13 16:37:58,396 WARN  util.MimeUtil - >>null
{noformat}",smartshark_2_2,800,nutch,"""[0.9979598522186279, 0.0020401515066623688]"""
2361,230620,Upgrade all jobs to new MapReduce API,"We should upgrade to the new Hadoop API for Nutch trunk as already has been done for the Nutchgora branch. If i'm not mistaken we can already upgrade to the latest 0.20.5 version that still carries the legacy API so we can, without immediately upgrading to 0.21 or higher, port the jobs to the new API without having the need for a separate branch to work on.

To the committers who created/ported jobs in NutchGora, please write down your advice and experience.

http://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api",smartshark_2_2,2206,nutch,"""[0.9983400106430054, 0.0016599803930148482]"""
2362,230621,Fetcher to stringify exception on // unexpected exception,During development we sometimes saw a less than helpful exception e.g. fetch of http://www.openindex.io/en/home.html failed with: java.lang.NullPointerException. This error must be a bit more descriptive.,smartshark_2_2,2282,nutch,"""[0.8672551512718201, 0.13274484872817993]"""
2363,230622,Pluggable url partitioner,"At present, the url partition logic is hard wired inside nutch core. It should be pluggable like FetchSchedule customized via nutch-site.xml.

There might be use cases where a single domain needs to be partioned on some custom logic. The existing UrlPartitioner cannot handle such cases. 

Hence the requirement.",smartshark_2_2,884,nutch,"""[0.998505711555481, 0.0014943683054298162]"""
2364,230623,Log mapreduce job counters in local mode,"A simple change in the log4j.properties would make the Hadoop job counters appear in the hadoop.log also in local mode:
{noformat}
log4j.logger.org.apache.hadoop.mapreduce.Job=INFO
{noformat}
This may provide useful information for debugging, esp. if counters are not explicitly logged by tools (see [@user|https://lists.apache.org/thread.html/1dd5410b479bd536fb3df98612db4b832cd0a97533099b0dc632eba9@%3Cuser.nutch.apache.org%3E]). This would make the output also more similar to (pseudo)distributed mode (Nutch is called via {{hadoop jar}}) Job counters and progress info are obligatorily logged.",smartshark_2_2,2476,nutch,"""[0.9985754489898682, 0.0014245767379179597]"""
2365,230624,Add more normalization rules to regex-normalize file.,Add more normalization rules to the urlnormalizer-regex configuration file. Some useful rules can be glimpsed from the [GoogleMini|http://code.google.com/apis/searchappliance/documentation/46/admin_crawl/Troubleshooting.html] documentation.,smartshark_2_2,380,nutch,"""[0.9983749389648438, 0.001625068485736847]"""
2366,230625,Add trivial comment to lib/native/README.txt,"This trivial issue simply adds missing comments to the above file. The WARN logging which is churned out has caused a small degree of confusion in the past, therefore this sorts that out :0)",smartshark_2_2,1904,nutch,"""[0.9984484910964966, 0.0015515295090153813]"""
2367,230626,Please delete old releases from mirroring system,"To reduce the load on the ASF mirrors, projects are required to delete old releases [1]

Please can you remove all non-current releases?
Thanks!

Also, please note that the download page refers to 1.7 in the first body paragraph. That should be 1.8


[1] http://www.apache.org/dev/release.html#when-to-archive",smartshark_2_2,445,nutch,"""[0.9982699155807495, 0.0017300666077062488]"""
2368,230627,URLUtil should not add additional slashes for file URLs,UrlUtil.toASCII(String url) and .toUNICODE(String url) add two slashes to file URLs if it contains a single slash: {{file:/path/index.html}} becomes {{file:///path/index.html}}. Both methods should keep the single slash to get a behavior consistent with URL.toString(). See NUTCH-1483 for details.,smartshark_2_2,666,nutch,"""[0.8204607963562012, 0.17953920364379883]"""
2369,230628,Image Search,"Per the discussion in the Nutch-User mailing list, there is a wish for an ""Image Search"" add-on component that will index images.

Must have:
- retrieve outlinks to image files from fetched pages
- generate thumbnails from images
- thumbnails are stored in the segments as ImageWritable that contains the compressed binary data and some meta data 

Should have:
- implemented as hadoop map reduce job
- should be seperate from main Nutch codeline as it breaks general Nutch logic of one url == one index document.

Could  have:
- store the original image in the segments

Would like to have:
- search interface for image index
- parameterizable thumbnail generation (width, height, quality)",smartshark_2_2,1654,nutch,"""[0.9984540939331055, 0.0015458674170076847]"""
2370,230629,JUnit test for microformats-reltag,This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.,smartshark_2_2,914,nutch,"""[0.9984594583511353, 0.0015405458398163319]"""
2371,230630,Remove FetcherOutput,The FetcherOutput class is not used anywhere and it and its references should be removed.,smartshark_2_2,2304,nutch,"""[0.9981507658958435, 0.0018492310773581266]"""
2372,230631,Index job resume switch to resume a failed job,"This is also useful in nutchgora to allow for continuous indexing with -all -resume, as it is for fetching, cron scripts can then be independent without having to know the batchid.",smartshark_2_2,1168,nutch,"""[0.9984347224235535, 0.0015652200672775507]"""
2373,230632,Create Outlinks with metadata,Having the possibility to specify metadata when creating an outlink is extremely useful as it allows to pass information from a source page to the pages it links to. We use that routinely within our custom parsers in combination with the url-meta plugin.,smartshark_2_2,820,nutch,"""[0.9984744191169739, 0.001525521045550704]"""
2374,230633,better url-normalizer basic,"Basic URL normalizer lacks 2 important features

Encode space in URL into %20 to unbreak httpclient and possibly others who do not expect space inside URL

Ability to decode %33 encoding in URL. This is important for avoiding duplicates",smartshark_2_2,1891,nutch,"""[0.9983394145965576, 0.0016605748096480966]"""
2375,230634,Increase robustness and crawling versatility of Nutch for the Deep Web,"Nutch fails to grab a page or crawl in a manner that is more productive in certain cases. This issue is to discuss those specific cases and try to generalize them into Nutch to make it even more robust and productive.

I came across three websites and got many issues. I have toned down those issues into fine points.

1. Some websites detect that the crawler is not a browser (marketwired) (cookie validations) and send it to the first page again and again.
2. Some data behind a click (detect which clicks: javascript void) of 'a tag' that is not a link exactly (an improvement for the selenium plugin)
3. When clicked something on a page and the page changed, how to get back the page before clicking further (canât obviously look for a back button or cross button. Can save the old state juxtapose with new info and only take the extra info)
4. Differentiate between a navigation link and a common link in a forum page so that both links can be used differently to decide the progress of the crawler (nav links decide the rounds and other links we can go one round)
5. Bring the capability of changing # to ? (pataxia.com). Right now url normalization completely removes the part after # thinking that it's a simple anchor tag.
6. Easy route-decision in property file to decide how the fetcher will behave (instead of going all BFS or DFS, there should be a away to make it go DEPTH-LIMITED search. Esp good for forums and the likes of it. And users can give some known inputs like depth etc. to direct the crawler if they know something specific about the site)
7. A forum can be roughly generalized into: a meta topic page (no nav links) -> post list (with nav links) -> post page (with nav links) : How to make nutch aware of this structure/heirachy. If manually give simple clues as well. Can be seen as an extension of the last point.
8. Sometimes even nav links are not actual links but ajax requests.

NOTE: Nav links (definition here): the structure on a web page (like a forum) which gives us an option to go to various pages by numbers or next, previous, first and or last pages.",smartshark_2_2,991,nutch,"""[0.9983670115470886, 0.0016330161597579718]"""
2376,230635,black- white list url filtering,Existing url filter mechanisms need to process each url against each filter pattern. For very large filter sets this may be does not scale very well.,smartshark_2_2,1608,nutch,"""[0.9980611205101013, 0.0019388138316571712]"""
2377,230636,Elasticsearch REST Indexer broken due to wrong depenency,"When trying to index into Elasticsearch using {{indexer-elastic-rest}} the following error is being thrown:
{code}
Exception in thread ""main"" java.lang.LinkageError: loader constraint violation: when resolving method ""org.slf4j.impl.StaticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory;"" the class loader (instance of org/apache/nutch/plugin/PluginClassLoader) of the current class, org/slf4j/LoggerFactory, and the class loader (instance of sun/misc/Launcher$AppClassLoader) for the method's defining class, org/slf4j/impl/StaticLoggerBinder, have different Class objects for the type org/slf4j/ILoggerFactory used in the signature
    at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:418)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)
    at org.apache.nutch.indexwriter.elasticrest.ElasticRestIndexWriter.<clinit>(ElasticRestIndexWriter.java:71)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at java.lang.Class.newInstance(Class.java:442)
    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:161)
    at org.apache.nutch.indexer.IndexWriters.<init>(IndexWriters.java:57)
    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)
    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:230)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:239)
{code}

[e66d44d|https://github.com/apache/nutch/commit/e66d44d9c290c550e78edb425a43e010b861172c#diff-aefa48b9ce916d2e33dc27b153c44977] removed the runtime dependency on {{slf4j-api-1.7.21.jar}} everywhere but in {{indexer-elastic-rest}}.
Possible fix: https://github.com/apache/nutch/pull/253",smartshark_2_2,2411,nutch,"""[0.472595751285553, 0.527404248714447]"""
2378,230637,protocol-http cannot handle colons after the HTTP status code,Some servers invalidly add colons after the HTTP status code in the status line (they can send _HTTP/1.1 404: Not found_ instead of _HTTP/1.1 404 Not found_ for instance). Browsers can handle that.,smartshark_2_2,2574,nutch,"""[0.33673563599586487, 0.6632643342018127]"""
2379,230638,Fix missing/wrong headers in source files,It seems several source files still do not contain the proper ASL headers. This includes older core in 1.3 (indexer.NutchField etc) and recent code in 2.0 (API for instance). This should be fixed (yet again). So if you spot one ;),smartshark_2_2,1787,nutch,"""[0.9948403239250183, 0.005159721244126558]"""
2380,230639,capture batchId and remove references to segments in 2.x crawl script.,"The concept of segment is replaced by batchId in 2.x
I'm currently getting rid of segments references in 2.x
This issue was flagged up and separate from NUTCH-1532 which I am working on.",smartshark_2_2,2339,nutch,"""[0.9939342141151428, 0.006065822206437588]"""
2381,230640,Fine tune Solr schema,The supplied schema is old and doesn't use more advanced fieldTypes such as Trie based (since Solr 1.4) and perhaps other improvements. We need to fine tune the schema.,smartshark_2_2,1800,nutch,"""[0.9985345602035522, 0.001465393346734345]"""
2382,230641,Increase scalability by only removing markers when they actually exist for DbUpdaterReducer,"After applying GORA-120 (this already is a huge performance boost by itself) one of the major bottlenecks of the DbUpdaterReducer is the deletion of the markers. The update reducer simply sets every row to delete its markers. A lot of rows do not actually have the markers but the deletes are fired away in any case. Because the markers are already always on the input, a simple check to see if they exist greaty improves performance.

In particular it is very expensive in HBase, because every single Delete inmediately triggers a connection to the regionservers. (They ignore the ""autoflush=false"" directive). Although deletes can be done in batch, this is currently not supported by Gora. For one it is very difficult to implement in the current HBaseStore with regard to multithreading, and secondly I noticed performance did not increase significantly.

By performance debugging on a real life cluster this currently seems to be the biggest bottleneck of the DbUpdaterReducer. (Remember only after applying GORA-120)",smartshark_2_2,2056,nutch,"""[0.9983343482017517, 0.0016656749648973346]"""
2383,230642,Avoid lock by MimeUtil in constructor of protocol.Content,"The constructor of the class o.a.n.protocol.Content instantiates a new MimeUtil object. That's not cheap as it always creates a new Tika object and there is a lock on the job/jar file when config files are read:
{noformat}
""FetcherThread"" #146 daemon prio=5 os_prio=0 tid=0x00007f70523c3800 nid=0x1de2 waiting for monitor entry [0x00007f70193a8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.util.zip.ZipFile.getEntry(ZipFile.java:314)
        - waiting to lock <0x00000005e0285758> (a java.util.jar.JarFile)
        at java.util.jar.JarFile.getEntry(JarFile.java:240)
        at java.util.jar.JarFile.getJarEntry(JarFile.java:223)
        at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042)
        at sun.misc.URLClassPath$JarLoader.findResource(URLClassPath.java:1020)
        at sun.misc.URLClassPath$1.next(URLClassPath.java:267)
        at sun.misc.URLClassPath$1.hasMoreElements(URLClassPath.java:277)
        at java.net.URLClassLoader$3$1.run(URLClassLoader.java:601)
        at java.net.URLClassLoader$3$1.run(URLClassLoader.java:599)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader$3.next(URLClassLoader.java:598)
        at java.net.URLClassLoader$3.hasMoreElements(URLClassLoader.java:623)
        at sun.misc.CompoundEnumeration.next(CompoundEnumeration.java:45)
        at sun.misc.CompoundEnumeration.hasMoreElements(CompoundEnumeration.java:54)
        at java.util.Collections.list(Collections.java:5239)
        at org.apache.tika.config.ServiceLoader.identifyStaticServiceProviders(ServiceLoader.java:325)
        at org.apache.tika.config.ServiceLoader.loadStaticServiceProviders(ServiceLoader.java:352)
        at org.apache.tika.config.ServiceLoader.loadServiceProviders(ServiceLoader.java:274)
        at org.apache.tika.detect.DefaultEncodingDetector.<init>(DefaultEncodingDetector.java:45)
        at org.apache.tika.config.TikaConfig.getDefaultEncodingDetector(TikaConfig.java:92)
        at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:248)
        at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:386)
        at org.apache.tika.Tika.<init>(Tika.java:116)
        at org.apache.nutch.util.MimeUtil.<init>(MimeUtil.java:69)
        at org.apache.nutch.protocol.Content.<init>(Content.java:83)
        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:316)
        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:341)
{noformat}

If there are many Fetcher threads this may cause a significant bottleneck, running a Fetcher with 120 threads I've found up to 50 threads waiting for this lock:
{noformat}
# pid 7195 is a Fetcher map task
% sudo -u yarn jstack 7195 \
      | grep -A25 'waiting to lock' \
      | grep -F 'org.apache.tika.Tika.<init>' \
      | wc -l
49
{noformat}

As MimeUtil is thread-safe [including the called Tika detector|https://www.mail-archive.com/user@tika.apache.org/msg00296.html], the best solution seems to cache the MimeUtil object in the actual protocol implementation as it is done in Nutch 2.x ([lib-http HttpBase, line #151|https://github.com/apache/nutch/blob/2.x/src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java#L151]).",smartshark_2_2,2596,nutch,"""[0.8394310474395752, 0.16056901216506958]"""
2384,230643,URLNormalizerChecker to run as TCP Telnet service,"Similar to NUTCH-2320, but then for normalizer checker.",smartshark_2_2,2423,nutch,"""[0.9981928467750549, 0.0018072167877107859]"""
2385,230644,Configurable length for Tlds,"Length of the tld  should be selectable, there is some available tld's like .travel and url-validator plugin filters this type of urls.",smartshark_2_2,473,nutch,"""[0.9985606074333191, 0.0014394047902897]"""
2386,230645,Handling common error-pages as 404,"Idea: Some pages from some software-packages/scripts report an ""http 200 ok"" even though a specific page could not be found. Example I just found  is:
http://www.deteimmobilien.de/unternehmen/nbjmup;Uipnbt/IfsctuAefufjnnpcjmjfo/ef
That's a typo3-page explaining in it's standard-layout and wording: ""The requested page did not exist or was inaccessible.""

So I had the idea if somebody might create a plugin that could find commonly used formulations for ""page does not exist"" etc. and turn the page into a 404 before feeding them  into the nutch-index  - although the server responded with status 200 ok.
",smartshark_2_2,94,nutch,"""[0.9787719249725342, 0.021228116005659103]"""
2387,230646,Add a combiner to improve performance on updatedb,"We have a lot of similar links with status ""linked"" generated at the ouput of the map task when we try to update the crawldb based on the segment fetched.

We can use a combiner to improve the performance.",smartshark_2_2,1453,nutch,"""[0.9983229041099548, 0.0016770396614447236]"""
2388,230647,"Type safe members , arguments for better readability ","Enable generics for some of the API, for better type safety and readability, in the process. 
",smartshark_2_2,699,nutch,"""[0.9983651041984558, 0.0016349311918020248]"""
2389,230648,CrawlDbReader -stats to show quantiles of score,"The command ""readdb -stats"" shows for the CrawlDatum score min., max. and average. Median and quartiles (quantiles, in general) would complete the statistics to get an impression how scores are distributed.",smartshark_2_2,2403,nutch,"""[0.9981842637062073, 0.0018157234881073236]"""
2390,230649,Inconsistent 'Modified Time' in crawl db,"The 'Modified time' in crawldb is invalid. It is set to (0-Timezone Difference)

*How to verify/reproduce:*
  Run 'nutch readdb /path/to/crawldb -dump yy' and then inspect content of 'yy'

The following improvements can be done:
1. Set modified time by DefaultFetchSchedule
2. Set ProtocolStatus.lastModified if modified time is available in protocol response headers


This issue is also discussed in dev mailing lists: http://www.mail-archive.com/dev@nutch.apache.org/msg19803.html#",smartshark_2_2,1247,nutch,"""[0.6018530130386353, 0.39814695715904236]"""
2391,230650,HostNormalizer,"Nutch would benefit from having a host normalizer. A host normalizer maps a given host to the desired host. A basic example is to map www.apache.org to apache.org. The Apache website is one of many on the internet that has a duplicate website on the same domain just because it allows both www and non-www to return HTTP 200 and proper content.

It is also able to handle wildcards such as *.example.org to example.org if there are multiple sub domains that actually point to the same website.

Large internet crawls tend to get polluted very quickly due to these problems. It also leads to skewed scores in the webgraph as different websites link to different versions of the same duplicate website. An example:

{code}
# Force all sub domains to non-www.
*.example.com example.com

# Force www sub domain to non-www.
www.example.net example.net

# Force non-www. sub domain to www
example.org www.example.org
{code}",smartshark_2_2,2299,nutch,"""[0.9982763528823853, 0.0017236900748685002]"""
2392,230651,Site search powered by Lucene/Solr,"Replace current Nutch site search with Lucene/Solr powered search hosted by Lucid Imagination (http://www.lucidimagination.com/search).  It allows one to search all of the Nutch (content from other parts of the Lucene ecosystem is also available) content from a single place, including web, wiki, JIRA and mail archives. Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. 

A preview of the site with the new search enabled is available at http://people.apache.org/~siren/site/
",smartshark_2_2,2309,nutch,"""[0.9983270764350891, 0.0016728790942579508]"""
2393,230652,New configuration for CommonCrawlDataDumper tool,"Hi all, you can find in attachment a new patch including support for new options for {{CommonCrawlDataDumper}}.
In particultar, new options are passed to {{CommonCrawlFormat}} object (which provides methods to create JSON output) using a configuration object ({{CommonCrawlConfig}}).

In particular, in this patch {{CommonCrawlDataDumper}} provides support for the following options:
* {{-SimpleDataFormat}}: enables timestamps in GMT epoche (milliseconds) format.
* {{-epochFilename}}: files extracted will be organized in a reversed-DNS tree based on the FQDN of the webpage, followed by a SHA1 hash of the complete URL. Scraped data will be stored in these directories as individual GMT-timestamped files using ""epoche time (in milliseconds)"" plus file extension.
* {{-jsonArray}}: organizes both request and response headers into a JSON array instead of using a JSON sub-object.
*{{-reverseKey}}: enables to use the same layout as described for -epochFilename option, with underscore in place of directory separators.

You can use the options above in addition to the options already supported, as described in the [Nutch wiki|https://wiki.apache.org/nutch/CommonCrawlDataDumper] page.
This patch starts from [NUTCH-1974|https://issues.apache.org/jira/browse/NUTCH-1974].

Thanks [~chrismattmann] and [~annieburgess] for supporting me on this work.",smartshark_2_2,755,nutch,"""[0.9984038472175598, 0.0015962125035002828]"""
2394,230653,Protocol-file should treat symbolic links as redirects,"(reported by [~angela_wang], see NUTCH-1884, [[1|https://www.mail-archive.com/dev@nutch.apache.org/msg15614.html]] and [[2|https://www.mail-archive.com/dev@nutch.apache.org/msg15610.html]])

If a file is a symbolic link or contains a link on it's path:, protocol-file follows the link immediately and returns a Content object with the canonical path (all symbolic links resolved) in field ""Location"". This may cause
- the Parse object not available under its expected URL (see NUTCH-1884)
- dubious CrawlDatums (status fetched!) in CrawlDb (first URL is a symbolic link to second item):
{noformat}
file:/var/www/redir_test.html   Version: 7
Status: 2 (db_fetched)
...
Signature: null
Metadata: 
        Content-Type=text/html
        _pst_=success(1), lastModified=0

file:/var/www/test.html Version: 7
Status: 2 (db_fetched)
...
Signature: 50fa8436398f0ecb6b15eaba0574ef23
Metadata: 
        Content-Type=text/html
        _pst_=success(1), lastModified=0
{noformat}
Because signature is null these will never result in duplicates in index.

Protocol-file should instead explicitly redirect to the link target. This should be the default, optionally we could add a property to restore the old behavior.

Should not be difficult to resolve: FileResponse already has status ""redirect"" for symlinks, but File.getProtocolOutput() then resolves the links internally. So we just need to return a redirect response before links are resolved/followed.",smartshark_2_2,667,nutch,"""[0.73947674036026, 0.2605232298374176]"""
2395,230654,Indexing filter of documents by the MIME type,"This allows to filter the indexed documents by the MIME type property of the crawled content. Basically this will allow you to restrict the MIME type of the contents that will be stored in Solr/Elasticsearch index without the need to restrict the crawling/parsing process, so no need to use URLFilter plugin family. Also this address one particular corner case when certain URLs doesn't have any format to filter such as some RSS feeds (http://www.awesomesite.com/feed) and it will end in your index mixed with all your HTML content.

A configuration can file specified on the {{mimetype.filter.file}} property in the {{nutch-site.xml}}. This file use the same format as the {{urlfilter-suffix}} plugin. If no {{mimetype.filter.file}} key is found an {{allow all}} policy is used instead, so all your crawled documents will be indexed.",smartshark_2_2,718,nutch,"""[0.9984096884727478, 0.0015902823070064187]"""
2396,230655,MimeType API deprecated and breaks with Tika 1.0,"We used Tika 1.0-SNAPSHOT in production and just switched to 1.1-SNAPSHOT. The new version triggers the following error:

{code}
2011-12-21 12:29:56,665 ERROR http.Http - java.lang.IllegalAccessError: tried to access method org.apache.tika.mime.MimeTypes.getMimeType([B)Lorg/apache/tika/mime/MimeType; from class org.apache.nutch.util.MimeUtil
2011-12-21 12:29:56,665 ERROR http.Http - at org.apache.nutch.util.MimeUtil.autoResolveContentType(MimeUtil.java:169)
2011-12-21 12:29:56,665 ERROR http.Http - at org.apache.nutch.protocol.Content.getContentType(Content.java:292)
2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.protocol.Content.<init>(Content.java:88)
2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:142)
2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.parse.ParserChecker.run(ParserChecker.java:82)
2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.parse.ParserChecker.main(ParserChecker.java:138)
{code}

",smartshark_2_2,2062,nutch,"""[0.8331595659255981, 0.16684038937091827]"""
2397,230656,Implement Missing NutchServer REST API Tests,"TestAPI.java was all commented. Reason was indicated as:

{quote}
CURRENTLY DISABLED. TESTS ARE FLAPPING FOR NO APPARENT REASON.
SHALL BE FIXED OR REPLACES BY NEW API IMPLEMENTATION
{quote}

So, we should implement that missing tests based on new AbstractNutchAPITestBase.",smartshark_2_2,1253,nutch,"""[0.9983766078948975, 0.0016233875649049878]"""
2398,230657,Integrate Solr/Nutch,"Hi:

After trying out Sami's patch regarding Solr/Nutch. Can be found here (http://blog.foofactory.fi/2007/02/online-indexing-integrating-nutch-with.html) and I can confirm it worked :-) And that lead me to request the following :

I would be very very great full if this could be included in nutch 0.9 as I am trying to eliminate my python based crawler which post documents to solr. As I am in the corporate enviornment I can't install trunk version in the production enviornment thus I am asking this to be included in 0.9 release. I hope my wish would be granted.

I look forward to get some feedback.

Thank you.

",smartshark_2_2,1769,nutch,"""[0.99857497215271, 0.0014250462409108877]"""
2399,230658,Adding goldstandard.txt default file in conf,"Since there is already the stopwords.txt example in the conf directory, instead the goldstandard is missing, it would be nice for newbies finding immediately that file in the directory without having to guess the position and how to create it.",smartshark_2_2,1208,nutch,"""[0.9977095127105713, 0.002290516160428524]"""
2400,230659,Log with Generic Class Name at Nutch 1.x,"There are many mistakes when some reference code is copied and created a new class and a logger is used. We can log with a generic class name to avoid it as like:

{code:java}
private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());
{code}

(cf. SOLR-8324)",smartshark_2_2,1296,nutch,"""[0.9983905553817749, 0.0016094574239104986]"""
2401,230660,Commandline Search,"we have gone through this http://issues.apache.org/jira/browse/NUTCH-330.

With script given in the above issue was fetching the results like title and url , In our case we need to display the Summary also.

Is there any way to obtain it from the given script name(CommandLineSearch.java.)

Please help me in achieve this.

thanks & regards,
Santhosh.Ch 


",smartshark_2_2,1754,nutch,"""[0.9475930333137512, 0.05240698903799057]"""
2402,230661,"Three new plugins that parse, index and query meta tags defined in the configuration","These plugins allow you to define meta tags in you're nutch-site file that you want to include in parseing, indexing and searching.  The query plugin must replace query-basic.  The format for adding query terms to nutch-site.xml is:

<property>
  <name>meta.names</name>
  <value>keywords,recommended</value>
  <description>This is a comma seperated list of meta tag names that will
  be parsed, indexed and searched against when parse-meta, index-meta and
  query-meta are used.</description>
</property>

<property>
  <name>meta.boosts</name>
  <value>1.0,5.0</value>
  <description>Comma seperated list of boost values when searching using
  query-meta.  The order of the values should match the order of meta.names.
  </description>
</property>

Meta tags found are assumed to have either a single value or be a comma seperated list of values.  The values found are added to the index as lucene keywords (i.e. meta name=keywords values=""First Thing, Second Thing"" would result in two keyword fields named ""keywords"".  The first would countain ""First Thing"" and the second would contain ""Second Thing"").

I had to replace the query-basic plugin in order to allow matches in the meta fields to return hits even if there were no matches in any of the default fields.  The query-basic field only returns hits when every search term is found in at least one default field.  I needed hits returned if matches were found in at least one field for every term, and/or the entire search phrase appeared in a meta index field.

One known bug is that common terms are not getting stripped out of the fields' values before they get indexed, so ""The Next Big Thing"" could not be matched because the query engine will strip out ""the"" from all queries.  I intend to fix this by stipping out common terms from meta fields before indexing them.

Another issue is that searching for ""Next Big Thing"" would not match meta index values for ""Next"", ""Big"" or ""Thing"".  You can consider that a bug or a feature depending on how you look at it.

These plugins were written for and only work on the 0.7.2 branch.

I'm going to attache a tarball of the source of these three plugins after I create the issue.  To use the plugins, you'll need to untar them in your src/plugins directory and add them to the ant build.xml directive (and of course add them in your nutch-site.xml file).  If these end up getting added to the project, I'll write up documentation on the wiki.",smartshark_2_2,1731,nutch,"""[0.9984160661697388, 0.0015839687548577785]"""
2403,230662,add support for internationalized domain names,"Internationalized domain names are gaining ground and so nutch should give a little bit more support to this feature, atleast we need punycode encoding/decoding functionality so we can display/enter internationalized domain names in ui.",smartshark_2_2,2277,nutch,"""[0.9975265860557556, 0.0024734782055020332]"""
2404,230663,Add support for vfsfile:// loading of plugins for JBoss,"In the file:
/src/java/org/apache/nutch/plugin/PluginManifestParser.java

There is a check to make sure that the plugin file location is a url formatted like ""file://path/plugins"".

When deployed on Jboss, the file protocol will sometimes be: ""vfsfile://path/plugins"".  The code with vfsfile can operate the same so I propose a change to the check to also allow this protocol.  This would allow Nutch to be deployed on the newer versions of JBoss without any modification.

Here is a simple patch:

Index: src/java/org/apache/nutch/plugin/PluginManifestParser.java
===================================================================
--- src/java/org/apache/nutch/plugin/PluginManifestParser.java	Mon Nov 09 20:20:51 EST 2009
+++ src/java/org/apache/nutch/plugin/PluginManifestParser.java	Mon Nov 09 20:20:51 EST 2009
@@ -121,7 +121,8 @@
       } else if (url == null) {
         LOG.warn(""Plugins: directory not found: "" + name);
         return null;
-      } else if (!""file"".equals(url.getProtocol())) {
+      } else if (!""file"".equals(url.getProtocol()) &&
+        !""vfsfile"".equals(url.getProtocol())) {
         LOG.warn(""Plugins: not a file: url. Can't load plugins from: "" + url);
         return null;
       }

",smartshark_2_2,1775,nutch,"""[0.9985698461532593, 0.001430085045285523]"""
2405,230664,Resolving Ivy dependencies in several plugins ,"When configuring Nutch 1.5-SNAPSHOT in Eclipse, I noticed that any plugins requiring additional libraries OVER AND ABOVE the ones specified in NUTCH_HOME/ivy/ivy.xml cannot resolve the dependencies. In specific the classes are 
{code}
- FeedParser <dependency org=""net.java.dev.rome"" name=""rome"" rev=""1.0.0"" conf=""*->master""/>
- URLAutomationFilter - <dependency org=""dk.brics"" name=""automaton"" rev=""???""/>
- SWFParser <dependency org=""com.google.gwt"" name=""gwt-incubator"" rev=""2.0.1""/>
- HTMLParser   <dependency org=""net.sourceforge.nekohtml"" name=""nekohtml"" rev=""1.9.15""/> 
{code}

Further to this, I cannot locate the dk.brics dependency!

Finally, the plugin/ivy.xml files for the above plugins cannot be parsed corectly due to the ${nutch.root} vairable.
",smartshark_2_2,1903,nutch,"""[0.9742329120635986, 0.025767067447304726]"""
2406,230665,Enable use of (Gora) SNAPSHOT dependencies,"For some time it has been on my radar to enable use of SNAPSHOT dependencies for use within Nutch. Specifically, this relates to gora-* SNAPSHOT's available here [0].
I am working on a patch which updates ivy.xml and ivysettings.xml t enable this, however it seems almost like black magic right now.
I'll upload the patch once I get my build working. 

[0] https://repository.apache.org/content/repositories/snapshots/org/apache/gora/",smartshark_2_2,492,nutch,"""[0.9984714388847351, 0.0015285857953131199]"""
2407,230666,protocol-selenium can't handle https,"fetch ofÂ any https pageÂ is failing with: org.apache.nutch.protocol.ProtocolNotFound: protocol not found for url=https
at org.apache.nutch.protocol.ProtocolFactory.getProtocol(ProtocolFactory.java:83)
at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:687)",smartshark_2_2,2541,nutch,"""[0.1802530586719513, 0.8197469115257263]"""
2408,230667,DomainStats tool should be named for what it's doing,"DomainStats tool can calculate on host, domain and suffix. The job name should reflect these types.",smartshark_2_2,2249,nutch,"""[0.9980461597442627, 0.0019537729676812887]"""
2409,230668,Integrate ÂµBlock into Nutch to block Ads,I feel ÂµBlock (https://github.com/gorhill/uBlock) or any other related plugin which can block Ads can be integrated into Nutch that way we can skip the Ads in the Nutch crawls.,smartshark_2_2,720,nutch,"""[0.998354434967041, 0.0016455544391646981]"""
2410,230669,bin/nutch fetch/parse handle crawl/segments directory,"I'm having issues porting scripts across different systems to support the step of extracting the latest/only segments resulting from the generate phase.
Variants include:
$ export SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1` #[1]
$ s1=`ls -d crawl/segments/2* | tail -1` #[2]
$ segment=`$HADOOP_HOME/bin/hadoop dfs -ls crawl/segments | tail -1 | grep -o [a-zA-Z0-9/\-]* |tail -1`
$ segment=`$HADOOP_HOME/bin/hdfs -ls crawl/segments | tail -1 | grep -o [a-zA-Z0-9/\-]* |tail -1`

And I'm not sure what windows users would have to do. Some users may also do with:
bin/nutch fetch with crawl/segments/2*

But I don't see a need in having the user extract/worry-about the latest/only segment, and have it a described step in every nutch tutorial. More over only fetch and parse expect a segment while other commands are fine with the directory of segments.

Therefore, I think it's beneficial if fetch and parse also handle directories of segments. 







[1] http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/
[2] http://wiki.apache.org/nutch/NutchTutorial#Command_Line_Searching",smartshark_2_2,1788,nutch,"""[0.8220645189285278, 0.1779354214668274]"""
2411,230670,changes to geoPosition plugin to make it work on nutch 0.9,"I have modified the geoPosition plugin (http://wiki.apache.org/nutch/GeoPosition) code to work with nutch 0.9.  (The code was built originally using nutch 0.7.)  I'd like to contribute my changes back to the nutch project.  I already communicated with the code's author (Matthias Jaekle), and he agrees with my mods.",smartshark_2_2,1759,nutch,"""[0.9983764886856079, 0.0016234833747148514]"""
2412,230671,Improve readability of logs/hadoop.log,"adding
log4j.logger.org.apache.nutch.plugin.PluginRepository=WARN
to conf/log4j.properties
dramatically improves the readability of the logs in logs/hadoop.log (removes all INFO)",smartshark_2_2,2038,nutch,"""[0.9976422190666199, 0.002357703633606434]"""
2413,230672,The crawl script should be able to skip an initial injection.,"When our crawl gets really big a new injection takes considerable time as it updates crawldb, the crawl script should be able to skip the injection and go directly to the generate call.",smartshark_2_2,1322,nutch,"""[0.9985017776489258, 0.0014981996500864625]"""
2414,230673,Fix TestFetcher for Nutchgora,This issue is part of a larger target which aims to fix broken JUnit tests for Nutchgora,smartshark_2_2,2163,nutch,"""[0.9980161190032959, 0.0019839138258248568]"""
2415,230674,Normalize duplicate slashes in URL's,"Many websites produce faulty URL's with multiple slashes e.g. http://cocoon.apache.org///////////////////////1.x/dynamic.html
This can be really nasty if the number of slashes varies, resulting in many URL's actually pointing to the same page and generating new (unique) URL's to the same or other duplicate pages.",smartshark_2_2,1908,nutch,"""[0.9774076342582703, 0.022592326626181602]"""
2416,230675,Display consistent usage of GeneratorJob with 1.X,"If we pass the generate argument to the nutch script, the Generator auto-spings into action and begins generating fetchlists. This should not be the case, instead it should print traditional usage to stdout. An example is below
{code}
lewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch generate
GeneratorJob: Selecting best-scoring urls due for fetch.
GeneratorJob: starting
GeneratorJob: filtering: true
GeneratorJob: done
GeneratorJob: generated batch id: 1339628223-1694200031
{code}

All I wanted to do was get the usage params printed to stdout but instead it generated my batch willy nilly.  ",smartshark_2_2,2237,nutch,"""[0.9838055372238159, 0.016194431111216545]"""
2417,230676,Bring back multiple segment support for Generate / Update,"We find it convenient to be able to run generate once for -topN 300M and have multiple independent segments to work with (lower overhead) -- then run update on all segments which succeeded simultaneously.

This reactivates -numFetchers and fixes updatedb to handle multiple provided segments again.



Radu Mateescu wrote the attached patch for us with the below description (lightly edited):

The implementation of -numFetchers in 0.8 improperly plays with the number of reduce tasks in order to generate a given number of fetch lists. Basically, what it does is this: before the second reduce (map-reduce is applied twice for generate), it sets the number of reduce tasks to numFetchers and ideally, because each reduce will create a file like part-00000, part-00001, etc in the ndfs, we'll end up with the number of desired fetched lists. But this behaviour is incorrect for the following reasons:
1. the number of reduce tasks is orthogonal to the number of segments somebody wants to create. The number of reduce tasks should be chosen based on the physical topology rather then the number of segments someone might want in ndfs
2. if in nutch-site.xml you specify a value for mapred.reduce.tasks property, the numFetchers seems to be ignored
 
Therefore , I changed this behaviour to work like this: 
 - generate will create numFetchers segments
 - each reduce task will write in all segments (assuming there are enough values to be written) in a round-robin fashion
The end results for 3 reduce tasks and 2 segments will look like this :
 
/opt/nutch/bin>./nutch ndfs -ls segments
060111 122227 parsing file:/opt/nutch/conf/nutch-default.xml
060111 122228 parsing file:/opt/nutch/conf/nutch-site.xml
060111 122228 Client connection to 192.168.0.1:5466: starting
060111 122228 No FS indicated, using default:master:5466
Found 2 items
/user/root/segments/20060111122144-0    <dir>
/user/root/segments/20060111122144-1    <dir>

 
/opt/nutch/bin>./nutch ndfs -ls segments/20060111122144-0/crawl_generate
060111 122317 parsing file:/opt/nutch/conf/nutch-default.xml
060111 122317 parsing file:/opt/nutch/conf/nutch-site.xml
060111 122318 No FS indicated, using default:master:5466
060111 122318 Client connection to 192.168.0.1:5466: starting
Found 3 items
/user/root/segments/20060111122144-0/crawl_generate/part-00000  1276
/user/root/segments/20060111122144-0/crawl_generate/part-00001  1289
/user/root/segments/20060111122144-0/crawl_generate/part-00002  1858

 
/opt/nutch/bin>./nutch ndfs -ls segments/20060111122144-1/crawl_generate
060111 122333 parsing file:/opt/nutch/conf/nutch-default.xml
060111 122334 parsing file:/opt/nutch/conf/nutch-site.xml
060111 122334 Client connection to 192.168.0.1:5466: starting
060111 122334 No FS indicated, using default:master:5466
Found 3 items
/user/root/segments/20060111122144-1/crawl_generate/part-00000  1207
/user/root/segments/20060111122144-1/crawl_generate/part-00001  1236
/user/root/segments/20060111122144-1/crawl_generate/part-00002  1841

",smartshark_2_2,1396,nutch,"""[0.9985846281051636, 0.0014153433730825782]"""
2418,230677,Subcollection to optionally write to configured fields,The subcollection plugin writes the contents of the name element of a given subcollection to the subcollection field. There are cases in which writing to fields other than subcollection is useful.,smartshark_2_2,2194,nutch,"""[0.9984052777290344, 0.0015947030624374747]"""
2419,230678,Timeouts in protocol-httpclient when crawling same host with >2 threads NUTCH-1613 is not a complete solution,"NUTCH-1613 provided a fix for the hardcoded limitation of 2 threads for protocol-httpclient.  However, just extending the hardwired 10 max threads and allocating them all to a single host only provides a partial solution.  It is still possible to exhaust the thread pool and observe timeouts depending on the settings of:

 - fetcher.threads.per.host (nutch-site.xml)
 - mapred.tasktracker.map.tasks.maximum (mapred-site.xml)

It would perhaps be more robust to set the httpclient thread pool as a derivative of these two configuration parameters as below:



{code}
    params.setMaxTotalConnections(maxThreadsTotal);

// Add the following lines ...


	// --------------------------------------------------------------------------------
	// Modification to increase the number of available connections for
	// multi-threaded crawls.
	// --------------------------------------------------------------------------------
	connectionManager.setMaxConnectionsPerHost(conf.getInt(""fetcher.threads.per.host"", 10));
	connectionManager.setMaxTotalConnections(conf.getInt(""mapred.tasktracker.map.tasks.maximum"", 5) * conf.getInt(""fetcher.threads.per.host"", 10));
	LOG.debug(""setMaxConnectionsPerHost: "" + connectionManager.getMaxConnectionsPerHost());
	LOG.debug(""setMaxTotalConnections  : "" + connectionManager.getMaxTotalConnections());
	// --------------------------------------------------------------------------------
{code}",smartshark_2_2,821,nutch,"""[0.16778984665870667, 0.832210123538971]"""
2420,230679,Fix [deprecation] javac warnings,"As part of this task, these warnings should be resolved, however this particular strand of warnings can either be resolved by adding

{code}
@SuppressWarnings(""deprecation"")
{code}

or by actually upgrading our class usage to rely upon non-deprecated classes. Which option is more appropriate for the project?",smartshark_2_2,2116,nutch,"""[0.9982534050941467, 0.0017465982818976045]"""
2421,230680,Use a combiner in LinDbMerger to improve the performance as in LinkDb,"We have defined a Combiner in LinkDb to improve the performance, we should do the same thing in LinkDbMerger.",smartshark_2_2,335,nutch,"""[0.9983479976654053, 0.0016520497156307101]"""
2422,230681,Elastic REST Indexer: Allow multiple hosts,Allow specifying a list of Elasticsearch hosts to index documents to. This would be especially helpful when working with a Elasticsearch cluster which contains of multiple nodes.,smartshark_2_2,2461,nutch,"""[0.9981794357299805, 0.0018205990782007575]"""
2423,230682,Upgrade to Tika 1.3,http://www.apache.org/dist/tika/CHANGES-1.3.txt,smartshark_2_2,2351,nutch,"""[0.9982855916023254, 0.001714327372610569]"""
2424,230683,Proposal for Nutch 3.x,This is a parent issue which contains a proposal for Nutch 3.x. It's based on my branch (mr2-mvn at https://github.com/allfro/nutch).  ,smartshark_2_2,952,nutch,"""[0.9983617663383484, 0.0016382768517360091]"""
2425,230684,Add 'version' field to Solr schema as required by new Solr servers,"The f<ield name=""_version_"" type=""long"" indexed=""true"" stored=""true""/> is absent from the current Solr schema in Nutch trunk. It is nothing other than a pain for people to manually add this to the schema if and when they wish to use Solr for indexing content.",smartshark_2_2,654,nutch,"""[0.9985356330871582, 0.0014644126640632749]"""
2426,230685,Standard metadata property names in the ParseData metadata,"Currently, people are free to name their string-based properties anything that they want, such as having names of ""Content-type"", ""content-TyPe"", ""CONTENT_TYPE"" all having the same meaning. Stefan G. I believe proposed a solution in which all property names be converted to lower case, but in essence this really only fixes half the problem right (the case of identifying that ""CONTENT_TYPE""
and ""conTeNT_TyPE"" and all the permutations are really the same). What about
if I named it ""Content     Type"", or ""ContentType""?

 I propose that a way to correct this would be to create a standard set of named Strings in the ParseData class that the protocol framework and the parsing framework could use to identify common properties such as ""Content-type"", ""Creator"", ""Language"", etc.

 The properties would be defined at the top of the ParseData class, something like:

 public class ParseData{

   .....

    public static final String CONTENT_TYPE = ""content-type"";
    public static final String CREATOR = ""creator"";

   ....

}


In this fashion, users could at least know what the name of the standard properties that they can obtain from the ParseData are, for example by making a call to ParseData.getMetadata().get(ParseData.CONTENT_TYPE) to get the content type or a call to ParseData.getMetadata().set(ParseData.CONTENT_TYPE, ""text/xml""); Of course, this wouldn't preclude users from doing what they are currently doing, it would just provide a standard method of obtaining some of the more common, critical metadata without pouring over the code base to figure out what they are named.

I'll contribute a patch near the end of the this week, or beg. of next week that addresses this issue.
",smartshark_2_2,152,nutch,"""[0.9938626289367676, 0.006137378979474306]"""
2427,230686,Indexer Plugin for Elastic Search,We should rewrite ElasticSearch indexer compatible with new Indexing Plugin Architect. ,smartshark_2_2,483,nutch,"""[0.9978266358375549, 0.0021733874455094337]"""
2428,230687,Cookies are not being read properly,"Cookies that do not begin with a period are not being accepted. For example ""cnn.com"" instead of the RFC "".cnn.com"". But A LOT of sites seem to not know the standard. It would be nice if the plugin accepted those cookies as well.
",smartshark_2_2,21,nutch,"""[0.31279271841049194, 0.6872073411941528]"""
2429,230688,Add ability to sort on more than one column,"Currently nutch only allows sorting to be specified by passing in a string of the field name to be sorted on, a null string results in sorting by score. I'd like to be able to sort on multiple fields, including score instead of being restricted to just one.",smartshark_2_2,1721,nutch,"""[0.9984664916992188, 0.0015335322823375463]"""
2430,230689,Upgrade from org.mortbay.jetty to org.eclipse.jetty,"The old org.mortbay.jetty libs (not maintained since 2008) are still used for the Nutch server and multiple unit tests. Nutch should be upgraded to the maintained org.eclipse.jetty libs/packages.

The old dependency causes the unit tests of the HTTP protocol plugins to fail when built with Java 9 or 10 with the following error while compiling JSP classes (see NUTCH-2512):
{noformat}
2018-06-06 11:03:02,335 ERROR mortbay.log (Slf4jLog.java:warn(87)) - /basic-http.jsp
java.lang.ClassCastException: java.base/jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to java.base/java.net.URLClassLoader
        at org.apache.jasper.compiler.JspRuntimeContext.<init>(JspRuntimeContext.java:94)
        at org.apache.jasper.servlet.JspServlet.init(JspServlet.java:100)
        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)
        at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:339)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
2018-06-06 11:03:02,356 WARN  mortbay.log (Slf4jLog.java:warn(76)) - /basic-http.jsp: javax.servlet.UnavailableException: java.lang.ClassCastException: java.base/jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to java.base/java.net.URLClassLoader
2018-06-06 11:03:02,386 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@127.0.0.1:47504
...
HTTP Status Code for http://127.0.0.1:47504/basic-http.jsp expected:<200> but was:<500>
junit.framework.AssertionFailedError: HTTP Status Code for http://127.0.0.1:47504/basic-http.jsp expected:<200> but was:<500>
        at org.apache.nutch.protocol.http.TestProtocolHttp.fetchPage(TestProtocolHttp.java:130)
        at org.apache.nutch.protocol.http.TestProtocolHttp.testStatusCode(TestProtocolHttp.java:80)
{noformat}
",smartshark_2_2,2567,nutch,"""[0.9984000325202942, 0.0015999570023268461]"""
2431,230690,Make Git ignore IDE project files and add note about IDE setup,Make Git ignore IDE project files and add note about IDE setup,smartshark_2_2,742,nutch,"""[0.9943990111351013, 0.00560098048299551]"""
2432,230691,Scoring filter should distribute score to all outlinks at once,"Currently ScoringFilter.distributeScoreToOutlink, as its name implies, takes only a single outlink and works on that. I would suggest that we change it to distributeScoreToOutlink_s_ so that it would take all the outlinks of a page at once. This has several advantages:

1) A ScoringFilter plugin returns a single adjust datum to set its score instead of returning several.
2) A ScoringFilter plugin can change the score of the original page (via adjust datum) even if there are no outlinks. This is useful if you have a ScoringFilter plugin that, say, scores pages based on content instead of outlinks.
3) Since the ScoringFilter plugin recieves all outlinks at once, it can make better decisions on how to distribute the score. For example, right now it is not possible to create a plugin that always distributes exactly a page's 'cash' to outlinks(that is, if a page has score 5, it will always distribute exactly 5 points to its outlinks no matter what the internal/external factors are) if internal / external score factors are not 1.",smartshark_2_2,317,nutch,"""[0.9984167814254761, 0.001583268865942955]"""
2433,230692,Create a whitelist of IPs/hostnames to allow skipping of RobotRules parsing,"Based on discussion on the dev list, to use Nutch for some security research valid use cases (DDoS; DNS and other testing), I am going to create a patch that allows a whitelist:

{code:xml}
<property>
  <name>robot.rules.whitelist</name>
  <value>132.54.99.22,hostname.apache.org,foo.jpl.nasa.gov</value>
  <description>Comma separated list of hostnames or IP addresses to ignore robot rules parsing for.
  </description>
</property>
{code}",smartshark_2_2,776,nutch,"""[0.9985223412513733, 0.0014777304604649544]"""
2434,230693,"Added -dir command line option to Indexer and SolrIndexer,  allowing to specify directory containing segments","The patches add -dir option, so the user can specify the directory in which the segments are to be found. The actual mode is to specify the list of segments, which is not very easy with hdfs. Also, the -dir option is already implemented in LinkDB and SegmentMerger, for example.",smartshark_2_2,1789,nutch,"""[0.9985992312431335, 0.0014007141580805182]"""
2435,230694,URLFilterChecker to run as TCP Telnet service,Allow testing URL filters for webapplications just like indexing filters checker.,smartshark_2_2,2422,nutch,"""[0.9983600974082947, 0.0016398667357861996]"""
2436,230695,HostDeduplicator for Nutch,A host deduplicator able to emit rules for the HostNormalizer. ,smartshark_2_2,825,nutch,"""[0.9983530044555664, 0.0016470396658405662]"""
2437,230696,Spurious Duplications for MD5,"We're seeing some incidence of a large number of documents being marked as duplicate in our crawl.

We traced it back to one of the crawl plugins returning an empty array for the content field.

We'd like to propose changing the MD5 signature generation from:
{code}
public byte[] calculate(Content content, Parse parse) {
    byte[] data = content.getContent();
    if (data == null)
      data = content.getUrl().getBytes();
    return MD5Hash.digest(data).getDigest();
  }
{code}
to:
{code}
public byte[] calculate(Content content, Parse parse) {
    byte[] data = content.getContent();
    if ((data == null) || (data.length == 0))
      data = content.getUrl().getBytes();
    return MD5Hash.digest(data).getDigest();
  }
{code}
to address the issue",smartshark_2_2,1331,nutch,"""[0.5766122341156006, 0.4233877658843994]"""
2438,230697,"I  want  crawl the websites including news.yahoo.com,game.yahoo.com,blog.yahoo.com,etc!","how do  I  config them in the crawl-urlfilter.txt? I  config  them below,but  it is not successful.
# The url filter file used by the crawl command.

# Better for intranet crawling.
# Be sure to change MY.DOMAIN.NAME to your domain name.

# Each non-comment, non-blank line contains a regular expression
# prefixed by '+' or '-'.  The first matching pattern in the file
# determines whether a URL is included or ignored.  If no pattern
# matches, the URL is ignored.

# skip file:, ftp:, & mailto: urls
-^(file|ftp|mailto):

# skip image and other suffixes we can't yet parse
-\.(gif|GIF|jpg|JPG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe)$

# skip URLs containing certain characters as probable queries, etc.
-[?*!@=]

# accept hosts in MY.DOMAIN.NAME
#+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/
+^http://([a-z0-9]*\.)*yahoo.com/
# skip everything else
#-.
but It can not work, and can not crawl the  domain name (DOMAIN.NAME) inluding  news.yahoo.com,game.yahoo.com,blog.yahoo.com
why?",smartshark_2_2,38,nutch,"""[0.945633053779602, 0.05436690151691437]"""
2439,230698,SegmentMerger to normalize,"SegmentMerger can not normalize, only filter. Issue to add a -normalize option.",smartshark_2_2,2225,nutch,"""[0.9985128045082092, 0.0014872007304802537]"""
2440,230699,Hadoop 0.19 requires an update of jets3t,"jets3t-0.6.0.jar is currently in the lib directory. Hadoop 0.19 relies on the version 0.6.1

I am getting java.lang.NoSuchMethodError: org.jets3t.service.S3Service.moveObject(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/jets3t/service/model/S3Object;Z)Ljava/util/Map;
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.rename(Jets3tNativeFileSystemStore.java:228)

when using distcp with jets3t-0.6.0.jar. I haven't tried with 0.6.1 yet but I suspect this is the cause. It won't hurt to upgrade jets3t anyway",smartshark_2_2,367,nutch,"""[0.9360310435295105, 0.06396892666816711]"""
2441,230700,Fetcher following outlinks to keep track of already fetched items,"When fetcher.follow.outlinks.depth is non-zero, fetcher follows outlinks. This patch keeps track of already fetched URL's and thus avoid fetching the same URL twice.

A Set is used to keep track of them, hashcodes to reduce memory usage. This is not used if fetcher doesn't follow outlinks.",smartshark_2_2,1380,nutch,"""[0.8715294003486633, 0.12847062945365906]"""
2442,230701,Indexer for Elasticsearch 2.x,Add an additional plugin for Elasticsearch 2.x,smartshark_2_2,1189,nutch,"""[0.9976816177368164, 0.0023183110170066357]"""
2443,230702,Form authentication should not be global and ignore <authScope>,"Taken from [~wastl-nagel]'s comments on NUTCH-827
bq. the form authentication is global and ignores <authScope>. So you have to restrict your crawl to the form authentication pages only. Ideally, also form authentication should be bound to a scope (one host, one URL prefix, etc.) same as HTTP authentication.",smartshark_2_2,1211,nutch,"""[0.8892040848731995, 0.11079597473144531]"""
2444,230703,Write Solr XML documents,"Documents need to be reindexed when index-time analysis is modified. Indexing individual segments from Nutch is tedious, especially for small segments. This issue should add a feature that can write XML batches.",smartshark_2_2,396,nutch,"""[0.9981184005737305, 0.0018816179363057017]"""
2445,230704,Top Level Domains Indexing / Scoring,"Top Level Domains (tlds) are the last part(s) of the host name in a DNS system. TLDs are managed by the Internet Assigned Numbers Authority. IANA divides tlds into three. infrastructure, generic(such as ""com"", ""edu"") and country code tlds(such as ""en"", ""de"" , ""tr"", ). Indexing the top level domain and optionally boosting is needed for improving the search results and enhancing locality. 


",smartshark_2_2,1436,nutch,"""[0.9971150159835815, 0.0028849930968135595]"""
2446,230705,Improve formatting of ant targets for clearer project help,This is a trivial formatting issue I will submit a patch shortly and fix it.,smartshark_2_2,1959,nutch,"""[0.9981011748313904, 0.0018988458905369043]"""
2447,230706,Switch CrawlCompletion arg parsing to Commons CLI,The current CrawlCompletion utility should be updated to use commons CLI instead of doing manual arg parsing and checking.,smartshark_2_2,1079,nutch,"""[0.9980220794677734, 0.0019779398571699858]"""
2448,230707,Delegate processing of URL domains to crawler commons,"We have code in src/java/org/apache/nutch/util/domain and a resource file conf/domain-suffixes.xml to handle URL domains. This is used mostly from URLUtil.getDomainName.

The resource file is not necessarily up to date and since crawler commons has a similar functionality we should use it instead of having to maintain our own resources.",smartshark_2_2,574,nutch,"""[0.9984083771705627, 0.0015916768461465836]"""
2449,230708,automatic adjust the CrawlDatum.fetchInterval according to the number of newly outlinks,"The spider must could  find the new urls  in time.  and the new urls usually are included in some url like index page,list page.

but  the score of url can not reflect it Adequately.

Could we adjust the CrawlDatum.fetchInterval according to the number of newly outlinks.",smartshark_2_2,292,nutch,"""[0.9889813661575317, 0.011018707416951656]"""
2450,230709,Speed up link inversion step in crawling script,"While working on a project where I have to index a huge number of URLs I encountered an issue with the link inversion step of the crawling script. A while ago Ian Lopata stumbled upon the same issue as described here: http://lucene.472066.n3.nabble.com/InvertLinks-Performance-Nutch-1-6-td4183004.html
{quote}
I am running the invertlinks step in my Nutch 1.6 based crawl process on a 
single node.  I run invertlinks only because I need the Inlinks in the 
indexer step so as to store them with the document.  I do not need the 
anchor text and I am not scoring.  I am finding that invertlinks (and more 
specifically the merge of the linkdb) takes a long time - about 30 minutes 
for a crawl of around 150K documents.  I am looking for ways that I might 
shorten this processing time.  Any suggestions? 
{quote}

Back then [~wastl-nagel] suggested turning off the normalizers and filters during the inversion step which speeds up the process a bunch.
In my case however I kind of depend on those so this is no real solution.

I opened this issue here in order to get some feedback on how we could improve things in a crawl script and speed up the process.",smartshark_2_2,2459,nutch,"""[0.9979124665260315, 0.0020875351037830114]"""
2451,230710,Authorization Support for REST API,Add authorization for Nutch REST API.,smartshark_2_2,1240,nutch,"""[0.9981653094291687, 0.0018347646109759808]"""
2452,230711,CrawlDBReader to dump on exception and HTTP code,The CrawlDBReader tool can dump based on status and URL regex but not on status db_gone combined with an HTTP exception and HTTP response code.,smartshark_2_2,895,nutch,"""[0.9937095642089844, 0.006290420424193144]"""
2453,230712,LinkDB to implement db.ignore.external.links,LinkDB needs an option to ignore external links.,smartshark_2_2,714,nutch,"""[0.9983605742454529, 0.0016394376289099455]"""
2454,230713,Allow predeterminate running order of index filters,"I've tested a patch for org.apache.nutch.indexer.IndexingFilters, allowing the user to state in which order the indexing filters are to be run based on a new
indexingfilter.order property. This is needed when a filter needs to rely on previously generated document fields as a source of input to generate further fields.

As suggested elsewhere, I based this on the urlfilter.order functionality:

<property>
  <name>indexingfilter.order</name>
  <value>org.apache.nutch.indexer.basic.BasicIndexingFilter org.apache.nutch.indexer.more.MoreIndexingFilter</value>
  <description>The order by which index filters are applied.
  If empty, all available index filters (as dictated by properties
  plugin-includes and plugin-excludes above) are loaded and applied in system
  defined order. If not empty, only named filters are loaded and applied
  in given order. For example, if this property has value:
  org.apache.nutch.indexer.basic.BasicIndexingFilter org.apache.nutch.indexer.more.MoreIndexingFilter
  then BasicIndexingFilter is applied first, and MoreIndexingFilter second.
  Since all filters are AND'ed, filter ordering does not have impact
  on end result, but it may have performance implication, depending
  on relative expensiveness of filters.
  </description>
</property>
",smartshark_2_2,225,nutch,"""[0.9984820485115051, 0.0015179644105955958]"""
2455,230714,In distributed mode URL's are not partitioned,In distributed mode URL's are not partitioned to a specific machine which means the politeness policy is voided,smartshark_2_2,1955,nutch,"""[0.6390148997306824, 0.3609851002693176]"""
2456,230715,Be explicit about target JVM when building (1.4.x?),"Below is patch for nutch build.xml.  It stipulates the target JVM is 1.4.x.  Without explicit target, a nutch built with 1.5.x java defaults to a 1.5.x java target and won't run in a 1.4.x JVM.  Can be annoying (From the ant javac doc, regards the target attribute: ""We highly recommend to always specify this attribute."").

[debord 282] nutch > svn diff -u build.xml
Subcommand 'diff' doesn't accept option '-u [--show-updates]'
Type 'svn help diff' for usage.
[debord 283] nutch > svn diff build.xml
Index: build.xml
===================================================================
--- build.xml   (revision 349779)
+++ build.xml   (working copy)
@@ -72,6 +72,8 @@
      destdir=""${build.classes}""
      debug=""${debug}""
      optimize=""${optimize}""
+     target=""1.4""
+     source=""1.4""
      deprecation=""${deprecation}"">
       <classpath refid=""classpath""/>
     </javac>
",smartshark_2_2,148,nutch,"""[0.9968572854995728, 0.0031426474452018738]"""
2457,230716,Port logging to slf4j,"We are already inheriting a dependency on slf4j from Solr so we might as well use it :-)
Any thoughts on this?",smartshark_2_2,1542,nutch,"""[0.9956436157226562, 0.004356355406343937]"""
2458,230717,Make fetcher thread time out configurable,The fetcher sets a time out value based of half the mapred.task.timeout value. This is not a proper value for all cases. Add an option (fetcher.thread.timeout.divisor) to configure the divisor used and default it to two.,smartshark_2_2,1925,nutch,"""[0.9986086487770081, 0.00139136565849185]"""
2459,230718,remove testresources/testcrawl from 2.x,"With NUTCH-1844 src/testresources/testcrawl have been removed from Nutch 1.x. They are still included in 2.x although definitely useless: data structures (crawldb, linkdb, segments) cannot be read by 2.x.",smartshark_2_2,1166,nutch,"""[0.9985411167144775, 0.0014589028432965279]"""
2460,230719,Better parsed text by default parser,"I found the parsed text by default parser, Neko in 1.0 nightly is not easy to process - it just add a space to the end of the tag. 
For easier analysis,  neko (or other parser) should change the behaviour to 
1.adding tab for inline element
2.add a tab+newline for block level element end
instead of  space

That will help another application to use the parsed text.",smartshark_2_2,296,nutch,"""[0.9985254406929016, 0.0014745591906830668]"""
2461,230720,Nutch script does not require HADOOP_HOME,"The Nutch script currently requires HADOOP_HOME to be set and point to a valid HADOOP setup in order to run in distributed mode. What is actually needs is not the location of the whole Hadoop setup but just to be able to call the executable 'hadoop'. This requires the users to add HADOOP_HOME/bin to their path. 
Moreover when using Nutch on Hadoop distribs such as Cloudera's CDH, the various parts of Hadoop are located in a separate directories so the concept of HADOOP_HOME is not necessarily relevant to find the hadoop executable. ",smartshark_2_2,1910,nutch,"""[0.889650285243988, 0.11034966260194778]"""
2462,230721,No longer able to set per-field boosts on lucene documents,"I'm working on upgrading from Nutch 0.9 to Nutch 1.1 and I've noticed that it no longer seems possible to set boosts on specific fields in lucene documents.  This is, in my opinion, a major feature regression and removes a huge component to fine tuning search.  Can this be added?",smartshark_2_2,2185,nutch,"""[0.8358298540115356, 0.16417020559310913]"""
2463,230722,Uses commons logging Code Guards,"""Code guards are typically used to guard code that only needs to execute in support of logging, that otherwise introduces undesirable runtime overhead in the general case (logging disabled). Examples are multiple parameters, or expressions (e.g. string + "" more"") for parameters. Use the guard methods of the form log.is<Priority>() to verify that logging should be performed, before incurring the overhead of the logging method call. Yes, the logging methods will perform the same check, but only after resolving parameters.""
(description extracted from http://jakarta.apache.org/commons/logging/guide.html#Code_Guards)
",smartshark_2_2,175,nutch,"""[0.9946132302284241, 0.005386759527027607]"""
2464,230723,Update the FileDumper tool to fetch only those URLs with status db_fetched in nutch,"The FileDumper tool is a tool that reads the crawled data from Nutch and dumps this data into its raw files. This tool currently dumps every single file irrespective of status, duplicates etc. This cause files that are fetched in error or files that have not been fetched because they were made unavailable by the server to also be dumped. 

The fix should be to fetch only those files that were fetched with status db_fetched by Nutch.",smartshark_2_2,675,nutch,"""[0.987699031829834, 0.012300960719585419]"""
2465,230724,Add support for Content-Encoding: deflated,"Add support for the ""deflated"" content-encoding, next to the already
implemented GZIP content-encoding. Patch attached. See also the
""Patch: deflate encoding"" thread on nutch-dev on August 7/8 2006.",smartshark_2_2,1438,nutch,"""[0.9983609318733215, 0.0016390347154811025]"""
2466,230725,DeduplicationJob to optionally group on host or domain,"Add optional grouping to DeduplicationJob.

Usage: DeduplicationJob <crawldb> [-group <none|host|domain>]",smartshark_2_2,1053,nutch,"""[0.9982267022132874, 0.001773345866240561]"""
2467,230726,Index filter for Page's latitude and longitude,"I see some discuss about page's ip storing. I think If we have page's ip, we can index page's geo position as latitude and longitude. That use for location based searches. 

[~icebergx5] I know you have a patch about this in your secret patches  :) Can you share us ?",smartshark_2_2,512,nutch,"""[0.9975929856300354, 0.0024069449864327908]"""
2468,230727,Incompatible neko and xerces versions,"The Nutch 1.4 distribution includes

 - nekohtml-0.9.5.jar (under .../runtime/local/plugins/lib-
nekohtml)
 - xercesImpl-2.9.1.jar (under .../runtime/local/lib)

These two JARs appear to be incompatible versions. When the HtmlParser (configured to use neko) is invoked during a local-mode crawl, the parse fails due to an AbstractMethodError. (Note: To see the AbstractMethodError, rebuild the HtmlParser plugin and add a
catch(Throwable) clause in the getParse method to log the stacktrace.)

I found that substituting a later, compatible version of nekohtml (1.9.11)
fixes the problem.

Curiously, and in support of the above, the nekohtml plugin.xml file in
Nutch 1.4 contains the following:

<plugin
   id=""lib-nekohtml""
   name=""CyberNeko HTML Parser""
   version=""1.9.11""
   provider-name=""org.cyberneko"">

   <runtime>
       <library name=""nekohtml-0.9.5.jar"">
           <export name=""*""/>
       </library>
   </runtime>
</plugin>

Note the conflicting version numbers (version tag is ""1.9.11"" but the
specified library is ""nekohtml-0.9.5.jar"").

Was the 0.9.5 version included by mistake? Was the intention rather to
include 1.9.11?",smartshark_2_2,510,nutch,"""[0.9656710028648376, 0.034329019486904144]"""
2469,230728,Meta-data per URL/site/section,"We have the need to index sites and attach additional meta-data-tags to them. Afaik this is not yet possible, or is there a ""workaround"" I don't see? What I think of is using meta-tags per start-url, only indexing content below that URL, and have the ability to limit searches upon those meta-tags. E.g.

http://www.example1.com/something1/   -> meta-tag ""companybranch1""
http://www.example2.com/something2/   -> meta-tag ""companybranch2""
http://www.example3.com/something3/   -> meta-tag ""companybranch1""
http://www.example4.com/something4/   -> meta-tag ""companybranch3""

search for everything in companybranch1 or across 1 and 3 or similar",smartshark_2_2,97,nutch,"""[0.9742466807365417, 0.025753282010555267]"""
2470,230729,IndexerMapReduce to use single instance of NutchIndexAction for deletions,"For every URL/document to be deleted a new instance of NutchIndexAction is created in IndexerMapReduce (in multiple positions):
{code}
NutchIndexAction action = new NutchIndexAction(null,
   NutchIndexAction.DELETE);
output.collect(key, action);
{code}
Since the index action does not hold any data specific to any URL/document it would be more efficient to re-use a single instance.",smartshark_2_2,778,nutch,"""[0.9982795715332031, 0.0017204382456839085]"""
2471,230730,Solr Document Size Limit,"There should be an option, perhaps named solr.content.limit, that defines the max size of documents added to Solr.  I've had issues with large documents in Solr, so I set the file.content.limit to 2MB.  However, this causes many files to not be parsed (mostly PDFs) because of only retrieving parts of the document.  With this new option, I could still correctly parse them, but only index the first 2MB (or however large it is set) in Solr.",smartshark_2_2,2283,nutch,"""[0.9943748712539673, 0.005625149700790644]"""
2472,230731,Upgrade Nutch to use Hadoop 0.19,"Upgrade Nutch to use a newer hadoop, version 0.18.2.  This includes performance improvements, bug fixes, and new functionality.  Changes some current APIs.",smartshark_2_2,1472,nutch,"""[0.9981735944747925, 0.0018263704841956496]"""
2473,230732,Remove pom.xml from source,See discussion on [http://mail-archives.apache.org/mod_mbox/nutch-user/201407.mbox/%3CCA+-fM0sQc_HHN77ARgypjr5N-A_=P_Lk4Tod-0SD0Dj8xk0z4A@mail.gmail.com%3E],smartshark_2_2,609,nutch,"""[0.9984174966812134, 0.0015825575683265924]"""
2474,230733,indexer fails if linkdb is missing,"If the linkdb is missing the indexer fails with
{noformat}
2015-06-17 12:52:10,621 ERROR ...cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: .../linkdb/current
{noformat}

If both db.ignore.internal.links and db.ignore.external.links there will be no LinkDb even if ""invertlinks"" is run (as consequence of NUTCH-1913). The script ""bin/crawl"" does not know about the values of these two properties and calls indexer with ""-linkdb .../linkdb"" which will then fail.

Since ""bin/crawl"" is agnostic to properties defined in nutch-site.xml we solution similar to NUTCH-1854: make the tool/job more tolerant and log a warning instead of raising an error.",smartshark_2_2,808,nutch,"""[0.07955018430948257, 0.9204498529434204]"""
2475,230734, Extract urls from plain texts,"Some parsers have no Outlinks returned. E.g. the Word-Parser.
This class is able to extract (absolute) hyperlinks from a plain String (content)  and generates outlinks from them.
This would be very usful for parser which have no explicite extraction of hyperlinks.

Excample:

Outlink[] links = OutlinkExtractor.getOutlinks(""Nutch is located at http://www.apache.org and ..."");

Will return an array of Outlinks containing the one element of ""http://www.apache.org"".

----
transfered from: http://sourceforge.net/tracker/index.php?func=detail&aid=1109328&group_id=59548&atid=491356
submitted  by: Stephan Strittmatter",smartshark_2_2,27,nutch,"""[0.9982200264930725, 0.001780016114935279]"""
2476,230735,Generate to log truncation caused by generate.max.per.host,"LOG.info() hosts which have had their generate lists truncated.

This can inform admins about potential abusers or excessively large sites that they may wish to block with rules.",smartshark_2_2,168,nutch,"""[0.9104698896408081, 0.0895300805568695]"""
2477,230736,Change Plain text parser to use parser.character.encoding.default property for fall back encoding,"The value of the property parser.character.encoding.default is used as a fallback character encoding (charset) when HTML parser cannot find the charset information in HTTP Content-Type header or in META HTTP-EQUIV tag.  But the plain text parser behaves differently.  It just uses the system encoding (Java VM file.encodings, which in turn derives from the OS and the locale of the environment from which the JVM was spawned).  This is not pretty.  To gurantee a consistent behavior, plain text parser should use the value of the same property.

Though not tested, these changes in ./src/plugin/parse-text/src/java/org/apache/nutch/parse/text/TextParser.java should do it:
Insert this statement in the class definition:
  private static String defaultCharEncoding =
    NutchConf.get().get(""parser.character.encoding.default"", ""windows-1252"");

Replace this:
      text = new String(content.getContent());    // use default encoding
with this:
      text = new String(content.getContent(), defaultCharEncoding );    // use default encoding
",smartshark_2_2,1449,nutch,"""[0.9859673380851746, 0.014032651670277119]"""
2478,230737,Nutch's Solr schema doesn't work with Solr 4.9 because of the RealTimeGet handler,"Nutch's schema.xml file doesn't work with Solr 4.9 which has the RealTimeGetHandler implemented out of the box:

http://stackoverflow.com/questions/19064361/solr-4-4-0-is-giving-error-code-500

The simple answer is to add a field to our schema.xml:

{noformat}
<field name=""_version_"" type=""long"" indexed=""true"" stored=""true""/>
{noformat}

This patch does that.
",smartshark_2_2,629,nutch,"""[0.484604150056839, 0.5153958797454834]"""
2479,230738,Crawling sites with 403 Forbidden robots.txt,"If a 403 error is encountered when trying to access the robots.txt file, Nutch does not crawl any pages from that site.  This behavior is consistent with the RFC recommendation for the robot exclusion protocol.  

However, Google does crawl sites that exhibit this type of behavior, because most webmasters of these sites are unaware of robots.txt conventions and do want their site to be crawled.",smartshark_2_2,18,nutch,"""[0.279401957988739, 0.720598042011261]"""
2480,230739,nutch-selenium plugin,"I updated the plugin [nutch-selenium|https://github.com/momer/nutch-selenium] plugin to run against trunk.
I feel that there is a good bit of work to be done here however early testing on my system are that it works. ",smartshark_2_2,744,nutch,"""[0.9983423948287964, 0.0016575948102399707]"""
2481,230740,ParseOutputFormat has redundant code,"In ParseOutputFormat, I see a code block:

{code}
         // collect outlinks for subsequent db update
         Outlink[] links = parseData.getOutlinks();
         int outlinksToStore = Math.min(maxOutlinks, links.length);
         if (ignoreExternalLinks) {
           try {
             fromHost = new URL(fromUrl).getHost().toLowerCase();
           } catch (MalformedURLException e) {
             fromHost = null;
           }
         } else {
           fromHost = null;
         }
{code}

The if(ignoreExternalLinks) part then gets subsequently set and 
reset in the ensuing for loop:

{code}
         int validCount = 0;
         CrawlDatum adjust = null;
         List<Entry<Text, CrawlDatum>> targets = new ArrayList<Entry<Text, CrawlDatum>>(outlinksToStore);
         List<Outlink> outlinkList = new ArrayList<Outlink>(outlinksToStore);
         for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {
           String toUrl = links[i].getToUrl();
           // ignore links to self (or anchors within the page)
           if (fromUrl.equals(toUrl)) {
             continue;
           }
           if (ignoreExternalLinks) {
             try {
               toHost = new URL(toUrl).getHost().toLowerCase();
             } catch (MalformedURLException e) {
               toHost = null;
             }
             if (toHost == null || !toHost.equals(fromHost)) { // external links
               continue; // skip it
             }
           }
{code}

Isn't that redundant? I don't think the first if block is needed.",smartshark_2_2,2197,nutch,"""[0.8784775733947754, 0.1215224340558052]"""
2482,230741,remove unused package o.a.n.html,"The package {{org.apache.nutch.html}} has been removed in trunk with NUTCH-837 but is still present in 2.x. It contains only one class/file (Entities.java) which is not used at all. It should be removed including Entities.java, or the class should be moved to package {{o.a.n.util}}.",smartshark_2_2,681,nutch,"""[0.9983733892440796, 0.0016265343874692917]"""
2483,230742,README.txt is lacking info that should be there,"from Jukkas email:

* The README.txt should start with ""Apache Nutch"" instead of ""Nutch""",smartshark_2_2,388,nutch,"""[0.9939270615577698, 0.006072950083762407]"""
2484,230743,Upgrade to Gora 0.6.1,"Apache Gora 0.6.1 was released recently.
We should upgrade before pushing Nutch 2.3.1 as it will come in very handy for the new Docker containers.",smartshark_2_2,974,nutch,"""[0.9983432292938232, 0.0016567149432376027]"""
2485,230744,ERROR conf.Configuration - Failed to set setXIncludeAware(true),"Each executed job results in a number of occurences of the exception below:

2011-01-27 13:40:34,457 ERROR conf.Configuration - Failed to set setXIncludeAware(true) for parser org.apache.xerces.jaxp.DocumentBuilderFactoryImpl@3801318b:java.lang.UnsupportedOperationException: This parser does not support specification ""null"" version ""null""
java.lang.UnsupportedOperationException: This parser does not support specification ""null"" version ""null""
        at javax.xml.parsers.DocumentBuilderFactory.setXIncludeAware(DocumentBuilderFactory.java:590)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1054)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1040)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:436)
        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:103)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)
        at org.apache.nutch.crawl.Injector.inject(Injector.java:230)
        at org.apache.nutch.crawl.Injector.run(Injector.java:248)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.nutch.crawl.Injector.main(Injector.java:238)

This can be fixed by upgrading xercesImpl from 2.6.2 to 2.9.1. If modified ivy and lib-xml's ivy configuration and can commit it. The question is, is upgrading the correct method? I've tested Nutch with 2.9.1 and except the lack of the annoying exception everything works as expected.",smartshark_2_2,1680,nutch,"""[0.23945604264736176, 0.760543942451477]"""
2486,230745,Upgrade to most recent JUnit 4.x to improve test flexibility,"I wanted to try using the @Ignore functionality within JUnit, however I don't think it is available in the current JUnit version we use in Nutch. We should upgrade.",smartshark_2_2,2210,nutch,"""[0.9983822107315063, 0.001617774018086493]"""
2487,230746,Set mapreduce.map.speculative for Hadoop 0.21 or higher,Configuration option has been renamed. Fetcher still uses old config to disable speculative exection.,smartshark_2_2,2321,nutch,"""[0.9980677962303162, 0.0019322683801874518]"""
2488,230747,BasicURLNormalizer to normalize https same as http ,"Most of the normalization done by BasicURLNormalizer (lowercasing host, removing default port, removal of page anchors, cleaning . and . in the path) is not done for URLs with protocol https.",smartshark_2_2,2238,nutch,"""[0.9971532821655273, 0.0028467117808759212]"""
2489,230748,DeleteDuplicates based on crawlDB only ,"The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.
I was wondering whether we could use the information from the crawlDB instead to detect URLs to delete then do the deletions in an indexer-neutral way. As far as I understand the content of the crawlDB contains all the elements we need for dedup, namely :
* URL 
* signature
* fetch time
* score

In map-reduce terms we would have two different jobs : 
* read crawlDB and compare on URLs : keep only most recent element - oldest are stored in a file and will be deleted later

* read crawlDB and have a map function generating signatures as keys and URL + fetch time +score as value
* reduce function would depend on which parameter is set (i.e. use signature or score) and would output as list of URLs to delete

This assumes that we can then use the URLs to identify documents in the indices.

Any thoughts on this? Am I missing something?

Julien


",smartshark_2_2,395,nutch,"""[0.9944092631340027, 0.005590730346739292]"""
2490,230749,Domain Ä°ndexing / Query Filter,"Hostname's contain information about the domain of th host, and all of the subdomains. Indexing and Searching the domains are important for intuitive behavior. 

From DomainIndexingFilter javadoc : 
Adds the domain(hostname) and all super domains to the index. 
 * <br> For http://lucene.apache.org/nutch/ the 
 * following will be added to the index : <br> 
 * <ul>
 * <li>lucene.apache.org </li>
 * <li>apache</li>
 * <li>org </li>
 * </ul>
 * All hostnames are domain names, but not all the domain names are 
 * hostnames. In the above example hostname lucene is a 
 * subdomain of apache.org, which is itself a subdomain of 
 * org <br>
 * 
 
Currently Basic indexing filter indexes the hostname in the site field, and query-site plugin 
allows to search in the site field. However site:apache.org will not return http://lucene.apache.org

 By indexing the domain, we can be able to search domains. Unlike 
 the site field (indexed by BasicIndexingFilter) search, searching the 
 domain field allows us to retrieve lucene.apache.org to the query 
 apache.org. 
 ",smartshark_2_2,1750,nutch,"""[0.5466601252555847, 0.45333990454673767]"""
2491,230750,Integrate index-html into Nutch build,"The plugin index-html (added by NUTCH-1944) is loosely integrated:
- code is in Nutch version control
- no build (compile, javadoc generation)
- src/plugin/index-html/src/java/org/apache/nutch/indexer/html/package.html contains a description how to do the integration

Well, the plugin should be available just by adding it to plugin.includes without any extra efforts.",smartshark_2_2,1055,nutch,"""[0.9983693957328796, 0.0016306823818013072]"""
2492,230751,Need to keep hotStore.flush() exception catching,"Still need exception checking for hoststorelflush() for those who have to use gora-core 0.2.1 otherwise Nutch 2.x will not compile.

<!-- Uncomment this to use SQL as Gora backend. It should be noted that the 
    gora-sql 0.1.1-incubating artifact is NOT compatable with gora-core 0.3. Users should 
    downgrade to gora-core 0.2.1 in order to use SQL as a backend. -->


Index: src/java/org/apache/nutch/host/HostDb.java
===================================================================
--- java/workspace/2.x/src/java/org/apache/nutch/host/HostDb.java	(revision 1487824)
+++ java/workspace/2.x/src/java/org/apache/nutch/host/HostDb.java	(working copy)
@@ -87,7 +87,11 @@
             CacheHost removeFromCacheHost = notification.getValue();
             if (removeFromCacheHost != NULL_HOST) {
               if (removeFromCacheHost.timestamp < lastFlush.get()) {
-                hostStore.flush();
+                try {
+                  hostStore.flush();
+                } catch (IOException e) {
+                  throw new RuntimeException(e);
+                }
                 lastFlush.set(System.currentTimeMillis());
               }
             }",smartshark_2_2,2341,nutch,"""[0.9870789051055908, 0.01292115543037653]"""
2493,230752,Modified injector to allow newly injected CrawlDatum to overwrite original,Before this patch if a CrawlDatum is already in the crawldb then it will be used in preference to the CrawlDatum created by the newly injected url. This patch gives the user the ability to force the injected CrawlDatum to be used instead. The use case for this patch was the requirement for injected urls to jump to the top of the TopN list so that we can garuntee they will be crawled immediately (usefull for intranet crawling where changes can trigger injects).,smartshark_2_2,1644,nutch,"""[0.9979078769683838, 0.002092118375003338]"""
2494,230753,Ignore external links based on domain,We currently have `db.ignore.external.links` which is a nice way of restricting the crawl based on the hostname. This adds a new parameter 'db.ignore.external.links.domain' to do the same based on the domain.,smartshark_2_2,1312,nutch,"""[0.9984765648841858, 0.0015234554884955287]"""
2495,230754,"allow parsers to return multiple Parse object, this will speed up the rss parser","allow Parser#parse to return a Map<String,Parse>. This way, the RSS parser can return multiple parse objects, that will all be indexed separately. Advantage: no need to fetch all feed-items separately.
see the discussion at http://www.nabble.com/RSS-fecter-and-index-individul-how-can-i-realize-this-function-tf3146271.html
",smartshark_2_2,1394,nutch,"""[0.9984000325202942, 0.0015999706229194999]"""
2496,230755,Mailing list is broken.,"All of the following addresses are failing:

nutch-user@nutch.apache.org
nutch-user-subscribe@nutch.apache.org
nutch-user-subscribe@lucene.apache.org

For the last one, the mailer daemon said 
""This mailing list has moved to user at nutch.apache.org.""

Below is the message I tried to send:

Hi people,

I've been banging my head against this problem for two days now.
Simply, I want to add a field with the value of a given meta tag.

I've been trying the parse-xml plugin, but that seems that it doesn't
work with version 1.0.  I've tried the code at
http://sujitpal.blogspot.com/2009/07/nutch-getting-my-feet-wet.html
and it hasn't worked.  I don't even know why.  I don't even know if my
plugin is being used... or even looked for!  Nutch seems to have a
infuriating ""Fail silently"" policy for plugins.  I put a
System.exit(1) in my filters just to see if my code is even being
encountered.  It has not in spite of my config telling it to.

Here's my config:
nutch-site.xml
...
<property>
 <name>plugin.includes</name>
 <value>protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|metadata</value>
</property>
...

parse-plugins.xml
...
<mimeType name=""application/xhtml+xml"">
   <plugin id=""parse-html"" />
   <plugin id=""metadata"" />
</mimeType>


<mimeType name=""text/html"">
      <plugin id=""parse-html"" />
      <plugin id=""metadata"" />
</mimeType>

<mimeType name=""text/sgml"">
      <plugin id=""parse-html"" />
      <plugin id=""metadata"" />
</mimeType>

<mimeType name=""text/xml"">
         <plugin id=""parse-html"" />
         <plugin id=""parse-rss"" />
        <plugin id=""metadata"" />
        <plugin id=""feed"" />
</mimeType>
...
<alias name=""metadata""
extension-id=""com.example.website.nutch.parsing.MetaTagExtractorParseFilter""
/>
...

I've also copied the plugin.xml and jar from my build/metadata to the
plugins root dir.

Nonetheless, Nutch runs and puts data in solr for me.  Afaik, Nutch is
completely unaware of my plugin despite my config options.  Is the
some other place I need to tell Nutch to use my plugin?  Is there some
other approach to do this without having to write a plugin?  This does
seem like a lot of work to simply get a meta tag into a field.  Any
help would be appreciated.

Sincerely,

John Sherwood",smartshark_2_2,1620,nutch,"""[0.9722509980201721, 0.027748964726924896]"""
2497,230756,Proposition: Enable Nutch to use a parser plugin not just based on content type,"Sorry, please close this issue.

I figured that if I set my parse plugin first. I can always be called first and than decide if I want to parse or not.",smartshark_2_2,50,nutch,"""[0.9985394477844238, 0.001460554776713252]"""
2498,230757,Injecting Crawl metadata,"the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separated by '='. A input line might look like this:
http://www.myurl.com  \t  categ=value1 \t categ2=value2

This functionality can be useful to store external knowledge and index it with a custom plugin",smartshark_2_2,1538,nutch,"""[0.9984405636787415, 0.0015594131546095014]"""
2499,230758,Faster RegexNormalize with more features,"The patch associated with this is backwards-compatible and has several improvements over the stock 0.8 RegexURLNormalizer:

1) About a 34% performance improvement, from only executing the superclass (BasicURLNormalizer) once in most cases, instead of twice as the stock version did. 

2) Support for expensive host-specific normalizations with good performance. Each <regex> block optionally takes a list of hosts to which to apply the associated regex. If supplied, the regex will only be applied to these hosts. This should have scalable performance; the comparison is O(1) regardless of the number of hosts. The format is:

    <regex>
        <host>www.host1.com</host>
        <host>host2.site2.com</host>
        <pattern> my pattern here </pattern>
        <substitution> my substitution here </substitution>
   </regex>

3)  Support for decoding URLs with escaped character encodings (e.g. %20, etc.). This is useful, for example, to decode ""jump redirects"" which have the target URL encoded within the source, as on Yahoo. I tried to create an extensible notion of ""options,"" the first of which is ""unescape."" The unescape function is applied *after* the substitution and *only* if the substitution pattern matches. A simple pattern to unescape Yahoo directory redirects would be something like:

<regex>
  <pattern>^http://[a-z\.]*\.yahoo\.com/.*/\*+(http[^&amp;]+)</pattern>
  <substitution>$1</substitution>
  <options>unescape</options>
</regex>

4) Added the notion of iterating the pattern chain. This is useful when the result of a normalization can itself be normalized. While some of this can be handled in the stock version by repeating patterns, or by careful ordering of patterns, the notion of iterating is cleaner and more powerful. The chain is defined to iterate only when the previous iteration changes the input, up to a configurable maxium number of iterations. The config parameter to change is: urlnormalizer.regex.maxiterations, which defaults to 1 (previous behavior). The change is performance-neutral when disabled, and has a relatively small performance cost when enabled.

Pardon any potentially unconventional Java on my part. I've got lots of C/C++ search engine experience, but Nutch is my first large Java app. I welcome any feedback, and hope this is useful.

Doug",smartshark_2_2,886,nutch,"""[0.998188316822052, 0.0018116796854883432]"""
2500,230759,Authentication Support for Web GUI,We should implement an authentication support for Web GUI.,smartshark_2_2,1290,nutch,"""[0.9980242252349854, 0.0019758513662964106]"""
2501,230760,Exchange component for indexing job,The exchange component acts in indexing job and decides which index writer a document should go to. It includes an extension point to allow developers to develop plugins with their own logic.,smartshark_2_2,2609,nutch,"""[0.9981314539909363, 0.0018686053808778524]"""
2502,230761,Check that Factory classes use the cache in a thread safe way,I found in [NUTCH-1604] that the ProtocolFactory class was not handling access to the cache properly. The same mechanism is used in other Factory classes so we should make sure that they are properly synchronized + make ObjectCache thread safe as well,smartshark_2_2,409,nutch,"""[0.886398434638977, 0.11360152065753937]"""
2503,230762,NutchField to support long,"NutchField has no support for Long in readfields. Usually this is not a problem because in reducers it is only written to the output. But when using NutchField in mappers, then a reducer cannot read a Long.

{code}
java.lang.RuntimeException: problem advancing post rec#0
        at org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:1217)
        at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.moveToNext(ReduceTask.java:250)
        at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:246)
        at org.apache.nutch.fetcher.Fetcher$FetcherReducer.reduce(Fetcher.java:1440)
        at org.apache.nutch.fetcher.Fetcher$FetcherReducer.reduce(Fetcher.java:1401)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:197)
        at org.apache.hadoop.io.Text.readString(Text.java:402)
        at org.apache.nutch.indexer.NutchField.readFields(NutchField.java:89)
        at org.apache.nutch.indexer.NutchDocument.readFields(NutchDocument.java:112)
        at org.apache.nutch.indexer.NutchIndexAction.readFields(NutchIndexAction.java:81)
        at org.apache.nutch.util.GenericWritableConfigurable.readFields(GenericWritableConfigurable.java:54)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)
        at org.apache.hadoop.mapred.Task$ValuesIterator.readNextValue(Task.java:1276)
        at org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:1214)
        ... 7 more
{code}",smartshark_2_2,433,nutch,"""[0.9274531602859497, 0.07254686951637268]"""
2504,230763,Create or locate class for org.apache.nutch.tools.compat.CrawlDbConverter,"Whilst updating the CommandLineOptions for release 1.3 on the wiki, I noticed that the above class does not exist in the expected location in /src folder. Having looked further afield, it appears that this class (which is meant to convert Nutch 0.9 WebDB to 1.3 format WebDB) does not exist.",smartshark_2_2,1814,nutch,"""[0.996070146560669, 0.003929825499653816]"""
2505,230764,Clustering API improvements,"This patch adds support for retrieving original document scores (from NutchBean), as well as cluster-level relevance scores (from Clusterer). Both methods may improve visual representation of the clusters, where individual items may be visually differentiated depending on their query relevance and cluster relevance. A modified cluster.jsp illustrates this feature.",smartshark_2_2,1586,nutch,"""[0.9982971549034119, 0.0017028675647452474]"""
2506,230765,SentenceDetectorME::sentPosDetect() with useTokenEnd=false,"I trained the SentenceModel with a german korpus and wondered about the results for the following input (a mark indicates the expected split):
{code:xml}
""I am hungry.Ich bin Mr. Bean.Ein guter Satz.""
             ^                ^
{code}
The result was 3 sentences. Good, but the split was not at the eosChar. It was after the token with the eosChar: ""I am hungry.Ich"" , ""bin Mr. Bean.Ein"", ...

After some debugging I found out that I have to set useTokenEnd=false in the SentenceDetectorFactory-ctor.
And then I found a *little bug in SentenceDetectorME* when the span is calculated:
{code:java}
  public Span[] sentPosDetect(String s) {
...
      if (bestOutcome.equals(SPLIT) && isAcceptableBreak(s, index, cint)) {
        if (index != cint) {
          if (useTokenEnd) {
            positions.add(getFirstNonWS(s, getFirstWS(s,cint + 1)));
          }
          else {
            positions.add(getFirstNonWS(s,cint)); // this should be positions.add(getFirstNonWS(s,cint + 1)); 
          }
          sentProbs.add(probs[model.getIndex(bestOutcome)]);
        }
        index = cint + 1;
      }
...
{code}

This change has only impact on models with useTokenEnd=false",smartshark_2_2,625,opennlp,"""[0.11753389984369278, 0.8824660778045654]"""
2507,230766,invoking .find() on a RegexNameFinder instance brings back Spans with identical start/end indices,"The RegexNameFinder class has a serious bug...Whenever it finds something it produces a Span with the same start/end index. This happens because 'sentencePosTokenMap' stores the same position for the start and end of the token.Conceptually this fine, after all it is the same token, however later on matcher.start()/end() is invoked to determine what to ask from the map.Well, if we've stored the same position we will get the same number and the Span will be ruined, right? The trick here is to store i+1 for the endIndex for that token in the map. That is essentially the position of next token, but since we're expecting tokenized text anyway everything is fine...Untokenized text breaks the system anyway so in my opinion it is safe to apply the forthcoming patch. A dirty approach would be to leave the map as is and simply replace 'matcher.end()' with 'matcher.end()+1' when we're doing the lookup.",smartshark_2_2,439,opennlp,"""[0.06314390152692795, 0.9368561506271362]"""
2508,230767,NameFinderSequenceValidator should not return true for CONT following a START of a different name type,"Writing the unittest for NameFinderSequenceValidator it was discovered that Start followed by a Continue of a different type was validated as correct (valid).

The simple test for continue in NameFinderSequenceValidator does not take the name type into account.",smartshark_2_2,975,opennlp,"""[0.6323801875114441, 0.3676197826862335]"""
2509,230768,Name Finders cross validation is broken,The recent refactory of the cmd line code broke the cross validation of the name finder.,smartshark_2_2,370,opennlp,"""[0.10988312214612961, 0.8901168704032898]"""
2510,230769,java.io.IOException with POSTaggerTrainer.conllx when -gram option is used,"If the -ngram option is used this exception is thrown:

Building ngram dictionary ... IO error while building NGram Dictionary: Stream not marked
Stream not marked
java.io.IOException: Stream not marked
       at java.io.BufferedReader.reset(BufferedReader.java:485)
       at opennlp.tools.util.PlainTextByLineStream.reset(PlainTextByLineStream.java:79)
       at opennlp.tools.util.FilterObjectStream.reset(FilterObjectStream.java:43)
       at opennlp.tools.util.FilterObjectStream.reset(FilterObjectStream.java:43)
       at opennlp.tools.cmdline.postag.POSTaggerTrainerTool.run(POSTaggerTrainerTool.java:80)
       at opennlp.tools.cmdline.CLI.main(CLI.java:222)",smartshark_2_2,543,opennlp,"""[0.07495834678411484, 0.925041675567627]"""
2511,230770,TokenNameFinderCrossValidator hangs when passed a NameEvaluationErrorListener... ,"TokenNameFinderCrossValidator works fine if passed null or any other listener except a ""NameEvaluationErrorListener"", for  the l""listeners"" parameter (last vararglist parameter in the constructor)...I have tested it with DetailedFMeasureListener and null and it works fine. When trying to pass an array with the 2 listeners i want (DetailedFMeasureListener, NameEvaluationErrorListener) it just hangs after the processing the last partition of data, rather than showing the F-score! It is not a show stopper because i can simply pass null and get my F-score, but it would be nice if could see the misclassifications as well...",smartshark_2_2,368,opennlp,"""[0.07097619771957397, 0.9290237426757812]"""
2512,230771,File Encoding Issues,"The input and output encodings are not working correctly or are not properly handled.  A good example is the CoNLL 2002 data if correctly encoded in UTF-8 does not correctly work for training without specifying -Dfile.encoding=UTF-8 for the Java Command.

We already specify the input and expected output encoding on the cmdline interface with the -encoding paramter.  For some reason this isn't being followed.

I'll work on fixing this for the next major release...  :-)
",smartshark_2_2,329,opennlp,"""[0.10592303425073624, 0.894076943397522]"""
2513,230772,NameSampleTypeFilter drops the id of the NameSample object,"The NameFinderTypeFilter filters out annotations which do not match the type list. This filter needs to create a new NameSample object instance, during the creation the id of the NameSample objetc is ignore.

To fix this pass the id correctly to the newly created NameSample object.",smartshark_2_2,493,opennlp,"""[0.14144660532474518, 0.8585534691810608]"""
2514,230773,Bad precision using FMeasure,"I noticed bad precision in FMeasure results. I think the issue is that the current implementation is summing divisions. It computes the precision and recall for every sample, and after adds the results for each sample to compute the overall result. By doing that, the error related to each division are summed and can impact the final result.
I found the problem while implementing the ChunkerEvaluator. To verify the evaluator I tried to compare the results we get using OpenNLP and the Perl script conlleval available at http://www.cnts.ua.ac.be/conll2000/chunking/output.html. The results were always different if I process more than one sentence, because the implementation was using FMeasure.updateScores() that was summing divisions.
To solve that and have the same results provided by conll I basically stopped using the Mean class.",smartshark_2_2,24,opennlp,"""[0.271980881690979, 0.728019118309021]"""
2515,230774,Brat parser crashes if .ann file contain attribute annotations,"A brat .ann annotation file that contains an attribute annotations crashes the parser. A parser for attribute annotation lines is missing in the brat formats support.

To fix this support for attribute annotations has to be added.",smartshark_2_2,637,opennlp,"""[0.07939538359642029, 0.9206045866012573]"""
2516,230775,Add language code constants,"There are many places in the code where languages are referenced by hardcoded strings such as ""eng"". Add a class that contains constants for the language codes and replace hardcoded string with the constant values. Use ISO 639-2 language codes.",smartshark_2_2,1197,opennlp,"""[0.9982762336730957, 0.0017237538704648614]"""
2517,230776,Remove all trailing white spaces,"The OpenNLP source code contains many lines which have one or more white space characters at the end of the line.

It would be nice to remove these before the next release. We will then continue doing this refactoring before each release.

Todays diff tools can diff the source code and ignore white space only differences.",smartshark_2_2,573,opennlp,"""[0.9982410669326782, 0.0017589033814147115]"""
2518,230777,OPENNLP.BAT: change -Xmx4096m to -Xmx1024m,"On Windows with 32-bit Java 6, with the original -Xmx4096 in the command, I get the following error message:
Error occurred during initialization of VM
The size of the object heap + VM data exceeds the maximum representable size

To allow it to run on this environment, the switch should be changed to -Xmx1024. The shell script also uses -Xmx1024.

Thanks.",smartshark_2_2,652,opennlp,"""[0.9770861268043518, 0.022913875058293343]"""
2519,230778,Add a dependency parser component,It would be nice to add a dependency parser component to OpenNLP.,smartshark_2_2,1044,opennlp,"""[0.99751877784729, 0.0024812850169837475]"""
2520,230779,GeoEntityLinker should dedupe entries returned from Gazateers,"Gazateers can have dupes, especially when returning a subset of fields. The GeoEntityLinker addon appears to return many duplicates from the GeoNames Gazateer. Deduping should be performed on a Span by Span basis.",smartshark_2_2,529,opennlp,"""[0.987201452255249, 0.012798529118299484]"""
2521,230780,Add implementations of Tokenizer and SentenceDetector using BreakIterator,Add implementations of Tokenizer and SentenceDetector using BreakIterator.,smartshark_2_2,1028,opennlp,"""[0.9982683658599854, 0.0017316641751676798]"""
2522,230781,Merge TrainingParameters and PluggableParameters,The PluggableParameters class was added to pull out the get(Int/String/Boolean)Parameters() methods from the AbstractTrainer.  Merge the functionality of the PluggableParameters into the TrainingParameters.,smartshark_2_2,913,opennlp,"""[0.9984837174415588, 0.0015162451891228557]"""
2523,230782,DictionarySerializer should not close the InputStream,"The DictionarySerializer closes the passed Input Stream after the dictionary is read. The Input Stream should stay open, because it is the responsibility of the one who created the Input Stream to close it. ",smartshark_2_2,117,opennlp,"""[0.7993938326835632, 0.20060612261295319]"""
2524,230783,Make constants in StringPattern constant,"The StringPattern defines a couple of constants, but they are not static final.",smartshark_2_2,199,opennlp,"""[0.9984355568885803, 0.0015644371742382646]"""
2525,230784,BeamSearch class is missing a junit test,"BeamFinder is missing a test class to make sure any modifications don't disturb the functionality of the classes that use this important class.

Issues is migrated from SourceForge:
https://sourceforge.net/tracker/?func=detail&aid=3044654&group_id=3368&atid=103368",smartshark_2_2,173,opennlp,"""[0.9971774816513062, 0.002822548383846879]"""
2526,230785,Move all the parameter classes in opennlp.tools.cmdline into the params sub-package,Small refactoring: Move all the parameter classes into the params sub-package.,smartshark_2_2,184,opennlp,"""[0.997982382774353, 0.002017548307776451]"""
2527,230786,Make statistical models available as maven artifacts and deploy them on the maven repo,"This would be really useful for third party application developers who use opennlp as a library to be able to load default models from the classpath.

For each model, we need to write a very short pom.xml file that will then be used by maven to build and deploy a versioned
jar that holds the model (the .bin.gz file in folder src/main/resource/opennlp/ for instance).",smartshark_2_2,898,opennlp,"""[0.9979719519615173, 0.0020280967000871897]"""
2528,230787,Include a formater that creates a stream of SentenceSample from AD corpus,This can be used to train a Portuguese Sentence Detector model.,smartshark_2_2,333,opennlp,"""[0.9978402853012085, 0.0021596362348645926]"""
2529,230788,Write a release note for our first (1.5.1-incubating) release here,"We need to write a release note which can be included in our distribution
and be published on our website.",smartshark_2_2,61,opennlp,"""[0.9978287816047668, 0.002171189058572054]"""
2530,230789,Add comments to the L-BFGS trainer sample param files,"The L-BFGS sample params file qn-trainer-l1.params should contain a comment explaining the possible parameters.

Which range should be used for L1? Why is L2 set to zero? Are there other parameters which could be set, but are not used in the sample?

A similar description should be created for the L2 file. It would also be nice if the L2 file could be renamed to the same naming schema as the L1 (qn-trainer-l1.params) file.",smartshark_2_2,590,opennlp,"""[0.9981751441955566, 0.0018248413689434528]"""
2531,230790,Add a code conventions page to the website,There should be a short page explaining the OpenNLP code conventions and offering downloads for format files which can be used by the major IDEs.,smartshark_2_2,305,opennlp,"""[0.9973548650741577, 0.0026451125741004944]"""
2532,230791,Add Apache Feather logo to web-site,"When moving the web-site, the incubator logo was removed.
It would be nice to add the apache logo and link to http://www.apache.org with the feather logo.
",smartshark_2_2,361,opennlp,"""[0.9971539974212646, 0.002846029819920659]"""
2533,230792,DictionaryLemmatizer dictionary and LemmatizerME training format different,"The LemmatizerME training data has a format of word\tpos\tlemma.
The DictionaryLemmatizer format is word\tlemma\tpos.

Can we make them the same.",smartshark_2_2,791,opennlp,"""[0.9979246854782104, 0.002075291471555829]"""
2534,230793,Write a test case for the NameFinderSequenceValidator class,"The NameFinderSequenceValidator is public now, and that makes it possible to write a test for it. The test should check that the sequence validation works as expected.",smartshark_2_2,967,opennlp,"""[0.9983991980552673, 0.0016007416415959597]"""
2535,230794,"Add incubator disclaimer to website, or remove notice if not necessary","The website has a ""fixme"" to add an incubator disclaimer. The disclaimer should be added to the website.",smartshark_2_2,10,opennlp,"""[0.9972527623176575, 0.0027472826186567545]"""
2536,230795,GeoEntityLinker should maintain an index of Regions,"Currently the GeoEntityLinker does not understand region references such as North Africa, or continental references such as South America. This can be fixed by adding a region type to the index and context files.",smartshark_2_2,596,opennlp,"""[0.9878081679344177, 0.012191882357001305]"""
2537,230796,"Opennlp supports RRB/LRB and RCB/LCB, but not RSB/LSB","OpenNlp supports Right / Left Round Bracket and Right / Left Curly Bracket, but not Right / Left Square Bracket.

Fix is simple, by adding the corresponding ""if()""s in 3 files: 
tools/parser/chunking/BuildContextGenerator
tools/parser/Parse
tools/tokenize/lang/en/TokenSampleStream


Important NOTE:

I attach the patch for this fix, made by svn diff in trunk/.
However, please note that this patch includes also the changes for the fix of OPENNLP-597.
Unfortunately, I cannot really go around this, as the changes for OPENNLP-597 were not yet merged, at the moment when I opened OPENNLP-598.
If this is a problem, I will re-create this patch, after OPENNLP-597 is closed & its changes merged.

Thank you.
",smartshark_2_2,517,opennlp,"""[0.39615559577941895, 0.603844404220581]"""
2538,230797,Replace occurrences of junit.framework.* with org.junit.*,Replace occurrences of junit.framework.* with org.junit.*,smartshark_2_2,828,opennlp,"""[0.9961305856704712, 0.0038694022223353386]"""
2539,230798,Lemmatizer serialization,"Hello,
why should we not implement the serialization for SimpleLemmatizer?",smartshark_2_2,722,opennlp,"""[0.9982384443283081, 0.001761582912877202]"""
2540,230799,Add cross validation cmd line tool for the POS Tagger,The cmd line interface should have a cross validation tool for the pos tagger.,smartshark_2_2,133,opennlp,"""[0.9976035952568054, 0.0023963991552591324]"""
2541,230800,Switch from Java 7 to Java 8,"Java 7 is now End Of Life and OpenNLP should be updated to use Java 8.

- Update the POMs
- Add comment to README / NOTES files",smartshark_2_2,782,opennlp,"""[0.9980418682098389, 0.001958064967766404]"""
2542,230801,POSTagger should check that tag dict only contains mappings to valid model outcomes,"Currently a tag dict could contain a mapping to a pos tag which cannot occur because it is not an outcome of the underlying statistical model. The POSModel code should validate that the tag dict only contains valid tags, and if not throw an exception.
A POSTagger with such an invalid model might fail during runtime.",smartshark_2_2,172,opennlp,"""[0.890465497970581, 0.10953450202941895]"""
2543,230802,Improve errror reporting in Brat format parser,The Brat format parser process lots of different files usually. In case there is an error it should always include the file name in the exception so the user has a chance to track down the error easily (e.g. without modifying OpenNLP code).,smartshark_2_2,788,opennlp,"""[0.9983691573143005, 0.0016308880876749754]"""
2544,230803,Semcor Parser/Converter,"We need a parser/converter for Semcor.
This is useful to be able to evaluate WSD related approaches.",smartshark_2_2,776,opennlp,"""[0.9979131817817688, 0.0020868040155619383]"""
2545,230804,Fix documentation to remove references to 'Save XXXModel to database' ,Fix documentation to remove references to 'Save XXXModel to database' since its not supported today and is up to  the user to implement that.,smartshark_2_2,994,opennlp,"""[0.9984140396118164, 0.0015859659761190414]"""
2546,230805,Add a language detection component,"Many of the components in OpenNLP are sensitive to the input language. It would be nice if OpenNLP would have a component to detect the language of an input text.

Two commonly used solutions today are:
Apache Tikas Language Identifier
Language Detection from Shuyo, Nakatani",smartshark_2_2,1070,opennlp,"""[0.9983224272727966, 0.0016776002012193203]"""
2547,230806,"GeoEntityLinker does not provide a method for setting the properties file location in order to get the database connection, it is currently hard coded","GeoEntityLinker does not provide a method for setting the properties file location in order to get the database connection, it is currently hard coded",smartshark_2_2,480,opennlp,"""[0.8441538214683533, 0.15584617853164673]"""
2548,230807,Improve resource loading for custom feature generators,Currently the feature generators are matched in the TokenNameFinderTool which is part of the cmd line interface. To improve this the logic should be moved to the GeneratorFactory.,smartshark_2_2,1004,opennlp,"""[0.9982921481132507, 0.0017078911187127233]"""
2549,230808,"Tokenize ""can't""","When I use OpenNLP's tokenizer to tokenize this sentence ""I can't do it."", I get tokens like ""[I] [ca] [n't] [do] [it] [.]"". Isn't it supposed to be something like ""[I] [can] ['] [t] [do] [it] [.]"" or ""[I] [can't] [do] [it] [.]""? I know LanguageTool's tokenizer gives tokens like ""[I] [can] ['] [t] [do] [it] [.]"".",smartshark_2_2,561,opennlp,"""[0.5915318727493286, 0.408468097448349]"""
2550,230809,demonstration how sensitive syntactic match is compared to bag-of-words approach,"per Jason's recommendation:  have you done
> standard similarity based on the standard bag-of-words model?

I do simple bag-of-words with its own list of stopwords and compare two approaches on the pair of  cases:
1) similar words but different meaning
2) different words but similar meaning",smartshark_2_2,446,opennlp,"""[0.9977949857711792, 0.0022050838451832533]"""
2551,230810,Javadoc of NaiveBayesTrainer class looks incorrect,"It seems that Javadoc of NaiveBayesTrainer class was copied from PerceptronTrainer and hence, it says ""Trains models using the perceptron algorithm."" :)",smartshark_2_2,1131,opennlp,"""[0.9961733222007751, 0.0038266624324023724]"""
2552,230811,Add formats support for the French Treebank,"The OpenNLP formats package should have support for the French TreeBank. It can be obtained without paying for it for research purposes.

Here is more information about it.
http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php",smartshark_2_2,670,opennlp,"""[0.9983640313148499, 0.0016360008157789707]"""
2553,230812,Source distribution is almost empty,When building with the apache-release profile the created source distribution is almost empty. Make sure all the projects and files end up in the source distribution.,smartshark_2_2,57,opennlp,"""[0.9964675903320312, 0.0035323866177350283]"""
2554,230813,PorterStemmer class needs to be public,The PorterStemmer class was accidentally labeled as package private. It should be changed to public so it can be used outside of the stemmer package.,smartshark_2_2,479,opennlp,"""[0.9979416728019714, 0.0020583607256412506]"""
2555,230814,Remove the legacy META-INF folder from  opennlp-maxent,"The META-INF folder contains a MANIFEST which was used a long time ago to print an error message if java -jar is used to start the maxent jar file. The mechanism is no longer in place since a couple of releases. 
Lets remove the META-INF folder.",smartshark_2_2,432,opennlp,"""[0.9983875751495361, 0.0016124977264553308]"""
2556,230815,Prepare the opennlp-1.5.1 Release Candiate 7,One more RC to include the recent changes to maxent.,smartshark_2_2,115,opennlp,"""[0.9985055923461914, 0.0014943802962079644]"""
2557,230816,GeoEntityLinker point clustering scorer should use standard Lucene spatial geohash,Lucene spatial provides an efficient geohash function. Since lucene is already a dependency the lucene spatial geohash function should be used rather than the current implementation.,smartshark_2_2,582,opennlp,"""[0.998308539390564, 0.0016914359293878078]"""
2558,230817,JavaDoc warnings,There are various JavaDoc warning on compilation,smartshark_2_2,273,opennlp,"""[0.9970935583114624, 0.0029064377304166555]"""
2559,230818,Update the coref code to compile against 1.6.0,It would be nice if the coref code would compile against an older release version and gets the code a bit updated so it complies mostly with checkstyle rules.,smartshark_2_2,1180,opennlp,"""[0.9977670907974243, 0.0022328917402774096]"""
2560,230819,JIRA No Longer Tracking Sources,"JIRA doesn't seem to be tracking changes to the SVN repo and marking activity towards JIRA issues.
Are we doing something wrong?  Or did an upgrade break something?",smartshark_2_2,819,opennlp,"""[0.923748791217804, 0.07625122368335724]"""
2561,230820,Evaluators should allow tools to register a report interface,OPENNLP-220 introduced the -misclassified argument that enables evaluators to print misclassified items while using the command line evaluators. We should expand it to allow any other tool that uses evaluators to register an interface to get that information.,smartshark_2_2,206,opennlp,"""[0.9984910488128662, 0.001508886693045497]"""
2562,230821,"Caching for parsing, chunking and phrase grouping","to speed up similarity computation, store parsing results in a hash, so that if a sentence has been parsed, chunked and prepared for matching once, we store it in a hash.
when the Processor is instantiated, hash is deserialized. When the processor is closed, this hash is serialized.",smartshark_2_2,448,opennlp,"""[0.9983661770820618, 0.0016338503919541836]"""
2563,230822,Enable UIMA Parser module,"The Parser module (presumably likely to be an important component for many projects?) is missing from the OpenNlpTextAnalyzer.xml pipeline.

opennlp-uima\descriptors\Parser.xml is present in e.g. SVN revision 1432812 but not referenced in OpenNlpTextAnalyzer.xml and createPear.xml.

The attached patches add Parser.xml to the OpenNlpTextAnalyzer pipeline.
",smartshark_2_2,440,opennlp,"""[0.9984400868415833, 0.0015599278267472982]"""
2564,230823,Create a detailed FMeasure results listener,"Create a evaluation listener that would output detailed FMeasure for samples that uses typed span. For example, it lets the user know individual precision and recall for person, organization, date in a NameFinder model.",smartshark_2_2,207,opennlp,"""[0.9984254837036133, 0.0015745373675599694]"""
2565,230824,Identify why some eval tests fail on AMD processors,"When running the eval-tests for the 1.8.1 tag some of the tests consistently fail on an EC2 instance. On another virtual machine the tests consistently pass. When the tests fail the failures are consistent with the following:

{quote}Failed tests: 
  ArvoresDeitadasEval.evalPortugueseChunkerQnMultipleThreads:208->chunkerCrossEval:128 expected:<0.9649180953528779> but was:<0.9650518197155942>
  ArvoresDeitadasEval.evalPortugueseSentenceDetectorMaxentQn:143->sentenceCrossEval:90 expected:<0.99261110833375> but was:<0.9927505074644777>
  Conll02NameFinderEval.evalSpanishOrganizationMaxentQn:390->eval:90 expected:<0.682961897915169> but was:<0.6798418972332015>
  ConllXPosTaggerEval.evalSwedishMaxentQn:152->eval:76 expected:<0.9347595473833098> but was:<0.9322842998585573>{quote}

Both systems are Ubuntu 16.04.2 running OpenJDK 1.8.0_131 but there must be some other differences affecting the tests. Those differences need to be identified.

*VM1 (Tests Consistently _Pass_)*
Apache Maven 3.3.9
Maven home: /usr/share/maven
Java version: 1.8.0_131, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-openjdk-amd64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""4.4.0-1022-aws"", arch: ""amd64"", family: ""unix""
LANG=en_US.UTF-8

*VM2 (Tests Consistently _Fail_)*
Apache Maven 3.3.9
Maven home: /usr/share/maven
Java version: 1.8.0_131, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-openjdk-amd64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""4.4.0-83-generic"", arch: ""amd64"", family: ""unix""
LANG=en_US.UTF-8

This VM also consistently fails when using Oracle JDK:
Java version: 1.8.0_131, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-oracle/jre

*VM3 (Tests Consistently _Pass_)*
Apache Maven 3.3.9
Maven home: /usr/share/maven
Java version: 1.8.0_131, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-openjdk-amd64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""4.4.0-83-generic"", arch: ""amd64"", family: ""unix""

*VM4 (Tests Consistently _Fail_)*
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T11:41:47-05:00)
Maven home: C:\Program Files (x86)\maven\bin\..
Java version: 1.8.0_92, vendor: Oracle Corporation
Java home: C:\Program Files\Java\jdk1.8.0_92\jre
Default locale: en_US, platform encoding: Cp1252
OS name: ""windows 10"", version: ""10.0"", arch: ""amd64"", family: ""dos""",smartshark_2_2,1144,opennlp,"""[0.9971522092819214, 0.0028477597516030073]"""
2566,230825,Create sample descriptor for Parser Analysis Engine,Write a sample descriptor for the parser and define the Parser type in the sample OpenNLP type system.,smartshark_2_2,338,opennlp,"""[0.9983786344528198, 0.0016214062925428152]"""
2567,230826,Create Chunk tool documentantion,Move the documentation from Wiki to Docbook.,smartshark_2_2,25,opennlp,"""[0.9976898431777954, 0.0023102161940187216]"""
2568,230827,"Fix/Suppresse ""Possible heap pollution from parameterized vararg type"" warning","The warning should either be fixed, or if false alarm be suppressed. ",smartshark_2_2,908,opennlp,"""[0.9978519678115845, 0.0021480803843587637]"""
2569,230828,Improve features related to abbreviation dictionary in Tokenizer,"I noticed, after doing some benchmark, that we can improve Tokenizer effectiveness by changing a little how features related with the abbreviation dictionary are collected.",smartshark_2_2,392,opennlp,"""[0.9982136487960815, 0.0017863884568214417]"""
2570,230829,Cannot get entities from trained model using DictionaryFeatureGenerator ,"Hello,
I have created the following training data.

{code:title=train.txt|borderStyle=solid}
Ciao mi chiamo <START:person> Damiano <END> ed abito a Roma  .
il mio indirizzo Ã¨ via del <START:person> Corso <END> nella provincia di Roma .
il mio cap Ã¨ lo 00144 nella capitale e e il mio nome Ã¨  <START:person> john <END> .
Abito a Roma in via tar dei tali 10 , <START:person> Mario <END> Ã¨ il mio amico .
Oggi ho incontrato <START:person> giovanni <END> e siamo andati a giocare a calcio .
{code}

And then this code:

{code:title=test.java|borderStyle=solid}

        Charset charset = Charset.forName(""UTF-8"");
        ObjectStream<String> lineStream =
                        new PlainTextByLineStream(new FileInputStream(""/home/damiano/person.train""), charset);
        ObjectStream<NameSample> sampleStream = new NameSampleDataStream(lineStream);

        TokenNameFinderModel model;

        Dictionary dictionary = new Dictionary();
        dictionary.put(new StringList(new String[]{""giovanni""}));
        dictionary.put(new StringList(new String[]{""maria""}));
        dictionary.put(new StringList(new String[]{""luca""}));
      
        BufferedOutputStream aa = null;
          
        AdaptiveFeatureGenerator featureGenerator = new CachedFeatureGenerator(
                 new AdaptiveFeatureGenerator[]{                                 
                    new WindowFeatureGenerator(new TokenFeatureGenerator(), 2, 2),
                    new WindowFeatureGenerator(new TokenClassFeatureGenerator(true), 2, 2),
                    new OutcomePriorFeatureGenerator(),
                    new PreviousMapFeatureGenerator(),
                    new BigramNameFeatureGenerator(),
                    new SentenceFeatureGenerator(true, false),
                    new DictionaryFeatureGenerator(""person"", dictionary)
                   });

        try {
            model = NameFinderME.train(""it"", ""person"", sampleStream, TrainingParameters.defaultParams(),
                    featureGenerator, Collections.<String, Object>emptyMap());
        }
        finally {
          sampleStream.close();
        }

        // Save trained model
        try (BufferedOutputStream modelOut = new BufferedOutputStream(new FileOutputStream(""/home/damiano/it-person-custom.bin""))) {
          model.serialize(modelOut);
        }
                
        // Read the trained model
        try (InputStream modelIn = new FileInputStream(""/home/damiano/it-person-custom.bin"")) {

            TokenNameFinderModel nerModel = new TokenNameFinderModel(modelIn);

            NameFinderME nameFinder = new NameFinderME(nerModel, featureGenerator, NameFinderME.DEFAULT_BEAM_SIZE);
          
            String sentence[] = new String[]{
                ""Ciao"", ""mi"", ""chiamo"", ""Damiano"", ""e"", ""sono"", ""di"", ""Roma"", "".""
            };
            
            Span nameSpans[] = nameFinder.find(sentence);                     
          
            System.out.println(Arrays.toString(Span.spansToStrings(nameSpans, sentence)));
        }      
{code}

When i try 
{code}
""Ciao"", ""mi"", ""chiamo"", ""Damiano"", ""e"", ""sono"", ""di"", ""Roma"", "".""
{code}
it correctly detect ""Damiano"" as PERSON, but if i change it with:
{code}
""Ciao"", ""mi"", ""chiamo"", ""maria"", ""e"", ""sono"", ""di"", ""Roma"", "".""
{code}
it does not detect ""maria"" as PERSON but I added ""maria"" in the dictionary so it should get it. Why not ?

Thanks!",smartshark_2_2,935,opennlp,"""[0.4374743103981018, 0.5625256896018982]"""
2571,230830,EntityLinker framework has no documentation,EntityLinker framework has no documentation,smartshark_2_2,753,opennlp,"""[0.9952883720397949, 0.004711674060672522]"""
2572,230831,Add formats support for germeval2014,"Details about the format can be found here:
https://sites.google.com/site/germeval2014ner/data",smartshark_2_2,1124,opennlp,"""[0.998296320438385, 0.0017036473145708442]"""
2573,230832,"OpenNLP TLP: Remove ""incubating"" from 1.5.3 version",The incubating label should be removed from all mentions of the 1.5.3 version.,smartshark_2_2,348,opennlp,"""[0.998276948928833, 0.0017230765661224723]"""
2574,230833,Move brat ner annotator to opennlp.git,The brat ner annotator service should be included in the OpenNLP Tools release distribution. For that it has to be moved from opennlp-sandbox.git to opennlp.git,smartshark_2_2,749,opennlp,"""[0.9975353479385376, 0.002464613877236843]"""
2575,230834,The inclusion of letter's mechanical meaning in NLP processes.,"Using the LNSR (""Lenser"") to Amplify Meaning

The understanding that letters, numbers and natual sounds have no underlying meaning is taken for granted by _virtually everyone_...

With Letter/Numeral/Sonic Revealer, the capacity to use letters, numbers and sounds-- with the defining ""functional keywords""--can give greater depth to documents and written works of all sorts!

Imagine the increase in refinement, as the deep and fundemental mechanics of written communication, are part of your organization's potental...


Possible Applications:

1. Speed-Speak: Using single, or strings, of letters numbers and sounds in a meaningful way...  For instance, cutting the sentence, ""Maintain within guidelines."", down to the letter, ""a"".



2. Program Computers Using English:. (Currently under development.)  Mathmatizing letter and number mechanics to program devices in common English.



3. A ""Working"" Word Definition; No Dictionary Required:

Using the 26 mechanical kewords, get an approximate functional definition--in English, Spanish, French--or any other language that uses the 26 letter alphabet...(Most useful for _deepening_ one's understanding of expressions.)


Goals:

To expand the use of the LNSR app and techniques; to increase efficiency, understanding and simplification in the above, and as yet undeveloped, applications.
",smartshark_2_2,1186,opennlp,"""[0.9564878344535828, 0.04351217299699783]"""
2576,230835,Add eval support to detokenizer,It would be nice to be able to evaluate a detokenizer on tokenizer training data.,smartshark_2_2,1187,opennlp,"""[0.9977644681930542, 0.0022355469409376383]"""
2577,230836,Java5: generics to avoid casts,In several places Java5 generics can be used to avoid casts and make the code safer and more readable,smartshark_2_2,275,opennlp,"""[0.9979994893074036, 0.002000457374379039]"""
2578,230837,Add a small icon to links which point to external sites,The website contains links which point to external sites. These links should be marked with a small icon like its done over at pdfbox or uima.,smartshark_2_2,15,opennlp,"""[0.997815728187561, 0.002184260170906782]"""
2579,230838,Write the OpenNLP White Paper,There should be an OpenNLP White Paper which documents OpenNLP in a way it is needed for academic papers and theses.,smartshark_2_2,835,opennlp,"""[0.9975016713142395, 0.0024982912000268698]"""
2580,230839,Maven compiler plugin should not overwrite version from partent pom,"The compiler plugin sets the version to 2.3.2. There might have been a reason for this a couple of years back, but today it is probably better to remove it and see if the project can be built with the version specified in the parent pom.",smartshark_2_2,673,opennlp,"""[0.9978310465812683, 0.002168965060263872]"""
2581,230840,"Sentence Detector View should not try to label headlines, bylines, etc.",Currently the sentence detector can be limited to detect text within annotations (e.g. paragraphs). Depending on the use case the user might just want to specify a (additional) list of annotations which should not intersect with sentences.,smartshark_2_2,880,opennlp,"""[0.9876204133033752, 0.012379552237689495]"""
2582,230841,Add evaluation tests for OntoNotes 4,"The OntoNotes corpus can be used to train all components in OpenNLP. For now it would be nice to add evaluation tests for the Pos Tagger, Name Finder and Parser.",smartshark_2_2,663,opennlp,"""[0.9983223080635071, 0.0016776304692029953]"""
2583,230842,OpenNLP TLP: Update incubator status site,"Update the Incubator site
Update the Incubator status page
Update the podling status page. All sections should now be filled in including EXIT. Take some time to read carefully since this page forms the final public record for graduation.
Edit the Incubator website to remove the podling from the list in project.xml. Here explains how.",smartshark_2_2,514,opennlp,"""[0.9982536435127258, 0.0017463454278185964]"""
2584,230843,The rat plugin should allow unapproved licenses during development ,"The build currently fails if there is one file with an unapproved license inside the project folder. The rat setup should be changed to be strict about licenses when we do a release, and allow test without license header during development.

Change the setup of the rat plugin to allow unapproved licenses during development, and non when the apache-release profile is activated.",smartshark_2_2,137,opennlp,"""[0.9982625842094421, 0.0017374834278598428]"""
2585,230844,"Demonstration on how similarity component improves search accuracy. A query is run through Bing search API, and less relevant hits are sorted out by similarity measure between the query and the snippet","Right now we use a fancy domain to demonstrate the usability of similarity component, such as content generation based on web mining. I believe it also makes sense to use a simpler case such as search relevance improvement by use of similarity component. We get search candidates by TF*IDF and then apply OpenNLP parsing and then Similarity component to assess relevance of a snippet to an answer.",smartshark_2_2,297,opennlp,"""[0.9982879757881165, 0.0017120513366535306]"""
2586,230845,Remove deprecated code form opennlp.util package,There are a few classes which are not used anymore or have been moved to other places. Lets remove that code now.,smartshark_2_2,815,opennlp,"""[0.9979391694068909, 0.0020607959013432264]"""
2587,230846,NameFinder multiple lines,"It would be great if the NameFinder would work over multiple lines. When feeding in documents to training, sometimes entities span multiple lines. Additionally, when predicting, it would be great to keep the formatting the original document had. ",smartshark_2_2,733,opennlp,"""[0.9984642267227173, 0.0015357760712504387]"""
2588,230847,Code Styling for NetBeans,"A default set of code styling defaults are needed for NetBeans IDE like the ones for Eclipse.

** Current NetBeans Lacks the ability to Export/Import these settings, however, we need them regarless.

I'll be attaching a zip file with a majority of the NetBeans defaults (as images); can someone look over the settings and comment?

Thanks",smartshark_2_2,853,opennlp,"""[0.9983183145523071, 0.0016817421419546008]"""
2589,230848,POS Tagger context generator should use feature generation classes,As part of the name finder refactoring a number of reusable feature generator classes have been created. The POS Tagger should use these classes and drop custom feature generation code as much as possible. ,smartshark_2_2,979,opennlp,"""[0.9982890486717224, 0.001710893353447318]"""
2590,230849,Add evaluation test for CONLL 2002 data,"The CONLL 2002 data contains named entities and can be used to evaluate the Name Finder. Currently the data is used to evaluate a release candidate in our manual test plan.

It would be nice to automate this manual evaluation and have a JUnit test which fails if the output doesn't match the expectation.
",smartshark_2_2,658,opennlp,"""[0.998501181602478, 0.0014988926704972982]"""
2591,230850,NameFinderME#find should return Spans with probabilities.,"Since Span objects will be able to store probs, the NameFInder should return probs with each Span.",smartshark_2_2,586,opennlp,"""[0.9972484707832336, 0.0027514975517988205]"""
2592,230851,[PATCH] Typos in Name Finder docs,"Hello, I think there are few typos in the Name Finder documentation page. Since I'm not a native speaker it would be good an extra pair of eyes reviewing my patch. Thanks for the awesome tool.",smartshark_2_2,483,opennlp,"""[0.9980498552322388, 0.001950204954482615]"""
2593,230852,Implement early stopping in NameFinderME,Implement early stopping in NameFinderME.,smartshark_2_2,1181,opennlp,"""[0.9981650710105896, 0.0018349444726482034]"""
2594,230853,CoNLL 02 NameFinderConverter can not export all tag types,"The NameFinderConverter factory for the CoNLL 2002 data format does not support exporting a data set with all tag types.
You can only export based on one type only, person, misc, organization, or location.
",smartshark_2_2,252,opennlp,"""[0.5558781623840332, 0.4441218376159668]"""
2595,230854,Add Arvores Deitadas TokenSampleStream,Add Arvores Deitadas TokenSampleStream to train a tokenizer model using corpus in AD format.,smartshark_2_2,349,opennlp,"""[0.9982501864433289, 0.001749735209159553]"""
2596,230855,Update Build to Java 7,Currently the CI server builds with java 1.5. Team consensus is to upgrade to 1.7 so code can utilize current funtionality.,smartshark_2_2,544,opennlp,"""[0.9978391528129578, 0.0021609251853078604]"""
2597,230856,Donation of nlp-utils to Apache OpenNLP,"As discussed on the user list (http://markmail.org/message/zio3jbrakj5qz5au) this is the issue to track the donation of nlp-utils (https://github.com/tteofili/nlp-utils) to Apache OpenNLP.
Main interested components are language modeling and ngram.",smartshark_2_2,557,opennlp,"""[0.9983842372894287, 0.0016157736536115408]"""
2598,230857,NameFinder and Dictionary Integration,"Now that we have a NameFinder Dictionary and improved NameFinder tools; it would be nice to be able to integrate the dictionary and model to help improve the finding of names.
This way, the name finder could be trained more on the surrounding text instead of attempting to memorize common names in the news that occur frequently.

I've already got the name finder corpus, created the dictionaries with the data from the US Census.

I just need to implement some method to help train the model; or be able to use the dictionaries post model creation to help with the finding of names.
",smartshark_2_2,469,opennlp,"""[0.9983282685279846, 0.0016716785030439496]"""
2599,230858,Add reject entity support to the name finder job and name finder view ,"Currently it is only possible to confirm an entity in the name finder view. In some cases entity must be rejected, because they cause lots of false positive hits. Add functionality to reject entities to the name finder job and name finder view.",smartshark_2_2,321,opennlp,"""[0.99853515625, 0.0014648728538304567]"""
2600,230859,Refactor the coreference package,"The coref package should be re-factored and aligned with the approches taken for the other components.
This issue is just an umbrella to link all coref refactoring issues together.",smartshark_2_2,158,opennlp,"""[0.9975058436393738, 0.002494156826287508]"""
2601,230860,Update OpenNLP Tools version to 1.5.2,Update the version class to output 1.5.2 instead of 1.5.1.,smartshark_2_2,123,opennlp,"""[0.9984972476959229, 0.0015027181943878531]"""
2602,230861,Added an interface for opennlp.tools.util.Span,"Added an interface, opennlp.tools.util.Spannable, to allow for the creation of objects that can span text and fulfill the same functionality as Span without necessarily needing to extend it (currently implemented only by opennlp.tools.util.Span).",smartshark_2_2,793,opennlp,"""[0.9984610080718994, 0.0015390312764793634]"""
2603,230862,Deprecate methods which still use cutoff and iterations args,All methods which still use the iterations and cutoff argument should be deprecated and the related method with the TrainingParameters argument should be called.,smartshark_2_2,210,opennlp,"""[0.998427152633667, 0.0015728507423773408]"""
2604,230863,Maxent/Perceptron training should report progess back via an API,"Currently any training progress is printed to the console. The code should be changed to report the training progress back via an API. A command line training tool could use this API to print the status messages to the console. Other applications, e.g. a training server could use the reported results to display them to a user in a ui interface.",smartshark_2_2,537,opennlp,"""[0.9984277486801147, 0.0015722945099696517]"""
2605,230864,Naive Bayesian Classifier,"I thought it would be nice to have a Naive Bayesian classifier in OpenNLP (it lacks one at present).

Implementation details:  We have a production-hardened piece of Java code for a multinomial Naive Bayesian classifier (with default Laplace smoothing) that we'd like to contribute.  The code is Java 1.5 compatible.  I'd have to write an adapter to make the interface compatible with the ME classifier in OpenNLP.  I expect the patch to be available 1 to 3 weeks from now.

Below is the email trail of a discussion in the dev mailing list around this dated May 19th, 2015.

<snip>
Tommaso Teofili via opennlp.apache.org 

to dev 
Hi Cohan,

I think that'd be a very valuable contribution, as NB is one of the
foundation algorithms, often used as basis for comparisons.
It would be good if you could create a Jira issue and provide more details
about the implementation and, eventually, a patch.

Thanks and regards,
Tommaso

</snip>

2015-05-19 9:57 GMT+02:00 Cohan Sujay Carlos 

> I have a question for the OpenNLP project team.
>
> I was wondering if there is a Naive Bayesian classifier implementation in
> OpenNLP that I've not come across, or if there are plans to implement one.
>
> If it is the latter, I should love to contribute an implementation.
>
> There is an ME classifier already available in OpenNLP, of course, but I
> felt that there was an unmet need for a Naive Bayesian (NB) classifier
> implementation to be offered as well.
>
> An NB classifier could be bootstrapped up with partially labelled training
> data as explained in the Nigam, McCallum, et al paper of 2000 ""Text
> Classification from Labeled and Unlabeled Documents using EM"".
>
> So, if there isn't an NB code base out there already, I'd be happy to
> contribute a very solid implementation that we've used in production for a
> good 5 years.
>
> I'd have to adapt it to load the same training data format as the ME
> classifier, but I guess that shouldn't be very difficult to do.
>
> I was wondering if there was some interest in adding an NB implementation
> and I'd love to know who could I coordinate with if there is?
>
> Cohan Sujay Carlos
> CEO, Aiaioo Labs, India",smartshark_2_2,756,opennlp,"""[0.9983269572257996, 0.0016730797942727804]"""
2606,230865,Allow length for Prefix and Suffix feature generators to be set,"Allow the length for PrefixFeatureGenerator and the SuffixFeatureGenerator to be configurable. Additionally, prevent these generators from creating duplicate features when the length is larger than the token's size.",smartshark_2_2,969,opennlp,"""[0.9984760880470276, 0.0015239691128954291]"""
2607,230866,row count incorrect if data file has more than 2^31 rows,"If a parquet file has more than 2^31 rows, the row count written into the file metadata is incorrect. 
The cause of the problem is the use of an int instead of long data type for numRows in ParquetMetadataConverter, toParquetMetadata:
    int numRows = 0;
    for (BlockMetaData block : blocks) {
      numRows += block.getRowCount();
      addRowGroup(parquetMetadata, rowGroups, block);
    }",smartshark_2_2,285,parquet-mr,"""[0.06644127517938614, 0.9335587620735168]"""
2608,230867,NPE when debug logging file metadata,"When debug logging is enabled and when all values are null in a block, Parquet throws NPE when pretty printing the metadata as the metadata doesn't have min/max defined.

{code:java|borderStyle=solid}
java.io.IOException: Could not read footer: java.lang.RuntimeException: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[""blocks""]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[""columns""]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[""statistics""]->org.apache.parquet.column.statistics.BinaryStatistics[""maxBytes""])
	at org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:247)
	at org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(ParquetFileReader.java:188)
	at org.apache.parquet.hadoop.ParquetReader.<init>(ParquetReader.java:124)
	at org.apache.parquet.hadoop.ParquetReader.<init>(ParquetReader.java:55)
	at org.apache.parquet.hadoop.ParquetReader$Builder.build(ParquetReader.java:264)
	at org.apache.parquet.hadoop.TestParquetVectorReader.testNullReads(TestParquetVectorReader.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
Caused by: java.lang.RuntimeException: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[""blocks""]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[""columns""]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[""statistics""]->org.apache.parquet.column.statistics.BinaryStatistics[""maxBytes""])
	at org.apache.parquet.hadoop.metadata.ParquetMetadata.toJSON(ParquetMetadata.java:72)
	at org.apache.parquet.hadoop.metadata.ParquetMetadata.toPrettyJSON(ParquetMetadata.java:62)
	at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:528)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:430)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:237)
	at org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:233)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[""blocks""]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[""columns""]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[""statistics""]->org.apache.parquet.column.statistics.BinaryStatistics[""maxBytes""])
	at org.codehaus.jackson.map.JsonMappingException.wrapWithPath(JsonMappingException.java:218)
	at org.codehaus.jackson.map.JsonMappingException.wrapWithPath(JsonMappingException.java:183)
	at org.codehaus.jackson.map.ser.std.SerializerBase.wrapAndThrow(SerializerBase.java:140)
	at org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:158)
	at org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)
	at org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)
	at org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)
	at org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)
	at org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:122)
	at org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:71)
	at org.codehaus.jackson.map.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:86)
	at org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)
	at org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)
	at org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)
	at org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:122)
	at org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:71)
	at org.codehaus.jackson.map.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:86)
	at org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)
	at org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)
	at org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)
	at org.codehaus.jackson.map.ser.StdSerializerProvider._serializeValue(StdSerializerProvider.java:610)
	at org.codehaus.jackson.map.ser.StdSerializerProvider.serializeValue(StdSerializerProvider.java:256)
	at org.codehaus.jackson.map.ObjectMapper._configAndWriteValue(ObjectMapper.java:2575)
	at org.codehaus.jackson.map.ObjectMapper.writeValue(ObjectMapper.java:2081)
	at org.apache.parquet.hadoop.metadata.ParquetMetadata.toJSON(ParquetMetadata.java:68)
	... 9 more
Caused by: java.lang.NullPointerException
	at org.apache.parquet.column.statistics.BinaryStatistics.getMaxBytes(BinaryStatistics.java:56)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.codehaus.jackson.map.ser.BeanPropertyWriter.get(BeanPropertyWriter.java:483)
	at org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:418)
	at org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)
	... 30 more
{code}",smartshark_2_2,275,parquet-mr,"""[0.07137686759233475, 0.9286230802536011]"""
2609,230868,ParquetMetadataConverter offset filter is broken,The commit for PARQUET-384 moved block filtering by offset that was happening in the record reader into ParquetMetadataConverter using the same codepath as the existing range filter. This broke the offset filtering because the offsets actually passed to the range filter are row group midpoints.,smartshark_2_2,927,parquet-mr,"""[0.06657638400793076, 0.933423638343811]"""
2610,230869,PrimitiveType.union erases original type,"The following ScalaTest test case
{code}
  test(""merge primitive types"") {
    val expected =
      Types.buildMessage()
        .addField(
          Types
            .required(INT32)
            .as(DECIMAL)
            .precision(7)
            .scale(2)
            .named(""f""))
        .named(""root"")

    assert(expected.union(expected) === expected)
  }
{code}
produces the following assertion error
{noformat}
message root {
  required int32 f;
}
 did not equal message root {
  required int32 f (DECIMAL(9,0));
}
{noformat}
This is because {{PrimitiveType.union}} doesn't handle original type properly. An open question is that, can two primitive types with the same primitive type name but different original types be unioned?",smartshark_2_2,409,parquet-mr,"""[0.08554136753082275, 0.9144586324691772]"""
2611,230870,DictionaryValuesWriter dictionaries are corrupted by user changes.,"DictionaryValuesWriter passes incoming Binary objects directly to Object2IntMap to accumulate dictionary values. If the arrays backing the Binary objects passed in are reused by the caller, then the values are corrupted but still written without an error.

Because Hadoop reuses objects passed to mappers and reducers, this can happen easily. For example, Avro reuses the byte arrays backing Utf8 objects, which parquet-avro passes wrapped in a Binary object to writeBytes.

The fix is to make defensive copies of the values passed to the Dictionary writer code. I think this only affects the Binary dictionary classes because Strings, floats, longs, etc. are immutable.",smartshark_2_2,287,parquet-mr,"""[0.06882164627313614, 0.9311783909797668]"""
2612,230871,InternalParquetRecordReader.close() should not throw an exception if initialization has failed,"InternalParquetRecordReader.close() currently throws a NullPointerException if initialize() didn't succeed and hence the ""reader"" remains null. 

This NPE happens with client usage idioms like this:

{code}
AvroParquetReader reader = ...
try {
       // read some parquet data
} finally {
  if (reader != null) { reader.close(); }
}
{code}

I believe close() should read as follows:

{code}
public void close() throws IOException {
    if (reader != null) {
      reader.close();
    }
  }
{code}

Thoughts?",smartshark_2_2,176,parquet-mr,"""[0.16906987130641937, 0.8309301733970642]"""
2613,230872,Should use ConcurrentHashMap instead of HashMap in ParquetMetadataConverter,"When using parquet in Spark Environment, sometimes got hang  with following thread dump:
""Executor task launch worker-0"" daemon prio=10 tid=0x000000004073d000 nid=0xd6c5 runnable [0x00007ff3fda40000]
java.lang.Thread.State: RUNNABLE
at java.util.HashMap.get(HashMap.java:303)
at parquet.format.converter.ParquetMetadataConverter.fromFormatEncodings(ParquetMetadataConverter.java:218)
at parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:543)
at parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:520)
at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:426)
at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:381)
at parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:161)
at parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.(NewHadoopRDD.scala:135)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:107)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:56)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)

From the source code of ParquetMetadataConverter:

private Map> encodingLists = new HashMap>();

It use HashMap instead of ConcurrentHashMap. Because HashMap is not thread safe and can cause hang when run in multithread environment. So it need to change to ConcurrentHashMap",smartshark_2_2,291,parquet-mr,"""[0.5330907106399536, 0.4669093191623688]"""
2614,230873,InternalParquetRecordReader will not read multiple blocks when filtering,"The InternalParquetRecordReader keeps track of the count of records it has processed and uses that count to know when it is finished and when to load a new row group of data. But when it is wrapping a FilteredRecordReader, this count is not updated for records that are filtered, so when the reader exhausts the block it is reading, it will continue calling read() on the filtered reader and will pass null values to the caller.

The quick fix is to detect null values returned by the record reader and update the count to read the next row group. But the longer-term solution is to correctly account for the filtered records.

The pull request for the quick fix is [#9|https://github.com/apache/incubator-parquet-mr/pull/9].",smartshark_2_2,195,parquet-mr,"""[0.09484041482210159, 0.905159592628479]"""
2615,230874,Add support for repeated columns in the filter2 API,"They currently are not supported. They would need their own set of operators, like contains() and size() etc.",smartshark_2_2,687,parquet-mr,"""[0.9985091090202332, 0.0014909273013472557]"""
2616,230875,Use the same environment setup script for Travis CI as local sandbox development,"Currently the environment setups are slightly different, and so a passing Travis CI build might have a problem with the sandbox build and vice versa.",smartshark_2_2,440,parquet-mr,"""[0.9983408451080322, 0.00165910879150033]"""
2617,230876,[C++] Parquet modular encryption,"CPP version of a mechanism for modular encryption and decryption of Parquet files. Allows to keep the data fully encrypted in the storage, while enabling a client to extract a required subset (footer, column(s), pages) and to authenticate / decrypt the extracted data.",smartshark_2_2,1376,parquet-mr,"""[0.9983443021774292, 0.001655653933994472]"""
2618,230877,[C++] Add optional $PARQUET_BUILD_TOOLCHAIN environment variable option for configuring build environment,"Some users will install a centralized build toolchain containing all of the build dependencies for Parquet. We should provide an environment variable option to help with this, rather than having to set each thirdparty $FOO_HOME variable separately.

see also ARROW-849",smartshark_2_2,862,parquet-mr,"""[0.9976897239685059, 0.0023102399427443743]"""
2619,230878,Upgrade Avro to 1.8.1,"Unit test failure occurs if using Avro 1.8.1:
{code}
[ERROR] Failed to execute goal org.apache.avro:avro-maven-plugin:1.8.1:idl-protocol (schemas) ... 
org.apache.avro.compiler.idl.ParseException: Encountered "" ""date"" ""date """" at line 23, column 14.
[ERROR] Was expecting one of:
[ERROR] <IDENTIFIER> ...
[ERROR] ""@"" ...
[ERROR] ""`"" ...
[ERROR] -> [Help 1]
{code}

See AVRO-1924 about the background.
It is not clear whether it is solvable on Avro side or it is more a limitation from Avro 1.8.1. Therefore, I would suggest fixing the issue in the unit test side. ",smartshark_2_2,696,parquet-mr,"""[0.9936853051185608, 0.006314657628536224]"""
2620,230879,Type builders don't honor new logical types,"Type builder still use OriginalType constructors even for new logical types. Therefore one can't build a timestamp type with UTC adjustment = false with the type builder, because they default utc adjustment to false (via OriginalType conversion)",smartshark_2_2,1404,parquet-mr,"""[0.4744211435317993, 0.5255789160728455]"""
2621,230880,Release 1.10 contains breaking changes for Hive,"Hive uses the initFromPage(int valueCount, ByteBuffer page, int offset) method that [got removed|https://github.com/apache/parquet-mr/commit/8bbc6cb95fd9b4b9e86c924ca1e40fd555ecac1d#diff-175c27f5147df0043ac57c7685629934L574] in PARQUET-787. As a result, Hive does not compile with Paruqet 1.10.",smartshark_2_2,1329,parquet-mr,"""[0.7280309200286865, 0.2719690501689911]"""
2622,230881,Add thoroughly parquet test encodings,"PARQUET-246 highlighted the need to have more thorough validation of the Parquet encodings. With the 2.0 encodings as high priority, we need to validate each encoding with its spec (and ensure there is a spec). 

We also need to add a suite of validations that are used for all encodings that test:
- Writing multiple pages (v1 and v2 pages) and multiple row groups
- Validating that each page can be read individually or with last-to-first page order and has the correct set of values (would have caught PARQUET-246).",smartshark_2_2,1241,parquet-mr,"""[0.9986171722412109, 0.0013827906223013997]"""
2623,230882,C++: Decoders should directly emit arrays with spacing for null entries,"Currently the decoders all emit the data arrays in the form of the PLAIN encoding, i.e. only the non-null entries with no memory wastage for null entries. But for random access we need to gap the output so that entry {{i}} is located at memory address {{base + i * type_size}}.",smartshark_2_2,741,parquet-mr,"""[0.9913937449455261, 0.00860620103776455]"""
2624,230883,[C++] Support writing sliced Arrow arrays,Follow up to ARROW-33. We may want to explicitly disallow any non-zero slice offsets until then,smartshark_2_2,779,parquet-mr,"""[0.9981632828712463, 0.0018367341253906488]"""
2625,230884,Simplify CapacityByteArrayOutputStream,"There's a lot of complicated logic around trying to re-use byte buffers as well as allocating slabs before they are needed as well as shrinking / consolidating. 

After discussing with [~julienledem] we should simplify and not bother reusing the byte buffers, and not allocate buffers until they are needed. The hope is this will reduce the memory overhead and that the number of byte buffers should be small so they should not cause trouble for GC.",smartshark_2_2,908,parquet-mr,"""[0.9983211159706116, 0.0016789129003882408]"""
2626,230885,Allow Parquet to build with Java 8,We should ensure that Parquet can be built and all tests pass when using Java 8. Note that this will not change the default target bytecode version (still at Java 6).,smartshark_2_2,349,parquet-mr,"""[0.9982524514198303, 0.0017475562635809183]"""
2627,230886,C++: Add conda packaging recipe,"Now that Thrift packages are available from the conda-forge community channel, we can also post dev build artifacts of parquet as part of Travis CI (or on demand): https://anaconda.org/conda-forge/thrift-cpp. ",smartshark_2_2,536,parquet-mr,"""[0.9981028437614441, 0.0018972443649545312]"""
2628,230887,add data_encodings to ColumnMetaData to enable dictionary based predicate push down,"To implement predicate push down based on dictionary we need to know if fall back happened.
If all data pages are dictionary encoded we can use the dictionary for predicate-push down.
If not we can not.",smartshark_2_2,186,parquet-mr,"""[0.9985044002532959, 0.0014956825179979205]"""
2629,230888,Add UUID logical type,"I think we should add a UUID logical type that is stored in a 16-byte fixed. The common string representation is 36 bytes instead of the 16 required. UUIDs are commonly used as unique identifiers, so it makes sense to have a good support. A binary representation will reduce memory when writing or building bloom filters and will reduce cycles needed to compare values.",smartshark_2_2,1058,parquet-mr,"""[0.9982571005821228, 0.0017428443534299731]"""
2630,230889,clean up abandoned PRs,"parquet-mr: #333
parquet-format: #38, #33",smartshark_2_2,621,parquet-mr,"""[0.9977667331695557, 0.0022332228254526854]"""
2631,230890,Parquet-thrift doesn't compile with Thrift 0.10.0,"Parquet-thrift doesn't compile with Thrift 0.10.0 due to THRIFT-2263. The default generator parameter used for {{--gen}} argument by Thrift Maven plugin is no longer supported, this can be fixed with an additional {{<generator>java</generator>}} parameter to Thrift Maven plugin.",smartshark_2_2,1203,parquet-mr,"""[0.9683042168617249, 0.03169582411646843]"""
2632,230891,Pig Predicate Pushdown using Filter2 API,Use the pushdown API from Pig to build filter predicates for parquet.  See PIG-3760.,smartshark_2_2,456,parquet-mr,"""[0.9982151985168457, 0.0017848315183073282]"""
2633,230892,[C++] Add option to link with shared boost libraries when building Arrow in the thirdparty toolchain,See discussion in https://github.com/apache/parquet-cpp/pull/231,smartshark_2_2,885,parquet-mr,"""[0.9983637928962708, 0.0016361752059310675]"""
2634,230893,Organize src code file structure to have a very clear folder with public headers.,We should organize the source code structure to have a folder where all the public headers are and nothing else. This makes it easy to understand what is the public API and which APIs needed to be looked at wrt to compatibility.,smartshark_2_2,475,parquet-mr,"""[0.9969147443771362, 0.0030851864721626043]"""
2635,230894,Consider adding checks that validate eq(null) / notEq(null) is not used on a required column in the filter2 API,Is this too strict?,smartshark_2_2,24,parquet-mr,"""[0.9978262782096863, 0.0021736377384513617]"""
2636,230895,Close Parquet github account to avoid confusion,"The old github repo has significant history (github issues and PRs) that we'd like to maintain.
But at the same time it is confusing and people mistake it for the main repo.
We need a solution for this.
",smartshark_2_2,699,parquet-mr,"""[0.9984095692634583, 0.0015905118780210614]"""
2637,230896,website should list mailing list adresses,Other apache projects are listing their mailing lists on their web-site. The information is not easy to find for parquet.,smartshark_2_2,351,parquet-mr,"""[0.9976711869239807, 0.0023288445081561804]"""
2638,230897,C++: Build dependencies using CMake External project,"Instead of the shell scripts, use CMake's ExternalProject support.",smartshark_2_2,722,parquet-mr,"""[0.9983437061309814, 0.0016562766395509243]"""
2639,230898,Improve write performance with wide schema sparse data,"In write path, when there are tons of sparse data, most of time is spent on writing nulls.

Currently writing nulls has the same code path as writing values, which is reclusive traverse all the leaves when a group is null.

Due to the fact that when a group is null all the leaves beneath it should be written with null value with the same repetition level and definition level, we can eliminate the recursion call to get the leaves",smartshark_2_2,1274,parquet-mr,"""[0.9982744455337524, 0.0017255643615499139]"""
2640,230899,Complete PlainEncoder implementation for all primitive types and test end to end,"As part of PARQUET-485, I added a partial {{Encoding::PLAIN}} encoder implementation. This needs to be finished, with a test suite that validates data round-trips across all primitive types. ",smartshark_2_2,439,parquet-mr,"""[0.9984795451164246, 0.001520432299003005]"""
2641,230900,Improve API to support Decimal type,Extend the `ColumnDescriptor` API to return `precision` and `scale` values from DecimalMetadata. Implement necessary checks if the `LogicalType` is Decimal.,smartshark_2_2,459,parquet-mr,"""[0.9983929991722107, 0.0016069617122411728]"""
2642,230901,Add visibility macros to be used for public and internal APIs of libparquet,e.g. {{PARQUET_EXPORT}} and {{PARQUET_NO_EXPORT}},smartshark_2_2,573,parquet-mr,"""[0.9982081651687622, 0.0017918668454512954]"""
2643,230902,[C++] Account for API changes in ARROW-33,Patch forthcoming,smartshark_2_2,774,parquet-mr,"""[0.9979199767112732, 0.0020800381898880005]"""
2644,230903,Convert flat SchemaElement vector to implied nested schema data structure,To assist with conversion to in-memory nested data structures. Related: PARQUET-441,smartshark_2_2,407,parquet-mr,"""[0.9981840252876282, 0.0018159254686906934]"""
2645,230904,Create Integration tests to validate statistics,In response to [PARQUET-251|https://issues.apache.org/jira/browse/PARQUET-251] create unit tests that validate the statistics fields for each column type.,smartshark_2_2,1283,parquet-mr,"""[0.9984525442123413, 0.001547483028843999]"""
2646,230905,Update Javadoc for Java 1.8,"After moving the build to Java 1.8, the release procedure no longer works because Javadoc generation fails.",smartshark_2_2,1215,parquet-mr,"""[0.9981961846351624, 0.0018037946429103613]"""
2647,230906,ParquetInputFormat.getFooters() should return in the same order as what listStatus() returns,"Because of how the footer cache is implemented, getFooters() returns the footers in a different order than what listStatus() returns.

When I provided url ""hdfs://.../part-00001.parquet,hdfs://.../part-00002.parquet,hdfs://.../part-00003.parquet"", ParquetInputFormat.getSplits(), which internally calls getFooters(), returned the splits in a wrong order.
",smartshark_2_2,1281,parquet-mr,"""[0.6078745722770691, 0.3921253979206085]"""
2648,230907,Add Uwe L. Korn to KEYS,"To sign parquet-cpp releases that can be verified by others, I would like to have my gpg key appended to the KEYS file. As there is only a single file (https://dist.apache.org/repos/dist/dev/parquet/KEYS) for all parquet RCs/releases at the end, I'm going to do a PR to the parquet-mr repo first and will then later make a PR to have that KEYS file replicated in parquet-cpp.",smartshark_2_2,680,parquet-mr,"""[0.9983086585998535, 0.0016913821455091238]"""
2649,230908,[C++] Use the same BOOST_ROOT and Boost_NAMESPACE for Thrift ,"When building Thrift using the ExternalProject facility, we do not pass on the variables for a custom Boost variant. Thus if the user uses a differently flavoured/located Boost, Thrift does not pick it up. As a cause of this, we explicitly build Thrift during the Arrow OS X Wheel build.",smartshark_2_2,1296,parquet-mr,"""[0.9918366074562073, 0.008163364604115486]"""
2650,230909,Retire _metadata generation ,"We can disable  _metadata file generation since it is not being used anymore to reduce the memory usage during commit phase.

We are keeping the _common_metadata since it is less memory expensive.",smartshark_2_2,1400,parquet-mr,"""[0.9983285069465637, 0.0016714684898033738]"""
2651,230910,Tasks for parquet-mr 1.6.0 release,"In order to get release 1.6 of parquet-mr out, we have to do some updates.

For Apache policy:
1. -Update the header in all Parquet source files (see PARQUET-72 for examples)-
2. -Add DISCLAIMER and KEYS files (copy from parquet-format)-
3. -Add the Apache Rat maven plugin to check source file license headers and make sure it passes-
4. -For the overall project, go through any external source that is bundled and update the root LICENSE and NOTICE files-
5. -For each module that produces a binary artifact, go through the content pulled in by the maven-shade-plugin or the maven-assembly-plugin and update a LICENSE and NOTICE file for that content (see PARQUET-109 for an example).-

We also need to update the build (see [PR #11|https://github.com/apache/incubator-parquet-format/pull/11]):
1. -Inherit from the ASF pom- to pull in Apache release settings (will be done for org.apache groupId)
2. -Update metadata, like SCM, mailing lists, and JIRA links-
3. -Go through modules and remove any LICENSE and NOTICE files that are included by shading or assembly (relevant content should be included in parquet-mr LICENSE and NOTICE)-",smartshark_2_2,93,parquet-mr,"""[0.9982467889785767, 0.0017532160272821784]"""
2652,230911,Release apache-parquet-1.7.0,"Need to release 1.7.0. The rename is done, we should just need to switch over to Apache release infrastructure. See:

* https://github.com/apache/parquet-format/commit/c3afbd22
* https://github.com/apache/parquet-format/commit/26b2c68f",smartshark_2_2,246,parquet-mr,"""[0.9985665678977966, 0.0014334192965179682]"""
2653,230912,Update committers lists to point to apache website,https://github.com/apache/parquet-mr/pull/355#issuecomment-235637473,smartshark_2_2,588,parquet-mr,"""[0.9984660148620605, 0.0015340604586526752]"""
2654,230913,"[C++] parquet::arrow should use thread pool, not ParallelFor","Arrow now has a global thread pool, parquet::arrow should use that instead of ParallelFor.",smartshark_2_2,1331,parquet-mr,"""[0.995790421962738, 0.0042096273973584175]"""
2655,230914,"Add ""Floating Timestamp"" logical type","Unlike current Parquet Timestamp stored in UTC, a ""floating timestamp"" has no timezone, it is up to the reader to interpret the timestamps based on their timezone.
This is the behavior of a Timestamp in the sql standard

",smartshark_2_2,1344,parquet-mr,"""[0.9983062744140625, 0.0016936571337282658]"""
2656,230915,Add new compression codecs to the Parquet spec,"After [recent tests|https://lists.apache.org/thread.html/2fc572ac5fd4ac414c39047b1e6e81c36c38fc0f92e85b9aa4e5493a@%3Cdev.parquet.apache.org%3E], I think we should add Zstd to the spec.

I'm also proposing we add LZ4 because it is widely available and outperforms snappy. As a successor for fast compression but not necessarily good compression ratios, I think it makes sense to have it.",smartshark_2_2,1074,parquet-mr,"""[0.9983927607536316, 0.0016071819700300694]"""
2657,230916,bzip2 compression ,"Hi,

I have a requirement to implement Parquet with bzip2 compression because it's splitable. Right now, we can't provide bzip2 in PIG. 

SET parquet.compression none/gzip/SNAPPY; 

Is there any way to compress to bzip2 on top parquet ?",smartshark_2_2,1042,parquet-mr,"""[0.9984315037727356, 0.0015684703830629587]"""
2658,230917,Limit the number of rows per block and per split,"We use Parquet to store raw metrics data and then query this data with Hadoop-Pig. 

The issue is that sometimes we end up with small Parquet files (~80mo) that contain more than 300 000 000 rows, usually because of a constant metric which results in a very good compression. Too good. As a result we have a very few number of maps that process up to 10x more rows than the other maps and we lose the benefits of the parallelization. 

The fix for that has two components I believe:
1. Be able to limit the number of rows per Parquet block (in addition to the size limit).
2. Be able to limit the number of rows per split.",smartshark_2_2,323,parquet-mr,"""[0.9971734285354614, 0.002826529089361429]"""
2659,230918,[C++] Bring parquet::arrow up to date with API changes in arrow::io ,See patches in ARROW-280 and ARROW-267,smartshark_2_2,641,parquet-mr,"""[0.9981641173362732, 0.0018359037349000573]"""
2660,230919,"C++: Update Arrow Hash, update Version in metadata.",Last release preparations,smartshark_2_2,785,parquet-mr,"""[0.9975911378860474, 0.0024088763166218996]"""
2661,230920,"Make an alternate, stricter thrift column projection API",See description here: https://github.com/apache/incubator-parquet-mr/pull/150,smartshark_2_2,224,parquet-mr,"""[0.9984875917434692, 0.0015124068595468998]"""
2662,230921,Parquet-Thrift does not handle dropping values in a map,"Thrift does not have a notion of null, and it does not treat map values as optional. We need to apply the same trick as we do for unions to map values -- keep at least 1 primitive column even when the values of the map are not requested.",smartshark_2_2,243,parquet-mr,"""[0.36813849210739136, 0.6318615078926086]"""
2663,230922,[C++] Add column selection to parquet::arrow::FileReader,"With multithreaded reads coming in PARQUET-835, it would be better to push down the subsetting into the FileReader rather than leaving this work at the application level. One artifact of this is the Python interface in Arrow (which handles subsetting in Python/Cython)",smartshark_2_2,749,parquet-mr,"""[0.9983301758766174, 0.001669801538810134]"""
2664,230923,implement the new page format for Parquet 2.0,"as defined in 
https://github.com/Parquet/parquet-format/pull/64
https://github.com/Parquet/parquet-format/issues/44

PR: https://github.com/apache/incubator-parquet-mr/pull/75
",smartshark_2_2,140,parquet-mr,"""[0.9984717965126038, 0.001528205000795424]"""
2665,230924,[C++] Read and write Arrow decimal values,"We now have 16-byte decimal values in Arrow which have been validated against the Java implementation. We need to be able to read and write these to Parquet format. 

To make these values readable by Impala or some other Parquet readers may require some work. It expects the storage size to match the decimal precision exactly. So in parquet-cpp we will need to write the correct non-zero bytes into a FIXED_LEN_BYTE_ARRAY of the appropriate size.

We should validate this against Java Parquet implementations",smartshark_2_2,1082,parquet-mr,"""[0.9984288811683655, 0.0015710388543084264]"""
2666,230925,"Extend RowGroupStatistics to include ""min"" ""max"" statistics",Only `null_count` and `distinct_count` are included as of now.,smartshark_2_2,487,parquet-mr,"""[0.998428463935852, 0.0015715701738372445]"""
2667,230926,Document recommendations for block size and page size given an expected number of writers,"I sent a mail on dev list but I seem to have a problem with email on dev list  so opening a bug here . 

I am on a multithreaded system where there are M threads , each thread creating   an   independent parquet writer  and writing on the hdfs  in its own independent files  . I have a  finite amount of RAM  say R  .  

Now when I created  parquet writer using default block and page size i get heap error (no memory )  on my set up  . so I reduced my block size and page size to very low and  my system stopped giving me these out of memory errors and started writing the file correctly . I am able to read these files correctly as well  . 

I should not have to make the memory low and parquet should automatically make sure i do not get these errors . 

But in case i have to keep track of the memory my question is as follows. 

Now keeping these  values very less is not a recommended practice as i would loose on performance . I am particularly concerned about  write performance .  What math formula  do you recommend that I  should use to find correct blockSize , pageSize to be passed to the parquet constructor to have   the right  WRITE  performance  . ie how can i decide what should be the right blockSize , pageSize  for a parquet writer given that i have M threads and total RAM memory available is R   . I don't understand dictionaryPageSize need and in case i    need to bother about that as well kindly let me know but i have kept enableDictionary flag as false . 

I am using the bellow constructor .
public More ...ParquetWriter(
162      Path file,
163      WriteSupport<T> writeSupport,
164      CompressionCodecName compressionCodecName,
165      int blockSize,
166      int pageSize,
167      int dictionaryPageSize,
168      boolean enableDictionary,
169      boolean validating,
170      WriterVersion writerVersion,
171      Configuration conf) throws IOException {
",smartshark_2_2,62,parquet-mr,"""[0.9867988228797913, 0.013201199471950531]"""
2668,230927,Deprecate ConversionPatterns,"Methods in {{ConversionPatterns}} doesn't conform to standard LIST and MAP  schema, and should be deprecated. We can either suggest users to use {{Types}} builder methods or create new wrapper methods for LIST and MAP.",smartshark_2_2,370,parquet-mr,"""[0.9983604550361633, 0.0016395155107602477]"""
2669,230928,Review usages of size_t and unsigned integers generally per Google style guide,"The Google style guide recommends generally avoiding unsigned integers for the bugs they can silently introduce. 

https://google.github.io/styleguide/cppguide.html#Integer_Types
",smartshark_2_2,460,parquet-mr,"""[0.9982557892799377, 0.0017441738164052367]"""
2670,230929,Provide vectorized ColumnReader interface,"Related to PARQUET-433. The library user should be able to retrieve a batch of column values, repetition levels, or definition levels with a particular size into a preallocated C array. ",smartshark_2_2,388,parquet-mr,"""[0.9982353448867798, 0.0017646764172241092]"""
2671,230930,[C++] Add installation instructions for use in other projects,"Apache Arrow headers are required to be able to use libparquet. This is not clear from the README. If users install using the thirdparty Arrow EP, they may have some difficulty using in another project",smartshark_2_2,943,parquet-mr,"""[0.9982182383537292, 0.001781766302883625]"""
2672,230931,Set the HDFS padding default to 8MB,"PARQUET-306 added the ability to pad row groups so that they align with HDFS blocks to avoid remote reads. The ParquetFileWriter will now either pad the remaining space in the block or target a row group for the remaining size.

The padding maximum controls the threshold of the amount of padding that will be used. If the space left is under this threshold, it is padded. If it is greater than this threshold, then the next row group is fit into the remaining space. The current padding maximum is 0.

I think we should change the padding maximum to 8MB. My reasoning is this: we want this number to be small enough that it won't prevent the library from writing reasonable row groups, but larger than the minimum size row group we would want to write. 8MB is 1/16th of the row group default, so I think it is reasonable: we don't want a row group to be smaller than 8 MB.

We also want this to be large enough that a few row groups in a  block don't cause a tiny row group to be written in the excess space. 8MB accounts for 4 row groups that are 2MB under-size. In addition, it is reasonable to not allow row groups under 8MB.",smartshark_2_2,1196,parquet-mr,"""[0.9981071949005127, 0.0018927751807495952]"""
2673,230932,parquet-tools command to get rowcount & size,"Parquet files contain metadata about rowcount & file size. We should have new commands to get rows count & size.
These command can be added in parquet-tools:
1. rowcount : This should add number of rows in all footers to give total rows in data. 
2. size : This should give compresses size in bytes and human readable format too.
These command helps us to avoid parsing job logs or loading data once again to find number of rows in data. This comes very handy in complex processes, stats generation, QA etc..",smartshark_2_2,891,parquet-mr,"""[0.9979472756385803, 0.0020527667365968227]"""
2674,230933,Column indexes,"Write the column indexes described in PARQUET-922.
 This is the first phase of implementing the whole feature. The implementation is done in the following steps:
 * Utility to read/write indexes in parquet-format
 * Writing indexes in the parquet file
 * Extend parquet-tools and parquet-cli to show the indexes
 * Limit index size based on parquet properties
 * Trim min/max values where possible based on parquet properties
 * Filtering based on column indexes

The work is done on the feature branchÂ {{column-indexes}}. This JIRA will be resolved after the branch has been merged toÂ {{master}}.",smartshark_2_2,1141,parquet-mr,"""[0.9982094764709473, 0.0017905760323628783]"""
2675,230934,Store `dictionary entries` of parquet columns that will be used for joins,"It would be great if Parquet would store `dictionary entries` for columns marked to be used for joins. 

When a column is used for a join (it could be a [surrogate key|https://en.wikipedia.org/wiki/Surrogate_key] or a [natural key|https://en.wikipedia.org/wiki/Natural_key]) - the value of a cloumn used for join itself is actually not so important. 

So we could join directly on `dictionary entries` instead of values 
and save CPU cycles. (no need to decompress etc)

Inspired by [Oracle In-memory columnar storage improvements in 12.2|https://blogs.oracle.com/In-Memory/entry/what_s_new_in_12]",smartshark_2_2,873,parquet-mr,"""[0.9984205961227417, 0.0015794006176292896]"""
2676,230935,[CPP]: Reduce buffer allocations (mallocs) on critical path,"The current implementation allocates and frees memory many times on the critical path. The scope of this JIRA is to reuse buffers where possible.
On production systems, too many mallocs/frees can impact performance. ",smartshark_2_2,816,parquet-mr,"""[0.9982355833053589, 0.00176442030351609]"""
2677,230936,"[C++] Update developer documentation for C++11, clang tools, style guide links, etc.",The dev documentation has fallen out of date. ,smartshark_2_2,569,parquet-mr,"""[0.997856080532074, 0.002143855206668377]"""
2678,230937,"[C++] ""parquet_reader"" should be ""parquet-reader""","Out of ""parquet-dump-schema"", ""parquet_reader"" and ""parquet-scan"", ""parquet_reader"" gratuitously follows a different naming convention.",smartshark_2_2,1217,parquet-mr,"""[0.998145580291748, 0.0018544684862717986]"""
2679,230938,"parquet depends on hadoop-lzo, which requires Twitter's Maven repo","I'm trying to add Parquet to my project:

{code}
libraryDependencies += ""org.apache.parquet"" % ""parquet"" % ""1.8.1""
libraryDependencies += ""org.apache.parquet"" % ""parquet-protobuf"" % ""1.8.1""
{code}

But the project appears to depend on hadoop-lzo, which requires adding Twitter's Maven repo as a resolver:
{code}
resolvers += ""twitter-repo"" at ""http://maven.twttr.com""
{code}

(now sbt resolves and fetches all dependencies)

{code}
# uname -a
Linux pw-246 3.13.0-85-generic #129-Ubuntu SMP Thu Mar 17 20:50:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.4 LTS
Release:        14.04
Codename:       trusty
# sbt sbtVersion
[info] Loading project definition from xxxxx
[info] Set current project to xxxx (in build file:xxxx)
[info] 0.13.11
{code}

I suggest that Apache host hadoop-lzo along with the main Parquet dependencies.",smartshark_2_2,570,parquet-mr,"""[0.9900122880935669, 0.009987737983465195]"""
2680,230939,PLAIN_DICTIONARY write support,Add support for writing columns with PLAIN_DICTIONARY encoding.,smartshark_2_2,597,parquet-mr,"""[0.9982630610466003, 0.001736941747367382]"""
2681,230940,Canonicalizer error when encrypting multiple elements,"When we are encrypting multiple elements like this:
for (int i = 0; i < elems.length; i++) {
  cipher.doFinal(doc, elems[i])
}

We are reusing the same canonicalizer. Unfortunately, the default canonicalizer behaves differently the first time it is called, so we get different results for identical subtrees. See the firstCall member used in Canonicalizer20010315.handleAttributesSubtree()

The sample test case reproduces this problem. The sample code encrypts the <elem> elements and decrypts them.

The input document:
<?xml version=""1.0"" encoding=""UTF-8""?><env:Envelope xmlns:env=""http://schemas.xlsoap.org/soap/envelope/""><env:Body><elem>11</elem><elem>22</elem></env:Body></env:Envelope>

becomes:
<?xml version=""1.0"" encoding=""UTF-8""?><env:Envelope xmlns:env=""http://schemas.xmlsoap.org/soap/envelope/""><env:Body><elem>11</elem><elem xmlns:env=""http://schemas.xmlsoap.org/soap/envelope/"">22</elem></env:Body></env:Envelope>

The XML Encryption spec says the encrypted subtree must be serialized following the rules of the XML 1.0 spec and the characters converted to bytes in UTF-8. The XML Encryption spec does not mandate a specific canonicalization. In fact, it warns in section 4.3.3 that Inclusive XML and Exclusive XML both have problems. (See SANTUARIO-309). Here the first Element encrypted is <elem>22</elem>, but I would not conclude it is serialized better than the subsequent elements.
",smartshark_2_2,288,santuario-java,"""[0.06549076735973358, 0.9345092177391052]"""
2682,230941,XmlWriterToTree can write out xmlns:null for a null prefix,"
XmlWriterToTree can write out xmlns:null for a null prefix, under certain circumstances.",smartshark_2_2,388,santuario-java,"""[0.08492191880941391, 0.9150781035423279]"""
2683,230942,XMLCipher fails to declare signature namespace on xenc:EncryptionMethod/ds:DigestMethod elements,"XMLCipher encryptKey generates the appropriate children of xenc:EncryptionMethod when RSA OAEP is parameterized.  If an explicit digest method is used however, the resulting ds:DigestMethod child element does not have a namespace declaration for the signature namespace, so the ""ds"" prefix is not resolveable.

My fix is copied from what is currently done for the xenc11:MGF element, a few lines below.",smartshark_2_2,379,santuario-java,"""[0.07257901132106781, 0.927420973777771]"""
2684,230943,XMLDSig XPathFilter2Transform bug involving intersect filter,"If the intersect filter is present, but the result of the XPath expression is an empty node-set, it ignores the filter rather than properly performing an intersection with an empty node-set, the result of which should always be an empty node-set. The problem is that the code does not distinguish between filters that are not included in the Transform and those that produce an empty node-set.

This is a fairly obscure case and should occur infrequently in practice, as it would signify that the Transform was probably created incorrectly if the intersect filter didn't select any nodes at all.

However, this used to work in a previous version of Santuario, thus it is a regression.",smartshark_2_2,269,santuario-java,"""[0.06310516595840454, 0.9368948340415955]"""
2685,230944,Signing XML with SHA1 with DSA throws exception when key is larger than 1024,"Getting the following exception when trying to sign xml document with SHA1 with DSA key that is 2048 bits long:

Exception in thread ""main"" java.lang.RuntimeException: A problem occured when trying to sign the document
	at XMLSignTest$XMLDSIGGenerator.signXMLDocument(XMLSignTest.java:113)
	at XMLSignTest.main(XMLSignTest.java:61)
Caused by: org.apache.xml.security.signature.XMLSignatureException: Invalid ASN.1 format of DSA signature
Original Exception was java.io.IOException: Invalid ASN.1 format of DSA signature
	at org.apache.xml.security.algorithms.implementations.SignatureDSA.engineSign(SignatureDSA.java:160)
	at org.apache.xml.security.algorithms.SignatureAlgorithm.sign(SignatureAlgorithm.java:173)
	at org.apache.xml.security.signature.XMLSignature.sign(XMLSignature.java:614)
	at XMLSignTest$XMLDSIGGenerator.signXMLDocument(XMLSignTest.java:109)
	... 1 more
Caused by: java.io.IOException: Invalid ASN.1 format of DSA signature
	at org.apache.xml.security.algorithms.implementations.SignatureDSA.convertASN1toXMLDSIG(SignatureDSA.java:284)
	at org.apache.xml.security.algorithms.implementations.SignatureDSA.engineSign(SignatureDSA.java:158)
	... 4 more

See attached eclipse demo project for reproducing, keystore and xml file:

https://drive.google.com/file/d/0B8qrjQTbDPd3Sld2bW5ncTlLQms/edit?usp=sharing",smartshark_2_2,376,santuario-java,"""[0.09696934372186661, 0.9030306935310364]"""
2686,230945,Provide a way to change the Signature prefix for the streaming code,"Please provide a possibility for changing the namespace for the element dsig:Signature.
",smartshark_2_2,398,santuario-java,"""[0.9984843134880066, 0.001515737152658403]"""
2687,230946,Implement KeyResolvers for PrivateKeys and SecretKeys,"The built-in KeyResolvers can resolve PubicKeys by searching through the StorageResolvers. They cannot resolve private or secret keys because the StorageResolvers can only iterate certificates. I proposed to extend StorageResolverSpi or invent a new kind of storage class but the project lead prefers to rely on custom KeyResolvers.

This patch implements KeyResolvers for private and secret keys for some common cases. These resolvers take their storage on the constructor. This means they cannot be used as static resolvers (because that requires a default constructor). The user must register the new resolvers explicitly as internal key resolvers.
",smartshark_2_2,286,santuario-java,"""[0.9985365867614746, 0.001463435241021216]"""
2688,230947,In XMLCipher support use of GCMParameterSpec for AES GCM modes,"XMLCipher currently unconditionally uses an IvParameterSpec to initialize the Java Cipher instance for data encryption and decryption. 

In Java 1.7 a new AlgorithmParameterSpec was introduced to support GCM modes, GCMParameterSpec.  This is intended to be the way that GCM mode ciphers are initialized, as it allows supplying both the IV and the authentication tag length.

In particular the security provider supplied with Oracle's Java 1.8 seems to require its use.  When handed an IvParameterSpec per the current XMLCipher behavior, it fails with:

{code}
java.security.InvalidAlgorithmParameterException: Unsupported parameter: javax.crypto.spec.IvParameterSpec@15bbf42f
    at com.sun.crypto.provider.CipherCore.init(CipherCore.java:509)
    at com.sun.crypto.provider.AESCipher.engineInit(AESCipher.java:339)
    at javax.crypto.Cipher.implInit(Cipher.java:801)
    at javax.crypto.Cipher.chooseProvider(Cipher.java:859)
    at javax.crypto.Cipher.init(Cipher.java:1370)
    at javax.crypto.Cipher.init(Cipher.java:1301)
    at org.apache.xml.security.encryption.XMLCipher.encryptData(XMLCipher.java:1129)
    at org.apache.xml.security.encryption.XMLCipher.encryptData(XMLCipher.java:1081)
    at org.apache.xml.security.encryption.XMLCipher.encryptData(XMLCipher.java:1012)
{code} 

The BouncyCastle security provider on the other hand works fine with IvParameterSpec and current XMLCipher.  They did however add support for GCMParameterSpec in their 1.50 release (although due to an semi-related bug, it won't work properly with XMLCipher until their v1.51, which is not yet released.  See: http://www.bouncycastle.org/jira/browse/BJA-473)

XMLCipher should ideally detect AES GCM mode algorithms and attempt to use GCMParameterSpec where possible, falling back to IvParameterSpec where it is not.

Patch to follow shortly.",smartshark_2_2,408,santuario-java,"""[0.9234620332717896, 0.07653799653053284]"""
2689,230948,Add support for GCM algorithms via a third-party Crypto provider,"
This task is to add support for AES GCM algorithms via a third party Crypto provider as detailed in the spec:

http://www.w3.org/TR/xmlenc-core1/#sec-AES-GCM

This change will allow a user to use a provider such as BouncyCastle to encrypt and decrypt messages using these algorithms.",smartshark_2_2,315,santuario-java,"""[0.9982946515083313, 0.0017053027404472232]"""
2690,230949,"XML parsing should allow configuration (e.g. EntityResolver, DOCTYPE allowed)","When transforming referenced data prior to digesting, the content may need to be parsed (in XMLSignatureInput.convertToNodes())

Currently the user cannot customize the XML parser configuration, to disallow document type declarations, or configure an entity resolver.

This may be an issue in some environments if the signed data has a document type declaration that should be resolved to a local copy of the DTD, or if the signed data should not contain a document type declaration, or is not permitted to reference external entities.

An example of how this issue can occur is a Base64 transform, followed by canonicalization:

                    <ds:Reference URI=""#signeddata"">
                        <ds:Transforms>
                            <ds:Transform Algorithm=""http://www.w3.org/2000/09/xmldsig#base64""/>
                            <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
                        </ds:Transforms>
                        <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
                        <ds:DigestValue>tZ/wPQtWp7Bca2Z1QqnpwZLRLZ4=</ds:DigestValue>
                    </ds:Reference>
",smartshark_2_2,259,santuario-java,"""[0.9966347813606262, 0.0033652575220912695]"""
2691,230950,Use requested SecureRandom for OAEP in XMLCipher.encryptKey,"There is currently no mechanism to tell XMLCipher.encryptKey() which SecureRandom to use for the random generation within OAEP, so the Sun RNG is used. 

To make this configurable, one could pass the SecureRandom algorithm to getProviderInstance, and encryptKey could use the requestedJCEProvider to create a SecureRandom instance.  Alternatively, add an optional secureRandom parameter to encryptKey.

Â ",smartshark_2_2,500,santuario-java,"""[0.9985729455947876, 0.0014270900283008814]"""
2692,230951,Xalan is still a required dependency,"It is stated in the release notes that ""Xalan is no longer a required dependency"". However, running signature creation without it produces an exception:
java.lang.NoClassDefFoundError: org/apache/xml/utils/PrefixResolver
	at org.apache.xml.security.transforms.implementations.TransformXPath.enginePerformTransform(TransformXPath.java:105)
	at org.apache.xml.security.transforms.Transform.performTransform(Transform.java:304)
	at org.apache.xml.security.transforms.Transform.performTransform(Transform.java:281)
	at org.apache.xml.security.transforms.Transforms.performTransforms(Transforms.java:272)
	at org.apache.xml.security.transforms.Transforms.performTransforms(Transforms.java:248)
	<...>

Both TransformXPath and TransformXPath2Filter use the same code:
XPathAPI xpathAPIInstance = new XalanXPathAPI();
if (!XalanXPathAPI.isInstalled()) {
  xpathAPIInstance = new JDKXPathAPI();
}

which is supposed to provide a way to get either Xalan or JDK -based XPathAPI implementation.
However, the XalanXPathAPI class in turn imports Xalan classes and hence just can not be loaded without it.

And BTW, I think it would be better to provide a factory to get the XPathAPI implementation from, instead of repeating the above code lines.
",smartshark_2_2,276,santuario-java,"""[0.9441449642181396, 0.05585501715540886]"""
2693,230952,AES-GCM support,Add AES-GCM support when OpenSSL 1.0.1 is used.,smartshark_2_2,295,santuario-java,"""[0.9980452060699463, 0.0019548332784324884]"""
2694,230953,Support streaming XML Signature,"
This task is to add support for streaming XML Signature for the 2.0 release.",smartshark_2_2,337,santuario-java,"""[0.9983362555503845, 0.0016637680819258094]"""
2695,230954,Conditional inclusion of XMKS code in library,XKMS support should be optional and disabled via configure.,smartshark_2_2,481,santuario-java,"""[0.9981478452682495, 0.0018521194579079747]"""
2696,230955,Provide download links on Java and C++ pages,Seems to me it would be useful to have links to the download page in the menus for the Java and C++ pages.,smartshark_2_2,112,santuario-java,"""[0.9969615340232849, 0.0030384520068764687]"""
2697,230956,Handle nextSibling in KeyInfo marshalling,"When marshalling the KeyInfo element, the nextSibling, given in the context, is ignored. This leads to unnecessary dom shuffling in user code, when other object elements exists.",smartshark_2_2,430,santuario-java,"""[0.9966316819190979, 0.00336826522834599]"""
2698,230957,Support streaming XML Signature XPointer references,"
This task is to add support for streaming XML Signature XPointer references, e.g.:

<dsig:Reference URI=""#xpointer(id('to-be-signed'))"">
        <dsig:Transforms>
          <dsig:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#WithComments"">
            <InclusiveNamespaces xmlns=""http://www.w3.org/2001/10/xml-exc-c14n#"" PrefixList=""bar #default"" />
          </dsig:Transform>
        </dsig:Transforms>
        <dsig:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" />
        <dsig:DigestValue>a1cTqBgbqpUt6bMJN4C6zFtnoyo=</dsig:DigestValue>
      </dsig:Reference>",smartshark_2_2,349,santuario-java,"""[0.998428463935852, 0.0015715244226157665]"""
2699,230958,Internal key store should be deleted on any CryptoAcquireContext() error ,"In the WinCAPICryptoProvider constructor, if CryptAcquireContext fails when obtaining the internal key store, it only calls CryptAcquireContext again with the CRYPT_DELETEKEYSET option if the error encountered was NTE_BAD_KEYSET. We have seen this API fail with error NTE_KEYSET_ENTRY_BAD as well, but in that case, the key store is not deleted and the initialization fails. Why not just call CryptAcquireContext with the CRYPT_DELETEKEYSET option if any error is encountered (not just NTE_BAD_KEYSET) since the code block tries to subsequently re-create the key store anyway?",smartshark_2_2,476,santuario-java,"""[0.6067396998405457, 0.39326027035713196]"""
2700,230959,Make it easier to see the signing/decrypting key for the streaming case,"
This task is to make it easier to see the signing/decrypting key for the streaming case. This might involve a review of the ""SecurityEvent"" architecture.",smartshark_2_2,334,santuario-java,"""[0.9978973865509033, 0.0021025524474680424]"""
2701,230960,Incorrect result for min/max over matrices with NaNs,"The following script should return {{NaN NaN NaN}} but currently returns {{6.0 10.0 NaN}}.

{code}
X = seq(1,10);
X[5,1] = NaN;
print(min(X)+"" ""+max(X)+"" ""+sum(X));
{code}",smartshark_2_2,1203,systemml,"""[0.06543007493019104, 0.9345698952674866]"""
2702,230961,Parfor optimizer fails checking for block-partitioned inputs,"{code}
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.sysml.parser.ParForStatementBlock.determineAccessPattern(ParForStatementBlock.java:588)
        at org.apache.sysml.parser.ParForStatementBlock.rDeterminePartitioningCandidates(ParForStatementBlock.java:539)
        at org.apache.sysml.parser.ParForStatementBlock.rDeterminePartitioningCandidates(ParForStatementBlock.java:526)
        at org.apache.sysml.parser.ParForStatementBlock.determineDataPartitionFormat(ParForStatementBlock.java:398)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizerRuleBased.rewriteSetDataPartitioner(OptimizerRuleBased.java:423)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizerRuleBased.optimize(OptimizerRuleBased.java:236)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizationWrapper.optimize(OptimizationWrapper.java:246)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizationWrapper.optimize(OptimizationWrapper.java:118)
        at org.apache.sysml.runtime.controlprogram.ParForProgramBlock.execute(ParForProgramBlock.java:601)
        at org.apache.sysml.runtime.controlprogram.Program.execute(Program.java:118)
        ... 13 more
Caused by: java.lang.NullPointerException
        at org.apache.sysml.parser.ParForStatementBlock$LinearFunction.equalSlope(ParForStatementBlock.java:1959)
        at org.apache.sysml.parser.ParForStatementBlock.isAlignedBlocking(ParForStatementBlock.java:595)
        at org.apache.sysml.parser.ParForStatementBlock.determineAccessPattern(ParForStatementBlock.java:580)
{code}",smartshark_2_2,987,systemml,"""[0.05914083868265152, 0.9408591389656067]"""
2703,230962,Compression decision lost after recompilation or codegen,"Even with forced compression (compressed.linalg=true), compression is currently not applied if the respective HOP DAG is recompiled or subject to code generation. The root cause is an incomplete deep copy of the HOP DAG which loses the compression flag.",smartshark_2_2,597,systemml,"""[0.06899215281009674, 0.9310078620910645]"""
2704,230963,Wrong transformapply output with col name specification and subset of cols,"Given a frame input with 3 rows and column names {{name1, name2}}, the following script produces incorrect results:
{code}
X = read(""X.csv"", data_type=""frame"", format=""csv"", header=TRUE);
spec = ""{ids: false, recode: [ name1, name2 ]}"";
[Y,M] = transformencode(target=X, spec=spec);
spec2 = ""{ids: false, recode: [ name2 ]}"";
Z = transformapply(target=X[,2], spec=spec2, meta=M)
print(toString(Z));
{code}

The output is supposed to be 3x1 and properly recoded but currently returns
{code}
NaN
NaN
NaN
{code}",smartshark_2_2,584,systemml,"""[0.07070299237966537, 0.9292970299720764]"""
2705,230964,Lost update on high frequency statistics maintenance,"The recently introduced lock-free statistics maintenance (SYSTEMML-2069) has an issue of lost updates under high-frequency updates of hot opcodes. For example, the following script

{code}

parfor(j in 1:24)
Â Â  for(i in 1:10000000)
Â Â Â Â  q = nrow(Y);

{code}

Â 

gives the following statistics (counts of 240,000,000 are expected here)

{code}

Â #Â  InstructionÂ  Time(s)Â Â Â Â Â  Count
Â 1Â  nrowÂ Â Â Â Â Â Â Â Â  95.118Â  239999993
Â 2Â  mvvarÂ Â Â Â Â Â Â Â  64.423Â  239999957

{code}

Â ",smartshark_2_2,1019,systemml,"""[0.08316785097122192, 0.9168321490287781]"""
2706,230965,Unexpected mapreduce task,"When trying to use scalar casting to get element from a list, unexpected mapreduce tasks are launched instead of CP mode. The scenario is to replace *C = 1* with *C = as.scalar(hyperparams[""C""])* inside the {{_gradient function_}} found in {{_src/test/scripts/functions/paramserv/mnist_lenet_paramserv.dml_}}. And then the problem could be reproduced by launching the method {{_testParamservBSPBatchDisjointContiguous_}} inside class _{{org.apache.sysml.test.integration.functions.paramserv.ParamservLocalNNTest}}_

Here is the stack:
{code:java}
18/07/31 22:10:27 INFO mapred.MapTask: numReduceTasks: 1
18/07/31 22:10:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
18/07/31 22:10:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
18/07/31 22:10:27 INFO mapred.MapTask: soft limit at 83886080
18/07/31 22:10:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
18/07/31 22:10:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
18/07/31 22:10:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
18/07/31 22:10:27 INFO mapreduce.Job: Running job: job_local792652629_0008
{code}

[~mboehm7], if possible, could you take a look on this? And I've double checked the creation of execution context in {{ParamservBuiltinCPInstruction}}. But it is instance of ExecutionContext not SparkExecutionContext.
",smartshark_2_2,1343,systemml,"""[0.14073291420936584, 0.8592671155929565]"""
2707,230966,Codegen failing in JMLC context due to unknowns,"In JMLC settings, we disable by default dynamic recompilation. For complex scripts, the missing corrections of dynamic recompilation, currently lead to codegen compiler/runtime failures due to unknowns which affect the generated code, especially for indexing operations.",smartshark_2_2,787,systemml,"""[0.07556040585041046, 0.9244396090507507]"""
2708,230967,Single-threaded csv frame reader incorrect output w/ multiple splits,"The single-threaded csv frame reader produces incorrect outputs if the used record reader returns multiple splits because each split content is written at starting position 0, which ultimately previously read content.",smartshark_2_2,558,systemml,"""[0.06342311203479767, 0.9365768432617188]"""
2709,230968,Embedded Statements in Matrix Creation,"Inlining statements into the 'matrix' statement fails as seen below:


{code:scala}
// SystemML kernel
val testKernel =
""""""
a = matrix(1 - 4.6 / 2.0, rows=1, cols=1)
write(a, """")
""""""

// Run kernel
import com.ibm.bi.dml.api.MLContext
val ml = new MLContext(sc)
ml.registerOutput(""a"")
val outputs = ml.executeScript(testKernel)
val a = outputs.getDF(sqlContext, ""a"").first()(1).asInstanceOf[Double]
{code}


{noformat}
-- line 2, column 0 -- for matrix statement, parameter data cannot have value type String or Boolean. 
	at com.ibm.bi.dml.parser.Expression.raiseValidateError(Expression.java:413)
	at com.ibm.bi.dml.parser.Expression.raiseValidateError(Expression.java:386)
	at com.ibm.bi.dml.parser.DataExpression.validateExpression(DataExpression.java:1108)
	at com.ibm.bi.dml.parser.StatementBlock.validate(StatementBlock.java:594)
	at com.ibm.bi.dml.parser.DMLTranslator.validateParseTree(DMLTranslator.java:146)
	at com.ibm.bi.dml.api.MLContext.executeUsingSimplifiedCompilationChain(MLContext.java:1202)
	at com.ibm.bi.dml.api.MLContext.compileAndExecuteScript(MLContext.java:1113)
	at com.ibm.bi.dml.api.MLContext.executeScript(MLContext.java:1044)
        ...
{noformat}


The background for this use case relates to the issue of not being able to write scalar values out in binary format, which is a separate issue.",smartshark_2_2,30,systemml,"""[0.08230280131101608, 0.9176972508430481]"""
2710,230969,"DML-BUG: ParFor is causing the odd errors in probably ""size propogation"".","Filling this bug after discussion with Matthias on the potential issue with ""size propogation""



Here is the main error, which also causes another errors.

Main Errors
=========
*
15/10/09 18:50:40 ERROR parfor.ParWorker: Failed to execute task (type=SET, iterations={[i=2]}), retry:1
com.ibm.bi.dml.runtime.DMLRuntimeException: ERROR: Runtime error in program block generated from statement block between lines 0 and 0 -- Error evaluating instruction: CPÂ°castdtsÂ°sl_Z1DÂ·MATRIXÂ·DOUBLEÂ·falseÂ°_Var19950Â·SCALARÂ·DOUBLE
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:338)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeInstructions(ProgramBlock.java:225)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.execute(ProgramBlock.java:167)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.ParWorker.executeSetTask(ParWorker.java:189)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.ParWorker.executeTask(ParWorker.java:152)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.LocalParWorker.run(LocalParWorker.java:110)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.ibm.bi.dml.runtime.DMLRuntimeException: Dimension mismatch - unable to cast matrix of dimension (2 x 1) to scalar.
	at com.ibm.bi.dml.runtime.instructions.cp.VariableCPInstruction.processInstruction(VariableCPInstruction.java:505)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:307)
	... 6 more
15/10/09 18:50:40 ERROR parfor.ParWorker: Failed to execute task (type=SET, iterations={[i=1]}), retry:1
com.ibm.bi.dml.runtime.DMLRuntimeException: ERROR: Runtime error in program block generated from statement block between lines 0 and 0 -- Error evaluating instruction: CPÂ°castdtsÂ°sl_Z1DÂ·MATRIXÂ·DOUBLEÂ·falseÂ°_Var19950Â·SCALARÂ·DOUBLE
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:338)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeInstructions(ProgramBlock.java:225)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.execute(ProgramBlock.java:167)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.ParWorker.executeSetTask(ParWorker.java:189)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.ParWorker.executeTask(ParWorker.java:152)
	at com.ibm.bi.dml.runtime.controlprogram.parfor.LocalParWorker.run(LocalParWorker.java:110)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.ibm.bi.dml.runtime.DMLRuntimeException: Dimension mismatch - unable to cast matrix of dimension (2 x 1) to scalar.
	at com.ibm.bi.dml.runtime.instructions.cp.VariableCPInstruction.processInstruction(VariableCPInstruction.java:505)
	at com.ibm.bi.dml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:307)
	... 6 more*",smartshark_2_2,99,systemml,"""[0.13118696212768555, 0.8688130378723145]"""
2711,230970,Address Optimizer Inconsistencies Due to Casting of Doubles To Ints,"Currently, there are a few inconsistencies with our optimizer in which accidental truncation of double values to integer values could lead to rewrite rules being mistakenly applied. As an example, a rewrite rule targeting the expression âsum(v^2)â could be mistakenly applied to the expression âsum(v^2.3)â.",smartshark_2_2,232,systemml,"""[0.058109868317842484, 0.9418901205062866]"""
2712,230971,Codegen optimizer failing for MLogreg special cases,"{code}
Caused by: java.lang.NullPointerException
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.rGetPlanCosts(PlanSelectionFuseCostBasedV2.java:955)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.rGetPlanCosts(PlanSelectionFuseCostBasedV2.java:962)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.rGetPlanCosts(PlanSelectionFuseCostBasedV2.java:944)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.getPlanCost(PlanSelectionFuseCostBasedV2.java:892)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.enumPlans(PlanSelectionFuseCostBasedV2.java:240)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.selectPlans(PlanSelectionFuseCostBasedV2.java:191)
	at org.apache.sysml.hops.codegen.opt.PlanSelectionFuseCostBasedV2.selectPlans(PlanSelectionFuseCostBasedV2.java:143)
	at org.apache.sysml.hops.codegen.template.CPlanMemoTable.pruneSuboptimal(CPlanMemoTable.java:268)
	at org.apache.sysml.hops.codegen.SpoofCompiler.optimize(SpoofCompiler.java:356)
{code}",smartshark_2_2,1291,systemml,"""[0.061659056693315506, 0.9383409023284912]"""
2713,230972,Regularization and tolerance parameters not being set correctly in Scala API,"Classifier/regressor objects behave as shown below:

scala> val lr = new LogisticRegression(""logReg"", sc)

scala> lr.setTol(0.001)
res17: org.apache.sysml.api.ml.LogisticRegression = logReg

scala> lr.getTol
res18: Double = 0.001

scala> lr.getRegParam
res19: Double = 0.001

scala> lr.setRegParam(0.000001)
res20: org.apache.sysml.api.ml.LogisticRegression = logReg

scala> lr.getTol
res21: Double = 1.0E-6

scala> lr.getRegParam
res22: Double = 1.0E-6
",smartshark_2_2,1951,systemml,"""[0.150978222489357, 0.8490217328071594]"""
2714,230973,Codegen failing on three-way multi-aggregate,"{code}
Caused by: org.apache.sysml.hops.HopsException: 44 TRead X
	at org.apache.sysml.hops.codegen.template.TemplateCell.rConstructCplan(TemplateCell.java:321)
	at org.apache.sysml.hops.codegen.template.TemplateCell.rConstructCplan(TemplateCell.java:192)
	at org.apache.sysml.hops.codegen.template.TemplateMultiAgg.constructCplan(TemplateMultiAgg.java:89)
	at org.apache.sysml.hops.codegen.SpoofCompiler.rConstructCPlans(SpoofCompiler.java:560)
	at org.apache.sysml.hops.codegen.SpoofCompiler.rConstructCPlans(SpoofCompiler.java:572)
	at org.apache.sysml.hops.codegen.SpoofCompiler.rConstructCPlans(SpoofCompiler.java:572)
	at org.apache.sysml.hops.codegen.SpoofCompiler.rConstructCPlans(SpoofCompiler.java:572)
	at org.apache.sysml.hops.codegen.SpoofCompiler.optimize(SpoofCompiler.java:364)
{code}",smartshark_2_2,1290,systemml,"""[0.07406828552484512, 0.9259316921234131]"""
2715,230974,Parfor sample script fails w/ dimension mismatch,"The parfor util script sample.dml fails with dimension mismatch in special cases, where the remote memory budget of map/reduce tasks is larger than the driver memory budget and the permutation matrix multiplication would be compiled to MR in local parfor but CP in remote parfor execution. 

In these cases, we trigger a forced recompile to CP which internally tries to reduce the overhead by recompiling only dags where the runtime plan contains MR instructions. This selective recompilation in invalid with permutation matrix multiplications that stretch two subsequent dags and the first dag does not necessarily contain MR instructions. 

Since meanwhile, the overhead of recompiling average dags (50-100 operators) is less than 1ms, we should always recompile the entire parfor body program in these cases. 

As a related note: Since we now support removeEmpty with selection vectors, we should rewrite these permutation matrix multiplications to remove empty w/ selection which is equivalent from a runtime perspective but would simplify debugging in comparison to the current multi-dag rewrite. ",smartshark_2_2,356,systemml,"""[0.4554815888404846, 0.5445184111595154]"""
2716,230975,"Potential infinite recursion in Explain#explain(DMLProgram, Program, ExplainType)","Here is related code:
{code}
        public static String explain(DMLProgram prog, Program rtprog, ExplainType type)
                throws HopsException, DMLRuntimeException, LanguageException {
                return explain(prog, rtprog, type);
{code}",smartshark_2_2,665,systemml,"""[0.07400641590356827, 0.9259935617446899]"""
2717,230976,NPE in executeRemoteSparkParFor,"The method ParForProgramBlock.releaseForcedRecompile(long tid) calls recompileProgramBlockHierarchy2Forced with execution type (et) null. This leads to a NullPointerException. 

I haven't fully figured out under which circumstances this occurs but it happens when calling an external function inside a forced parfor_spark. 

The ParForProgramBlock.executeRemoteSparkParFor method sets the flagForced to true which then in turn calls the above method with et==null.",smartshark_2_2,1942,systemml,"""[0.06476294994354248, 0.9352370500564575]"""
2718,230977,Multi-class Mlogreg fails w/ invalid generated code,"{code}
public final class TMP43 extends SpoofRowwise {
  public TMP43() {
    super(RowType.FULL_AGG, -1, false, 2);
  }
  protected void genexec(double[] a, int ai, SideInput[] b, double[] scalars, double[] c, int len, int rowIndex) {
    double[] TMP37 = LibSpoofPrimitives.vectMinusWrite(0, a, ai, len);
    double[] TMP38 = LibSpoofPrimitives.vectPowWrite(TMP37, 2, 0, TMP37.length);
    double TMP39 = LibSpoofPrimitives.vectSum(TMP38, 0, TMP38.length);
    c[0] += TMP39;
  }
  protected void genexec(double[] avals, int[] aix, int ai, SideInput[] b, double[] scalars, double[] c, int alen, int len, int rowIndex) {
    double[] TMP40 = LibSpoofPrimitives.vectMinusWrite(0, a, ai, len);
    double[] TMP41 = LibSpoofPrimitives.vectPowWrite(TMP40, 2, 0, TMP40.length);
    double TMP42 = LibSpoofPrimitives.vectSum(TMP41, 0, TMP41.length);
    c[0] += TMP42;
  }
}
{code}

which gives the following error

{code}
Caused by: org.codehaus.commons.compiler.CompileException: Line 19, Column 60: Unknown variable or type ""a""
        at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11004)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6017)
{code}",smartshark_2_2,742,systemml,"""[0.08538904786109924, 0.9146109819412231]"""
2719,230978,Parfor optimizer fails w/ complex indexing expressions in spark exec modes,"The function nn/layers/conv2d_depthwise.dml::forward throws the following exception in spark execution modes. The reason why it does not show up in hadoop execution modes, is because it originates from a codepath for block partitioning, which is only supported in Spark.

{code}
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.sysml.parser.ParForStatementBlock.determineAccessPattern(ParForStatementBlock.java:616)
        at org.apache.sysml.parser.ParForStatementBlock.rDeterminePartitioningCandidates(ParForStatementBlock.java:567)
        at org.apache.sysml.parser.ParForStatementBlock.rDeterminePartitioningCandidates(ParForStatementBlock.java:554)
        at org.apache.sysml.parser.ParForStatementBlock.determineDataPartitionFormat(ParForStatementBlock.java:417)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizerRuleBased.rewriteSetDataPartitioner(OptimizerRuleBased.java:434)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizerRuleBased.optimize(OptimizerRuleBased.java:244)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizationWrapper.optimize(OptimizationWrapper.java:243)
        at org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizationWrapper.optimize(OptimizationWrapper.java:117)
        at org.apache.sysml.runtime.controlprogram.ParForProgramBlock.execute(ParForProgramBlock.java:579)
        at org.apache.sysml.runtime.controlprogram.FunctionProgramBlock.execute(FunctionProgramBlock.java:115)
        ... 18 more
Caused by: java.lang.NullPointerException
        at org.apache.sysml.parser.ParForStatementBlock.rParseBinaryExpression(ParForStatementBlock.java:1898)
        at org.apache.sysml.parser.ParForStatementBlock.getLinearFunction(ParForStatementBlock.java:1728)
        at org.apache.sysml.parser.ParForStatementBlock.determineAccessPattern(ParForStatementBlock.java:599)
        ... 27 more
{code}",smartshark_2_2,539,systemml,"""[0.07062526047229767, 0.9293747544288635]"""
2720,230979,Full standard deviation fails due to wrong data type ,"The following script currently fails due to incorrect hop construction of {{sd(Xi)}} at language level:
{code}
X = rand(rows=1000, cols=100);
R = matrix(0, ncol(X), 4)
parfor(i in 1:ncol(X), log=DEBUG) {
   Xi = X[,i];
   R[i,1] = mean(Xi);                  # mean
   R[i,2] = sd(Xi);                    # standard deviation
   R[i,3] = moment(Xi,3)/(sd(Xi)^3);   # skewness
   R[i,4] = moment(Xi,4)/(sd(Xi)^4)-3; # kurtosis
}
{code}

The stacktrace is as follows

{code}
Caused by: org.apache.sysml.runtime.DMLRuntimeException: Variable '_Var1703' does not exist in the symbol table.
	at org.apache.sysml.runtime.controlprogram.context.ExecutionContext.getMatrixObject(ExecutionContext.java:186)
	at org.apache.sysml.runtime.controlprogram.context.ExecutionContext.setMatrixOutput(ExecutionContext.java:451)
	at org.apache.sysml.runtime.instructions.cp.MatrixBuiltinCPInstruction.processInstruction(MatrixBuiltinCPInstruction.java:59)
	at org.apache.sysml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:286)
	... 6 more
{code}

which is cause by the following instruction which has an output type MATRIX although it should be SCALAR.
{code}
--------CP uavar _mVar3.MATRIX.DOUBLE _mVar5.MATRIX.DOUBLE 24
{code}

Interestingly, this only shows up when there are multiple consumers with matrix and scalar data types.
",smartshark_2_2,580,systemml,"""[0.06765400618314743, 0.9323460459709167]"""
2721,230980,Compilation failure functions without return clause,"The IPA rewrite for inlining small functions currently leads to invalid hops in case of special cases of no returns and remaining transient writes from previous sequence of statement blocks that are combined into a single block.

{code}
foo = function (String msg) {
    verbose = FALSE
    if (verbose)
        print(msg)
}

foo(""This is an test error message."")
{code}",smartshark_2_2,1260,systemml,"""[0.07649296522140503, 0.923507034778595]"""
2722,230981,StepLinreg is failing w/ recompilation issue,"Our step-wise LinregDS is currently failing with the following NPE on dynamic recompilation 
{code}

Caused by: java.lang.NullPointerException
        at org.apache.sysml.lops.BinaryScalar.getOpcode(BinaryScalar.java:111)
        at org.apache.sysml.lops.BinaryScalar.getInstructions(BinaryScalar.java:87)
        at org.apache.sysml.lops.compile.Dag.generateControlProgramJobs(Dag.java:1406)
        at org.apache.sysml.lops.compile.Dag.doGreedyGrouping(Dag.java:1176)
        at org.apache.sysml.lops.compile.Dag.getJobs(Dag.java:270)
        at org.apache.sysml.hops.recompile.Recompiler.recompileHopsDag(Recompiler.java:239)
        at org.apache.sysml.runtime.controlprogram.ProgramBlock.execute(ProgramBlock.java:147)
{code}

The root cause is that some rewrite modified the input solve to a scalar, which causes the instruction generation of solve to fail because it is not defined over scalars.",smartshark_2_2,603,systemml,"""[0.06276185810565948, 0.9372382164001465]"""
2723,230982,Dataframe converters incorrectly use positional access,"Both our dataframe-matrix and dataframe-frame converters assume positional access of the __INDEX column. This is incorrect as, for example, a serialization into json and subsequent deserialization loses this order (i.e., sorts by column name). This is a bug because our converters cannot even read data frames produced by SystemML in a previous script.",smartshark_2_2,1659,systemml,"""[0.06322752684354782, 0.9367724657058716]"""
2724,230983,"Perftest Mlogreg multinomial, icp 1 failing w/ index-out-of-bounds","{code}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 4004
	at org.apache.sysml.runtime.codegen.SpoofOperator.getValue(SpoofOperator.java:219)
	at codegen.TMP59.genexec(Unknown Source)
	at org.apache.sysml.runtime.codegen.SpoofCellwise.executeDenseNoAgg(SpoofCellwise.java:433)
	at org.apache.sysml.runtime.codegen.SpoofCellwise.executeDense(SpoofCellwise.java:305)
	at org.apache.sysml.runtime.codegen.SpoofCellwise.execute(SpoofCellwise.java:242)
	at org.apache.sysml.runtime.instructions.cp.SpoofCPInstruction.processInstruction(SpoofCPInstruction.java:87)
	at org.apache.sysml.runtime.controlprogram.ProgramBlock.executeSingleInstruction(ProgramBlock.java:264)
	... 18 more
{code}

The reason is incorrect dimension propagation in a previous row template with outer operations and indexing, where the column range indexing leads to a smaller size than the input would suggest.",smartshark_2_2,771,systemml,"""[0.06940201669931412, 0.9305980205535889]"""
2725,230984,"Fix license, notice, and disclaimer for standalone jar artifact","The standalone jar artifact (see ""create-standalone-jar"" in pom.xml, which delegates to ""src/assembly/standalone-jar.xml"") has issues with its license, notice, and disclaimer files.

#1) There is no META-INF/DISCLAIMER incubator file, which ""should"" be there.

#2) It contains a META-INF/NOTICE file for ""Apache Wink :: JSON4J""  and a META-INF/NOTICE.txt file for ""Apache Commons CLI"". However, there is no META-INF/NOTICE file for SystemML.

#3) It contains a META-INF/LICENSE file which is the general Apache license and a META-INF/LICENSE.txt file which is also the general Apache license. However, the LICENSE file most likely should include a listing of the license information for all the projects contained in the project (similar to src/assembly/standalone/LICENSE).",smartshark_2_2,370,systemml,"""[0.9976584911346436, 0.0023415458854287863]"""
2726,230985,Implementation of language and compiler extension,"This part aims to add an additional language support for the âparamservâ function in order to be able to compile this new function. Since SystemML already supports the parameterized builtin function, we can easily extend an additional operation type and generate a new instruction for the âparamservâ function. Recently, we have also added a new âevalâ built-in function which is capable to pass a function pointer as argument so that it can be called in runtime. Similar to it, we would need to extend the inter-procedural analysis to avoid removing unused constructed functions in the presence of second-order âparamservâ function. Because the referenced functions, i.e., the aggregate function and update function, should be present in runtime.",smartshark_2_2,1227,systemml,"""[0.9983288645744324, 0.0016711200587451458]"""
2727,230986,Simplify script invocation section of Beginner's Guide to DML,"New user needs to be able run a DML script in as short a time as possible. Shortest way is probably to download a standalone release (when it is available) and run a ""hello world"" script. Current version of the doc mentions building the project. The build command is not shown on this page and building the project can be a potential turnoff. Better option is to download a standalone release and use that.

Additionally, move Eclipse Debug Configuration out of the Beginner's Guide since it would make more sense in a separate document that focuses more on contributing code to the SystemML project.

",smartshark_2_2,1568,systemml,"""[0.9976623058319092, 0.002337712561711669]"""
2728,230987,Performance csv/textcell reblock of sparse matrices,During csv and textcell reblock any temporary blocks are created in dense format. This leads to unnecessary allocation and dense-sparse block conversions in case of sparse matrices.,smartshark_2_2,1542,systemml,"""[0.9979519248008728, 0.0020481590181589127]"""
2729,230988,Enable Prefetching of Mini-Batches,"For efficient training of large deep learning models, a mini-batch training approach is preferred.  On SystemML with the Spark backend, this currently equates to grabbing a mini-batch from an RDD (via a PartitionPruning RDD -- see SYSTEMML-951), and then using entirely single-node instructions for each mini-batch.  While the fetching of partitions has been made efficient, we currently have to pause after each training step to grab the next partition.  For large models, training time is already an issue even for GPUs with saturated input pipelines.  Thus, we need to enable prefetching of mini-batches that runs in parallel to the training loop.  One possibility would be to create an input queue that is fed from a prefetch thread, and that then feeds the training loop. ",smartshark_2_2,1758,systemml,"""[0.9983497858047485, 0.0016502599464729428]"""
2730,230989,Support matrices with zero rows/columns,"* Basic compiler/runtime support zero rows/columns
* I/O support zero rows/columns all formats
* Handling of special builtin functions like removeEmpty.
* Handling of aggregation functions",smartshark_2_2,986,systemml,"""[0.9981422424316406, 0.0018578078597784042]"""
2731,230990,Outer comparison w/ vector > 16 gives incorrect results,"{code}
test_vector = seq (16, 1, -1);
test_matrix = outer (test_vector1, t(test_vector2), "">"");
print(toString(test_matrix))
{code}",smartshark_2_2,1306,systemml,"""[0.06401745975017548, 0.9359825849533081]"""
2732,230991,Update github README,Updating README to include assumptions and make it easier for beginners.,smartshark_2_2,1390,systemml,"""[0.9979471564292908, 0.002052824944257736]"""
2733,230992,Add Vector As Supported Type In Frame Conversions,"Currently, we are able to pass in Spark DataFrames with Vector type columns as input to SystemML scripts to be converted to a SystemML {{matrix}}.  We should add the Vector type to the {{frame}} conversion code too.

Example:  Given a Spark DataFrame of schema <double, vector, string>, we should be able to convert that to a SystemML {{frame}} with a bunch of double columns and the final string column.

cc [~mboehm7], [~acs_s]",smartshark_2_2,1615,systemml,"""[0.9984655380249023, 0.0015344986459240317]"""
2734,230993,Add Scala LogisticRegression API For Spark ML Pipeline,"I wrote a scala ml pipeline wrapper for LogisticRegression Model as a example for scala user.

I propose a scala version example since some weakness for java version.

It's not naturally to extend scala class in java code. We need know function style after compile, like
@Override
public void org$apache$spark$ml$param$shared$HasElasticNetParam$setter$elasticNetParam_$eq(DoubleParam arg0) {}

I assume it's set function, but do nothing here

Hard to follow ml parameter style, but define parameter like below

private IntParam icpt = new IntParam(this, ""icpt"", ""Value of intercept"");
private DoubleParam reg = new DoubleParam(this, ""reg"", ""Value of regularization parameter"");",smartshark_2_2,337,systemml,"""[0.9984198808670044, 0.001580086536705494]"""
2735,230994,Tests do not run with R version 3.2.x,"R installation is required for SystemML testing. Current latest version of R is 3.2.x, which I installed on my Mac using brew. If I run LinearRegressionTest.java, with R 3.2.x installed, I see the following in the console:

Error in t(V) : could not find function ""checkAtAssignment""
Calls: t -> t
Execution halted

Manually installing R 3.1.3 on my Mac fixed this.

The tests should run using the latest versions of R.",smartshark_2_2,113,systemml,"""[0.9886271357536316, 0.011372925713658333]"""
2736,230995,Fix Python Tests to Run on Python 2 & 3,"Currently, the Python {{mllearn}} tests do not work on Python 3.  This simply fixes the tests so that they run on both Python 2 & 3.",smartshark_2_2,1513,systemml,"""[0.9982945322990417, 0.0017054617637768388]"""
2737,230996,Remove antlr tool jar from standalone tar.gz and zip artifacts,"The antlr tool jar (antlr4-4.5.3.jar) is not required for the standalone tar.gz and zip artifacts. Therefore, it can be removed from the standalone.xml assembly. Any license references should also be updated.

(As a side note: The antlr runtime library is included in the main SystemML jar as a compile-scope dependency.)",smartshark_2_2,1593,systemml,"""[0.9982415437698364, 0.0017584201414138079]"""
2738,230997,Package algorithm files in folder in several artifacts,"The main algorithm files are written to the base directory of the following artifacts:

systemml-0.10.0-incubating-SNAPSHOT.jar
systemml-0.10.0-incubating-SNAPSHOT-inmemory.jar
systemml-0.10.0-incubating-SNAPSHOT-sources.jar
systemml-0.10.0-incubating-SNAPSHOT-standalone.jar

It would be good to package these algorithm files either into a scripts/ folder or a scripts/algorithms folder.

As an example, here are the current primary jar file base directory contents:
{code}
$ tree -L 1 systemml-0.10.0-incubating-SNAPSHOT
systemml-0.10.0-incubating-SNAPSHOT
âââ ALS-CG.dml
âââ ALS-DS.dml
âââ ALS_predict.dml
âââ ALS_topk_predict.dml
âââ Cox-predict.dml
âââ Cox.dml
âââ CsplineCG.dml
âââ CsplineDS.dml
âââ GLM-predict.dml
âââ GLM.dml
âââ KM.dml
âââ Kmeans-predict.dml
âââ Kmeans.dml
âââ LinearRegCG.dml
âââ LinearRegDS.dml
âââ META-INF
âââ MultiLogReg.dml
âââ PCA.dml
âââ StepGLM.dml
âââ StepLinearRegDS.dml
âââ Univar-Stats.dml
âââ apply-transform.dml
âââ bivar-stats.dml
âââ com
âââ decision-tree-predict.dml
âââ decision-tree.dml
âââ l2-svm-predict.dml
âââ l2-svm.dml
âââ m-svm-predict.dml
âââ m-svm.dml
âââ naive-bayes-predict.dml
âââ naive-bayes.dml
âââ obsolete
âââ org
âââ random-forest-predict.dml
âââ random-forest.dml
âââ stratstats.dml
âââ transform.dml
{code}

Placing the *.dml files in a scripts/ or scripts/algorithms/ folder would give the following contents in the base directory:

{code}
$ tree -L 1 systemml-0.10.0-incubating-SNAPSHOT
systemml-0.10.0-incubating-SNAPSHOT
âââ META-INF
âââ com
âââ org
âââ scripts
{code}",smartshark_2_2,410,systemml,"""[0.9977306723594666, 0.0022693565115332603]"""
2739,230998,Vector Free L-BFGS implementation,"This is for the implementation of vector free L-BFGS, as in the paper http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf , to avoid the parameter server. 

Example implementation for spark-ml lib : @ https://github.com/yanboliang/spark-vlbfgs
",smartshark_2_2,697,systemml,"""[0.9983586668968201, 0.0016413578996434808]"""
2740,230999,Efficient spark right indexing via lookup,"So far all versions of spark right indexing instructions require a full scan over the data set. In case of existing partitioning (which anyway happens for any external format - binary block conversion) such a full scan is unnecessary if we're only interested in a small subset of the data. This task adds an efficient right indexing operation via 'rdd lookups' which access at most <num_lookup> partitions given existing hash partitioning. 

cc [~mwdusenb@us.ibm.com]  

In detail, this task covers the following improvements for spark matrix right indexing. Frames are not covered here because they allow variable-length blocks. Also, note that it is important to differentiate between in-core and out-of-core matrices: for in-core matrices (i.e., matrices that fit in deserialized form into aggregated memory), the full scan is actually not problematic as the filter operation only scans keys without touching the actual values.

(1) Scan-based indexing w/o aggregation: So far, we apply aggregations to merge partial blocks very conservatively. However, if the indexing range is block aligned (e.g., dimension start at block boundary or range within single block) this is unnecessary. This alone led to a 2x improvement for indexing row batches out of an in-core matrix.

(2) Single-block lookup: If the indexing range covers a subrange of a single block, we directly perform a lookup. On in-core matrices this gives a minor improvement (but does not hurt) while on out-of-core matrices, the improvement is huge in case of existing partitioner as we only have to scan a single partition instead of the entire data.

(3) Multi-block lookups: Unfortunately, Spark does not provide a lookup for a list of keys. So the next best option is a data-query join (in case of existing partitioner) with {{data.join(filter).map()}}, which works very well for in-core data sets, but for out-of-core datasets, unfortunately, does not exploit the potential for partition pruning and thus reads the entire data. I also experimented with a custom multi-block lookup that runs multiple lookups in a multi-threaded fashion - this gave the expected pruning but was very ugly due to an unbounded number of jobs. 

In conclusion, I'll create a patch for scenarios (1) and (2), while scenario (3) requires some more thoughts and is postponed after the 0.11 release. One idea would be to create a custom RDD that implements {{lookup(List<T> keys)}} by constructing a pruned set of input partitions via {{partitioner.getPartition(key)}}. cc [~freiss] [~niketanpansare] [~reinwald]

",smartshark_2_2,1656,systemml,"""[0.9982275366783142, 0.0017724876524880528]"""
2741,231000,Extend scheme Disjoint_Round_Robin,"It aims to extend theÂ scheme of data partition.
 * Disjoint_Round_Robin: for each worker, use a permutation multiply or simpler a removeEmpty such as {{removeEmpty(target=X, margin=rows, select=(seq(1,nrow(X))%%k)==id)}}.",smartshark_2_2,1245,systemml,"""[0.998503565788269, 0.0014964910224080086]"""
2742,231001,Need better documentation for PYDML function parameters,"The various PyDML functions need improved documentation describing their functionality and the parameters that they take.

For example, the full() function in the PyDML Language Reference shows:
x = full(4, 4, 3)
But there's no description of what each parameter is. Additionally, the ""Python syntax for DML"" shows an example, but the parameters are in the wrong order. This can make it frustrating to write PyDML.

So right now issues like this can require someone to have to read source code to figure out the correct parameter syntax.",smartshark_2_2,350,systemml,"""[0.9983128309249878, 0.0016872364794835448]"""
2743,231002,Dependency isolation matrix block library,"Modify the local linear algebra routines in SystemML such that those routines do not depend on any unnecessary external libraries. When possible, err on the side of reimplementing functionality locally instead of using an external library. These changes should make it easier for other projects to incorporate these linear algebra routines without adding incompatible dependencies.",smartshark_2_2,1382,systemml,"""[0.998173713684082, 0.0018263247329741716]"""
2744,231003,Aggregation service,The aggregation service is independant of local or remote workers. It is responsible for executing the parameter updating.,smartshark_2_2,1235,systemml,"""[0.9971253275871277, 0.002874611411243677]"""
2745,231004,Clean Up and Reorganize Documentation Targeted At Data Scientists,"This JIRA issue aims to clean up and reorganize a lot of the existing documentation. The goal here is to work towards cleaning up our external message and targeting specific types of users in order to increase ease of adoption.

My vision is that we target data scientists using Spark first and foremost. Without this focus in our documentation, the project is seemingly too confusing, and will deter this key user demographic from adoption. Once these users are onboard, engine developers will follow.

This PR is a first effort towards this goal, and provides a nicer, cleaned-up version of the docs. We should collectively work to improve these, with clear separation between data scientists, systems engineers/researchers, etc.",smartshark_2_2,1442,systemml,"""[0.998038113117218, 0.0019619499798864126]"""
2746,231005,kNN,"DML script for scalable kNN
1. kNN
2. k-selection 
3. feature importance
4. feature selection

The task includes:
a. DML scripts
b. test scripts (if possible with data generator)
c. user documentation",smartshark_2_2,274,systemml,"""[0.9980217218399048, 0.0019782446324825287]"""
2747,231006,Memory efficiency JMLC matrix and frame conversions,"The current JMLC conversion functions cause a very inefficient and memory intensive code path with leads to unnecessary OOMs that can be easily avoided. This task aims to add and improve these primitives to allow convenient data conversions with much better memory efficiency. 

For example consider a scenario of a 500k x 90 input model available as csv file in the classpath, which string representation requires 1GB. The typical codepath currently use looks as follows:
{code}
ResourceStream(model_file)
-> prep
---> StringBuilder -> String [3GB tmp, 1GB]
-> convertToDoubleMatrix
---> byte[] -> ByteInputStream [2GB]
---> MatrixBlock [360MB]
---> double[][] [400MB]
-> setMatrix
---> MatrixBlock [360MB]
{code} 

which requires at least 4GB of memory due to strong references to all intermediates. The goal of this task is to reduce this to the following, which only requires 360MB of memory:

{code}
ResourceStream(model_file)
-> convertToMatrix
---> MatrixBlock [360MB]
-> setMatrix
---> by references
{code} 
",smartshark_2_2,511,systemml,"""[0.998214602470398, 0.001785388682037592]"""
2748,231007,Passing function pointers,"Usage: To simplify the usage of parameter servers, it might be very useful to allow the passing of function pointers alternatively to function names and namespaces. For example, there is a test that uses a namespace that is defined in a different file which might lead to inconsistencies.",smartshark_2_2,1187,systemml,"""[0.9983600974082947, 0.0016399371670559049]"""
2749,231008,Give content more breathing space using CSS,Change ml-container width from 64em to 75em. Add padding class to feature content.,smartshark_2_2,1682,systemml,"""[0.9982218146324158, 0.001778178266249597]"""
2750,231009,Improve the efficiency of matrix subsetting,"For the {{rangeReIndex}} operation, it needs to read the whole input matrix into memory first and do the subsetting in the memory. It is not efficient because it needs to read a lot of unnecessary data, and even will be out of memory if the size of input matrix exceeds the available memory. 

The plan here is to read the keys in the Hadoop sequence file, and identify the keys overlapped with the input range index. Then the values specified by the identified keys will be read.",smartshark_2_2,2301,systemml,"""[0.9983355402946472, 0.0016645049909129739]"""
2751,231010,Write scripts to run SystemML performance suite,"The scripts for running the SystemML performance suite (see SYSML-38) are currently only semi-automated. A fair amount of manual intervention is required to run the algorithms tests from beginning to end. This task covers automating the performance testing process to the point where a single command line will:
* Generate data
* Run all algorithms multiple times at multiple data sizes
* Generate a full set of reports and graphs
* Log any failures and either continue or stop after each failure",smartshark_2_2,8,systemml,"""[0.9979939460754395, 0.002006086753681302]"""
2752,231011,Language feature updates,This epic tracks all language feature updates that remove features or change their external behavior.,smartshark_2_2,2082,systemml,"""[0.9980300068855286, 0.001969958422705531]"""
2753,231012,Log4j not configured when execute DMLScript,"Right now, if I set up the SystemML project in Eclipse and execute DMLScript (the main() entrypoint to SystemML), log4j is not configured, so I obtain the typical log4j initialization warning messages:

{code}
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
{code}

One standard approach to this issue is to add a log4j.properties to the root of the classpath. With a maven project, this can be done by creating a src/main/resources/log4j.properties file (src/main/resources is on the classpath).

This log4j.properties file can be excluded from the build so it's not packaged with the project.

The presence of this file makes it easier for developers to obtain log information, thus increasing consumability.",smartshark_2_2,127,systemml,"""[0.9470628499984741, 0.05293717607855797]"""
2754,231013,Change parser/hops block sizes from long to int,"The runtime supports only blocks of int by int dimensions. Currently, there is a mismatch between compiler and runtime components which use long and int block sizes, respectively. This causes unnecessary memory overhead and casting. Hence, this task aims to create consistency, starting from all data structures such as hops and data identifiers.",smartshark_2_2,1063,systemml,"""[0.9984337687492371, 0.001566236955113709]"""
2755,231014,Remove unused constants from project,"Unused (unreferenced) constants should be removed from the project because they are misleading.

Here is a list of unused constants in the project:
{code}
org.apache.sysml.api.MLMatrix.LOG
org.apache.sysml.hops.Hop.BREAKONSCALARS
org.apache.sysml.hops.Hop.SPLITLARGEMATRIXMULT
org.apache.sysml.hops.OptimizerUtils.BIT_SIZE
org.apache.sysml.lops.AppendCP.OPCODE
org.apache.sysml.lops.Checkpoint.STORAGE_LEVEL
org.apache.sysml.parser.LanguageException.LanguageErrorCodes.GENERIC_ERROR
org.apache.sysml.runtime.controlprogram.parfor.opt.CostEstimator.DEFAULT_MEM_ESTIMATE_MR
org.apache.sysml.runtime.controlprogram.parfor.opt.OptimizerRuleBased.MAX_REPLICATION_FACTOR_PARTITIONING
org.apache.sysml.runtime.controlprogram.parfor.opt.PerfTestTool.DEFAULT_SORT_IO_MEM
org.apache.sysml.runtime.controlprogram.parfor.opt.PerfTestTool.MAX_DIMSIZE
org.apache.sysml.runtime.controlprogram.parfor.opt.PerfTestTool.MAX_SORT_IO_MEM
org.apache.sysml.runtime.controlprogram.parfor.opt.PerfTestTool.MIN_DIMSIZE
org.apache.sysml.runtime.controlprogram.parfor.opt.PerfTestTool.MIN_SORT_IO_MEM
org.apache.sysml.runtime.controlprogram.parfor.ProgramConverter.EXT_FUNCTION
org.apache.sysml.runtime.controlprogram.parfor.ProgramConverter.NOT_SUPPORTED_EXECUTION_CONTEXT
org.apache.sysml.runtime.controlprogram.ParForProgramBlock.LIVEVAR_AWARE_CLEANUP
org.apache.sysml.runtime.instructions.Instruction.NAME_VALUE_SEPARATOR
org.apache.sysml.runtime.matrix.mapred.MapperBase.LOG
org.apache.sysml.runtime.matrix.mapred.MRConfigurationNames.DFS_DATANODE_DATA_DIR_PERM
org.apache.sysml.runtime.matrix.mapred.MRConfigurationNames.MR_APPLICATION_CLASSPATH
org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration.MAX_COL_DIMENSION
org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration.MAX_ROW_DIMENSION
org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration.PARFOR_NUMITERATOINS
org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration.PARFOR_NUMTASKS
org.apache.sysml.runtime.transform.GenTfMtdMR.DELIM
org.apache.sysml.runtime.util.UtilFunctions.ADD_PRIME2
org.apache.sysml.udf.PackageFunction.LOG
{code}
",smartshark_2_2,1818,systemml,"""[0.9983652234077454, 0.0016347419004887342]"""
2756,231015,Add bias_multiply operator,"bias_multiply performs similar operation as bias_add, except it does element-wise multiplication instead of addition",smartshark_2_2,2114,systemml,"""[0.9981308579444885, 0.001869109459221363]"""
2757,231016,Fix missing license header files - RAT build failing,"mvn clean package -Prat fails
",smartshark_2_2,236,systemml,"""[0.9941028952598572, 0.00589710334315896]"""
2758,231017,Umbrella: Refactor unit tests,This JIRA is an umbrella for subtasks involving work to refactor unit tests to increase parallelism and reduce the time to run the tests.,smartshark_2_2,210,systemml,"""[0.998221218585968, 0.0017787249526008964]"""
2759,231018,Unnecessary rdd computation for nnz maintenance on write,"Our primitive for writing binary block RDDs to HDFS (as used in guarded collect), first computes the number of non-zeros (nnz) and subsequently writes out the data. This leads to redundant RDD computation, which can be expensive for large DAGs of RDD operations. Explicitly computing the nnz is unnecessary as we could simply piggyback this computation onto the write via an accumulator as done in multiple other places in SystemML. ",smartshark_2_2,1850,systemml,"""[0.9981889128684998, 0.0018110572127625346]"""
2760,231019,New simplification rewrites for stratstats,"1) colSums(X%*%Y) -> colSums(X) %*% Y
2) rowSums(X%*%Y) -> X %*% rowSums(Y)
3) a=t(X), b=t(X^2) -> a=t(X), b=t(X)^2",smartshark_2_2,1840,systemml,"""[0.9982853531837463, 0.0017145680030807853]"""
2761,231020,Update FrameConverterTest RowGenerator,Update schema handling in RowGenerator of FrameConverterTest to prevent potential error in new Spark version.,smartshark_2_2,1503,systemml,"""[0.9983901977539062, 0.0016098814085125923]"""
2762,231021,Add mllearn and scala wrappers for ALS,See https://apache.github.io/incubator-systemml/algorithms-matrix-factorization.html#matrix-completion-via-alternating-minimizations for usage,smartshark_2_2,1603,systemml,"""[0.9984862804412842, 0.0015137330628931522]"""
2763,231022,Update team members page with github profiles,Utilize github profile information and display contributor picture on who we are page.,smartshark_2_2,1512,systemml,"""[0.9976592063903809, 0.0023408003617078066]"""
2764,231023,Regression tests delete each others' temporary directories,"When finishing a test case, the superclass for SystemML regression tests attempts to delete all files and directories that match the pattern system-ml/*temp*. Some of the tests (I'm not sure which ones) create temporary directories with names like ""system-ml/temp941_553"". When multiple tests run in parallel from the Maven build, this cleanup code interferes with the tests that create temporary directories, leading to test failures such as the following:

  ReplaceTest>AutomatedTestBase.tearDown:1284 unable to remove temporary files: Directory /Users/freiss/git/systemml/system-ml/temp917_539 is not empty
  ReplaceTest>AutomatedTestBase.tearDown:1284 unable to remove temporary files: Directory /Users/freiss/git/systemml/system-ml/temp941_553 is not empty
  RoundTest>AutomatedTestBase.tearDown:1284 unable to remove temporary files: Directory /Users/freiss/git/systemml/system-ml/temp941_553 is not empty
  FullCumprodTest>AutomatedTestBase.tearDown:1284 unable to remove temporary files: Directory /Users/freiss/git/systemml/system-ml/temp1115_724 is not empty
  FullCumprodTest>AutomatedTestBase.tearDown:1284 unable to remove temporary files: Directory /Users/freiss/git/systemml/system-ml/temp1115_724 is not empty",smartshark_2_2,98,systemml,"""[0.9940821528434753, 0.005917825270444155]"""
2765,231024,Spark data partitioner,"In the context of ml, it would be more efficient to support the data partitioning in distributed manner. This task aims to do the data partitioning on Spark which means that all the data will be firstly splitted among workers and then execute data partitioning on worker side according to scheme, and then the partitioned data which stay on each worker could be directly passed to run model training work without materialization on HDFS.",smartshark_2_2,1287,systemml,"""[0.9982808828353882, 0.0017191717633977532]"""
2766,231025,Add Jupyter Python Notebook Example,New Jupyter Python Notebook to showcase SystemML 2-layer autoencoder for acoustic signal modeling.,smartshark_2_2,672,systemml,"""[0.9977564215660095, 0.0022435863502323627]"""
2767,231026,Determine required avro jars for bin artifacts,"The current -bin (tgz and zip) artifacts have the following avro jars in them:
{code}
avro-1.7.4.jar
avro-ipc-1.7.7-tests.jar
avro-ipc-1.7.7.jar
avro-mapred-1.7.7-hadoop2.jar
{code}

Determine if avro-ipc-1.7.7-tests.jar, avro-ipc-1.7.7.jar, and avro-mapred-1.7.7-hadoop2.jar are needed. If not, exclude them from bin artifacts.

If any are needed, determine if a single version (1.7.4 or 1.7.7) should be used, and use that version.
",smartshark_2_2,2118,systemml,"""[0.9984080195426941, 0.0015919838333502412]"""
2768,231027,Refactor MLContext in Scala,"Our {{MLContext}} API relies on a myriad of optional parameters as conveniences for end-users, which has led to our Java implementation growing in size.  Moving to Scala will allow us to use default parameters and continue to expand the capabilities of the API in a clean way.",smartshark_2_2,1432,systemml,"""[0.998210072517395, 0.0017899564700201154]"""
2769,231028,Revisit the weight and bias of fully connected layer,"Since all our bias are column vector (which is consistent with Keras/Caffe), whereas bias of fully connected layer is a row-vector. Similarly, the weight that is passed to caffe is transpose of weights passed to SystemML (since both store in row-major NCHW format).

Making the dimensions consistent will simplify loading of Caffe/Keras models.

[~mwdusenb@us.ibm.com]",smartshark_2_2,2233,systemml,"""[0.9984368681907654, 0.001563123194500804]"""
2770,231029,Enable lazily freeing cuda allocated memory chunks,"The current version of deallocating cuda memory chunks is done asynchronously. That came about as a result of the {{cudaFree}} operations being expensive and so the thought process of doing cudaFree asynchronously was that the cudaFree could happen when the CPU was busy with other work. In tight loops where most operations are done on the GPU, the asynchronous cudaFree weren't really asynchronous. Operations waiting to use the GPU would pay the penalty for the cudaFree operation.

After adding extra instrumentation, it was determined that {{cudaAlloc}} operations were fairly expensive as well. 
Most GPU operations are done in loops with constantly allocating and deallocating the same size of memory chunks per loop. What would be more efficient is to ""clear out"" or set the memory to 0 instead.",smartshark_2_2,2121,systemml,"""[0.9983697533607483, 0.0016303028678521514]"""
2771,231030,Deep Learning,"This epic covers the addition of deep learning to SystemML, including:

* Core DML layer abstractions for deep (convolutional, recurrent) neural nets, with simple forward/backward API: affine, convolution (start with 2D), max-pooling, non-linearities (relu, sigmoid, softmax), dropout, loss functions.
* Modularized DML optimizers: (mini-batch, stochastic) gradient descent (w/ momentum, etc.).
* Additional DML language support as necessary (tensors, built-in functions such as convolution, function pointers, list structures, etc.).
* Integration with other deep learning frameworks (Caffe, Torch, Theano, TensoFlow, etc.) via automatic DML code generation.
* etc.

---
*Plan*:

\[*DONE*\] Phase 1:  *MVPs*
* Create mathematically correct DML deep learning library for running basic feed-forward and convolutional neural nets on a singlenode.
* Create mathematically correct built-in operators for convolution and max pooling for singlenode operation.

\[*CURRENT*\] Phase 2:  *Singlenode*
* Improve performance of DML deep learning library in singlenode operation.
* Expand DML deep learning library to include additional commonly-used layers, such as RNNs and LSTMs, as well as additional optimizers.
* Improve built-in operators for convolution and max pooling to be highly performant in singlenode operation.
* Implement performant GPU acceleration for built-in operators (and end-to-end deep learning algorithms) in singlenode operation.
* Add general engine improvements to improve bottlenecks, such as left-indexing within DML-bodied functions.
* Add end-to-end deep learning algorithm examples, such as a ""LeNet"" convolutional neural net.

Phase 3: *Distributed*
* Expand deep learning support to include *distributed operations* with large models.  This includes improvements to the DML deep learning library, the built-in operators, the GPU acceleration, and general engine improvements.

Phase 4: *APIs/Wrappers*
* Explore integration with Caffe, creating a SystemML interpreter for Caffe model definitions.
* Explore integration with Keras, creating a SystemML backend for Keras.",smartshark_2_2,1373,systemml,"""[0.9982725381851196, 0.001727501512505114]"""
2772,231031,Native dataset support in parfor spark datapartition-execute,"This task aims for a deeper integration of Spark Datasets into SystemML. Consider the following example scenario, invoked through MLContext with X being a DataSet<Row>:

{code}
X = read(...)
parfor( i in 1:nrow(X) ) {
    Xi = X[i, ]
    v[i, 1] = ... some computation over Xi
}
{code}

Currently, we would convert the input dataset to binary block (1st shuffle) at API level and subsequently pass it into SystemML. For large data, we would then compile a single parfor data-partition execute job that slices row fragments, collects row fragments int partitions (2nd shuffle), and finally executes the parfor body per partition. 

Native dataset support would allow us to avoid these two shuffles and compute the entire parfor in a data-local manner. In detail, this involves the following extensions:
* API level: Keep lineage of input dataset leveraging our existing lineage mechanism in {{MatrixObject}}
* Parfor datapartition-execute: SYSTEMML-1367 already introduced the data-local processing for special cases (if ncol<=blocksize). Given the lineage, we can simply probe the input to datapartition-execute and, for row partitioning, use directly the dataset instead of the reblocked matrix rdd in a data-local manner. This does not just avoid the 2nd shuffle but due to lazy evaluation also the 1st shuffle if no operation other than parfor accesses X (except zipwithindex if no ids are passed in, as this transformation triggers computation)
* Cleanup: Prevent cleanup (unpersist) of lineage objects of type dataset as they are passed from outside.

",smartshark_2_2,2077,systemml,"""[0.9984657764434814, 0.0015342597616836429]"""
2773,231032,Compressed Linear Algebra,"This epic tracks the integration and extension of compressed linear algebra (CLA), as described in the following paper, into SystemML:

A. Elgohary, M. Boehm, P. J. Haas, F. R. Reiss, B. Reinwald: Compressed Linear Algebra for Large-Scale Machine Learning; In PVLDB, Volumne 9, Issue 12, 2016.
http://www.vldb.org/pvldb/vol9/p960-elgohary.pdf

Initially, we will integrate the existing prototype of compressed matrix blocks, compression planning, and operations over compressed matrix blocks (matrix-vector, vector-matrix, mmchain, tsmm, append, sum/rowSums/colSums, and scalar operations). CLA can be enabled via an optional configuration in SystemML-config, initially disabled by default. Furthermore, we aim to add support for a variety of operations and additional column encoding formats.

",smartshark_2_2,684,systemml,"""[0.9982091188430786, 0.001790890353731811]"""
2774,231033,Standalone log4j.properties needs to be simplified,"The standalone log4j.properties file at src/main/standalone/log4j.properties that is included in the standalone build of SystemML is currently massive, consisting of over 260 lines!

This should be changed to include a minimal configuration, such as a console appender with a log level of INFO.",smartshark_2_2,2,systemml,"""[0.9972793459892273, 0.0027206239756196737]"""
2775,231034,Remove fused sel+ operator,"The fused operator sel+ (select positive values) is applied for patterns like (X>0)*X and max(X,0) in order to eliminate unnecessary intermediates. It stems from a time when max was sparse-unsafe and hence inefficient over sparse data. However, meanwhile we mark scalar operators as conditionally sparse-safe depending on the given scalar constant c, which applies for max if c<=0. Hence, this sel+ operator is meanwhile completely useless and should be removed.

Furthermore, we should also generalize the rewrites to rewrite the selection of negative values (X<0)*X to min(X,0)",smartshark_2_2,924,systemml,"""[0.9984257221221924, 0.0015743289841338992]"""
2776,231035,Discuss and agree on the input data format for Deep Learning,"Examples of issues to be discussed:
1. Whether the labels need to be one-based or zero-based.
2. Should one apply z-scoring etc.
3. Input dataset and input labels needs to be seperated in two files and user provides the path to the directory.

As a part of this task, we can provide simple wrappers that allows the user to either generate or download frequently used datasets, transform it into the format needed by SystemML.",smartshark_2_2,449,systemml,"""[0.9983965754508972, 0.0016033906722441316]"""
2777,231036,Add `X.T` Transpose Attribute Method To PyDML,"Currently, PyDML allows one to transpose a matrix using {{X.transpose()}}, which is valid in Python (NumPy) as well.  However, it is much more common (for clear, understandable code) in Python/NumPy to simply use the transpose attribute, {{X.T}}.

We should support the latter {{X.T}} syntax in PyDML.

Example (PyDML):
{code}
X = matrix(""[1 2 3; 4 5 6; 7 8 9; 10 11 12]"")
Z = X.transpose()
{code}",smartshark_2_2,362,systemml,"""[0.9984330534934998, 0.0015669289277866483]"""
2778,231037,Package datagen scripts with standalone SystemML distribution,"The random data generation scripts in scripts/datagen are a useful way for people to become familiar with SystemML, assuming they generate data files in a manner that can easily be consumed by the algorithms being executed.

Right now the scripts/datagen directory is not packaged into the standalone distribution. We should probably include this directory.",smartshark_2_2,390,systemml,"""[0.9979637861251831, 0.00203617662191391]"""
2779,231038,Performance issue sparse-dense maxpooling backward,"The current implementation of sparse-dense maxpooling_backward uses for every p-q combination a full scan over the sparse row just to find the applicable range. We should instead flip this dense implementation to a ""real"" sparse implementation that scans the sparse input just once.",smartshark_2_2,837,systemml,"""[0.9982835054397583, 0.0017165133031085134]"""
2780,231039,Create Spark Shell docs for new MLContext,Update MLContext programming guide Spark Shell section to use new MLContext,smartshark_2_2,1430,systemml,"""[0.996246874332428, 0.0037531310226768255]"""
2781,231040,Convert roadmap to md file using page-md layout,"There is no need to have fancy html for a bullet list of roadmap items, also, having it in html makes it more difficult to community members to update it as it requires html and css skills.",smartshark_2_2,2173,systemml,"""[0.9965516328811646, 0.003448405535891652]"""
2782,231041,Incorporate sparsity in lower bound costs,"So far, the codegen optimizer computes the lower bound costs for cost-based pruning without proper awareness of sparsity exploiting operators. This led to suboptimal plans because sub areas of the search space are pruned too eagerly.",smartshark_2_2,997,systemml,"""[0.9976935982704163, 0.002306401962414384]"""
2783,231042,Create tiny sample notebook to import Caffe VGG model and do prediction.,"Once this notebook is ready and functional, it will demonstrate following
  - Caffe VGG model can be imported into SystemML
  - This model can be used to classify the image (prediction)

Assumptions:
  - Functionality to import Caffe model is reasonably working
  - Caffe is not installed on the system
  
  ",smartshark_2_2,2282,systemml,"""[0.9982903599739075, 0.0017096366500481963]"""
2784,231043,Support both ceil and ceiling built-in functions,"The builtin function ceil unnecessarily differs from R's ceiling, which might cause confusion. Hence, this task aims to rename ceil to ceiling.",smartshark_2_2,701,systemml,"""[0.9983800649642944, 0.0016199767123907804]"""
2785,231044,Describe release candidate build and deployment,"Describe the steps involved in building a release candidate and deploying the release candidate to Apache servers so that multiple project members can understand and help with the process.

Add these steps to ""Release Candidate Build and Deployment"" section of Release Process document (release-process.md). See
http://apache.github.io/incubator-systemml/release-process.html
",smartshark_2_2,1812,systemml,"""[0.9980599284172058, 0.0019400008022785187]"""
2786,231045,Create ParserFactory,"Code such as the following does not really follow a recognized design pattern:
{code}
AParserWrapper parser = AParserWrapper.createParser(false);
{code}

For ease of code maintenance, this should be replaced by something such as:
{code}
ParserWrapper parser = ParserFactory.createParser(false);
{code}
",smartshark_2_2,2101,systemml,"""[0.99801105260849, 0.0019889455288648605]"""
2787,231046,Distributions should NOT unpack directly on current folder,It's best practice that distributions unpack on it's own folder. SystemML distros should follow this pattern.,smartshark_2_2,34,systemml,"""[0.9980500936508179, 0.001949929166585207]"""
2788,231047,Performance features sparse-sparse leftindexing,"This task aims to improve the sparse-sparse left indexing performance, which includes:
* Shallow copy of sparse rows into empty target rows (0 aligned)
* Exact preallocation of sparse rows for coping rows into empty target rows (not 0 aligned)
* New set index range with input sparse row to avoid repeated binary search and shifting.",smartshark_2_2,899,systemml,"""[0.9982253909111023, 0.0017746116500347853]"""
2789,231048,Suboptimal codegen plans for ALS due to wrong cost model memoization,"Due to a change from 1 to 0-based ID sequences in SystemML, there is a codegen cost model memoization issue for special cases of the first cost vector per optimization problem. For ALS-CG over amazon this has severe performance implications causing a slowdown of more than 2x.",smartshark_2_2,1034,systemml,"""[0.9959431290626526, 0.004056802950799465]"""
2790,231049,Shutdown the thread pool of agg service,The problem is that the execution ofÂ paramserv dml script could not termintate as expected. And it hangs at the end of the execution because of the hanging thread pool which is not shutdown.,smartshark_2_2,1250,systemml,"""[0.5181295871734619, 0.4818703830242157]"""
2791,231050,Make debugger command parsing more strict,"Most debugger functions allow a short form and a long form of the command (e.g., s, step).  The parsing of the tokens should be more strict to prevent typos from being interpreted as commands.",smartshark_2_2,23,systemml,"""[0.9983683228492737, 0.0016316701658070087]"""
2792,231051,Implementation of Sobol quasi random sequence generator,"The quasi random sobol sequence generator, samples points uniformly from a unit hypercube or unit sphere.",smartshark_2_2,929,systemml,"""[0.9979313611984253, 0.0020686460193246603]"""
2793,231052,Design Thinking Audit and Guidance,"Perform usability testing of SystemML, ideally with assistance from IBM Design.
Audit the usability of the system from the perspective of several classes of users:
* Library user: Treats SystemML as a collection of canned algorithms
* Light customization user: Uses one of the provided algorithms as a starting point, but makes minor modifications to cover a specific use case
* Heavy customization user: Writes a DML script from scratch

For all types of users, the testing should cover the entire data science life cycle:
* Small-scale testing on a laptop
* Large-scale testing on a cluster
* Deployment in a production system

After gathering information from this testing, document issues and develop a plan for resolving these issues.",smartshark_2_2,3,systemml,"""[0.9982224106788635, 0.0017776146996766329]"""
2794,231053,Hadoop dependency version mismatch in standalone distribution,"The standalone distro has some version mismatch on the hadoop client jars distributed in the lib folder :

hadoop-auth-2.4.0.jar
hadoop-client-2.2.0.jar
hadoop-common-2.4.0.jar
hadoop-hdfs-2.4.0.jar
hadoop-mapreduce-client-app-2.2.0.jar
hadoop-mapreduce-client-common-2.4.1.jar
hadoop-mapreduce-client-core-2.4.1.jar
hadoop-mapreduce-client-jobclient-2.2.0.jar
hadoop-mapreduce-client-shuffle-2.2.0.jar
hadoop-yarn-api-2.4.1.jar
hadoop-yarn-client-2.4.1.jar
hadoop-yarn-common-2.4.1.jar
hadoop-yarn-server-common-2.4.1.jar

We should make them all 2.4.1",smartshark_2_2,44,systemml,"""[0.9923902153968811, 0.0076097906567156315]"""
2795,231054,Fix website NOTICE,"Project name in NOTICE is wrong.
Copyright date needs to be updated.",smartshark_2_2,1645,systemml,"""[0.9973827004432678, 0.0026172602083534002]"""
2796,231055,Rework the function block recompilation,It aims to reorganise the recompilation of update and aggregation function block for workers and agg service. The idea is to use a same reference of program instead of different one for workers and agg service in order to avoid the side effect.,smartshark_2_2,1253,systemml,"""[0.998376727104187, 0.001623299322091043]"""
2797,231056,Create JMLC example in docs using input parameters,"The JMLC documentation examples show input variables but not input parameters ($ values). The documentation should include an example of passing in one or more input parameters.
",smartshark_2_2,2272,systemml,"""[0.9983672499656677, 0.001632707193493843]"""
2798,231057,Fix value type promotion semantics of all scalar-scalar operations,"Right now our value type promotion semantics are inconsistent across matrices and scalars and across different types of binary scalar-scalar operations (relational, arithmetic, builtin). This task aims to fix this and establish the following value type promotion semantics:

* if any input type is string, both inputs are interpreted as strings
* else if any input type is double, both inputs are interpreted as doubles 
* else if any input type is long, both inputs are interpreted as long
* else if any input type is boolean, both inputs are interpreted as boolean

Furthermore, (1) boolean-strings casts are handled in a language-dpendent manner (for concatenation with +), and (2) boolean true/false is interpreted as 1/0 double/long (consistent with matrix relational operations).


Beside consistent semantics, this change also allows us to cleanup all value functions, which unnecessarily implement, for example, combinations of double-long which internally apply these promotion rules.",smartshark_2_2,2136,systemml,"""[0.968271017074585, 0.03172894939780235]"""
2799,231058,New order with multiple order-by columns,"There are multiple use cases, where users have to emulate an order with multiple order-by columns as follows (which works due to stable sort):
{code}
X = order(target=X, by=3, ...)
X = order(target=X, by=2, ...)
X = order(target=X, by=1, ...)
{code}

This causes unnecessary overhead by sorting the dataset in the number of order-by columns. Hence, this task aims to support an order function that allows both scalar and matrix {{by}} parameters.",smartshark_2_2,786,systemml,"""[0.9983941912651062, 0.0016058061737567186]"""
2800,231059,Create Python wrapper API for new MLContext API,"The new MLContext API needs a Python wrapper API to access MLContext from Python. This will allow Jupyter notebooks to use the new API.

The old Python API is located at src/main/java/org/apache/sysml/api/python/SystemML.py

The new MLContext API is located in the org.apache.sysml.api.mlcontext package.

Examples of using the new MLContext API from Spark Shell can be found at http://apache.github.io/incubator-systemml/spark-mlcontext-programming-guide.html
",smartshark_2_2,1490,systemml,"""[0.9981915354728699, 0.0018084653420373797]"""
2801,231060,Update lang ref for read/write function boolean params,"DML requires boolean as uppercase (TRUE/FALSE), so the read/write function parameters that are boolean (such as header) need to reflect this in the DML language ref.

At the same time, parsing JSON with TRUE/FALSE instead of true/false throws a parsing exception, so metadata files need to use true/false.

So, the docs need to reflect both of these (true/false for metadata, TRUE/FALSE for DML function parameters).
",smartshark_2_2,277,systemml,"""[0.9979060888290405, 0.002093964023515582]"""
2802,231061,Obtain cloud resource meta data,"This task aims to automatically collect meta data about available cloud resources, including node types, max number of node constraints, number of virtual cores, available memory, available local storage, and monetary cost.",smartshark_2_2,2264,systemml,"""[0.9982848763465881, 0.0017150382045656443]"""
2803,231062,Implement generator for the layers used in Lenet proto,Implemented in the PR https://github.com/niketanpansare/incubator-systemml/tree/d9e6efaf297b1a22fcbe3eb0b7f75f07e19969db/src/main/java/org/apache/sysml/api/dl/layer,smartshark_2_2,442,systemml,"""[0.9985089898109436, 0.001491017290391028]"""
2804,231063,Incomplete codegen candidate exploration,"The code generation candidate exploration via open-fuse-merge-close showed incomplete partial fusion plans for complex DAG structures. This task aims to resolve these issues, including (1) better debug output of memo table entries, (2) fixes of plan enumeration, and (3) avoid too eager pruning.",smartshark_2_2,578,systemml,"""[0.6461169123649597, 0.35388311743736267]"""
2805,231064,Add custom kernels for sparse binary elementwise operations,[~nakul02],smartshark_2_2,1560,systemml,"""[0.9982790946960449, 0.0017208599019795656]"""
2806,231065,PydmlSyntacticValidator has messages about doubles instead of floats,"The PydmlSyntacticValidator class contains references to the 'double' type in messages, such as:

helper.notifyErrorListeners(""The builtin function \'"" + functionName + ""\' accepts exactly 3 arguments (constant *double* value, number of rows, number of columns)"", fnName);

helper.notifyErrorListeners(""The builtin function \'"" + functionName + ""\' accepts exactly 1 argument (either string or *double* value)"", fnName);

According to PyDML Language Reference:

Two data types (matrix and scalar) and four value types (*float*, int, str, and bool) are supported.

I assume the user should always see messages about floats (rather than doubles) to avoid confusion, even if doubles are actually used in the actual implementation.",smartshark_2_2,208,systemml,"""[0.7626373171806335, 0.23736268281936646]"""
2807,231066,Update license and notice for main jar artifact,"The main project jar file that is built (such as systemml-0.10.0-incubating-SNAPSHOT.jar) contains classes from some other projects since they are set to the default scope (compile). These projects can be seen in META-INF/DEPENDENCIES:

{code}
// ------------------------------------------------------------------
// Transitive dependencies of this project determined from the
// maven pom organized by organization.
// ------------------------------------------------------------------

SystemML


From: 'abego Software GmbH, Germany' (http://abego-software.de)
  - abego TreeLayout Core (http://code.google.com/p/treelayout/) org.abego.treelayout:org.abego.treelayout.core:jar:1.0.1
    License: BSD 3-Clause ""New"" or ""Revised"" License (BSD-3-Clause)  (http://treelayout.googlecode.com/files/LICENSE.TXT)

From: 'an unknown organization'
  - Guava: Google Core Libraries for Java (http://code.google.com/p/guava-libraries/guava) com.google.guava:guava:bundle:14.0.1
    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)

From: 'ANTLR' (http://www.antlr.org)
  - ANTLR 4 Runtime Annotations (http://www.antlr.org/antlr4-annotations) org.antlr:antlr4-annotations:jar:4.3
    License: The BSD License  (http://www.antlr.org/license.html)
  - ANTLR 4 Runtime (http://www.antlr.org/antlr4-runtime) org.antlr:antlr4-runtime:jar:4.3
    License: The BSD License  (http://www.antlr.org/license.html)

From: 'The Apache Software Foundation' (http://www.apache.org/)
  - Apache Wink :: JSON4J (http://www.apache.org/wink/wink-json4j/) org.apache.wink:wink-json4j:jar:1.4
    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)
{code}

The LICENSE and NOTICE files (in META-INF) included in the jar most likely need to be updated to reference these libraries.",smartshark_2_2,420,systemml,"""[0.9983722567558289, 0.0016278171679005027]"""
2808,231067,Move MLContext API info from lang ref to MLContext prog guide,"The last section of the DML Language Reference currently pertains to the MLContext API. Since there is now an ""Spark MLContext Programming Guide"", the information should either be moved to the new location or if it is redundant it should be removed.
",smartshark_2_2,1487,systemml,"""[0.9973310232162476, 0.0026689895894378424]"""
2809,231068,Install CUDA along with CuDNN on Jenkins,"Please install:
1. CUDA 7.5
2. CuDNN v4 from http://developer.download.nvidia.com/compute/redist/cudnn/v4/cudnn-7.0-win-x64-v4.0-prod.zip
3. Download JCuda binaries version 0.7.5b and JCudnn version 0.7.5. Link: http://www.jcuda.org/downloads/downloads.html ... The library path for test-cases is set in AutomatedTestbase class: https://github.com/apache/incubator-systemml/pull/165/files#diff-bcda036e4c3ff62cb2648acbbd19f61aR113

Once these changes are in (and once GPU backend in feature-complete) we can set TEST_GPU flag in AutomatedTestbase class to true. Since it will take few weeks to make GPU backend feature-complete, this is a low-priority task

[~nakul02] [~akchin]",smartshark_2_2,2196,systemml,"""[0.9985823631286621, 0.0014176488621160388]"""
2810,231069,Add Sigmoid layer in nn library and Caffe2DML,http://caffe.berkeleyvision.org/tutorial/layers/sigmoid.html,smartshark_2_2,2158,systemml,"""[0.9984055161476135, 0.0015945314662531018]"""
2811,231070,Update community page contents,"Mailing list contents on the bottom of the page are ""copy + paste"" from top section around lists, JIRA, etc and need to be updated. 

There is also few issues with existing archive links",smartshark_2_2,1677,systemml,"""[0.9975860118865967, 0.002414020011201501]"""
2812,231071,Implementation of language extension,It aims to extend the parsing and validation at language level.,smartshark_2_2,1228,systemml,"""[0.9982717037200928, 0.0017282250337302685]"""
2813,231072,Update MLContext with max heavy hitters option for statistics,"Builds on SYSTEMML-1102.
Updates the MLContext (Scala & Python APIs) to be able to access the max heavy hitters options.",smartshark_2_2,1938,systemml,"""[0.9983841180801392, 0.0016159374499693513]"""
2814,231073,Add a new 2D depthwise transpose convolution layer,"A depthwise transpose convolution (1) applies a different filter to each unique group of M input channels separately, thus condensing each group of M input channels to 1 output channel, and (2) concatenates the results into a single volume with C/M output channels. This is in contrast to a regular 2D transpose convolution, in which all of the filters would be applied to all of the input channels at once.",smartshark_2_2,645,systemml,"""[0.9982962012290955, 0.001703773159533739]"""
2815,231074,Explore model-parallel constructs in DML,An example of such construct is providing access to the parameter server.,smartshark_2_2,696,systemml,"""[0.9983991980552673, 0.0016008240636438131]"""
2816,231075,Slow performance of external UDF invocation,"Due to repeated class loading and input/output parsing, we currently only reach about 75K/s invocations of external functions, which is very slow compared to >500K for dml-bodied functions.",smartshark_2_2,789,systemml,"""[0.9981306195259094, 0.0018693694146350026]"""
2817,231076,Improve Metadata File JSON Object Output Formatting  ,"The format of the metadata file JSON object should be updated to have the commas placed at the end of the lines, rather than at the beginning of the lines.

Currently, the metadata files written are formatted with the commas placed at the beginning of the lines, which is somewhat strange looking:

{code}
{ 
    ""data_type"": ""matrix""
    ,""value_type"": ""double""
    ,""rows"": 1
    ,""cols"": 2
    ,""nnz"": 2
    ,""format"": ""text""
    ,""description"": { ""author"": ""SystemML"" } 
}
{code}

We should place the commas at the end of the lines, as follows:

{code}
{ 
    ""data_type"": ""matrix"",
    ""value_type"": ""double"",
    ""rows"": 1,
    ""cols"": 2,
    ""nnz"": 2,
    ""format"": ""text"",
    ""description"": { ""author"": ""SystemML"" } 
}
{code}

This applies to matrix and scalar metadata.",smartshark_2_2,248,systemml,"""[0.9984075427055359, 0.0015924901235848665]"""
2818,231077,Notebook DML - Gather Requirements,"Questions:
- what should the Notebook achieve
- which technology is best suited (usability, easy/quick to implement, ...)
- what features should be supported (tab completion, syntax highlighting, algorithm reference pop-ups, ...)
- how long would which features take to implement
- ...",smartshark_2_2,1462,systemml,"""[0.9980900883674622, 0.0019099159399047494]"""
2819,231078,Store Explain And Statistics Outputs as Strings in MLContext,"Currenty, statistics and explain outputs are simply printed out, and can be easily lost within the other logging output.  It would be quite useful to simply have MLContext retain these outputs as Strings, and allow the user to retrieve them after execution, i.e. something like {{ml.getExplain()}} and {{ml.getStatistics()}}.

cc [~deron]",smartshark_2_2,1616,systemml,"""[0.9985033273696899, 0.0014966903254389763]"""
2820,231079,"Generalize Binary Operations to (vector, matrix) Operands","Currently, our engine supports matrix-vector binary operations.  For example, we make use of broadcasting to allow for binary operations between matrices and vectors that have a common size in each the row or column dimension.  However, we do not  support vector-matrix binary operations.

Works correctly --> op(matrix, vector):
{code}
a = rand(rows=4, cols=16)
b = rand(rows=1, cols=16)
out = a * b
print(toString(out))
{code}

Fails --> op(vector, matrix):
{code}
a = rand(rows=4, cols=16)
b = rand(rows=1, cols=16)
out = b * a
print(toString(out))
{code}

{code}
org.apache.sysml.parser.LanguageException: Invalid Parameters : ERROR: tests/broadcasting_test.dml -- line 4, column 7 -- Mismatch in dimensions for operation (b MULT a)
        at org.apache.sysml.parser.Expression.raiseValidateError(Expression.java:549)
        at org.apache.sysml.parser.BinaryExpression.checkAndSetDimensions(BinaryExpression.java:188)
        at org.apache.sysml.parser.BinaryExpression.validateExpression(BinaryExpression.java:141)
        at org.apache.sysml.parser.StatementBlock.validate(StatementBlock.java:592)
        at org.apache.sysml.parser.DMLTranslator.validateParseTree(DMLTranslator.java:141)
        at org.apache.sysml.api.DMLScript.execute(DMLScript.java:588)
        at org.apache.sysml.api.DMLScript.executeScript(DMLScript.java:350)
        at org.apache.sysml.api.DMLScript.main(DMLScript.java:211)
Exception in thread ""main"" org.apache.sysml.api.DMLException: org.apache.sysml.parser.LanguageException: Invalid Parameters : ERROR: tests/broadcasting_test.dml -- line 4, column 7 -- Mismatch in dimensions for operation (b MULT a)
        at org.apache.sysml.api.DMLScript.executeScript(DMLScript.java:364)
        at org.apache.sysml.api.DMLScript.main(DMLScript.java:211)
Caused by: org.apache.sysml.parser.LanguageException: Invalid Parameters : ERROR: tests/broadcasting_test.dml -- line 4, column 7 -- Mismatch in dimensions for operation (b MULT a)
        at org.apache.sysml.parser.Expression.raiseValidateError(Expression.java:549)
        at org.apache.sysml.parser.BinaryExpression.checkAndSetDimensions(BinaryExpression.java:188)
        at org.apache.sysml.parser.BinaryExpression.validateExpression(BinaryExpression.java:141)
        at org.apache.sysml.parser.StatementBlock.validate(StatementBlock.java:592)
        at org.apache.sysml.parser.DMLTranslator.validateParseTree(DMLTranslator.java:141)
        at org.apache.sysml.api.DMLScript.execute(DMLScript.java:588)
        at org.apache.sysml.api.DMLScript.executeScript(DMLScript.java:350)
        ... 1 more
{code}",smartshark_2_2,2223,systemml,"""[0.9984777569770813, 0.0015222519868984818]"""
2821,231080,Direct Compilation Option,The overhead of ML frameworks seam to outweigh the benefits they provide (https://flaredata.github.io/2016/09/30/q6-spark-vs-c-on-a-laptop.html). So is it possible to direct execution speed of hand written C on SystemML?,smartshark_2_2,1654,systemml,"""[0.998344898223877, 0.0016551086446270347]"""
2822,231081,Add MLContext test class for SVM scripts,"This jira plans to add a test class for all the operations and functions in {{l2-svm-predict.dml}}, {{l2-svm.dml}}, {{m-svm-predict.dml}} and {{m-svm.dml}} scripts.",smartshark_2_2,914,systemml,"""[0.99835205078125, 0.001648009056225419]"""
2823,231082,Extend the update strategy ASP,It aims to implement the update strategy of ASP.,smartshark_2_2,1243,systemml,"""[0.9982892870903015, 0.001710711163468659]"""
2824,231083,Add gradient matching functionality,As described in this paper : https://arxiv.org/pdf/1706.04859.pdf,smartshark_2_2,760,systemml,"""[0.9982490539550781, 0.0017509165918454528]"""
2825,231084,Simplify project structure (similar to Spark project),"Simplify the SystemML project structure:
- move contents of the ""system-ml"" folder to the top level folder (""systemml"")
- inside the top-level folder there should be the following sub directories:
{code}
    bin/       ... run scripts (*.sh, *.bat)
    build/     ... build config files from system-ml/src/assembly/*.xml
    conf/      ... project/runtime configuration
    docs/
    scripts/   ... DML scripts from system-ml/scripts/(*.dml, *.sh)
    src/
    target/    ... generated, not in GitHub repo
{code}

The intention is to make the folder structure more intuitive and bring it inline with how the Spark project is set up.

Related Task: [SYSML-281: Simplify running in standalone mode|http://stc-jira.svl.ibm.com:8080/browse/SYSML-281]",smartshark_2_2,194,systemml,"""[0.9965270161628723, 0.0034729624167084694]"""
2826,231085,Create MLContext base class for testing,"Currently several MLContext test classes contain boilerplate code run before and after tests. This code should be abstracted to an MLContextTestBase class which inherits from AutomatedTestBase.
",smartshark_2_2,473,systemml,"""[0.9972559809684753, 0.0027440341655164957]"""
2827,231086,Add download link for previous releases,"Add link at bottom of download page for Previous Releases:
http://archive.apache.org/dist/incubator/systemml/
",smartshark_2_2,1834,systemml,"""[0.9977268576622009, 0.0022731521166861057]"""
2828,231087,Performance sparse reshape w/o unnecessary reallocations,"Often reshape operations convert from n1 to n1*n2 columns. In this special case where we have a N:1 mapping of input to output rows, we should determine the number of nnz per output row exactly and allocate it once in order to reduce unnecessary garbage collection overhead.",smartshark_2_2,716,systemml,"""[0.9982075691223145, 0.0017924460116773844]"""
2829,231088,Performance issues codegen rowwise (column aggregation) w/ wide matrices,"On scenarios with wide matrices of millions of features, the codegen rowwise template shows performance issues due to unnecessary multi-threading which requires additional memory per thread for partial aggregation which leads to cache thrashing. We should similarly to the mmchain operator establish a threshold for maximum temporary results and fall back to sequential operations if this threshold is exceeded.",smartshark_2_2,565,systemml,"""[0.997635006904602, 0.002364989835768938]"""
2830,231089,Remove unused methods from project,The project contains a large number of unreferenced methods. Many of these can be removed since they are essentially dead code. This can assist in project maintenance.,smartshark_2_2,1929,systemml,"""[0.9970607161521912, 0.0029392847791314125]"""
2831,231090,Add Local Response Normalization layer in nn library and Caffe2DML,"http://caffe.berkeleyvision.org/tutorial/layers/lrn.html

AlexNet needs this layer",smartshark_2_2,1990,systemml,"""[0.9983878135681152, 0.001612159889191389]"""
2832,231091,Fix PyDML 1-Based Indexing Bug,"Currently, PyDML has a *bug* in which it uses 1-based indexing, as in R & DML, rather than the correct 0-based indexing that Python uses.  This simply aims to fix this issue in the Parser, and then update the few PyDML test scripts to use the correct 0-based indexing.",smartshark_2_2,367,systemml,"""[0.07329753786325455, 0.9267024397850037]"""
2833,231092,Enable removal of out and expected test data for functions.aggregate.ColSumTest,Cleanup generated src/test/scripts/functions/aggregate/ColSumTest.dmlt file.,smartshark_2_2,102,systemml,"""[0.9980340600013733, 0.001965987728908658]"""
2834,231093,Release: Python artifact missing jar,"The python artifact (e.g., systemml-0.12.0-incubating-python.tgz) created for release is missing the jar file.  This is due to jar file name not matching when building a release vs. building a snapshot.",smartshark_2_2,1813,systemml,"""[0.9973695278167725, 0.002630397444590926]"""
2835,231094,Inconsistent namespace naming depending on OS,"The current OS-specific namespace naming by absolute/relative file paths creates problems for second order functions that aim to load functions by string. Since this is unnecessary, we now use consistent unix notation.",smartshark_2_2,1263,systemml,"""[0.9980288147926331, 0.0019711109343916178]"""
2836,231095,Logical namespace handling user-defined functions,"At script level functions might have logical namespace names such as {{foo::bar()}}, where foo is the namespace name, and bar is the function name. To handle namespace conflicts, SYSTEMML-631 internally replaced the logical namespaces with filenames. For reasons such as improved statistics output and the handling of namespace functions in the recently introduced {{eval}} function (SYSTEMML-2077), it would be good to keep the logical namespace as well.

This task aims to (1) extend the {{FunctionStatementBlock}} and {{FunctionProgramBlock}} data structures to keep the logical namespace name, (2) extend the parser and compiler accordingly, and (3) modify the statistics maintenance to use the function key (i.e., concatenation of logical namespace and function name) as the opcode.",smartshark_2_2,1089,systemml,"""[0.9978480339050293, 0.002151992404833436]"""
2837,231096,Add SystemML.jar to root of bin artifacts,"In terms of documentation and understandability, having SystemML.jar at the root of the -bin.tgz/-bin.zip artifacts is useful. It is common practice on our project to describe using ""SystemML.jar"" (such as in the 'distrib' artifacts), but in the -bin artifacts, there is no ""SystemML.jar"" at the root, and the SystemML jar is located in the lib directory with a name such as lib/systemml-1.0.0-SNAPSHOT.jar.

Therefore, add SystemML.jar to the root of the -bin artifacts (and also keep the existing versioned jar in the lib directory).
",smartshark_2_2,2360,systemml,"""[0.9973334074020386, 0.0026666601188480854]"""
2838,231097,Java Scala MLContext API accept SparkSession,"Create MLContext constructor to accept the newer SparkSession, since currently the constructors only take the older SparkContext and JavaSparkContext. Update any other API methods if appropriate.

Also see SYSTEMML-1346",smartshark_2_2,2103,systemml,"""[0.9985207915306091, 0.0014792172005400062]"""
2839,231098,Add support for matrix-vector GPU axpy operation,"Here's a short snippet to invoke the axpy rewrite and reproduce the issue on GPU:
{code}
n = 100
m = 10
#a = 2
a = as.scalar(rand(rows=1, cols=1))
x = rand(rows=n, cols=m)
y = rand(rows=1, cols=m)
z = x + a*y  # broadcasting
if (1==1){}
print(sum(z))
{code}",smartshark_2_2,2106,systemml,"""[0.9976493716239929, 0.0023505946155637503]"""
2840,231099,Misc performance issues codegen templates,"This task covers the following performance improvements:

* Avoid multi-threaded operations for small inputs (all templates)
* MAgg: select shared sparse-safe input as driver
* Row: used flipped outer computation depending on size",smartshark_2_2,605,systemml,"""[0.9981347322463989, 0.0018651924328878522]"""
2841,231100,Compilation failure on inferring size of reshapes w/ zero rows/columns,"{code}
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.sysml.hops.ReorgOp.inferOutputCharacteristics(ReorgOp.java:469)
	at org.apache.sysml.hops.Hop.computeMemEstimate(Hop.java:626)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:717)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:716)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:716)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:716)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:716)
	at org.apache.sysml.hops.Hop.refreshMemEstimates(Hop.java:716)
	at org.apache.sysml.hops.recompile.Recompiler.recompile(Recompiler.java:358)
	at org.apache.sysml.hops.recompile.Recompiler.recompileHopsDag(Recompiler.java:162)
	at org.apache.sysml.hops.recompile.Recompiler.rRecompileProgramBlock(Recompiler.java:690)
	at org.apache.sysml.hops.recompile.Recompiler.rRecompileProgramBlock(Recompiler.java:668)
	at org.apache.sysml.hops.recompile.Recompiler.rRecompileProgramBlock(Recompiler.java:649)
	at org.apache.sysml.hops.recompile.Recompiler.recompileProgramBlockHierarchy(Recompiler.java:424)
	at org.apache.sysml.runtime.controlprogram.FunctionProgramBlock.execute(FunctionProgramBlock.java:99)
{code}",smartshark_2_2,1259,systemml,"""[0.06875879317522049, 0.9312412142753601]"""
2842,231101,Add DISCLAIMER to additional packages,"needs to be added to:
system-ml-*-incubating-standalone.tar.gz
system-ml-*-incubating-standalone.zip
system-ml-*-incubating.tar.gz
system-ml-*-incubating.zip
",smartshark_2_2,254,systemml,"""[0.9973715543746948, 0.0026284444611519575]"""
2843,231102,Rename python package from systemml-*-python.tgz to systemml-*-python.tar.gz,"I encountered this issue because pypi has migrated to a new process: https://packaging.python.org/guides/migrating-to-pypi-org/#uploading

As noted in the above document, the recommended way to upload python packages to pypi is now via `twine`. However, if we use `twine` with our current package naming scheme (i.e. tgz), then it complains `ValueError: Unknown distribution format: 'systemml-0.15.0-python.tgz'`. Hence, I would recommend to use suffix `tar.gz` in our subsequent releases. This way we also are compatible with default package naming convention of pypi: `tar.gz`.

[~acs_s] [~gweidner] [~deron] [~dusenberrymw] [~reinwald] Suggestions ? Any takers ?",smartshark_2_2,686,systemml,"""[0.9982427358627319, 0.001757327583618462]"""
2844,231103,Runtime refactoring core matrix block library,"Pull the local (non-distributed) linear algebra components of SystemML into a separate package. Define a proper object-oriented Java API for creating and manipulating local matrices. Document this API. Refactor all tests of local linear algebra functionality so that those tests use the new API. Refactor the distributed linear algebra operators (both Spark and Hadoop map-reduce) to use the new APIs for local linear algebra. 

*Overall Refactoring Plan*
The MatrixBlock class will be the core locus of refactoring. The file is over 6000 lines long, has dependencies on the HOPS and LOPS layers, and contains a lot of sparse matrix code that really ought to be in SparseBlock. Even if itâs modified in place, MatrixBlock will bear little resemblance to its current form after the refactoring is completed. I recommend setting aside the current MatrixBlock class and creating new classes with equivalent functionality by copying appropriate blocks of code from the old class. 

Major changes to make relative to MatrixBlock:
* We should create a new DenseMatrixBlock class that only covers dense linear algebra.
* Sparse-specific code should be moved into the SparseBlock class. 
* Common functionality across dense and sparse should go into the MatrixValue superclass.
* There should be a new class with a name like âMatrixâ (weâll need one anyway to serve as the public API) that contains a pointer to a MatrixValue and can switch between different representations. Ideally this class should be designed so that, in the future, it can serve as a matrix ADT that will wrap both local and distributed linear algebra.
* Several fields (maxrow, maxcolumn, numGroups, and various estimates of future numbers of nonzeros) are used for stashing data that is only for internal SystemML use. Either put these into a different data structure or provide a generic mechanism for tagging a matrix block with additional application-specific data.
* Clean up and simplify the multiple different initialization methods (different variants of the constructors and the methods init() and reset()). There should be one canonical method for each major type of initialization. Other methods that are shortcuts (i.e. reset() with no arguments) should call the canonical method internally.
* Consider refactoring the variants of ternaryOperations() that support ctable() into something simpler that is called ctable() â perhaps a Java API that can take null values for the optional arguments. 

Other changes outside MatrixBlock:
* The matrix classes currently depend on Hadoop I/O classes like Writable and DataInputBuffer. A local linear algebra library really shoudnât require Hadoop. I/O methods that use Hadoop APIs should be factored out into a separate package. In paticular, MatrixValue needs to be separated from Hadoopâs WritableComparable API.
* The contents of the following packages need to move to the new library: sysml.runtime.functionobjects and sysml.runtime.matrix.operators
* The library will need local input and output functions. I havenât found suitable functions yet, but they may be hidden somewhere; in that case the existing functions should be adjacent to the other local linear algebra code.
* Utility functions under classes in sysml.runtime.util will need to be replicated.
* The more obscure subclasses of MatrixValue (MatrixCell, WeightedCell, etc.) do NOT need to be moved over.
",smartshark_2_2,2190,systemml,"""[0.9982731342315674, 0.0017268650699406862]"""
2845,231104,IPA repetitions until fixpoint,This task aims to increase the number of IPA repetitions from 2 to 3 with an explicit check for fixpoint conditions where the size information for function calls does not change anymore in order to reduce overhead for scenarios with simple function call patterns.,smartshark_2_2,804,systemml,"""[0.9785975813865662, 0.021402429789304733]"""
2846,231105,NullPointerException on a valid Word file,"On the attached file, which opens fine in Word, the Tika parser throws the following error:

java.lang.NullPointerException: 
	at org.apache.poi.hwpf.model.ListTables.getLevel:141
	at org.apache.poi.hwpf.usermodel.Paragraph.newParagraph:125
	at org.apache.poi.hwpf.usermodel.Range.getParagraph:766
	at org.apache.tika.parser.microsoft.WordExtractor.parse:178
	at org.apache.tika.parser.microsoft.OfficeParser.parse:169
	at org.apache.tika.parser.microsoft.OfficeParser.parse:130",smartshark_2_2,2414,tika,"""[0.07732685655355453, 0.9226731061935425]"""
2847,231106,Backwards Compatibility for Metadata.DATE is Incorrect,"Metadata.DATE was always somewhat ambiguous, but during the consolidation in TIKA-930 it was incorrectly assumed that most parsers used it as a creation date.

Metadata.DATE needs to instead be part of the TikaCoreProperties.MODIFIED composite property.",smartshark_2_2,876,tika,"""[0.2941869795322418, 0.7058130502700806]"""
2848,231107,WordParser fails on many Word files,"WordParser fail on some word files. A negative value is sent to substring

",smartshark_2_2,1237,tika,"""[0.0796160027384758, 0.9203839898109436]"""
2849,231108,NPE in JDBCTableReader,"NPE when there is a null String in a SQLite DB.

Caused by: java.lang.NullPointerException
	at org.apache.tika.parser.jdbc.JDBCTableReader.addAllCharacters(JDBCTableReader.java:252)
	at org.apache.tika.parser.jdbc.JDBCTableReader.handleCell(JDBCTableReader.java:135)
	at org.apache.tika.parser.jdbc.JDBCTableReader.nextRow(JDBCTableReader.java:95)
	at org.apache.tika.parser.jdbc.AbstractDBParser.parse(AbstractDBParser.java:90)
	at org.apache.tika.parser.jdbc.SQLite3Parser.parse(SQLite3Parser.java:78)",smartshark_2_2,2016,tika,"""[0.06372509151697159, 0.9362748861312866]"""
2850,231109,RuntimeException when parsing word (.doc) documents. Works in Tika 1.4 but not 1.5,"Parsing the attached document works in Tika 1.4, but not in Tika 1.5. See output below. However, using Tika 1.4 is not a proper temporary solution as it leaves tons of special characters and functions in the output. See my post on SO: https://stackoverflow.com/questions/21929040

{noformat}
$ java -jar tika-app-1.4.jar Ansvarsvakt\ rutine01.06.11.doc > /dev/null
$
$ java -jar tika-app-1.5.jar Ansvarsvakt\ rutine01.06.11.doc > /dev/null 
Exception in thread ""main"" org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.OfficeParser@193936e1
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:142)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:418)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:112)
Caused by: java.lang.IllegalArgumentException: This paragraph is not the first one in the table
        at org.apache.poi.hwpf.usermodel.Range.getTable(Range.java:932)
        at org.apache.tika.parser.microsoft.WordExtractor.handleParagraph(WordExtractor.java:188)
        at org.apache.tika.parser.microsoft.WordExtractor.handleHeaderFooter(WordExtractor.java:172)
        at org.apache.tika.parser.microsoft.WordExtractor.parse(WordExtractor.java:98)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:199)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:167)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
{noformat}

Sidenote: If I open the document in Abiword and just click ctrl+s to save the document (with no changes), Tika 1.5 parses it just fine.",smartshark_2_2,2228,tika,"""[0.1558372676372528, 0.8441627621650696]"""
2851,231110,Missing spaces on html parsing,"On parsing such html code:

text<p>more<br>yet<select><option>city1<option>city2</select>

resulting text is:

textmore
yetcity1city2

But must be:

text
more
yet city1 city2

Code sample:

import java.io.*;
import org.apache.tika.metadata.*;
import org.apache.tika.parser.*;

public class test {

   public static void main(String[] args) throws Exception {
      Metadata metadata = new Metadata();
      metadata.set(Metadata.CONTENT_TYPE, ""text/html"");
      String content = ""text<p>more<br>yet<select><option>city1<option>city2</select>"";

      InputStream in = new ByteArrayInputStream(content.getBytes(""UTF-8""));
      AutoDetectParser parser = new AutoDetectParser();
      Reader reader = new ParsingReader(parser, in, metadata, new ParseContext());
      char[] buf = new char[10000];
      int len;
      StringBuffer text = new StringBuffer();
      while((len = reader.read(buf)) > 0) {
         text.append(buf, 0, len);
      }
      System.out.print(text);
   }
}
",smartshark_2_2,426,tika,"""[0.14500226080417633, 0.8549976944923401]"""
2852,231111,Parser fails on files that parsed with v0.7,"I am including 3 files. Two of these (Employee Application.pdf; pertoc.html) parsed without error under Tika 0.7. The third (I-9.pdf) fails under both versions and is included more as another test case for PDF parse errors.

Perltoc.html fails with a Null Pointer exception in org.apache.tika.parser.html.HtmlParser@53786b79. Employee Application.pdf fails in  org.apache.tika.parser.pdf.PDFParser@39fb9fb3, also with a Null Pointer exception. Both cases repro with tikaapp.jar built from the 0.8 release.

I-9 (which also fails under 0.7, but for different reasons) also fails with a Null Pointer exception at the same location as Employee Application.pdf.

Looks like I have to create the bug before I can attach files...",smartshark_2_2,512,tika,"""[0.15132319927215576, 0.8486767411231995]"""
2853,231112,Different NullPointerException on a valid Excel file,"On the attached Excel file, which opens fine in Excel, the Tika parser throws the following error:

java.lang.NullPointerException
	at org.apache.poi.xssf.usermodel.XSSFDrawing.<init>(XSSFDrawing.java:89)
	at org.apache.poi.xssf.usermodel.XSSFDrawing.<init>(XSSFDrawing.java:97)
	at org.apache.poi.xssf.eventusermodel.XSSFReader$SheetIterator.getShapes(XSSFReader.java:308)
	at org.apache.tika.parser.microsoft.ooxml.XSSFExcelExtractorDecorator.buildXHTML(XSSFExcelExtractorDecorator.java:152)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:109)
	at org.apache.tika.parser.microsoft.ooxml.XSSFExcelExtractorDecorator.getXHTML(XSSFExcelExtractorDecorator.java:97)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:112)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)",smartshark_2_2,2413,tika,"""[0.0837293192744255, 0.9162706136703491]"""
2854,231113,WordParser fails on some Word files,"WordParser fail on some word files. A negative value is sent to TextPiece.substring in POI for some corner case in the algorithm.

",smartshark_2_2,1238,tika,"""[0.07549714297056198, 0.9245028495788574]"""
2855,231114,TaggedIOException can be passed non Serializable objects,"TaggedIOException can contain tags. It's used to contain TaggedInputStream which isn't serializable.

This can cause the ForkServer to fail when trying to report issues. See TIKA-827

2 solutions
* make the tag transient
* replace the InputStream instance in the tag by a serializable object specific to the input stream.

I opt for the first one as I really don't think we need more complexity",smartshark_2_2,663,tika,"""[0.37993985414505005, 0.62006014585495]"""
2856,231115,NPE thrown with password protected Pages file,"When trying to view a password-protected Pages file in Tika GUI, you get an NPE:

org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.iwork.IWorkPackageParser@30583058
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)
	at org.apache.tika.gui.ParsingTransferHandler.importFiles(ParsingTransferHandler.java:94)
	at org.apache.tika.gui.ParsingTransferHandler.importData(ParsingTransferHandler.java:77)
	at javax.swing.TransferHandler.importData(TransferHandler.java:756)
	at javax.swing.TransferHandler$DropHandler.drop(TransferHandler.java:1479)
	at java.awt.dnd.DropTarget.drop(DropTarget.java:445)
	at javax.swing.TransferHandler$SwingDropTarget.drop(TransferHandler.java:1204)
	at sun.awt.dnd.SunDropTargetContextPeer.processDropMessage(SunDropTargetContextPeer.java:531)
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchDropEvent(SunDropTargetContextPeer.java:844)
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchEvent(SunDropTargetContextPeer.java:768)
	at sun.awt.dnd.SunDropTargetEvent.dispatch(SunDropTargetEvent.java:42)
	at java.awt.Component.dispatchEventImpl(Component.java:4498)
	at java.awt.Container.dispatchEventImpl(Container.java:2110)
	at java.awt.Component.dispatchEvent(Component.java:4471)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4588)
	at java.awt.LightweightDispatcher.processDropTargetEvent(Container.java:4323)
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4174)
	at java.awt.Container.dispatchEventImpl(Container.java:2096)
	at java.awt.Window.dispatchEventImpl(Window.java:2490)
	at java.awt.Component.dispatchEvent(Component.java:4471)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:610)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:280)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:195)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:185)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:180)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:172)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:133)
Caused by: java.lang.NullPointerException
	at org.apache.tika.parser.iwork.IWorkPackageParser$IWORKDocumentType.detectType(IWorkPackageParser.java:125)
	at org.apache.tika.parser.iwork.IWorkPackageParser$IWORKDocumentType.access$000(IWorkPackageParser.java:71)
	at org.apache.tika.parser.iwork.IWorkPackageParser.parse(IWorkPackageParser.java:166)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 30 more

I tried viewing the contents in 7-zip, but it tells me it can't understand the compression format.",smartshark_2_2,1334,tika,"""[0.08532951772212982, 0.9146705269813538]"""
2857,231116,Lotus Notes .eml Files Not Always Detected Properly,"Lotus Notes doesn't guarantee that any of the magic matches defined under mime-type message/rfc822 will be at the start of the file.

Instead many X-Notes-Item headers may precede those headers.",smartshark_2_2,862,tika,"""[0.14448460936546326, 0.8555153608322144]"""
2858,231117,PDF form data isn't included in extracted content.,"When extracting content from PDFs, PDF form data isn't extracted. 

The following code extracts this data via PDF box, but it seems like something Tika should be doing.

PDDocumentCatalog docCatalog = load.getDocumentCatalog();
if (docCatalog != null) {
  PDAcroForm acroForm = docCatalog.getAcroForm();
  if (acroForm != null) {
	@SuppressWarnings(""unchecked"")
	List<PDField> fields = acroForm.getFields();
	if (fields != null && fields.size() > 0) {
	  documentContent.append("" "");
	  for (PDField field : fields) {
		if (field.getValue()!=null) {
		  documentContent.append(field.getValue());
		  documentContent.append("" "");
		}
	  }
	}
  }
}
",smartshark_2_2,1010,tika,"""[0.2639819383621216, 0.7360181212425232]"""
2859,231118,encrypted pdf files aren't handled properly,"While I was working on extracting full texts out of a bunch of pdf documents, I realized an odd behaviour of Tika when processing encrypted documents (those documents that restrict the execution of specific actions, e.g. editing or printing). To extract content from an encrypted pdf document you do not have to decrypt the document in every case. For instance, when creating an (encrypted) pdf document the author can decide to allow content extraction without the need of providing a password. Unfortunately, Tika's pdf parser isn't aware of this at the moment. Therefore, I suggest a minor change inside the parse method in class org.apache.tika.parser.pdf.PDFParser by introducing an additional check (""is copying allowed"") before trying to decrypt the document.

To be more precise, I'll provide a code snippet:

public void parse(...) throws ... {
  PDDocument pdfDocument = PDDocument.load(stream);
  try {
    //decrypt document only if copying is not allowed
    if (!pdfDocument.getCurrentAccessPermission().canExtractContent()) {
      if (pdfDocument.isEncrypted()) {
        try {
          pdfDocument.decrypt("""");
        } catch (Exception e) {
          // Ignore
        }
      }
    }
    ...

Another solution to this problem would be to eliminate the ""isEncrypted"" check since PDFBox seems to handle the extraction of content out of encrypted documents correctly (and throws an IOException in case of failure).",smartshark_2_2,216,tika,"""[0.17613713443279266, 0.8238628506660461]"""
2860,231119,System property added while catching exception on parsing PDF encrypted doc,"I'm using Tika 1.7. I'm parsing an encrypted PDF document which raise an exception. So far, so good.

My concern is that after that I have a new System property set {{sun.font.CFontManager}}. 

Code to reproduce the error:

{code:java}
@Test
public void testSystem() {
    Properties props = System.getProperties();
    assertThat(props.get(""sun.font.fontmanager""), nullValue());
    try {
        tika().parseToString(new URL(""https://github.com/elasticsearch/elasticsearch-mapper-attachments/raw/master/src/test/resources/org/elasticsearch/index/mapper/xcontent/encrypted.pdf""));
    } catch (Throwable e) {
    }
    assertThat(props.get(""sun.font.fontmanager""), nullValue());
}
{code}


With Tika 1.7:

{code}
[2015-02-11 16:43:36,166][INFO ][org.apache.pdfbox.pdfparser.PDFParser] Document is encrypted
[2015-02-11 16:43:36,837][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,837][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,838][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,838][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,839][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,840][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,840][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,841][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,841][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,842][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException

java.lang.AssertionError: 
Expected: null
     but: was ""sun.font.CFontManager""
 <Click to see difference>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.elasticsearch.plugin.mapper.attachments.test.TikaSystemTest.testSystem(TikaSystemTest.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{code}


With Tika 1.6:

{code}
[2015-02-11 16:38:42,922][INFO ][org.apache.pdfbox.pdfparser.PDFParser] Document is encrypted
{code}

Note also that it logs a lot of errors which was not the case in Tika 1.6.",smartshark_2_2,1274,tika,"""[0.18263787031173706, 0.8173621296882629]"""
2861,231120,Out of memory exception when parsing TTF file,"   When parsing attached file using tika-app-1.4.jar, CPU usage is high and it never seems to finish.

When parsing using attached java code, I get an out of memory exception.

Let me know what other information I can provide.

Thank you!",smartshark_2_2,1079,tika,"""[0.8888189792633057, 0.11118096858263016]"""
2862,231121,TaggedIOException from ZipException on a valid PowerPoint file,"On the following PPT file, which PowerPoint opens and displays fine:

https://dl.dropboxusercontent.com/u/92341073/ny2004b.ppt

The Tika parser throws the following exception:

org.apache.tika.io.TaggedIOException: incorrect data check
	at org.apache.tika.io.TaggedInputStream.handleIOException(TaggedInputStream.java:133)
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:103)
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:99)
	at java.io.BufferedInputStream.read1(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.FilterInputStream.read(Unknown Source)
	at java.nio.file.Files.copy(Unknown Source)
	at java.nio.file.Files.copy(Unknown Source)
	at org.apache.tika.io.TikaInputStream.getPath(TikaInputStream.java:587)
	at org.apache.tika.io.TikaInputStream.getFile(TikaInputStream.java:615)
	at org.apache.tika.parser.microsoft.POIFSContainerDetector.getTopLevelNames(POIFSContainerDetector.java:377)
	at org.apache.tika.parser.microsoft.POIFSContainerDetector.detect(POIFSContainerDetector.java:443)
	at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:112)
	at org.apache.tika.parser.DelegatingParser.parse(DelegatingParser.java:72)
	at org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor.parseEmbedded(ParsingEmbeddedDocumentExtractor.java:102)
	at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedResource(AbstractPOIFSExtractor.java:140)
	at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedResource(AbstractPOIFSExtractor.java:116)
	at org.apache.tika.parser.microsoft.HSLFExtractor.handleSlideEmbeddedResources(HSLFExtractor.java:368)
	at org.apache.tika.parser.microsoft.HSLFExtractor.parse(HSLFExtractor.java:138)
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:149)
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:117)
	at gov.nih.niaid.fscanner.Extract.ExtractContents(Extract.java:62)
	at gov.nih.niaid.temp.Main.main(Main.java:60)
Caused by: java.util.zip.ZipException: incorrect data check
	at java.util.zip.InflaterInputStream.read(Unknown Source)
	at org.apache.poi.util.BoundedInputStream.read(BoundedInputStream.java:121)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read1(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:99)
	... 22 more",smartshark_2_2,2254,tika,"""[0.10842075198888779, 0.891579270362854]"""
2863,231122,Tika client cannot extract files from embedded archive formats,"Â 

This may be related toÂ TIKA-2395. When trying to extract the files fromÂ 

tika/tika-parsers/src/test/resources/test-documents/test-documents.tgzÂ 

Â 

% coursier launch org.apache.tika:tika-app:1.17 --main org.apache.tika.cli.TikaCLI -- --extract test-documents.tgz

I see the exception:

Â 

Exception in thread ""main"" org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.pkg.CompressorParser@62628e78

at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:286)

at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)

at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143)

at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:205)

at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:486)

at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:145)

at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.base/java.lang.reflect.Method.invoke(Method.java:564)

at coursier.cli.qR.a(Unknown Source)

at coursier.cli.qQ.j(Unknown Source)

at coursier.cli.qW.a(Unknown Source)

at d.h.a.c(Unknown Source)

at b.b.c_(Unknown Source)

at d.b.d.E.g(Unknown Source)

at d.b.e.aW.g(Unknown Source)

at d.b.f.b.aa.a(Unknown Source)

at coursier.cli.qQ.b(Unknown Source)

at coursier.cli.Q.b(Unknown Source)

at b.J.c_(Unknown Source)

at d.F.h(Unknown Source)

at b.F.a(Unknown Source)

at coursier.cli.Coursier.main(Unknown Source)

at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.base/java.lang.reflect.Method.invoke(Method.java:564)

at coursier.Bootstrap.main(Bootstrap.java:428)

Caused by: java.io.IOException: mark/reset not supported

at java.base/java.io.InputStream.reset(InputStream.java:474)

at org.apache.tika.parser.microsoft.POIFSContainerDetector.detect(POIFSContainerDetector.java:444)

at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:84)

at org.apache.tika.cli.TikaCLI$FileEmbeddedDocumentExtractor.parseEmbedded(TikaCLI.java:1045)

at org.apache.tika.parser.pkg.CompressorParser.parse(CompressorParser.java:222)

at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)

... 28 more

Â 

However, I can browse the document fine using:

Â 

% coursier launch org.apache.tika:tika-app:1.17 --main org.apache.tika.cli.TikaCLI -- test-documents.tgz

Â 

This issue affects:Â test-documents.rar,Â test-documents.tar.Z,Â test-documents.tbz2, andÂ test-documents.tgz

But itÂ does not affect test-documents.7z,Â test-documents.cab,Â test-documents.ddf,Â test-documents.dmg,Â test-documents.tar, orÂ test-documents.zip

Â 

Â 

Â This makes me suspect that it has something to do with extracting files from packages that are embedded in other archive parsers.

Â ",smartshark_2_2,2816,tika,"""[0.3332107365131378, 0.6667892336845398]"""
2864,231123,HTMLParser gets an early </body> event,"I am using the IdentityMapper in the HTMLparser with this simple document:

{code}
<html><head><title> my title </title>
</head>
<body>
<frameset rows=\""20,*\""> 
<frame src=\""top.html\"">
</frame>
<frameset cols=\""20,*\"">
<frame src=\""left.html\"">
</frame>
<frame src=\""invalid.html\""/>
</frame>
<frame src=\""right.html\"">
</frame>
</frameset>
</frameset>
</body></html>
{code}

Strangely the HTMLHandler is getting a call to endElement on the body *BEFORE*  we reach frameset. As a result the variable bodylevel is decremented back to 0 and the remaining entities are ignored due to the logic implemented in HTMLHandler.

Any idea?


",smartshark_2_2,394,tika,"""[0.10665702074766159, 0.8933430314064026]"""
2865,231124,"OutlookExtractor ""buffer underrun"" when parsing .msg with embedded .msg","When parsing certain .msg files containing certain attachments (e.g. other .msg files), I get this error:

{noformat}
...
Caused by: org.apache.poi.util.LittleEndian$BufferUnderrunException: buffer underrun
        at org.apache.poi.util.LittleEndian.readInt(LittleEndian.java:662)
        at org.apache.poi.hmef.CompressedRTF.decompress(CompressedRTF.java:73)
        at org.apache.poi.util.LZWDecompresser.decompress(LZWDecompresser.java:81)
        at org.apache.poi.hmef.attribute.MAPIRtfAttribute.<init>(MAPIRtfAttribute.java:42)
        at org.apache.tika.parser.microsoft.OutlookExtractor.parse(OutlookExtractor.java:270)
...
{noformat}

I think the issue is with {{MAPIRtfAttribute}} not liking it when receiving an empty byte array from {{OutlookExtractor}}.  I was able to eliminate the error at around line 269 of {{OutlookExtractor}} with Tika 1.16 code (or around line 322 with Tika 1.17) with the following:

{code:java}
            //--- START FIX ---
            ByteChunk chunk = (ByteChunk) rtfChunk;
            if (chunk != null && chunk.getValue() != null 
                    && chunk.getValue().length > 0 && !doneBody) {
                //ByteChunk chunk = (ByteChunk) rtfChunk;
            //--- END FIX ---
{code}

I am not sure if that is a real fix or more should be done than just getting rid of the error to make sure all is extracted properly from all files.

I cannot share the sample file I have to test since it was given to me as sensitive content and I could not recreate a faulty msg file.

Thanks",smartshark_2_2,1687,tika,"""[0.1044214516878128, 0.8955785036087036]"""
2866,231125,"NullPointerException in tika-app, parsing PDF content","I try to extract text from some pdf files with the tika app. In version 0.10 the error 
ERROR - Error: Could not parse predefined CMAP file for '--UCS2'
is printed on the command line, but text extraction works and is correct.

In version 1.0 I get the same error message on the command line, but also receive an exception and no text is extracted:
org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.pdf.PDFParser@62bc36ff
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)
	at org.apache.tika.gui.TikaGUI.actionPerformed(TikaGUI.java:238)
	at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:1995)
	at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2318)
	at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:387)
	at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:242)
	at javax.swing.AbstractButton.doClick(AbstractButton.java:357)
	at javax.swing.plaf.basic.BasicMenuItemUI.doClick(BasicMenuItemUI.java:809)
	at javax.swing.plaf.basic.BasicMenuItemUI$Handler.mouseReleased(BasicMenuItemUI.java:850)
	at java.awt.Component.processMouseEvent(Component.java:6288)
	at javax.swing.JComponent.processMouseEvent(JComponent.java:3267)
	at java.awt.Component.processEvent(Component.java:6053)
	at java.awt.Container.processEvent(Container.java:2041)
	at java.awt.Component.dispatchEventImpl(Component.java:4651)
	at java.awt.Container.dispatchEventImpl(Container.java:2099)
	at java.awt.Component.dispatchEvent(Component.java:4481)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4577)
	at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4238)
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4168)
	at java.awt.Container.dispatchEventImpl(Container.java:2085)
	at java.awt.Window.dispatchEventImpl(Window.java:2478)
	at java.awt.Component.dispatchEvent(Component.java:4481)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:643)
	at java.awt.EventQueue.access$000(EventQueue.java:84)
	at java.awt.EventQueue$1.run(EventQueue.java:602)
	at java.awt.EventQueue$1.run(EventQueue.java:600)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:98)
	at java.awt.EventQueue$2.run(EventQueue.java:616)
	at java.awt.EventQueue$2.run(EventQueue.java:614)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:613)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)
Caused by: java.lang.NullPointerException
	at com.sun.org.apache.xml.internal.serializer.ToHTMLStream.endElement(ToHTMLStream.java:907)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.endElement(TransformerHandlerImpl.java:273)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.gui.TikaGUI$2.endElement(TikaGUI.java:519)
	at org.apache.tika.sax.TeeContentHandler.endElement(TeeContentHandler.java:94)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:273)
	at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:216)
	at org.apache.tika.parser.pdf.PDF2XHTML.endDocument(PDF2XHTML.java:112)
	at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:323)
	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:61)
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:96)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 43 more

I tried the same pdf files (and can switch forth and back between version 0.10 and 1.0, this behavior is stable) and it looks like the exact same pdfbox version is inside the tika-app-0.10.jar and tika-app-1.0.jar. It would be great if version 1.0 could do what 0.10 can. Sorry that I cannot provide the pdf.",smartshark_2_2,638,tika,"""[0.12087218463420868, 0.8791278600692749]"""
2867,231126,ExecutableParser doesn't call start document,The ExecutableParser doesn't call start document which causes errors when producing XHTML ,smartshark_2_2,2019,tika,"""[0.10350414365530014, 0.8964958786964417]"""
2868,231127,NullPointerException trying to parse detached .pk7s signature,"Our Pkcs7Parser tries to pull the signed content out and then parses
that, but if the signature is detached then there is no content (we
get null return from CMSSignedDataParser.getSignedContent) and we hit
NPE:

{noformat}
Exception in thread ""main"" org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.crypto.Pkcs7Parser@5545757a
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:138)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:399)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:111)
Caused by: java.lang.NullPointerException
	at org.apache.tika.parser.crypto.Pkcs7Parser.parse(Pkcs7Parser.java:64)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 5 more
{noformat}

I think fix is trivial: if we get null return then throw a
TikaException saying there's nothing to extract.
",smartshark_2_2,820,tika,"""[0.07043657451868057, 0.9295634031295776]"""
2869,231128,Garbled metadata when dealing with encrypted PDF files.,"The code exhibiting this issue is very simple:

        InputStream input = new FileInputStream(file);
        ContentHandler textHandler = new BodyContentHandler();
        tikaParser.parse(input, textHandler, metadata);
        input.close();
        System.out.println(metadata);

The output:
title=?a???â¬Ã·&â¼??â?Å¢jK???Å¾?âM?Aâ<â]1
=â¬\bK Author=âg?â?â¦ Content-Type=application/pdf creator=?k?â?â¦Ã`;Ã?)??/Â¶???Ä?3n
Ãâ¼46ÃO

Other than that, the extracted text is 100% correct.",smartshark_2_2,790,tika,"""[0.09233451634645462, 0.907665491104126]"""
2870,231129,XWPFWordExtractorDecorator does not extract bookmarks,XWPFWordExtractorDecorator does not extract bookmarks,smartshark_2_2,272,tika,"""[0.16258086264133453, 0.8374190926551819]"""
2871,231130,Problem in Tika().detect for xml file signed in CADES,"We have a xml file with base64 attachment signed with CADES signature. 
In this case TIKA recognize the resulted file mime type as ""text/plain"" and not ""application/pkcs7-signature"" as we expected.",smartshark_2_2,2105,tika,"""[0.16735243797302246, 0.8326475620269775]"""
2872,231131,TikaCLI: invalid characters in embedded document name causes FNFE when trying to save,"Attached document hits this on Windows:

{noformat}
C:\>java.exe -jar tika-app-1.3.jar -z -x c:\data\idit\T-DS_Excel2003-PPT2003_1.xls
Extracting 'file0.png' (image/png) to .\file0.png
Extracting 'file1.emf' (application/x-emf) to .\file1.emf
Extracting 'file2.jpg' (image/jpeg) to .\file2.jpg
Extracting 'file3.emf' (application/x-emf) to .\file3.emf
Extracting 'file4.wmf' (application/x-msmetafile) to .\file4.wmf
Extracting 'MBD0016BDE4/?Â£âº.bin' (application/octet-stream) to .\MBD0016BDE4\?Â£âº.bin
Exception in thread ""main"" org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.microsoft.OfficeParser@75f875f8
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:248)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:139)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:415)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:109)
Caused by: java.io.FileNotFoundException: .\MBD0016BDE4\?Â£âº.bin (The filename, directory name, or volume label syntax is incorrect.)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:205)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:156)
        at org.apache.tika.cli.TikaCLI$FileEmbeddedDocumentExtractor.parseEmbedded(TikaCLI.java:722)
        at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedOfficeDoc(AbstractPOIFSExtractor.java:201)
        at org.apache.tika.parser.microsoft.ExcelExtractor.parse(ExcelExtractor.java:158)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:194)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:161)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
{noformat}

TikaCLI manages to create the sub-directory, but because the embedded fileName has invalid (for Windows) characters, it fails.

On Linux it runs fine.

I think somehow ... we have to sanitize the embedded file name ...",smartshark_2_2,1008,tika,"""[0.11554012447595596, 0.8844598531723022]"""
2873,231132,"Extracting as HTML loses links in xlsx, ppt, and pptx files","I am trying to convert documents to HTML, then looking through the HTML for anchor tags to find links to external URLs.  This works fine when looking at some document types, including PDFs, Open Document formats, Microsoft Word formats .doc and .docx, and the older Microsoft Excel .xls format, but it does not work for any Microsoft Powerpoint formats (.ppt or .pptx) and it does not work for the newer Excel .xlsx format.  For the .ppt, .pptx, and .xlsx formats, the text is extracted properly and formatted into HTML, but the link is not converted to an anchor tag.

I am running tika in --server --html mode.

I included samples of .xlsx, .ppt, and .pptx files that do not properly extract links, and also included samples of .ods and .odp files that do extract links properly.",smartshark_2_2,2726,tika,"""[0.16197028756141663, 0.8380297422409058]"""
2874,231133,classloaders issue loading resources when extending Tika,"I noticed that ServiceLoader is using different classloader when loading 'services' like Parsers, etc (java.net.FactoryURLClassLoader) than MimeTypesFactory (org.eclipse.jetty.webapp.WebAppClassLoader) when loading mime types definitions. As result - it works completely different:

When jar with custom parser and custom-mimetypes.xml is added to solr.war - both resources are located and loaded (META-INF\services\org.apache.tika.parser.Parser and org\apache\tika\mime\custom-mimetypes.xml) and everything works fine.

When jar with custom parser is in Solr core lib and configured in solrconfig.xml - only META-INF\services\org.apache.tika.parser.Parser is loaded, but custom-mimetypes.xml is ignored.

MimeTypesFactoryÂ ignores custom classLoader provided in TikaConfig and always using only context provided one:
ClassLoader cl = MimeTypesReader.class.getClassLoader();",smartshark_2_2,947,tika,"""[0.3104284405708313, 0.6895715594291687]"""
2875,231134,Fix logic error in batch driver that prevents correct restarting of child process,"Thanks to work on TIKA-1285, I discovered a logic bug in the driver process that prevents correct restarting of the child process.  This should only happen under very heavy load, but this needs to be fixed asap.",smartshark_2_2,1595,tika,"""[0.11680591106414795, 0.8831941485404968]"""
2876,231135,Incorrectly declared SUPPORTED_TYPES in ChmParser.,"[This link|http://www.iana.org/assignments/media-types/application/vnd.ms-htmlhelp] assigns the official mime type for these files to ""application/vnd.ms-htmlhelp"". In the wild there are also two other types used:

* application/chm
* application/x-chm

tika-mimetypes.xml uses the correct official mime type, but ChmParser declares that it supports only ""application/chm"". For this reason content that uses the official mime type (e.g. coming via Detector or parsed using AutoDetectParser, or simply declared in metadata) fails to parse due to unknown mime type.

The fix seems simple - ChmParser should declare also all of the above types in its SUPPORTED_TYPES.",smartshark_2_2,1029,tika,"""[0.17316056787967682, 0.8268394470214844]"""
2877,231136,Underlined text is not decorated as such when extracting from word documents,"When extracting from doc and docx bold and italic text decoration is extracted, however underlining is not.  Can be demonstrated in WordParserTest or OOXMLParserTest (change to docx) with the following test case.

{code:title=WordParserTest.java|borderStyle=solid}
    @Test
    public void testTextDecoration() throws Exception {
      XMLResult result = getXML(""testWORD_various.doc"");
      String xml = result.xml;

      assertTrue(xml.contains(""<b>Bold</b>""));
      assertTrue(xml.contains(""<i>italic</i>""));
      assertTrue(xml.contains(""<u>underline</u>""));

    }
{code}",smartshark_2_2,2706,tika,"""[0.1284395158290863, 0.8715605139732361]"""
2878,231137,URI is not hierarchical exception when location model resource is inside a jar in classpath,"{code:title=Stacktrace|borderStyle=solid}
The following error happens when location NER model resource is packaged inside a jar and GeoTopicParser is enabled.

Caused by: java.lang.IllegalArgumentException: URI is not hierarchical
	at java.io.File.<init>(File.java:418)
	at org.apache.tika.parser.geo.topic.GeoParserConfig.<init>(GeoParserConfig.java:33)
	at org.apache.tika.parser.geo.topic.GeoParser.<init>(GeoParser.java:54)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at java.lang.Class.newInstance(Class.java:442)
	at org.apache.tika.config.TikaConfig$XmlLoader.loadOne(TikaConfig.java:559)
	at org.apache.tika.config.TikaConfig$XmlLoader.loadOverall(TikaConfig.java:492)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:166)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:149)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:142)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:138)
	at edu.usc.cs.ir.cwork.tika.Parser.<init>(Parser.java:45)

{code}

Refernces :
http://stackoverflow.com/questions/18055189/why-my-uri-is-not-hierarchical",smartshark_2_2,1902,tika,"""[0.1573546677827835, 0.8426452875137329]"""
2879,231138,IWorkPackageParser / IWorkParser not registering properly,"If you try to use AutoDetectParser to handle an iWork document, it'll fail with:
 org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)

However IWorkPackageParser works fine. It seems the IWorkParser needs just the individual zip part, but is registered as the handler for the individual mime types, so breaks.",smartshark_2_2,541,tika,"""[0.09967033565044403, 0.9003296494483948]"""
2880,231139,WordMLParser fails to parse a word xml file,"h3. Problem
I have a sample word xml file (attached as File5.xml) that can be parsed by neither OOXMLParser (yields an exception that was {{Caused by: org.apache.poi.openxml4j.exceptions.NotOfficeXmlFileException: The supplied data appears to be a raw XML file. Formats such as Office 2003 XML are not supported}}) nor by OfficeParser (yields an exception like: {{org.apache.poi.poifs.filesystem.NotOLE2FileException: The supplied data appears to be a raw XML file. Formats such as Office 2003 XML are not supported}}

I found TIKA-1958 which mentioned the new WordMLParser, so downloaded the source, built, and updated my tika version to 1.14. However, when parsing with WordMLParser, the output text content I get is the empty string {{""""}}, but I'm expecting something more like:
{noformat}
It means that the guy that you are trading with was reported for a scam attempt. As the others mentioned, some of these BOFA could be false.
What's important is the current trade that you are doing.
If everything seems to be in order then there is nothing wrong with going through with the trade.
Auti, Sneha (QAPM)
{noformat}

h3. Replication
You can replicate with the below Spock test
{noformat}
    def ""display error with WordMLParser""(){
        setup:
        File input = new File(""/Users/sstory/Downloads/File5.xml"") //modify for your path
        Parser parser = new WordMLParser()
        //Parser parser = new OOXMLParser()
        //Parser parser = new OfficeParser()
        org.xml.sax.ContentHandler textHandler = new BodyContentHandler(-1)
        Metadata metadata = new Metadata()
        ParseContext context = new ParseContext()
        
        when:
        parser.parse(input.newInputStream(), textHandler, metadata, context)
        String result = textHandler.toString()

        then:
        !result.isEmpty()
        result.contains(""the guy that you are trading with"")
        result.contains(""BOFA"")
    }
{noformat}",smartshark_2_2,2292,tika,"""[0.09870307892560959, 0.901296854019165]"""
2881,231140,TesseractOCRParser does not work in Windows,"STR:
* Case 1:
** Setting tesseractPath to a common installation path of Tesseract:  C:\Program Files (x86)\Tesseract-OCR
** the checking available Tesseract command returns always false

* Case 2:
** Even setting to no space value in tesseractPath, says C:\Tesseract-OCR
** the checking & running command of tesseract on Windows is not correct: C:\Tesseract-OCR\tesseract, it must be C:\Tesseract-OCR\tesseract.exe",smartshark_2_2,1189,tika,"""[0.21916645765304565, 0.7808335423469543]"""
2882,231141,Tika config xml shouldn't read nested parser definitions as top level,"Spotted while looking at TIKA-1642, if you have some Tika config xml like:
{code}
<properties>
  <parsers>
    <parser class=""org.apache.tika.parser.ctakes.CTAKESParser"">
       <parser class=""org.apache.tika.parser.DefaultParser""/>
    </parser>
  </parsers>
</properties>
{code}

Then because of the way that TikaConfig is fetching the elements, it will process the DefaultParser once as a child of CTakes, then a second time at the top level",smartshark_2_2,1556,tika,"""[0.6936307549476624, 0.30636924505233765]"""
2883,231142,Oracle internal API jdeps request for information,"We have been asked to provide information to Oracle around the internal API usage in Apache Tika to support move to JDK 9, which contains significant changes.

{quote}
Hi David,

My name is Rory O'Donnell, I am the OpenJDK Quality Group Lead.  

I'm contacting you because your open source project seems to be a very popular dependency for other open source projects.
As part of the preparations for JDK 9, Oracleâs engineers have been analyzing open source projects like yours to understand usage. One area of concern involves identifying compatibility problems, such as reliance on JDK-internal APIs. 

Our engineers have already prepared guidance on migrating some of the more common usage patterns of JDK-internal APIs to supported public interfaces.  The list is on the OpenJDK wiki [0].

As part of the ongoing development of JDK 9, I would like to inquire about your usage of  JDK-internal APIs and to encourage migration towards supported Java APIs if necessary.

The first step is to identify if your application(s) is leveraging internal APIs. 

  Step 1: Download JDeps. 
Just download a preview release of JDK8(JDeps Download). You do not need to actually test or run your application on JDK8.  JDeps(Docs) looks through JAR files and identifies which JAR files use internal APIs and then lists those APIs.    
  Step 2: To run JDeps against an application. The command looks like:
jdk8/bin/jdeps -P -jdkinternals *.jar > your-application.jdeps.txt

The output inside your-application.jdeps.txt will look like:

your.package (Filename.jar)
      -> com.sun.corba.se            JDK internal API (rt.jar)
3rd party library using Internal APIs:
If your analysis uncovers a third-party component that you rely on, you can contact the provider and let them know of the upcoming changes. You can then either work with the provider to get an updated library that won't rely on Internal APIs, or you can find an alternative provider for the capabilities that the offending library provides.

Dynamic use of Internal APIs:
JDeps can not detect dynamic use of internal APIs, for example through reflection, service loaders and similar mechanisms.

Rgds,Rory 

[0] https://wiki.openjdk.java.net/display/JDK8/Java+Dependency+Analysis+Tool
-- 
Rgds,Rory O'Donnell
Quality Engineering Manager
Oracle EMEA , Dublin, Ireland 
{quote}",smartshark_2_2,1596,tika,"""[0.9980912804603577, 0.001908694626763463]"""
2884,231143,Tika sites refers to incorrect svn repo URL,"
On http://tika.apache.org/source-repository.html

the URLs for browsing or checking out the source repo are no longer correct - It seems that the repo has been moved to http://svn.apache.org/repos/asf/tika

so the text below from the docs is incorrect:

The following is a link to the online source repository.

http://svn.apache.org/viewvc/lucene/tika/trunk

Anonymous access

The source can be checked out anonymously from SVN with this command:

$ svn checkout http://svn.apache.org/repos/asf/lucene/tika/trunk tika-site
{quote}


",smartshark_2_2,354,tika,"""[0.9948544502258301, 0.005145507398992777]"""
2885,231144,CompositeParser.getParser() should use mimetype hierarchy when falling back,"CompositeParser.getParser() doesn't use supertypes when falling back - if it can't get a parser for the exact mimetype, then it goes
straight to the fallback parser.

So, for example, if the file mimetype is application/<whatever>+xml, and no parser exists for it, then you get the default ""do nothing"" parser versus the XML parser.
",smartshark_2_2,353,tika,"""[0.8295190930366516, 0.170480877161026]"""
2886,231145,TikaConfig fails when a parser can't be loaded due to an Error,"When running in Java 1.4, some of the Parser classes can't be loaded due to dependencies (POI, etc.) that require Java 5 or higher. These cases throw UnsupportedClassVersionErrors or somilar Throwables that aren't caught by the current ""catch (Exception e)"" clause. We should catch all Throwables there.",smartshark_2_2,192,tika,"""[0.6505980491638184, 0.34940192103385925]"""
2887,231146,Add Various links and UTI values in tika-metadata.xml,In TIKA-1012 we added <tika:link> and <tika:uti> this patch fills in a few values,smartshark_2_2,893,tika,"""[0.9983356595039368, 0.0016643211711198092]"""
2888,231147,BoilerPipe Integration,Found a library that might be worth considering for integration into your package. It provides one of the best open source text extraction algorithms to find the main text within an HTML page.,smartshark_2_2,475,tika,"""[0.9982472658157349, 0.0017527193995192647]"""
2889,231148,Update description and fix typos in site,"The description of Tika on the homepage of the site should be expanded to describe what technology Tika uses, what it's used for, and where to look for how it works. Patch coming shortly.",smartshark_2_2,1102,tika,"""[0.9978101849555969, 0.0021898814011365175]"""
2890,231149,PDF embeded with document can not parse.,"I insert a Excel file into the pdf file.
But can not extracte embedded excel resources.

The attachment file PDF2XHTML.java_diff.html is the diff file.
Please confirm it.",smartshark_2_2,1585,tika,"""[0.6496743559837341, 0.3503256142139435]"""
2891,231150,"Fulltext, summary, and outlinks should not be added to the parsers' metadata.","We no longer need to add the above to the outgoing metadata.  To save memory and processing time, we should modify the code so that these are no longer added.

This can be done by no longer using ParserPostProcessor, or by disabling the code in that class that adds the properties.",smartshark_2_2,110,tika,"""[0.9973031282424927, 0.002696844283491373]"""
2892,231151,Replace usages of classes in org.apache.tika.io with current alternatives,"Many of the classes in org.apache.tika.io were inlined from commons-io in TIKA-249, but these days most components use commons-io anyway, so in order to clean the dependencies on org.apache.tika.io in preparation of adding commons-io to tika-core, the following can be done:
- Replace usages of classes in org.apache.tika.io within non-core components with the corresponding classes in commons-io
- Replace usages of org.apache.tika.io.IOUtils.UTF_8 with java.nio.charset.StandardCharsets.UTF_8 (in all components, including tika-core)
- Replace other uses of String encoding names of standard charsets with their corresponding Charsets instances from StandardCharsets (this is logically related to IOUtils as these constants should have been there as UTF_8 was before Java 7)",smartshark_2_2,1649,tika,"""[0.9980291724205017, 0.0019708373583853245]"""
2893,231152,Upgrade to PDFBox 2.0.5 when available,Upgrade when available.,smartshark_2_2,2460,tika,"""[0.9982149600982666, 0.0017851212760433555]"""
2894,231153,Build a parser to extract data from .dif format,"An initial crawl of the Acadis website (https://www.aoncadis.org/home.htm) revealed that a number of the files on this website are of the .dif type. Currently, Tika categorizes these files as text/plain since it does not have a parser for this type of file. The need is to provide metadata support and to build a parser for this kind of file.",smartshark_2_2,1199,tika,"""[0.998418927192688, 0.0015810271725058556]"""
2895,231154,Investigate Word .doc WMF/EMF/PICT attachmetns,"As spotted when working on TIKA-1644, many of the govdocs1 Word .doc files have embedded image resources which are coming through as WMF, EMF or PICT. In at least some of the cases, these files don't have the typical header that would be expected for that file, but do have PDF header some tens or a few hundred bytes into the file. (Some of the files do come out correctly though, so it doesn't look universal)

It's possible that this is all as expected and normal. However, it's possible that something in the POI code for pulling out the embedded resources is either truncating or failing to truncate the header, or some how otherwise failing to correctly pull these out. The result is that they aren't coming through quite as they should do as embedded resources.

This is probably going to mean lots of time with the file format specs, some time creating some slightly-unusual test files with these formats of attachments in, then finally looking at the govdocs ones",smartshark_2_2,1550,tika,"""[0.9909522533416748, 0.009047740139067173]"""
2896,231155,Integrate tika-java7 component,Code requiring Java 7 doesn't need to be in a separate module now that TIKA-1536 (upgrade to Java 7) is done.,smartshark_2_2,2719,tika,"""[0.9982852339744568, 0.0017147223697975278]"""
2897,231156,Provide a Detector JAXRS endpoint,"As identified in TIKA-1335, the Tika Server now has an endpoint which will tell you what Detectors are available to it, but not one that will trigger detection. That means your only way to do detection is to request the metadata, and check the content type, but that isn't always as accurate as an explicit detection call (eg if a general parser picks up the file)

We should therefore add in a new endpoint that just does the detection",smartshark_2_2,1084,tika,"""[0.9986127614974976, 0.0013871642295271158]"""
2898,231157,Update ASM dependency to 5.0.4,"Currently the Class file parser uses ASM 4.1. This older version cannot read Java 8 / Java 9 class files (fails with Exception).

The upgrade to ASM 5.0.4 is very simple, just Maven dependency change. The code change is only to update the visitor version, so it gets new Java 8 features like lambdas reported, but this is not really required, but should be done for full support.

FYI, in LUCENE-6729 we want to upgrade the Lucene Expressions module to ASM 5, too.

You can hot-swap ASM 4.1 with ASM 5.0.4 without recompilation (so we have no problem with Lucene using a newer version). Since ASM 4.x the updates are more easy (no visitor interfaces anymore, instead abstract classes), so it does not break if you just replace the JAR file. So just see this as a recommendatation, not urgent! Solr/Lucene will also work without this patch (it just replaces the shipped ASM by newer version in our packaging).",smartshark_2_2,2753,tika,"""[0.998490571975708, 0.0015093775000423193]"""
2899,231158,"cannot extract text in text-box for Excel 2007 file(.xlsx, .xlsm)","When I launch Tika gui from command-line and drag and drop .xlsx file that have textbox, no text in the textbox are extracted.

When drag and drop .xls file, text in the textbox are extracted.",smartshark_2_2,1019,tika,"""[0.5664199590682983, 0.4335800111293793]"""
2900,231159,Small improvements to how embedded docs are parsed in AbstractPOIFSExtractor.handleEmbeddedOfficeDoc,"I noticed some minor things in this method:

  * It does too much work (writes the tmpFile out) if the
    EmbeddedDocumentExtractor didn't want to actually parse file
    file.

  * It writes the tmpFile when it won't use it in the OLE10_NATIVE
    case (because we use a TikeInputStream from the in-RAM byte[]
    instead).

Also I fixed a typo in the method name (embeded -> embedded) -- is
that OK?  It's a protected method, and a few of the office parsers
invoke it.

Finally I cutover to TemporaryResources to track the possible tmpFile
and open TikaInputStream against it.

Separately, it's inefficient now that we must serialize a sub-dir
(DirectoryEntry) in the NPOIFileSystem to a tmp file only to re-parse
it back to an NPOIFileSystem in OfficeParser; I'd like to look into
instead (somehow) directly passing the NPOIFileSystem's DirectoryEntry
to OfficeParser... but that looks like a bigger change.
",smartshark_2_2,523,tika,"""[0.9968356490135193, 0.003164338879287243]"""
2901,231160,OpenGraph meta tags to allow multiple values,"HtmlHandler should use Metadata.add() for Open Graph properties instead of the HtmlHandler.addHtmlMetadata() method which uses Metadata.set(). The og:* properties can be multivalued. The Metadata.set() method overwrites previous entries because it doesn't use Metadata.appendedValues().

",smartshark_2_2,822,tika,"""[0.9813907146453857, 0.018609266728162766]"""
2902,231161,Added types to Grobid quantities parser,"Grobid Quantities returns information about the measurement(""type""), one example could be : length",smartshark_2_2,2072,tika,"""[0.9985626339912415, 0.001437329570762813]"""
2903,231162,Automatic whitespace for block elements in XHTMLContentHandler,"As discussed in TIKA-171, it would be a good idea to make the XHTMLContentHandler automatically add extra whitespace to separate block level elements from each other. This would prevent extracted words to accidentally get concatenated in clients that only care about the character events.",smartshark_2_2,29,tika,"""[0.9983693957328796, 0.0016305703902617097]"""
2904,231163,Add experimental SAX/Streaming XWPF/docx extractor,"I'd like to contribute an experimental streaming extractor for docx.  I should have something ready for committing in a few weeks.  I'll attach drafts as they're ready.

At least for a couple of releases, I'd like to keep it in o.a.t.parser.microsoft.ooxml.experimental if that makes sense.",smartshark_2_2,2291,tika,"""[0.9983592629432678, 0.0016407413640990853]"""
2905,231164,Avoid org.json dependency,"See the Apache discussion about this, e.g. TC-50",smartshark_2_2,2576,tika,"""[0.9983970522880554, 0.0016029097605496645]"""
2906,231165,Tika pom.xml is missing dependencies on bouncycastle jars needed by PDFBox,"While processing a bunch of PDFs off the web, I ran into a ClassNotFoundException thrown inside of PDFBox:

java.lang.NoClassDefFoundError: org/bouncycastle/jce/provider/BouncyCastleProvider
	at org.apache.pdfbox.pdmodel.PDDocument.openProtection(PDDocument.java:1092)
	at org.apache.pdfbox.pdmodel.PDDocument.decrypt(PDDocument.java:573)
	at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:235)
	at org.apache.pdfbox.util.PDFTextStripper.getText(PDFTextStripper.java:180)
	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:56)
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:69)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
	at bixo.parser.SimpleParser.parse(SimpleParser.java:153)
Caused by: java.lang.ClassNotFoundException: org.bouncycastle.jce.provider.BouncyCastleProvider
	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)

I believe the issue is that the PDFBox pom.xml declares the dependency on the missing BouncyCastleProvider jar as ""optional"".

   <dependency>
     <groupId>bouncycastle</groupId>
     <artifactId>bcprov-jdk14</artifactId>
     <version>136</version>
     <optional>true</optional>
   </dependency>

As explained in the Maven documentation, this means that Tika needs to explicitly include the jar:

http://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html

I see a few other optional dependencies in the PDFBox pom.xml, but perhaps the only one that's really critical is the above.
",smartshark_2_2,329,tika,"""[0.951705813407898, 0.048294227570295334]"""
2907,231166,Drop support for Java 1.4,"Since TIKA-175 we've been supporting clients that still need to run on Java 1.4 environments. I guess nobody is on Java 1.4 anymore, so it should be safe to drop the retrotranslator settings.",smartshark_2_2,501,tika,"""[0.9983914494514465, 0.0016084985109046102]"""
2908,231167,Tika MIME updates for *.cdf and *.xar and custom zero length file detector based on TREC-DD-Polar,Updated tika-mimetypes.xml and detector to identify new file types in TREC DD Polar dataset.,smartshark_2_2,2167,tika,"""[0.998201847076416, 0.001798190176486969]"""
2909,231168,Add mimetypes for famous programming languages,"I have noticed that some famous programming lanuages get not detected in the tika-mimetypes.xml-file:

* Clojure: text/x-clojure
* CoffeeScript: text/x-coffeescript
* Go: text/x-go
* Less: text/x-less
* OCaml: text/x-ocaml
* Property files, INI-Files
* reStructuredText: text/x-rst
* _Scala_: text/x-scala
* Smalltalk: text/x-stsrc
* Yaml: text/x-yaml

The mimetypes are taken from the CodeMirror editor.

It would be very helpfull for me, if also these types will get detected.",smartshark_2_2,872,tika,"""[0.9985120892524719, 0.0014878840884193778]"""
2910,231169,Tika 1.17 uses vulnerable Jackson version 2.9.2,See https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-17485,smartshark_2_2,1734,tika,"""[0.9880324602127075, 0.011967558413743973]"""
2911,231170,Clean up jdom version conflict,Not sure best way to fix this.  grib 4.5.5 uses jdom 2.0.4 and rometools uses 2.0.2.  Both are now getting packed into tika-app.  Should we exclude the earlier (jdom from rometools) and hope for the best?,smartshark_2_2,2052,tika,"""[0.9985617995262146, 0.0014381872024387121]"""
2912,231171,Leftover temp files after running Tika tests,"After running ""mvn test"", a number of files like "" tmp12106_testXML.xml"" are left in /tmp.

This is apparently due to TestParsers.testZipExtraction()",smartshark_2_2,165,tika,"""[0.99104905128479, 0.008950965479016304]"""
2913,231172,Speech recognition,"Like OCR for image files (TIKA-93), we could try using speech recognition to extract text content (where available) from audio (and video!) files.

The CMU Sphinx engine (http://cmusphinx.sourceforge.net/) looks promising and comes with a friendly license.",smartshark_2_2,1776,tika,"""[0.9982945322990417, 0.0017054806230589747]"""
2914,231173,Update CXF version to 3.0.2,"CXF 3.0.2 JAX-RS front-end offers a complete JAX-RS 2.0 support, has fewer dependencies and is smaller compared to CXF 2.7.x one. It is also backward-compatible with the applications written against JAX-RS 1.1.
Lets do this upgrade after Tika 1.6 is out.

CXF 3.1.0 is Java 7 based and is still under development.",smartshark_2_2,1175,tika,"""[0.9983065128326416, 0.0016934305895119905]"""
2915,231174,Updated site layout,"Our current web site could do with some web design love. I'm no designer, but at http://people.apache.org/~jukka/tika-site-proposal/ I have a quick proposal for a new layout that's IMO notably better than before. WDYT?",smartshark_2_2,467,tika,"""[0.9983243346214294, 0.0016756112454459071]"""
2916,231175,Clean up initialization of parsers,"With my addition of DigestingParser, parser initialization became more bloated than it needs to be. Also consider moving to one parser per thread or just one parser vs doc (as it currently is following example in server).",smartshark_2_2,1575,tika,"""[0.9983749389648438, 0.0016250143526121974]"""
2917,231176,Bouncy Castle version binary incompatibility,"One file in our Common Crawl stash demonstrates a Bouncy Castle version conflict...incompatible binaries with Jackcess and our current version of Bouncy Castle.

java.lang.NoSuchMethodError: org.bouncycastle.crypto.StreamCipher.processBytes([BII[BI)V
 at com.healthmarketscience.jackcess.impl.BaseCryptCodecHandler.streamDecrypt(BaseCryptCodecHandler.java:91)
 at com.healthmarketscience.jackcess.impl.BaseJetCryptCodecHandler.decodePage(BaseJetCryptCodecHandler.java:62)
 at com.healthmarketscience.jackcess.impl.PageChannel.readPage(PageChannel.java:224)
 at com.healthmarketscience.jackcess.impl.UsageMap.read(UsageMap.java:130)
 at com.healthmarketscience.jackcess.impl.PageChannel.initialize(PageChannel.java:117)
 at com.healthmarketscience.jackcess.impl.DatabaseImpl.<init>(DatabaseImpl.java:516)
 at com.healthmarketscience.jackcess.impl.DatabaseImpl.open(DatabaseImpl.java:389)
 at com.healthmarketscience.jackcess.DatabaseBuilder.open(DatabaseBuilder.java:248)
 at TestIt.testIt(TestIt.java:19)

A full description and test file are attached [here|https://sourceforge.net/p/jackcessencrypt/feature-requests/2/#b65d].

There was an API change in 1.51 that causes this problem.  1.50 works with the one test file, and 1.51 does not work.  We're currently using 1.52.

It looks like POI is using 1.51 in trunk, now. According to PDFBox trunk's build.xml, they're using 1.50, but their pom.xml has 1.51.

Two options that I see:
1) close our eyes and hope it doesn't affect too many people before Jackcess Encrypt upgrades... perhaps add a try/catch for this one version conflict?  Is there any shade magic we can do on our end ... or (I'm assuming) would that have to be done by Jackcess (or an upgrade, of course)?
2) downgrade our bc-prov to 1.50 (from 1.52).

Other options?
",smartshark_2_2,1872,tika,"""[0.9109545350074768, 0.08904541283845901]"""
2918,231177,Invalid closing script tag not handled gracefully by HtmlParser,"When an HTML file contains an invalid closing script tag, all content after that tag is interpreted as script data and therefore ignored.

Reduced test case file attached.

To reproduce:

1) create a file with the following HTML

{code:html}
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"" ""http://www.w3.org/TR/html4/loose.dtd"">
<html>
	<head>
		<script lang=""javascript""></script language>
	</head>
	<body>
		<p>This is a test.</p>
	</body>
</html>
{code}

2) {{java -jar tika-app-1.12.jar -t test.html}}

Expected result:

{{This is a test.}}

What is actually returned:

Nothing.",smartshark_2_2,2270,tika,"""[0.323411762714386, 0.676588237285614]"""
2919,231178,Detection problem: message/rfc822 file is detected as text/plain.,"When using {{DefaultDetector}} mime type for {{.eml}} files is different (you can test it on {{testRFC822}} and {{testRFC822_base64}} in {{tika-parsers/src/test/resources/test-documents/}}).

Main reason for such behavior is that only magic detector is really works for such files. Even if you set {{CONTENT_TYPE}} in metadata or some {{.eml}} file name in {{RESOURCE_NAME_KEY}}.

As I found {{MediaTypeRegistry.isSpecializationOf(""message/rfc822"", ""text/plain"")}} returns {{false}}, so detection by {{MimeTypes.detect(...)}} works only by magic.",smartshark_2_2,1597,tika,"""[0.543272078037262, 0.45672792196273804]"""
2920,231179,A more accurate facility for detecting Charset Encoding of HTML documents,"Currently, Tika uses icu4j for detecting charset encoding of HTML documents as well as the other naturally text documents. But the accuracy of encoding detector tools, including icu4j, in dealing with the HTML documents is meaningfully less than from which the other text documents. Hence, in our project I developed a library that works pretty well for HTML documents, which is available here: https://github.com/shabanali-faghani/IUST-HTMLCharDet

Since Tika is widely used with and within some of other Apache stuffs such as Nutch, Lucene, Solr, etc. and these projects are strongly in connection with the HTML documents, it seems that having such an facility in Tika also will help them to become more accurate.",smartshark_2_2,2156,tika,"""[0.9983417987823486, 0.001658136839978397]"""
2921,231180,Split Tika to separate modules,"As discussed recently on the mailing list [1], it would be good to split Tika to components based on use cases and external dependencies.

The proposed split is:

* tika-core - core parts of Tika; everything but cli, gui, and the parser.* packages
* tika-parsers - format-specific parser classes; with dependencies to external libraries
* tika-app - depends on all of the above; adds cli and gui; standalone jar packaging

[1] http://markmail.org/message/tavirkqhn6r2szrz",smartshark_2_2,191,tika,"""[0.9982946515083313, 0.0017052993644028902]"""
2922,231181,Figure out file types for several unknown OLE files in Common Crawl,"We're getting around 300 exceptions from ""application/x-tika-msoffice"" files in our current slice of Common Crawl documents that look roughly like this:

{noformat}
java.lang.IllegalArgumentException: Position 86528 past the end of the file
    at org.apache.poi.poifs.nio.FileBackedDataSource.read
{noformat}

I suspect these are non-MS OLE file formats.  Any help identifying the file types and patching our OLE mime detector would be great.",smartshark_2_2,1732,tika,"""[0.9969612956047058, 0.003038666909560561]"""
2923,231182,Parser for tar files,"Similar to TIKA-149, but for tar files.",smartshark_2_2,81,tika,"""[0.9982290863990784, 0.0017709842650219798]"""
2924,231183,Better parsing of Mbox files,"MboxParser currently looses metadata of all emails, except first. It does not extract/parse emails, nor decode parts. It should handle embedded emails like other container parsers do, so emails will be automatically parsed by RFC822Parser. I will try to add a patch for this.",smartshark_2_2,1036,tika,"""[0.9982409477233887, 0.0017590515781193972]"""
2925,231184,Refactor PDFParser to enable easier parameter setting,"It would be handy to be able to set PDFParser parameters (extractAnnotationText, etc) in a config file and via ParseContext.",smartshark_2_2,1031,tika,"""[0.9982660412788391, 0.0017340454505756497]"""
2926,231185,Detecting KML / KMZ files,"KML format is subtype of application/xml with a ""kml"" root node and (an optional?) ""http://www.opengis.net/kml/2.2"" namespace.

  <mime-type type=""application/vnd.google-earth.kml+xml"">    
    <root-XML localName=""kml""/>
    <root-XML namespaceURI=""http://www.opengis.net/kml/2.2"" localName=""kml""/>    
    <acronym>KML</acronym>
    <_comment>Keyhole Markup Language</_comment>
    <glob pattern=""*.kml""/>    
    <sub-class-of type=""application/xml""/>
  </mime-type>	

KMZ files (https://developers.google.com/kml/documentation/kmzarchives) are zip archives with a KML file inside (the file should be called doc.kml) and one or more folder. A naive approach consists in adding a further check in ZipContainerDetector (find attached). ",smartshark_2_2,1051,tika,"""[0.990928590297699, 0.009071439504623413]"""
2927,231186,Support XMP metadata keys for more of the common EXIF tags,"Following the work on TIKA-442, we now have some XMP inspired image metadata keys and type properties. This currently only covers 4 core exif tags

There are a couple of other common tags that we should probably handle in the same way. The plan would be to define more entries in the TIFF metadata area, using the same definitions as in XMP, and update the Exif parser to additionally map onto these. (The raw exif tags would continue to be output too, for compatibility)

The tags I propose to support are:
* exif:ExposureTime
* exif:FNumber
* exif:Flash
* exif:FocalLength
* exif:IsoSpeedRatings
* exif:Manufacturer
* exif:Model
* exif:Software
* exif:Orientation
* exif:XResolution
* exif:YResolution
* exif:ResolutionUnit

Plus one extra date, which we don't really seem to have a suitable existing key for, but maybe should be made more generic?
* exif:DateTimeOriginal",smartshark_2_2,410,tika,"""[0.9983711838722229, 0.001628842088393867]"""
2928,231187,Tika support for .NET,As i understand currently latest Tika Version is in .jar format which then again needs to converted to .Net assemblies to make it compatible and able to use with .Net framework. Do we have a built in utility of Tika which can be used with .Net projects ?,smartshark_2_2,1265,tika,"""[0.9976644515991211, 0.002335613826289773]"""
2929,231188,Version conflict with non-ASL jai-imageio-jpeg2000 and edu.ucar jj2000,"For users who want to extract jp2000 from PDFs for inline-image OCR, they have to add non- ASL 2.0 compatible:

{noformat}
<dependency>
    <groupId>com.github.jai-imageio</groupId>
    <artifactId>jai-imageio-jpeg2000</artifactId>
    <version>1.3.0</version>
  </dependency>
{noformat}

However, this creates a conflict with GRIB's jj2000:

{noformat}
   <dependency>
      <groupId>edu.ucar</groupId>
      <artifactId>jj2000</artifactId>
      <version>5.2</version>
    </dependency> 
{noformat}

[~mcaruanagalizia] (I'm guessing?) identified this conflict [here|https://github.com/ICIJ/extract/blob/master/pom.xml] and fixes it by upgrading jj2000 to 5.3.  However, that doesn't exist in maven central, but only in [Boundless|http://example.com].

What do we do?

# We could exclude the jj2000 dependency from GRIB, and that functionality won't work for GRIB folks
# We could add a warning if we see {{jai-imageio-jpeg2000}} is on the classpath to instruct users to exclude jj2000.
# Other options?
",smartshark_2_2,2602,tika,"""[0.9141884446144104, 0.08581148833036423]"""
2930,231189,Add Lingo24 Language Detector,Add LanguageDetector for the Lingo24 Premium MT API's /langid resource,smartshark_2_2,2453,tika,"""[0.9984220266342163, 0.0015779576497152448]"""
2931,231190,Improve doc and docx parsing to include more things,"There are several parts of the word documents (.doc and .docx) that we don't currently extract, but which would be nice to have.

These include:
* Hyperlinks
* Images (img tag referencing the name of the embeded image)
* Headings (when the default heading styles are used)
* Style information (when a style other than Default or a body is used on a paragraph, markup the p tag with it)

I'm proposing to add support for these in the near future",smartshark_2_2,416,tika,"""[0.9983550906181335, 0.0016449358081445098]"""
2932,231191,iCalendar not properly recognized as text/calendar,"At the moment the detection of text/calender is covered by the following mime-type-element:

{code:xml}
  <mime-type type=""text/calendar"">
    <magic priority=""50"">
      <match value=""BEGIN:VCALENDAR"" type=""string"" offset=""0"">
        <match value=""VERSION:2.0"" type=""string"" offset=""15:30""/>
      </match>
    </magic>
    <glob pattern=""*.ics""/>
    <glob pattern=""*.ifb""/>
    <sub-class-of type=""text/plain""/>
  </mime-type>
{code}

This recognition will fail, if VERSION:2.0 is not the first property after BEGIN:VCALENDAR.
Since this is not always the case (check: [https://tools.ietf.org/html/rfc5545|https://tools.ietf.org/html/rfc5545] 3.6. Calendar Components) recognition may fail for calendar objects with PRODID or other properties:

 Section ""4. iCalendar Object Examples"" shows some of these cases:

{code}
       BEGIN:VCALENDAR
       PRODID:-//xyz Corp//NONSGML PDA Calendar Version 1.0//EN
       VERSION:2.0
       BEGIN:VEVENT
       DTSTAMP:19960704T120000Z
       UID:uid1@example.com
       ORGANIZER:mailto:jsmith@example.com
       DTSTART:19960918T143000Z
       DTEND:19960920T220000Z
       STATUS:CONFIRMED
       CATEGORIES:CONFERENCE
       SUMMARY:Networld+Interop Conference
       DESCRIPTION:Networld+Interop Conference
         and Exhibit\nAtlanta World Congress Center\n
        Atlanta\, Georgia
       END:VEVENT
       END:VCALENDAR
{code}

or

{code}
       BEGIN:VCALENDAR
       METHOD:xyz
       VERSION:2.0
       PRODID:-//ABC Corporation//NONSGML My Product//EN
       BEGIN:VEVENT
       DTSTAMP:19970324T120000Z
       SEQUENCE:0
       UID:uid3@example.com
       ORGANIZER:mailto:jdoe@example.com
       ATTENDEE;RSVP=TRUE:mailto:jsmith@example.com
       DTSTART:19970324T123000Z
       DTEND:19970324T210000Z
       CATEGORIES:MEETING,PROJECT
       CLASS:PUBLIC
       SUMMARY:Calendaring Interoperability Planning Meeting
       DESCRIPTION:Discuss how we can test c&s interoperability\n
        using iCalendar and other IETF standards.
       LOCATION:LDB Lobby
       ATTACH;FMTTYPE=application/postscript:ftp://example.com/pub/
        conf/bkgrnd.ps
       END:VEVENT
       END:VCALENDAR
{code}

I suggest to either 
a) widen the offset of the VERSION-match from 15:30 to 15:200 or sth. like that (not so good approach, since we don't know how Long the PRODID might be) 
or
b) to add sub-matches for CALSCALE, PRODID, METHOD. (This might still not cover everything, since there are x-prop and iana-prop properties. For now I can only confirm that there are PRODID or METHOD as first property after BEGIN:VCALENDAR.)


Regards

Andreas",smartshark_2_2,1693,tika,"""[0.2037380486726761, 0.7962619662284851]"""
2933,231192,Add page count to metadata ,"Very small patch, which adds page count to PDF's metadata. ",smartshark_2_2,385,tika,"""[0.9984161853790283, 0.0015837617684155703]"""
2934,231193,Update nekohtml version,Latest version currently available is 1.9.9.,smartshark_2_2,70,tika,"""[0.9982506632804871, 0.0017493072664365172]"""
2935,231194,Try to be more parsimonious creating TikaConfigs and ParseContexts,"If we run the AutoDetectParser() against the files in our unit tests (around 600 files*), there are 701 new instantiations of TikaConfig.  The time is around 20 seconds.  If we modify AutoDetectParser to pass its TikaConfig via the ParseContext if one isn't already specified, that drops to 234 instantiations, and parse time goes to ~17 seconds.

Let's make this simple change and look for other areas to decrease the number of times our parsers are creating a new TikaConfig.

*Note I did not include the testCHM2.chm monster in these runs.",smartshark_2_2,2432,tika,"""[0.9985893368721008, 0.0014106930466368794]"""
2936,231195,Separate NOTICEs and LICENSEs for binary and source packages,"The Tika source tree or a source release does not contain our dependencies, so the associated LICENSE and NOTICE files do not need to cover them.

However, the Maven build automatically includes the depencencies in the -standalone jar, which thus should contain LICENSE and NOTICE files that do cover the dependencies.",smartshark_2_2,85,tika,"""[0.9983239769935608, 0.0016759686404839158]"""
2937,231196,"Improve CharsetDetector to recognize UTF-16LE/BE,UTF-32LE/BE and UTF-7 with/without BOMs correctly","I would like to help to improve the recognition accuracy of the CharsetDetector.

Therefore I created a testset of plain/text-files to check the quality of org.apache.tika.parser.txt.CharsetDetector: charset.tar.gz
(Testset created out of http://source.icu-project.org/repos/icu/icu4j/tags/release-4-8/main/tests/core/src/com/ibm/icu/dev/test/charsetdet/CharsetDetectionTests.xml)

The Testset was processed using TIKA1.17 (ID: 877d621, HEAD from 26.10.2017) and ICU4J 59.1 CharsetDetector with custom UTF-7 improvements. Here are the results:

{noformat}
TIKA-1.17
charset.tar.gz
Correct recognitions: 165/341
{noformat}

{noformat}
TIKA-1.17+ UTF-7 recognizer:
charset.tar.gz
Correct recognitions: 213/341
{noformat}

{noformat}
ICU4j 59.1 + UTF-7 recognizer:
charset.tar.gz
Correct recognitions: 333/341
{noformat}




As UTF-7 recognizer I used these two simple classes:

{code:java}
package test.utils;

import java.util.Arrays;

/**
 * Pattern state container for the Boyer-Moore algorithm
 */
public final class BoyerMoorePattern
{

    private final byte[] pattern;

    private final int[] skipArray;

    public BoyerMoorePattern(byte[] pattern)
    {
        this.pattern = pattern;
        skipArray = new int[256];
        Arrays.fill(skipArray, -1);
        // Initialize with pattern values
        for (int i = 0; i < pattern.length; i++)
        {
            skipArray[pattern[i] & 0xFF] = i;
        }
    }

    /**
     * Get the pattern length
     * 
     * @return length as int
     */
    public int getLength()
    {
        return pattern.length;
    }

    /**
     * Searches for the first occurrence of the pattern in the input byte array.
     * 
     * @param data - The data we want to search in
     * @param startIdx - The startindex
     * @param endIdx - The endindex
     * @return offset as int or -1 if not found at all
     */
    public final int searchPattern(byte[] data, int startIdx, int endIdx)
    {
        int patternLength = pattern.length;
        int skip = 0;
        for (int i = startIdx; i <= endIdx - patternLength; i += skip)
        {
            skip = 0;
            for (int j = patternLength - 1; j >= 0; j--)
            {
                if (pattern[j] != data[i + j])
                {
                    skip = Math.max(1, j - skipArray[data[i + j] & 0xFF]);
                    break;
                }
            }
            if (skip == 0)
            {
                return i;
            }
        }

        return -1;
    }

    /**
     * Searches for the first occurrence of the pattern in the input byte array.
     * 
     * @param data - The data we want to search in
     * @param startIdx - The startindex
     * @return offset as int
     */
    public final int searchPattern(byte[] data, int startIdx)
    {
        return searchPattern(data, startIdx, data.length);
    }

    /**
     * Searches for the first occurrence of the pattern in the input byte array.
     * 
     * @param data - The data we want to search in
     * @return offset as int or -1 if not found at all
     */
    public final int searchPattern(byte[] data)
    {
        return searchPattern(data, 0, data.length);
    }
}
{code}



{code:java}
package test;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;
import java.util.logging.Logger;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.commons.io.IOUtils;
import org.apache.tika.detect.EncodingDetector;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.txt.CharsetDetector;
import org.apache.tika.parser.txt.CharsetMatch;

import test.utils.BoyerMoorePattern;


public class MyEncodingDetector implements EncodingDetector {
	
	public Charset detect(InputStream input, Metadata metadata)
			throws IOException {

		
		CharsetDetector detector;
		CharsetMatch match;

		detector = new CharsetDetector();

		detector.setText(input);
		match = detector.detect();

		match = detector.detect();
		
		String charsetName = isItUtf7(match, IOUtils.toByteArray(input)); // determines whether the match is UTF-7 or not
		
		if (charsetName != null) {
			return Charset.forName(charsetName);
		}
		return null;
	}


    /**
     * Checks for BOM and determines whether it is UTF-7 or not.
     * 
     * @param match - The default match we expect, if it is not UTF-7
     * @param data - The bytearray we want to check
     * 
     * @return match
     */
    private String isItUtf7(CharsetMatch match, byte[] data)
    {
        if (isUTF7withBOM(data) || isUTF7withoutBOM(data)) {
            return ""UTF-7"";
        } else {
        	if (match != null) {
        		return match.getName();
        	}
        	return null;
        }        
    }
    
    private boolean isUTF7withBOM(byte[] data) {
        if ((data.length > 4 && data[0] == 43 && data[1] == 47 && data[2] == 118)
                && (data[3] == 56 || data[3] == 57 || data[3] == 43 || data[3] == 47))
        {
            // Checkin byte-array for ""byte order marks"" (BOM):
            // 43 47 118 56
            // 43 47 118 57
            // 43 47 118 43
            // 43 47 118 47
            return true;
        }
        return false;
    }
    
    private boolean isUTF7withoutBOM(byte[] data) {
        byte[] utf7StartPattern = ""+"".getBytes();
        byte[] utf7EndPattern = ""-"".getBytes();
        BoyerMoorePattern bmpattern = new BoyerMoorePattern(utf7StartPattern); // create a new pattern with the bytes
        int startPosSP = bmpattern.searchPattern(data);
        
        BoyerMoorePattern empattern = new BoyerMoorePattern(utf7EndPattern); // create a new pattern with the bytes
        int startPosEP = empattern.searchPattern(data);
		
        if (startPosSP != -1 && startPosEP != -1) {
        	// the pattern was found, so we can create a regular expression for the basic pattern now

        	Pattern p = Pattern.compile(""\\+[a-zA-Z]\\w{2,}\\-"");	// a word with length of at least 3 characters or more
        	Matcher m = p.matcher(new String(data));
        	
        	int numberMatches = 0;
        	while (m.find()) {
        		numberMatches++;
        	}
        	
        	System.out.println(""Number of possible UTF-7 regex matches: "" + numberMatches);

        	int minimumMatches = 3;
        	
        	if (numberMatches > minimumMatches) {	// if there are more than minimumMatches ""+xxx-"" words the expected encoding shall be UTF-7
        		return true;
        	}
        }
        
        return false;
    }
}
{code}



There might be some false positive (FP) recognitions with the current regex and the number of matches.
A better approach might be to set the minimumMatches in dependence of the amount of text given to the detector.

This is just a simple first try, nothing for productivity. It even does not cover all possible UTF-7 strings.



By the way:

I am perfectly aware of the fact that the current testset does only cover a few encodings. However, the specified files address the main weakness of the current CharsetDetector.

I don't know the history that lead to the creation of the CharsetDetector in TIKA and why ICU4J was rebuild with extensions like the cp866 ngram detection, instead of participating in icu4j development.
Wouldn't it be better to forward the changes of the CharsetDetector to the ICU4J developers so they can implement missing encodings?

Is it planned to include the newest version of ICU4J in future releases of TIKA?

What about neural networks to determine some or all charsets? (given that there are enough testfiles)",smartshark_2_2,2674,tika,"""[0.9983487129211426, 0.0016513261944055557]"""
2938,231197, Tess4jOCRParser - A simpler Java version of TesseractOCRParser,"Right now, TesseractOCRParser calls tesseract and imagemagick from command line. Intention of this new parser ""Tess4jOCRParser"" is to use the Tess4J API instead of the runtime.exec way to executing tesseract out of process.  ",smartshark_2_2,2525,tika,"""[0.9983260035514832, 0.0016740343999117613]"""
2939,231198,Add more mimetypes for famous programming languages,"Would it be possible to add the Mime Types of these programming lanuages from the attached custom-mimetypes.xml into the tika-mimetypes.xml-file?

* Asciidoc: [text/x-asciidoc|http://discuss.asciidoctor.org/Mimetype-for-Asciidoc-td211.html]
* D: text/x-d
* HAML: text/x-haml
* Haxe: text/x-haxe
* R: text/x-rsrc
* XQuery: application/xquery

These mimetypes are also used in other projects like the CodeMirror editor.",smartshark_2_2,931,tika,"""[0.998374342918396, 0.001625659642741084]"""
2940,231199,Referenced version of Apache SIS (org.apache.sis) is branch EOL,"Currently referenced 0.6.x version of Apache SIS (org.apache.sis) is branch EOL.  Request moving to latest active 0.8.x branch.
",smartshark_2_2,2798,tika,"""[0.9977904558181763, 0.0022095239255577326]"""
2941,231200,Automatic checks against backwards-incompatible API changes,"As we get closer to 1.x we should add tooling like the Maven Clirr plugin [1] to guard against accidental backwards-incompatible API changes.

[1] http://mojo.codehaus.org/clirr-maven-plugin/",smartshark_2_2,500,tika,"""[0.998340368270874, 0.0016595805063843727]"""
2942,231201,Tika 2.0 - Add Encoding Detector and Language Detectors to Dynamic Service Loader,Currently only Parser and Detector classes are added to the dynamic ServiceLoader list.  We should extend this to include the EncodingDetector and LanguageDetector  ,smartshark_2_2,2182,tika,"""[0.9983503818511963, 0.0016496526077389717]"""
2943,231202,New Detector and Parser classes for Time Stamped Data Envelope file format,"Hello,
I'm Fabio Evangelista from Rome. I'm working for an italian Public Administration company and i'm using Apache Tika in my Java applications to detect and parse a broad kinds of file formats. During that activity, after following your good guide on Tika project page, I've made with success new type of Detector and Parser classes for a particular crypto timestamp type with these caracteristics:

Format name:               Time Stamped Data Envelope

Mime Type:                   application/timestamped-data

File extension:              .tsd

TSD file hax magic code at the start of the file:   30 80 06 0B 2A 86 48 86 F7

I've integrated and tested successfully with my applications those new classes in Tika 1.13 tika-core.jar and tika-parsers.jar. What should I do to submit my new classes to you? Should I to push those in a particular git branch or, is there a particular process to follow to submit my classes?

Thank you for you patience and best regards.
Fabio.",smartshark_2_2,2509,tika,"""[0.9984318614006042, 0.0015681650256738067]"""
2944,231203,Upgrade to PDFBox 1.8.1,"The PDFBox project has just announced version 1.8.1 of the PDFBox library containing many fixes and improvements that will be useful to Tika. Therefore, it will be useful to upgrade once available in the Maven Repo.",smartshark_2_2,2334,tika,"""[0.9981966614723206, 0.0018033243250101805]"""
2945,231204,Subtypes for common text formats currently included in text/plain,"Currently, we have a very large number of file extension globs all feeding into the {{text/plain}} mimetype. This includes not only variations on actual plain text, but also lots of other text-based formats (eg config or makefile/autoconf files). This list dates back quite a while (TIKA-85 seems to have added most of them)

While this simplifies things in Tika, it has the downside of making it very tricky for people to add custom parsers for these text-based formats (eg [https://stackoverflow.com/questions/48411421/define-a-mime-type-for-txt-files-for-tika] where they want to handle .cfg differently to other .txt)

Because of howÂ {{AutoDetectParser}} works, as long as there's no more specific parser defined, if we create some new {{text/}} subtypes which extend {{text/plain}} then there won't be any change in parsing behaviour. The only change would be for detection, where a more specific type would be returned

I therefore propose that we pull some of these (file-magic-less) globs out into other {{text/}} mimetypes with a parent of {{text/plan}} , grouped roughly by type",smartshark_2_2,2804,tika,"""[0.9984033703804016, 0.0015965983038768172]"""
2946,231205,Basic parser for old Excel files (eg Excel 4),"In TIKA-1487, we added mime magic for the pre-OLE2 excel file formats. Based on the reading of the OpenOffice Excel docs for that, it looks like it should be possible to produce a basic parser to extract key bits of info (eg strings) from these older file formats. 

This would likely largely be done by having a custom record iterator for the older formats, then passing the handful of ""interesting"" records to POI's record classes (maybe with some tweaks for the older formats) to have the binary data parsed, then returned by the parser",smartshark_2_2,1227,tika,"""[0.9984086155891418, 0.0015914167743176222]"""
2947,231206,Geographic metadata namespace ,"As discussed in TIKA-443, we should have a new Geographic namespace. Initially, this will hold just:
* LATITUDE = geo:latitude
* LONGITUDE = geo:longitude

Later we can add things like height, bearing etc as required

This namespace will be used by both the geographic formats, as well as things like JPEG (exif geo tags) and HTML (icbm geo tags)",smartshark_2_2,362,tika,"""[0.9983900785446167, 0.001609977101907134]"""
2948,231207,Bug in TagSoup causes IOException,"When uploading documents to a jackrabbit 2.1 repository the following exception was received.  It looks like a bug in tagsoup 1.2 (if you search the tagsoup yahoo group you can see that it may be caused by '&' characters in the html being parsed):
27.05.2010 14:57:18 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 180)
org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.html.HtmlParser@eba477
       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:126)
       at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
       at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:174)
       at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
       at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Pushback buffer overflow
       at java.io.PushbackReader.unread(Unknown Source)
       at org.ccil.cowan.tagsoup.HTMLScanner.unread(HTMLScanner.java:274)
       at org.ccil.cowan.tagsoup.HTMLScanner.scan(HTMLScanner.java:487)
       at org.ccil.cowan.tagsoup.Parser.parse(Parser.java:449)
       at org.apache.tika.parser.html.HtmlParser.parse(HtmlParser.java:177)
       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
       ... 10 more",smartshark_2_2,573,tika,"""[0.08007439970970154, 0.9199255704879761]"""
2949,231208,"Parser and ParserFactory should work with resources' InputStreams but not their Files, URLs, or Strings.","To simplify the parser code, we want to move any code relating to resources as Files, URLs, and Strings outside to code in other areas.
",smartshark_2_2,4,tika,"""[0.9984259605407715, 0.0015740545932203531]"""
2950,231209,"protected Parser.parse(InputStream stream, Iterable<Content> contents)","In order to push towards a stateless Parser interface, I'd like to propose implementing the current Parser.getContents() method (as it exists after TIKA-26) in terms of a stateless abstract method with the following signature:

    protected abstract String parse(InputStream stream, Iterable<Content> contents) throws IOException, TikaException;

This method would return the fulltext content of the given stream as the String return value and place any extra metadata into the given set of Content instances. With this information the current Parser.getContents() method could populate a fulltext (and summary) Content entry and any regexp Content entries universally for any Parser classes. Also, we could centralize state handling and exception processing to a single class.",smartshark_2_2,154,tika,"""[0.9984069466590881, 0.0015930974623188376]"""
2951,231210,Fix dependency convergence,"Hi,

We tried to upgrade Tika to 1.17 in Hibernate Search and we had some dependency convergence issues:

{code}
Dependency convergence error for com.healthmarketscience.jackcess:jackcess:2.1.8 paths to dependency are:
+-org.hibernate:hibernate-search-engine:5.10.0-SNAPSHOT
Â  Â  +-org.apache.tika:tika-parsers:1.17
 Â  Â Â Â  Â Â +-com.healthmarketscience.jackcess:jackcess:2.1.8
and
+-org.hibernate:hibernate-search-engine:5.10.0-SNAPSHOT
 Â  Â Â +-org.apache.tika:tika-parsers:1.17
 Â  Â Â Â  Â Â +-com.healthmarketscience.jackcess:jackcess-encrypt:2.1.2
 Â  Â Â Â  Â Â Â  Â Â +-com.healthmarketscience.jackcess:jackcess:2.1.0
{code}

We could fix them downstream in Hibernate Search but I thought it would be better if Tika could ensure the convergence of its dependencies using the Maven enforcer plugin so that all the downstream projects can benefit from it.

Thanks.",smartshark_2_2,1673,tika,"""[0.990964412689209, 0.009035593830049038]"""
2952,231211,MS Write File,"We're currently identifying MS Write Files by suffix "".wri"" in one place in our mime defs, but we're also using MS Write File's magic {{0x31be0000}} to identify the file as an MSWord (doc) file in a different definition.

In govdocs1, there are a handful of .wri files with suffix .doc.  We're getting an Invalid Header exception for these files.

I think it would be better to move their magic out of our .doc definition to the .wri definition and use the EmptyParser.

Any objections?",smartshark_2_2,2393,tika,"""[0.9919487237930298, 0.008051267825067043]"""
2953,231212,Ability to Allow Empty and Duplicate Tika Values for XML Elements,"In some cases it is beneficial to allow empty and duplicate Tika metadata values for multi-valued XML elements like RDF bags.

Consider an example where the original source metadata is structured something like:
{code}
<Person>
  <FirstName>John</FirstName>
  <LastName>Smith</FirstName>
</Person>
<Person>
  <FirstName>Jane</FirstName>
  <LastName>Doe</FirstName>
</Person>
<Person>
  <FirstName>Bob</FirstName>
</Person>
<Person>
  <FirstName>Kate</FirstName>
  <LastName>Smith</FirstName>
</Person>
{code}

and since Tika stores only flat metadata we transform that before invoking a parser to something like:
{code}
 <custom:FirstName>
  <rdf:Bag>
   <rdf:li>John</rdf:li>
   <rdf:li>Jane</rdf:li>
   <rdf:li>Bob</rdf:li>
   <rdf:li>Kate</rdf:li>
  </rdf:Bag>
 </custom:FirstName>
 <custom:LastName>
  <rdf:Bag>
   <rdf:li>Smith</rdf:li>
   <rdf:li>Doe</rdf:li>
   <rdf:li></rdf:li>
   <rdf:li>Smith</rdf:li>
  </rdf:Bag>
 </custom:LastName>
{code}

The current behavior ignores empties and duplicates and we don't know if Bob or Kate ever had last names.  Empties or duplicates in other positions result in an incorrect mapping of data.

We should allow the option to create an {{ElementMetadataHandler}} which allows empty and/or duplicate values.",smartshark_2_2,939,tika,"""[0.9983555674552917, 0.0016444025095552206]"""
2954,231213,Add a junk text detector to Tika,"It would be helpful to have a detector that flags documents whose extracted text is junk.  This could be used as a component of TIKA-1332 or as a standalone detector.  See TIKA-1332 for some initial ideas of what statistics we might use for such a detector.

Two use cases:
* Parser developers could quickly see whether changes in code lead to less ""junky"" documents or more ""junky"" documents.  This would also aid in prioritizing manual review of output comparison (see discussion in TIKA-1419).
* Search system integrators could use that information to set document specific relevancy rankings or to avoid indexing a document",smartshark_2_2,2481,tika,"""[0.9982629418373108, 0.0017370532732456923]"""
2955,231214,OpenDocument basic style support,"I've added basic support for list and text styles. Paragraph styles are omitted on purpose -- one could use the style names as class names, though.

Only bold, italic, and underlined text is supported.

Lists now differentiate between ordered and unordered lists.

Test case included. I've also changed the ODFParserTest to make a bit more use of the methods of its super class.
",smartshark_2_2,1462,tika,"""[0.9984797835350037, 0.001520237186923623]"""
2956,231215,Add ApertureParser,"We should have a parser component that uses Aperture (http://aperture.sourceforge.net/) for parsing. Such a component should probably not be used by default in Tika, but would be nice for people who are using the Tika API (once it's stable) but still want to leverage some of the features of Aperture.",smartshark_2_2,469,tika,"""[0.9983052015304565, 0.0016947629628702998]"""
2957,231216,Make HtmlParser customizable through ParseContext,"In TIKA-304 we added the mapSafeElement() and isDiscardElement() methods to HtmlParser so that subclasses could better customize how incoming HTML elements get mapped to the XHMTL output from Tika. This works fairly well but requires you to modify the Tika configuration file or to explicitly inject a custom HtmlParser subclass instance to the CompositeParser instance you're using (AutoDetectParser, etc.).

Now that we have the ParseContext mechanism available to simplify such customization, it would be nice to allow you to provide a custom ""HTML mapper"" instance through the parse context and have HtmlParser call that mapper (if available) for the mapSafeElement() and isDiscardElement() operations.",smartshark_2_2,244,tika,"""[0.9983404874801636, 0.0016594642074778676]"""
2958,231217,Add .svn to .gitignore,This is for folks who may be working on TIKA issues on their own Git branches. It is an extremely trivial change.  ,smartshark_2_2,969,tika,"""[0.998015284538269, 0.0019847219809889793]"""
2959,231218,Weird <scope> associated to vorbis-java-core tests in vorbis-java-tika,"Here is how the dependencu is declared:
{code}
    <dependency>
      <groupId>${project.groupId}</groupId>
      <artifactId>vorbis-java-core</artifactId>
      <version>${project.version}</version>
      <classifier>tests</classifier>
      <scope>test,provided</scope>
    </dependency>
{code}

It's the first time I see such a multiple scope.

Among other things it make the Maven WAR plugin include this dependency as if it was a build scope dependency.


I don't know if this worked in old version of Maven but org.apache.maven.model.Dependency has only one scope for sure.",smartshark_2_2,983,tika,"""[0.9618830680847168, 0.03811691701412201]"""
2960,231219,Standardizing current Object Recognition REST parsers,"# This involves adding apiBaseUris and refactoring current Object Recognition REST parsers,
# Refactoring dockerfiles related to those parsers.
#  Moving the logic related to checking minimum confidence into servers",smartshark_2_2,2704,tika,"""[0.9982419013977051, 0.0017580436542630196]"""
2961,231220,Enable PMD reports,"PMD (http://pmd.sourceforge.net/) is another static analysis tool for Java. The reports seem useful and enabling them in Maven is trivial.
",smartshark_2_2,76,tika,"""[0.9983212351799011, 0.0016787424683570862]"""
2962,231221,secure-processing not supported by some JAXP implementations (2),"Related to http://issues.apache.org/jira/browse/TIKA-271

Some of the parsers try to set the secure-processing feature that JAXP requires all parser implementations to support. Unfortunately some XML parsers don't support the feature, which causes the following exception: 

org.xml.sax.SAXNotRecognizedException: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.

",smartshark_2_2,234,tika,"""[0.4819467067718506, 0.5180532932281494]"""
2963,231222,Avoid the <resource/> entry on ${basedir},"At least the m2eclipse Eclipse plugin is having trouble with the <resource/> entry we use to include the LICENSE, NOTICE, and README files in META-INF of the resulting jar artifact. The problem is ${basedir} (i.e. the root of the whole Tika checkout) used as the resource directory, which causes Eclipse to treat the whole Tika checkout as a source directory and prevents nesting of the real src/main/java and other source directories.

We could achieve the same effect (bundling of the META-INF files) for example by separately configuring the copy-resources goal of the maven-resources-plugin.",smartshark_2_2,30,tika,"""[0.9980667233467102, 0.0019332966767251492]"""
2964,231223,RFC822Parser should configure Mime4j not to fail reading mails containing more than 1000 chars in one headers text (even if folded),"Standard configuration of Mime4j accepts only 1000 characters per line and 1000 charackters per header. The streaming approach of tika should not need theese limitations, an exception is being thrown and none of the data read is available.

Solution:
Replace all occurences of:

Parser parser = new RFC822Parser();

by:

MimeEntityConfig config = new MimeEntityConfig();
config.setMaxLineLen(-1);
config.setMaxContentLen(-1);
Parser parser = new RFC822Parser(config);
",smartshark_2_2,580,tika,"""[0.883747935295105, 0.11625207960605621]"""
2965,231224,Tika 2.0 - Checklist of small things to be solved/checked before release of 2.0,"As we make changes to trunk (1.x), there are some small issues that will probably be taken care of by the time we release 2.0.  Let's use this issue to track those issues.

* Make sure that we don't have a jdom version conflict between Rome and Grib (TIKA-1950).",smartshark_2_2,2051,tika,"""[0.9982138872146606, 0.0017860822845250368]"""
2966,231225,New ContentHandler for plain text output that has no problem with missing white space after XHTML block tags,"One problem with mapping document content to plain text is incorrect whitespace handling:
The normal way to parse documents to plain text is to instantiate a parser and pass the SAX events from the parser to a BodyContentHandler(TextContentHandler(Writer)). This appends all output to a writer (see example on web site).

This works good for thumb parsers that just create a single <p>> tag in XHTML output whith all content of the document in it (including newlines).

As soon, as a more inteligent parser is used (e.g. HTML Parser) that creates multiple nodes and a feature-rich XHTML document, the problems begin. The TextContentHandler just strips all tags away and only characters() events are forwarded to the Writer. When the original document (e.g. a HTML document) does not contain additional whitespace and linefeeds (e.g. it is correct and possible to create a XHTML document with all content in one text line, but consisting of several paragraphs. In this case </p><p> events between paragraphs are stripped and there is no whitespace anymore between the two paragraphs.

My patch contains a new XHTMLToTextContentHandler, that checks the elements and inserts whitespace to the output depending on the XHTML tag type. HTML block tags like <p/> get a newline at the end, but HTML inline tags do not add whitespace. This mapping is done by a simple Set<String> of tag names extracted from the XHTML 1.0 spec. To make it even better, tables are printed out with white space and tabs between cells.

With this patch, I am able to correctly index a lot of document with Lucene.

The patch also changes some tests to correctly check for the '\n' at the end of plain text streams (which are included because of the single <p>-paragraph around plain text).",smartshark_2_2,905,tika,"""[0.5605852007865906, 0.43941476941108704]"""
2967,231226,"Fix key location, keys file and download link","Thanks to [~sebb@apache.org] for pointing out areas for improvement in our release process: https://lists.apache.org/thread.html/3ca68b758fea4dbc890d5d600590b680a1fe745fe7bd559e7e19789e@%3Cdev.tika.apache.org%3E

1) Change link in announcement email to http://tika.apache.org/download.html
2) Move keys to the standard link: https://www.apache.org/dist/tika/KEYS
-3) At the least, I need to add back my 1.15 release key- Same key I'm using now...if anyone else needs to add a key, please do.

Other areas for improvement in our announce/release process?",smartshark_2_2,2780,tika,"""[0.9982251524925232, 0.0017748490208759904]"""
2968,231227,Provide constructor for AutoDetectParser that has explicit list of supported parsers,"To reduce the size of the Tika dependency chain, it's useful to exclude the supporting jars for types that don't need to process (e.g. Microsoft docs, PDFs, etc). This can easily remove 20MB of 3rd party jars.

With 0.8-SNAPSHOT, the TikaConfig(Classpath) constructor now finds and instantiates all Parser-based classes found on the classpath. Which can trigger errors when 3rd party jars are missing.

One solution, as proposed by Jukka, is to provide an alternative constructor for AutoDetectParser which includes the list of supported parsers, and avoids creating the default TikaConfig.
",smartshark_2_2,413,tika,"""[0.9983933568000793, 0.0016066404059529305]"""
2969,231228,Add alternative search provider on site,"Add additional search provider (to existed Lucid Find) search-lucene.com.

Initiated in discussion: http://www.search-lucene.com/m/uTJxE2kcRv1

Requirements (copied from discussion):

Jukka Zitting: ""Ideally the search box would allow the user to choose which provider
to use. Something like a cookie that remembers the user's selection
would be nice. If the user doesn't make an explicit selection of the
provider, then one should be selected randomly for the first search
and remembered afterwards for consistency.

It would be great if the required logic was implemented on the client
side using javascript, as otherwise we'd need to start messing up with
CGI scripts, etc.""",smartshark_2_2,405,tika,"""[0.9984985589981079, 0.0015013973461464047]"""
2970,231229,Warnings during Site generation,"[INFO] Javadoc Warnings 
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\gui\TikaGUI.java:68: warning - Missing closing '}' character for inline tag: ""{@link AutoDetectParser)
[WARNING] instance as the default parser.""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\mime\HexCoDec.java:62: warning - @param argument ""starIndex"" is not a parameter n ame.
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\mime\HexCoDec.java:95: warning - @param argument ""starIndex"" is not a parameter name.
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\mime\MimeTypes.java:445: warning - @param argument ""stream"" is not a parameter name.
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\ParserPostProcessor.java:37: warning - Tag @link: reference not found: Content
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\ParsingReader.java:42: warning - End Delimiter } missing for possible SeeTag in comment string: ""Reader for the text content from a given binary stream. This class
[WARNING] starts a background thread and uses a {@link Parser}
[WARNING] ({@link AutoDetectParser) by default) to parse the text content from
[WARNING] a given input stream. The {@link BodyContentHandler} class and a pipe
[WARNING] is used to convert the push-based SAX event stream to the pull-based
[WARNING] character stream defined by the {@link Reader} interface.""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\ParsingReader.java:196: warning - @param argument ""cbuff"" is not a parameter name.
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\mp3\Mp3Parser.java:36: warning - Tag @see:illegal character: ""58"" in ""http://www.id3.org/ID3v1""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\mp3\Mp3Parser.java:36: warning - Tag @see:illegal character: ""47"" in ""http://www.id3.org/ID3v1""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\mp3\Mp3Parser.java:36: warning - Tag @see:illegal character: ""47"" in ""http://www.id3.org/ID3v1""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\mp3\Mp3Parser.java:36: warning - Tag @see:illegal character: ""47"" in ""http://www.id3.org/ID3v1""
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\mp3\Mp3Parser.java:36: warning - Tag @see: reference not found: http://www.id3.org/ID3v1
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\parser\pkg\bzip2\CBZip2InputStream.java:49: warning - Tag @link: reference not found: BZip2OutputStream
[WARNING] CBZip2OutputStream
[WARNING] C:\ws\tika\src\main\java\org\apache\tika\utils\ParseUtils.java:149: warning - @param argument ""URL"" is not a parameter name.
[INFO] Generating ""Source Xref"" report.",smartshark_2_2,38,tika,"""[0.9951934218406677, 0.00480649434030056]"""
2971,231230,Add applefile parser,"Now that we have detection for applefile (regular file w extra apple header), we are losing content that used to be extracted. For example, pdf files that have this extra header are now detected as applefile and then not parsed.  I found a spec and will commit on Monday.",smartshark_2_2,2129,tika,"""[0.9884119629859924, 0.011587974615395069]"""
2972,231231,Default parser/detector loading should warn on missing/empty classes,"As mentioned on-list, with the parser modularisation changes in 2.x, the chances of a newbie getting something wrong goes up. We should therefore change the default in 2.x to warn (rather than silently ignore) if parsers or detectors are missing / none are defined

This remains configurable with Tika Config XML, explicit TikaConfig object setup etc, so it can be easily silenced if wanted. It's just the default which will warn people if they've made a mistake!",smartshark_2_2,1909,tika,"""[0.9986441731452942, 0.001355891115963459]"""
2973,231232,Refactor/merge new experimental docx/pptx components,"We can get rid of a fair amount of duplicate code by merging the docx and pptx SAX handlers.  If we find significant differences in future desired functionality, we can split them back out.",smartshark_2_2,2347,tika,"""[0.9980131387710571, 0.0019868342205882072]"""
2974,231233,Incorrectly MimeType detection for Apache Lucene web site,"Tika 1.5 detect many page from apache lucene web site as xml, for example this page 
http://lucene.apache.org/core/discussion.html

Here are error log:, it failed to parse becuase it use xml parser

Apache Tika was unable to parse the document
at http://lucene.apache.org/core/discussion.html.

The full exception stack trace is included below:

org.apache.tika.exception.TikaException: XML parse error
	at org.apache.tika.parser.xml.XMLParser.parse(XMLParser.java:78)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)
	at org.apache.tika.gui.TikaGUI.openURL(TikaGUI.java:293)
	at org.apache.tika.gui.TikaGUI.actionPerformed(TikaGUI.java:247)
	at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2018)",smartshark_2_2,1427,tika,"""[0.9173898100852966, 0.08261017501354218]"""
2975,231234,modify TikaFileTypeDetector to use new detect method accepting java.nio.file.Path,Utilize the new org.apache.tika.Tika.detect(Path) method,smartshark_2_2,1886,tika,"""[0.9983602166175842, 0.0016398153966292739]"""
2976,231235,Create a org.apache.tika.sax package for SAX utilities in Tika,"We have a number of helper and decorator classes for the SAX ContentHandler interface. Instead of having these utility classes in org.apache.tika.parser, it would be better to create a separate org.apache.tika.sax package for them.",smartshark_2_2,136,tika,"""[0.9980403780937195, 0.0019596770871430635]"""
2977,231236,Metadata.formatDate causes blocking in concurrent use,Currently this is a synchronized method that uses a single instance of DateFormat. Instead it could use a pool of ThreadLocal DateFormat instances and avoid the sync blocking.,smartshark_2_2,705,tika,"""[0.4409079849720001, 0.5590920448303223]"""
2978,231237,Inconsistent formatting in parsers pom.xml file,"Indentation inconsistency in tika-parsers/pom.xml under ""Provided Dependencies"" comment.",smartshark_2_2,1562,tika,"""[0.9957038760185242, 0.0042961067520082]"""
2979,231238,No logging in tika-core,"As discussed on the mailing list, it would make things simpler if the Tika core didn't log anything and thus had no logging dependencies.",smartshark_2_2,205,tika,"""[0.9983261227607727, 0.0016738318372517824]"""
2980,231239,TeeContentHandler constructor should use varargs,"Currently the TeeContentHandler only branches the incoming SAX event stream to two underlying handlers. There is no reason it couldn't handle any number of underlying handlers, and the most straightforward way to do that is to allow varargs in the constructor:

    public TeeContentHandler(ContentHandler... handlers)

Doing this introduces a minor backwards incompatibility with the 0.1 release, but I think it's worth it especially since we explicitly made no API stability promises in 0.1.",smartshark_2_2,64,tika,"""[0.998431384563446, 0.0015685592079535127]"""
2981,231240,Optimize type detection speed,It would be good to do some simple benchmarks on the type detection code (Tika.detect) to see if there are obvious performance optimizations we could make. There are some use cases like attaching file type information directory listings where type detection speed is important and not necessarily dwarfed by IO waits.,smartshark_2_2,246,tika,"""[0.9981556534767151, 0.0018443819135427475]"""
2982,231241,Detection is not working properly for detecting HWP 5.0 file,"HWP file has two formats which are HWP 3.0 and HWP 5.0.
'tika-app-1.10.jar' detects HWP 3.0 format's file correctly.
But, not for HWP 5.0.
Used commands and returned results are addresses below.

> java -jar tika-app-1.10.jar --detect test_3.0.hwp
> application/x-hwp

> java -jar tika-app-1.10.jar --detect test_5.0.hwp
> application/x-tika-msoffice",smartshark_2_2,1840,tika,"""[0.46572941541671753, 0.5342705845832825]"""
2983,231242,Let's run Tika against a large batch of docs nightly,"Many thanks to [~lewismc] for TIKA-1301!  Once we get nightly builds up and running again, it might be fun to run Tika regularly against a large set of docs and report metrics.

One excellent candidate corpus is govdocs1: http://digitalcorpora.org/corpora/files.

Any other candidate corpora?  
[~willp-bl], have anything handy you'd like to contribute? 
[http://www.openplanetsfoundation.org/blogs/2014-03-21-tika-ride-characterising-web-content-nanite]

 ;) ",smartshark_2_2,1081,tika,"""[0.9984087347984314, 0.0015912940725684166]"""
2984,231243,Add parameter to tika-app to supply password for decryption,"Now that TIKA-850 is in, we need a way to pass a password into tika-app.jar so that we can parse encrypted docs from commandline or GUI.",smartshark_2_2,922,tika,"""[0.998386025428772, 0.0016139568760991096]"""
2985,231244,Make handling of macros equivalent btwn VBA in MSOffice and JS in PDFs,"The current default behavior is to extract VBA macros from MSOffice files but not to extract JS from PDFs.  Now that we have a config for MSOffice files, I propose changing the default behavior to NOT extract VBA macros from MSOffice files.  Users can opt in to extraction of macros via configuration.",smartshark_2_2,2469,tika,"""[0.9985476136207581, 0.0014523841673508286]"""
2986,231245,Better mime type for ooxml files,"Every ooxml type has it's own mime type, these should be correctly provided by Tika. A list of correct mime types can be found here: http://blogs.msdn.com/vsofficedeveloper/pages/Office-2007-Open-XML-MIME-Types.aspx",smartshark_2_2,200,tika,"""[0.9983144998550415, 0.0016854655696079135]"""
2987,231246,OCR in PDF files,"As described in this [stackoverflow-post|http://stackoverflow.com/questions/32354209/apache-tika-extract-scanned-pdf-files] i'm having troubles extracting text out of scanned PDF files. By scanned PDF files i mean PDF files that consist only of images. Because each page is an image i can't extract them using a custom ParsingEmbeddedDocumentExtractor. I also tried using the setExtractInlineImages method of the PDFParserConfig but this didn't work aswell.
There was already a [ticket|https://issues.apache.org/jira/browse/TIKA-93] regarding the OCR support and including the [PDF file|https://issues.apache.org/jira/secure/attachment/12627866/testOCR.pdf] i'm using for my tests.
Here is a JUnit-test about my issue:
{code:title=PDFOCRTest.java|borderStyle=solid}
@Test
public void testPDFOCRExtraction() throws IOException, SAXException, TikaException {
	File file = new File(filePath);
	InputStream stream = new FileInputStream(file);
	
	BodyContentHandler handler = new BodyContentHandler(Integer.MAX_VALUE);
	Metadata metadata = new Metadata();
	PDFParserConfig config = new PDFParserConfig();
	config.setExtractInlineImages(true);
	ParseContext context = new ParseContext();
	context.set(PDFParserConfig.class, config);
	
	PDFParser pdfParser = new PDFParser();
	pdfParser.setPDFParserConfig(config);
	pdfParser.parse(stream, handler, metadata, context);
	String text = handler.toString().trim();
	assertFalse(text.isEmpty());
}
{code}",smartshark_2_2,1836,tika,"""[0.7972710132598877, 0.2027289718389511]"""
2988,231247,Add table tags to parsed RTF documents,"In Tika 1.5, the RTF TextExtractor class at line 1156 has a TODO for extracting tables out of RTF documents.  I can create a table on a Mac using TextEdit, and in Microsoft Word (although curiously, not WordPad), but Tika should have the ability to extract the table contents to XHTML.

I found the RTF table element descriptions at the following website:

http://www.biblioscape.com/rtf15_spec.htm#Heading40",smartshark_2_2,1060,tika,"""[0.9981332421302795, 0.0018667359836399555]"""
2989,231248,List all the document formats supported by Tika,There should be a page on our web site that lists all the document formats supported by Tika and describes what parts of the document contents are extracted and how. This would make it easier for people to understand the capabilities (and restrictions) of Tika.,smartshark_2_2,170,tika,"""[0.9981805086135864, 0.0018195214215666056]"""
2990,231249,More fault-tolerant loading of parsers and detectors,"Currently Tika will fail to start even if a single configured parser or detector can not be loaded. Such cases occur often when required parser libraries or other dependencies are not available, and it would be good if Tika could degrade more gracefully in such situations.",smartshark_2_2,787,tika,"""[0.9983968138694763, 0.0016031638951972127]"""
2991,231250,Remove logging of duplicate parser definitions,"In TIKA-620 (revision 1085003) we added logging for cases where more than one parser is defined for the same media type. This is good for catching issues like classpath conflicts, but not for cases like in Jackrabbit where we use configuration [1] to explicitly override the parsers of certain media types.

Because of this issue and the general preference of no logging in tika-core I'd like to remove this logging functionality. If it's needed in some use cases, we could instead create a utility tool that goes through a list of parsers and finds all the duplicates. Deployments that want to avoid duplicates could then use this tool to flag them.

[1] http://svn.apache.org/repos/asf/jackrabbit/branches/2.2/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/query/lucene/tika-config.xml",smartshark_2_2,605,tika,"""[0.9985554814338684, 0.0014445295091718435]"""
2992,231251,Add ParsingReader,"Lucene Java takes a Reader as input when including content in a search index. Tika instead generates SAX events or (with WriteOutContentHandler) writes content to a Writer, which requires extra work when integrating with Lucene and other tools that expect a Reader.

To cover that case we should implement a ParsingReader class that takes an InputStream and a Tika Parser and returns the parsed text content through the Reader interface.",smartshark_2_2,71,tika,"""[0.9984694123268127, 0.0015305652050301433]"""
2993,231252,Date not extracted from email saved as plain txt,"HI have two email testfiles:

(1) A file that has been created by using ""save as"" in Mac Mail (this creates a .txt file)
(2) A file that has been created by dragging an email from Mac Mail to the Desktop (this creates an .eml file)

If I feed the files with

curl -T filename http://localhost:9998/detect/stream

I get the response ""message/rfc822"" for both files.

If I run

curl -T filename http://localhost:9998/meta

I get the metadata, but in the case of (1) I do not get the DATE extracted, while in case (2) I do.
",smartshark_2_2,2086,tika,"""[0.33964717388153076, 0.6603528261184692]"""
2994,231253,"Parsing ""HTML"" as DcXML","As of r881342, I encounter the following error when trying to parse the ""HTML"" from <http://www.nheri.org/> with the AutoDetectParser. (I will attach my example.) It works fine if I use the HtmlParser explicitly.

org.apache.tika.exception.TikaException: TIKA-237: Illegal SAXException from org.apache.tika.parser.xml.DcXMLParser@45d64c37
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:130)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
	at org.cdlib.was.tikaIndexer.parser$.main(Parser.scala:18)
	at org.cdlib.was.tikaIndexer.parser.main(Parser.scala)
Caused by: org.xml.sax.SAXParseException: The markup in the document following the root element must be well-formed.
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:195)
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:174)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:388)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1414)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$TrailingMiscDriver.next(XMLDocumentScannerImpl.java:1422)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
	at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:198)
	at org.apache.tika.parser.xml.XMLParser.parse(XMLParser.java:72)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)

",smartshark_2_2,300,tika,"""[0.3994993269443512, 0.6005007028579712]"""
2995,231254,Avoid NullPointerException in org.apache.tika.langdetect.OptimaizeLangDetector if models haven't been loaded,"In an intuitive usage of
{code:java}
LanguageDetector languageDetector = LanguageDetector.getDefaultLanguageDetector();
List<LanguageResult> languageResults = languageDetector.detectAll(someNonEmptyString);
{code}

`org.apache.tika.langdetect.OptimaizeLangDetector` might/will (?) be chosen as default detector, but since there's no call to `LanguageDetector.loadModels()` `LanguageDetector.detector` is `null` in `detectAll` which causes an unhelpful error situation which one has to investigate in the code. A simple check whether `detector` is `null` and throwing an `IllegalStateException(""models haven't been loaded yet (forgot to call loadModels?"")` would be much more intuitive.

If that corresponds to the expected behaviour (it's my first week with Tika), I can provide a patch or pull request.

experienced with 1.16-75-g4455a6f08",smartshark_2_2,2618,tika,"""[0.12116989493370056, 0.878830075263977]"""
2996,231255,"Let's extract the MAPI subtype (NOTE, STICKY, etc.) for msg files","MAPI includes a broad range of file types, including: NOTE (regular email), POST, CONTACT, APPOINTMENT, STICKYNOTE and TASK (others?).  We are treating every msg file as a NOTE.  As a first step towards finer grained parsing, let's at least pull out the MAPI type and store that in the metadata.  Recommendations for keys?",smartshark_2_2,2447,tika,"""[0.9984663724899292, 0.0015336595242843032]"""
2997,231256,Tika 2.0.0 -- Overarching task list for what we need to do before 2.0.0,"Let's use this issue to track issues that absolutely, positively have to be completed before we release Tika 2.0.",smartshark_2_2,2430,tika,"""[0.9980736970901489, 0.001926335971802473]"""
2998,231257,Simplify token counting in tika-eval,We can skip double analysis by moving the isAlphabeticOrIdeographic check into the CommonTokensCounter.,smartshark_2_2,2437,tika,"""[0.998254120349884, 0.0017459015361964703]"""
2999,231258,Use POST for tika-server form resources,"According to the [HTML Spec|http://www.w3.org/TR/html401/interact/forms.html#h-17.13.1], HTML forms can only use POST and GET. But, the {{multipart/form-data}} methods in tika-server use PUT.",smartshark_2_2,1266,tika,"""[0.9953975081443787, 0.004602453671395779]"""
3000,231259,Add configurability for language detection to BasicContentHandlerFactory,"The RecursiveParserWrapper needs a new handler via a ContentHandlerFactory for each embedded document.  There is currently no way to configure the BasicContentHandlerFactory to include language detection.  Let's add that.

This request is driven by [~philipp.steinkrueger@uni-koeln.de]'s request on TIKA-1982.",smartshark_2_2,2101,tika,"""[0.9984063506126404, 0.0015937072457745671]"""
3001,231260,Using of maven-changes-plugin instead of hand made changes.txt,Suggestion would be to use the maven-changes-plugin to product a change list in relationship to the distributed releases and the current snapshot release as part of the generated site. So it's part of the project reports and give good overview of the made changes etc.,smartshark_2_2,146,tika,"""[0.9984388947486877, 0.0015610987320542336]"""
3002,231261,Image not extracted via -z or -J in ODT,Sam Bayer identified this issue and shared the attached triggering document.  We should extract images from odt.,smartshark_2_2,2450,tika,"""[0.8121597766876221, 0.18784020841121674]"""
3003,231262,Lucene Document builder,"Build lucene document from tika using java.io.file, inputstream or file path. Lucene field configuration is possible.",smartshark_2_2,471,tika,"""[0.9896486401557922, 0.010351368226110935]"""
3004,231263,HtmlParser should strip linefeeds out of links,"A number of HTML pages contain links where the URL has a linefeed in the middle of it.

Browsers such as Firefox will automatically remove the character but Tika passes it back, which results in a broken URL.",smartshark_2_2,1913,tika,"""[0.6671383380889893, 0.33286166191101074]"""
3005,231264,Add a parser for sas7bdat,EPAM recently agreed to migrate to Apache 2.0 so that we can incorporate parso into Tika for sas7bdat files: https://github.com/epam/parso/issues/19 !!!,smartshark_2_2,1816,tika,"""[0.9984040856361389, 0.001595897600054741]"""
3006,231265,Upgrade to Apache POI 3.11 beta 1,"All being well, in a week there'll be a new release of Apache POI available, 3.11 beta 1

This issue is to track the upgrade, any required changes, and fixing any TODOs that this upgrade permits",smartshark_2_2,1129,tika,"""[0.9980140924453735, 0.0019859003368765116]"""
3007,231266,Add UnsupportedFormatException (extends TikaException),"It would be handy to have an UnsupportedFormatException to handle those cases where one mime type covers different versions, and the relevant parser only handles a subset.

When possible, we should try to differentiate by mime type, however, if there really is one mime for different versions, or if we have to rely on the parser to determine format version (e.g. when magic isn't enough), parsers should throw this Exception.",smartshark_2_2,2352,tika,"""[0.9985112547874451, 0.0014887062134221196]"""
3008,231267,Implement extraction of non-global variables from netCDF3 and netCDF4,"Speaking to Eric Nienhouse at the ongoing NSF funded Polar Cyberinfrastructure hackathon in NYC, we became aware that variables parameters contained within netCDF3 and netCDF4 are just as valuable (if not more valuable) as global attribute values. 
AFAIK, right now we only extract global attributes however we could extend the support to cater for the above observations.  ",smartshark_2_2,2754,tika,"""[0.9984484910964966, 0.0015515390550717711]"""
3009,231268,Provide version number in tika-server,Tika server currently does not provide its version number.,smartshark_2_2,722,tika,"""[0.9983363747596741, 0.0016636033542454243]"""
3010,231269,"Add configuration layer to configure, Parsers default configurable properties.","In several conditions, we want to change the default tika configuration, for some specific parsers, to change the behaviour of libraries/components wrapped in the parsers.

Currently, the ParseContext class allows, for means of passing this default configuration.

Add means, to load from a configuration database/files default configuration to override, currently hard coded, defaults.

E.g. #TIKA-640, there are no means to change the defaults, by means of configuration files, with out having to change, tika source.

- Sugestion

- Add a  ParserConfigurationFactory, to delegate configuration loading, to a database/property based file etc.
- Allow for configuration factory instance to be configured via file, like tika-config.xml for Parsers.

interface ParserConfigurationFactory {
/**
* Create or fille the given ParseContext with the default configuration for the parser class...
*/
    <P extends Parser> ParseContext getDefaultConfig(Class<P> class, ParseContext ctx);
}

Replace when creating a ParseContext, by 

ParseContext ctx = ParserConfigurationFactorySingleton.getSingleton().getDefaultConfig(...) ...

Filling the configuration, given a set of properties, could be done via a class <ParserName>ConfigBuilder that know how to build and set the configuration properties in specific objects (such as MimeConfig), related to 3rd party external libraries ...

Best regards,
mc",smartshark_2_2,1634,tika,"""[0.9984831213951111, 0.001516851712949574]"""
3011,231270,ODF tests are for the wrong package,"The ODF related tests still call the deprecated parser, not the new one, and are in the wrong package",smartshark_2_2,606,tika,"""[0.9941270351409912, 0.005872905254364014]"""
3012,231271,Parent task for integration of Any23 into Tika,This issue should act as parent for all issues relating to integration of Any23 in to Tika. A document should be maintained herewith which details the execution plan for the migration of code. ,smartshark_2_2,1074,tika,"""[0.9982314705848694, 0.0017685098573565483]"""
3013,231272,Update NetCDF .jar file on Maven Central,"I am working to update the NetCDFParser file.  When using the most-recent .jar file available from http://www.unidata.ucar.edu/ at the command line I receive a note about a depreciated API: 

javac -classpath ../../../../tika-core/target/tika-core-1.6-SNAPSHOT.jar:../../../../toolsUI-4.3.jar org/apache/tika/parser/netcdf/NetCDFParser.java

Note: org/apache/tika/parser/netcdf/NetCDFParser.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.

After updating the NetCDFParser file with non-deprecated methods (e.x. changing ""dimension.getName()"" to ""dimension.getFullName()"") however, I get failed unit tests in maven, which I assume is because the Maven Central Repo has the lapsed version of the .jar file needed for NetCDF files (
http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22edu.ucar%22%20AND%20a%3A%22netcdf%22) .

Can anyone provide insight into how I get the updated .jar file into the Maven Central Repository? Is there an alternative method to update Tika so I can run my unit tests in Maven?

",smartshark_2_2,1429,tika,"""[0.9984316229820251, 0.0015682978555560112]"""
3014,231273,jhighlight license concerns,"jhighlight jar is a Tika dependency.  The Lucene team discovered that, while it claims to be a CDDL/LGPL dual-license, some of its functionality is LGPL only:

{code}
Solr's contrib/extraction contains jhighlight-1.0.jar which declares itself as dual CDDL or LGPL license. However, some of its classes are distributed only under LGPL, e.g.

com.uwyn.jhighlight.highlighter.
  CppHighlighter.java
  GroovyHighlighter.java
  JavaHighlighter.java
  XmlHighlighter.java

I downloaded the sources from Maven (http://search.maven.org/remotecontent?filepath=com/uwyn/jhighlight/1.0/jhighlight-1.0-sources.jar) to confirm that, and also found this SVN repo: http://svn.rifers.org/jhighlight/tags/release-1.0, though the project's website seems to not exist anymore (https://jhighlight.dev.java.net/).

I didn't find any direct usage of it in our code, so I guess it's probably needed by a 3rd party dependency, such as Tika. Therefore if we e.g. omit it, things will compile, but may fail at runtime.
{code}

Is it possible to remove this dependency for future releases, or allow only optional inclusion of this package?  It is of concern to the ManifoldCF project because we distribute a binary package that includes Tika and its required dependencies, which currently includes jHighlight.
",smartshark_2_2,1474,tika,"""[0.9792513847351074, 0.020748678594827652]"""
3015,231274,Integrate MIT Information Extraction(MITIE) into Tika to perform Named Entity Recognition,"Hello folks, 

MIT Information Extraction provides support for free state-of-the-art information extraction that includes named entity recognition and binary relation detection.

They contain pre trained models and functions to train new models.
I propose that we can use their precompiled jar or maven central project and supply it at runtime to enable MITIE Named Entity Recognition in Tika.

",smartshark_2_2,2061,tika,"""[0.9982611536979675, 0.0017388277919963002]"""
3016,231275,TagSoup HTML parser is project EOL,"The TagSoup HTML parser is project EOL, and the last update was to create the 1.2.1 version (that Tika references) back in Aug 2011.
I cannot find any TagSoup forks that are still active but there are many alternative (and perhaps better if you believe the reviews and wikipedia comparisons) html parsers out there.
Perhaps the most active is already pulled in by Tika as a transitive dependency of edu.ucar:grib, and that is jsoup with over 1,000 usages and updates as recent as a few months ago:
https://mvnrepository.com/artifact/org.jsoup/jsoup
https://jsoup.org/
Requesting consideration of moving away from the long EOL'd TagSoup to an active and modern HTML parser like jsoup that is already a transitive Tika dependency.
",smartshark_2_2,2787,tika,"""[0.9947453737258911, 0.005254692863672972]"""
3017,231276,Identify and parse the Apple iBooks format,"With the release of iBooks Author 1.0, Apple have created a new eBook format very similar to ePub. Tika could be extended to identify and parse this new format, re-using the existing ePub code wherever possible.

I have created an initial patch, which I will attach to this issue.",smartshark_2_2,1323,tika,"""[0.9983294606208801, 0.0016705074813216925]"""
3018,231277,Tika 2.0 - Clean up tika-parsers pom dependencies and a few other things,"In 2.x, we currently specify some parser versions in both the sub parser module and the tika-parsers module.  Let's clean up the pom in tika-parsers (if possible) so that we don't have to worry about coordinating upgrades to e.g. POI across two poms.",smartshark_2_2,1939,tika,"""[0.9983267188072205, 0.0016733284574002028]"""
3019,231278,Ignore NPOIFS IOOBE in PPT attachments,"TIKA-2588 has us trying to parse more embedded streams as npoifs.  Some of these are throwing IOOBE in our regression set.  Rather than throw a runtime exception while trying to parse an embedded stream, let's treat this like any other embedded stream IOException.",smartshark_2_2,1706,tika,"""[0.9694191813468933, 0.030580827966332436]"""
3020,231279,Increase buffer size for meta tag sniffing,"Some web pages (such as makler.su, see attached) have lots of script data before the body of the HTML.

When this happens, the sniffing code fails to find the charset info in the meta tag, because it currently only sniffs the first 4K.

Bumping it to 8K would cover all of the cases that I (Ken) have seen during a test crawl.",smartshark_2_2,302,tika,"""[0.9897895455360413, 0.010210413485765457]"""
3021,231280,DOAP file to recognize Tika on projects.a.o,"It would be great to have Tika listed on projects.apache.org. To do so, we need to get DOAP, well to create a DOAP file that is. Instructions are here [1]. I'll commit a first cut shortly.

[1] http://projects.apache.org/create.html",smartshark_2_2,340,tika,"""[0.9983372688293457, 0.0016627246513962746]"""
3022,231281,Self-hosted documentation for the JAX-RS Server,"Currently, if you fire up the JAX-RS Tika Server, and go to the root of the server in a web browser, you get an empty page back. You have to know to head over to https://wiki.apache.org/tika/TikaJAXRS find out what the available URLs are

We should self-host some simple documentation on the server at the root of it, so that people can discover what it offers. Ideally, this should be largely auto-generated based on the endpoints, so that we don't risk missing things when we add new features

This will also allow us to potentially offer a sample running version of the server for people to discover Tika with",smartshark_2_2,1268,tika,"""[0.998557984828949, 0.0014420602237805724]"""
3023,231282,Add GoogleTranslate implementation of Translation API,"Add an implementation of the Translation API that uses the Google Translate v2 API and Apache CXF: 
https://www.googleapis.com/language/translate/v2

",smartshark_2_2,1604,tika,"""[0.998310923576355, 0.0016891267150640488]"""
3024,231283,Changes to RFC822Parser to support turning off strict parsing,"Currently in RFC822Parser if Apache-Mime4J fails while parsing any field, then parsing the whole document will fail. This causes problems on the Enron Corpus - see https://issues.apache.org/jira/browse/TIKA-657

RFC822Parser is configured from a MimeEntityConfig object. MimeEntityConfig contains an option for ""strict parsing"". Currently MailContentHandler only performs strict parsing, I.E. if a MimeException is encountered when processing any fields in MailContentHandler.field then processing the document fails. However, we may prefer not to have strict parsing I.E. continue even if processing one or more fields fails. This can be achieved by placing a try / catch block around the logic inside MailContentHandler.field(), and only rethrowing the error if strictParsing is enabled, otherwise we log the error.

I enclose a diff for RFC822Parser and MailContentHandler that does this. I have also made some other minor changes to MailContentHandler: there was some repeated code for handling To:, Cc: and Bcc: fields, so I have replaced that with a single private method, and rewritten stripOutFieldPrefix, to avoid manipulating the String using re-assignment. 
",smartshark_2_2,591,tika,"""[0.9683855772018433, 0.03161439299583435]"""
3025,231284,Need to support URL's for input resources.,"It would be extremely helpful to support URL's instead of just File's for input resources.  This would enable us to use class loaders to find resources, and in general support resources that are not available via the filesystem.

Patch coming...
",smartshark_2_2,18,tika,"""[0.9983007311820984, 0.0016993192257359624]"""
3026,231285,Print the supported Metadata models and their associated met keys in tika-app,"It would be really nice to know what metadata models Tika supports, and the associated met keys, in a way that's callable from the command line. In the spirit of what Nick did in TIKA-470, I'm going to add it as an option to tika-app's CLI interface.",smartshark_2_2,464,tika,"""[0.9983550906181335, 0.001644859672524035]"""
3027,231286,HTML with charset unicode handled as utf-16 instead utf-8,"HTML files are detected as utf-16 when meta content is set to ""unicode"".

{code:XML}
<meta http-equiv=""Content-Type"" content=""text/html; charset=""unicode"">
Â {code}
Â 
Shouldn't the default be utf-8?

The attached sample file is shown correctly in:
Chromium Version 55.0.2883.75
Firefox 50.1.0
IE 11


I am aware that there is no charset ""unicode"" (available character encodings: [http://www.iana.org/assignments/character-sets/character-sets.xhtml|http://www.iana.org/assignments/character-sets/character-sets.xhtml])

Unfortunately there are many wrong encodings used out there.

All unknown encodings should be validated or at least be set to default utf-8.


Regards 

Andreas",smartshark_2_2,1678,tika,"""[0.39455071091651917, 0.6054493188858032]"""
3028,231287,Investigate rare IllegalArgumentException in macro extraction,poi bug [60279|https://bz.apache.org/bugzilla/show_bug.cgi?id=60279],smartshark_2_2,2285,tika,"""[0.9557245969772339, 0.044275376945734024]"""
3029,231288,Additional XML type: application/x-xml,"The following MediaType is not yet supported by Tika (not as a Media Type or an Alias): application/x-xml


I am no Media-Type expert, but if someone here at Tika is, then I suggest looking into it and if he sees fit then add it to the Tika Registry.",smartshark_2_2,1044,tika,"""[0.9982859492301941, 0.0017139877891167998]"""
3030,231289,Provide a JAX-RS to detect only the mediatype,"Currently I can use the JAX-RS server to detect the mediatype using the meta endpoint. The problem I have with this is that I need to send the entire document to get all metadata. 

To detect the mediatype, only a few bytes are often necessary and so I'd like to only send, say 8K or so, to the server and let it tell me the mediatype.

In order to accomplish this, it would be good to modify the /meta endpoint to address the individual fields that might be returned:

/meta/mediatype
/meta/author
/meta/lastModified

The parts currently following the path could be turned into a query parameter, which I think is more appropriate anyways (also easier to manipulate with tools like jquery).

If sufficient data is not available, I'd just return with a BAD_REQUEST.

If this would be of interest to TIKA, I think I could possibly implement this.",smartshark_2_2,886,tika,"""[0.9984824061393738, 0.0015176581218838692]"""
3031,231290,Improve embedded file name extraction in PDFParser,"When we extract embedded files from PDFs, we are currently using the key in the PDEmbeddedFilesNameTreeNode as the file name that we store as the value of Metadata.RESOURCE_NAME_KEY in the embedded document's  metadata.

I think we should try to get the file name from PDComplexFileSpecification's getFilename() first.  If that is null, then we should fall back to the key value.",smartshark_2_2,1117,tika,"""[0.9983407258987427, 0.0016592629253864288]"""
3032,231291,Missing license headers,"RAT (http://code.google.com/p/arat/) points out the following files without a proper license header:

    src/main/java/org/apache/tika/metadata/package.html
    src/main/resources/tika-config.xml
    src/site/SITE-README.txt
    src/test/resources/log4j/log4j.properties

Note that RAT also flags some of our test documents, but AFAIK they don't need license headers.",smartshark_2_2,139,tika,"""[0.9973359704017639, 0.002663983264937997]"""
3033,231292,Improving accuracy of Tesseract parser for Serial Number and Part Number (Numeric) Extraction,"Tesseract OCR parser works well with images containing English text. However, there is possibility of improvement in case of alphanumeric and numeric content which require training Tesseract with the relevant cases in order to better extract content from images. Such a customization can be helpful in extraction of serial numbers from images of counterfeit electronics and other applications focussing on atypical textual content.",smartshark_2_2,2137,tika,"""[0.9982390403747559, 0.0017610001377761364]"""
3034,231293,Possible ConcurrentModificationException while accessing Metadata produced by ParsingReader,"Oracle PipedReader and PipedWriter classes have a bug that do not allow them to execute concurrently, because they notify each other only when the pipe is full or empty, and do not after a char is read or written to the pipe. So i modified ParsingReader to use modified versions of PipedReader and PipedWriter, similar to gnu versions of them, that work concurrently. However, sometimes and with certain files, i am getting the following error:

java.util.ConcurrentModificationException
                at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
                at java.util.HashMap$KeyIterator.next(Unknown Source)
                at java.util.AbstractCollection.toArray(Unknown Source)
                at org.apache.tika.metadata.Metadata.names(Metadata.java:146)

It is because the ParsingReader.ParsingTask thread is writing metadata while it is being read by the ParsingReader thread, with files containing metadata beyond its initial bytes. It will not occur with the current implementation, because java PipedReader and PipedWriter block each other, what is a performance bug that affect ParsingReader, but they could be fixed in a future java release. I think it would be a defensive approach to turn access to the private Metadata.metadata Map synchronized, what could avoid a possible future problem using ParsingReader.",smartshark_2_2,1353,tika,"""[0.06730502098798752, 0.9326949715614319]"""
3035,231294,Dealing with PDF documents from scanning programs,"Hey,

sorry I didn't post this to mailing list, I kinda didn't get the confirmation.

The issue is that often people don't even realize there is a difference in pdf documents (extracted from openoffice/ms office or pdf from a scanner software). And if Tika processes such a document, it detects pdf content type, but there are only images in there. I don't know how to deal with that. There should be a function that decides on the type of PDF document so that I can take it and use some OCR software for the PDF from scanner software.

If there is a way to do that, could please anybody explain how to do that ?",smartshark_2_2,1308,tika,"""[0.9576492309570312, 0.04235072806477547]"""
3036,231295,Mimetype entry for DITA,"Currently, we don't have mimetype entries for DITA. There is a provisional mimetype

According to http://docs.oasis-open.org/dita/v1.2/cs01/spec/non-normative/DITA-mime-type.html there is a provisional mimetype of ""application/dita+xml"" for DITA files

There are then three kinds of DITA file, which apparently all use the same mimetype:
 DITA Topic - .dita
 DITA Map - .ditamap
 DITA Conditional Processing Profile - .ditaval

DITA is XML based, so we should be able to do XML detection in addition to filename matching",smartshark_2_2,633,tika,"""[0.9946202039718628, 0.005379740614444017]"""
3037,231296,Merlin passes invalid OID to getExtensionValue,"From org.apache.ws.security.components.crypto.Merlin:

    public boolean validateCertPath(X509Certificate[] certs)
...
            while (cacertsAliases.hasMoreElements()) {
                String alias = (String) cacertsAliases.nextElement();
                X509Certificate cert = (X509Certificate) this.cacerts
                        .getCertificate(alias);
                TrustAnchor anchor = new TrustAnchor(cert, cert
                        .getExtensionValue(""NameConstraints""));
                set.add(anchor);
            }

            // Add certificates from the keystore
            Enumeration aliases = this.keystore.aliases();
            while (aliases.hasMoreElements()) {
                String alias = (String) aliases.nextElement();
                X509Certificate cert = (X509Certificate) this.keystore
                        .getCertificate(alias);
                TrustAnchor anchor = new TrustAnchor(cert, cert
                        .getExtensionValue(""NameConstraints""));
                set.add(anchor);
            }

From J2SE API docs:
http://java.sun.com/j2se/1.5.0/docs/api/java/security/cert/X509Extension.html#getExtensionValue(java.lang.String)

getExtensionValue(String oid) expects its parameter to be an OID (in this case, ""2.5.29.30"").  It appears that the default JCE provider simply returns null (indicating extension not present).  However, this behaviour is not always the case.  Notably, the Bouncy Castle JCE provider will throw the (unchecked) exception IllegalArgumentException if the argument does not appear to be an OID.  This will cause cert path validation to fail with an exception on any JVM configured to use such a JCE provider (whether or not name constraints are used on any certs in the chain to be validated).

In addition, when used with a JCE that does not exhibit this behaviour, the code will identify some invalid cert paths as valid.  i.e. if a cert in the path has a naming constraint and is used to sign a cert which the name constraints would disallow, the path will still be seen as valid.
",smartshark_2_2,629,wss4j,"""[0.08074403554201126, 0.9192559719085693]"""
3038,231297,Incorrect Symmetric Key Derivation Length validation,"
There is an error in validating Signature Key Lengths against policy when derived keys are used with the Symmetric binding. For the ""Basic256"" policy for example, symmetric key lengths must be 256 bits, but derived keys for signature purposes can be 192 bits. The problem is that the SignatureVerifier class in Santuario asks for a secret key using XMLSecurityConstants.Sym_Sig, and eventually AbstractInboundSecurityToken ends up creating a new AlgorithmSuiteSecurityEvent using this. We need some way to distinguish between the two cases for validating against the AlgorithmSuite policy.",smartshark_2_2,135,wss4j,"""[0.0773237943649292, 0.9226762652397156]"""
3039,231298,Cannot resolve WSS signature reference URI that points to AssertionID attribute of SAML 1.1 token,"When a SAML 1.1 token is referenced by an XML Signature reference URI in the SOAP message, WSS4J cannot find the assertion element.  It looks like {{WSSecurityUtil.findSAMLAssertionElementById}} doesn't remove the hash symbol ({{#}}) before searching.",smartshark_2_2,212,wss4j,"""[0.10686826705932617, 0.8931317925453186]"""
3040,231299,AES_128_GCM does not work for attachments,"When trying use stax APIs to encrypt and decrypt attachments using AES_GCM_128, it breaks with error saying 
Caused by: java.lang.IllegalStateException: Cipher not initialized
    at javax.crypto.Cipher.d(Unknown Source)
    at javax.crypto.Cipher.doFinal(Unknown Source)
    at javax.crypto.CipherInputStream.close(Unknown Source)

Please note that same code works for AES_128 with attachments.

Also, I looked into the code stax code, found that org.apache.xml.security.stax.impl.processor.input.AbstractDecryptInputProcessor.processEvent(InputProcessorChain, boolean) has 

final int ivLength = JCEAlgorithmMapper.getIVLengthFromURI(algorithmURI) / 8; 

Not sure if ivLength used for GCM could be the problem?",smartshark_2_2,268,wss4j,"""[0.12559200823307037, 0.8744080066680908]"""
3041,231300,Rampart failing to extract keyinfo from SAML assertion,"Validation of a SAML 1.1 and 2.0 token is failing due to an inability to extract KeyInfo from the Subject of a SAML assertion. The issue appears to be independant of using SAML 1.1/2.0 or symmetric/asymmetric bindings.

The extract of the SAML subject (symmetric binding) is as follows:

<saml:Subject>
			<saml:NameIdentifier Format=""urn:oasis:names:tc:SAML:2.0:nameid-format:kerberos"">DOMAIN\JOE</saml:NameIdentifier>
			<saml:SubjectConfirmation>
				<saml:ConfirmationMethod>urn:oasis:names:tc:SAML:1.0:cm:holder-of-key</saml:ConfirmationMethod>
				<KeyInfo xmlns=""http://www.w3.org/2000/09/xmldsig#"">
					<e:EncryptedKey xmlns:e=""http://www.w3.org/2001/04/xmlenc#"">
						<e:EncryptionMethod Algorithm=""http://www.w3.org/2001/04/xmlenc#rsa-oaep-mgf1p"">
							<DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
						</e:EncryptionMethod>
						<KeyInfo>
							<o:SecurityTokenReference xmlns:o=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd"">
								<X509Data>
									<X509IssuerSerial>
										<X509IssuerName>CN=Root Agency</X509IssuerName>
										<X509SerialNumber>-147027885241304943914470421251724308948</X509SerialNumber>
									</X509IssuerSerial>
								</X509Data>
							</o:SecurityTokenReference>
						</KeyInfo>
						<e:CipherData>
							<e:CipherValue>YPzUKhWVjM56ugTvgLF8nbCULmITwiE2lGFWf5rsRwm7v+g/J2cswNJoK5oBpROUXJRV3P10PRtWloXNU3eR8kZRn7nutFp5iEpRW4FHcoRMTK3KHYILz7EaBwYsNaGJ45PD6IeJvjGb79/5N9boePWZBMl708vsXp63fXNbUPo=</e:CipherValue>
						</e:CipherData>
					</e:EncryptedKey>
				</KeyInfo>
			</saml:SubjectConfirmation>
		</saml:Subject>

The extract of the SAML subject (asymmetric binding) is as follows:

<Subject>
			<NameID Format=""urn:oasis:names:tc:SAML:2.0:nameid-format:kerberos"">DOMAIN\JOE</NameID>
			<SubjectConfirmation Method=""urn:oasis:names:tc:SAML:2.0:cm:holder-of-key"">
				<SubjectConfirmationData xmlns:a=""http://www.w3.org/2001/XMLSchema-instance"" a:type=""KeyInfoConfirmationDataType"">
					<KeyInfo xmlns=""http://www.w3.org/2000/09/xmldsig#"">
						<KeyValue>
							<RSAKeyValue>
								<Modulus>qq45WmIc5AJQF8f06R8KMm9G4RsT4Vi9Xx5psYuyDzD1M7480CQ7dEmOLkYnOP/qwLNiKgvG/Xm2rGYf1fgQD+dH+jwoirACflwEGUk7nU88mZOeJwqfXq4oWdsGVAMREyJQnW2q5+KJQt6/Pt3UBWquTvJnwes9/0WrQUAUX9s=</Modulus>
								<Exponent>AQAB</Exponent>
							</RSAKeyValue>
						</KeyValue>
					</KeyInfo>
				</SubjectConfirmationData>
			</SubjectConfirmation>
		</Subject>

In both cases the SAMUtil.java (as part of WSSJ-1.5.8) fails to extract the certificate info. The following lines are an extract from the source code for which this is failing:

                    Element e = samlSubj.getKeyInfo();
                    X509Certificate[] certs = null;
                    try {
                        KeyInfo ki = new KeyInfo(e, null);

                        if (ki.containsX509Data()) {
                            X509Data data = ki.itemX509Data(0);
                            XMLX509Certificate certElem = null;


In both use cases the ki instance return false for ki.containsX509Data() and hence the processing fails.

In an attempt to see whether the KeyInfo constructor limited itself to certain types of certificate references, I manually modified the SAML assertion subject using TCPMon to include the X509 Base64 representation of the certificate as follows and found that this actually works, as a result of the ki.containsX509Data() statement returns true:

<saml:Subject>
			<saml:NameIdentifier Format=""urn:oasis:names:tc:SAML:2.0:nameid-format:kerberos"">DOMAIN\JOE</saml:NameIdentifier>
			<saml:SubjectConfirmation>
				<saml:ConfirmationMethod>urn:oasis:names:tc:SAML:1.0:cm:holder-of-key</saml:ConfirmationMethod>
				<KeyInfo xmlns=""http://www.w3.org/2000/09/xmldsig#"">
					<X509Data>
						<X509Certificate>MIIBsTCCAV+gAwIBAgIQKCrSOt8UsKxIAPFEMK316zAJBgUrDgMCHQUAMBYxFDASBgNVBAMTC1Jvb3QgQWdlbmN5MB4XDTA4MDgxMzE1MDkyM1oXDTM5MTIzMTIzNTk1OVowFDESMBAGA1UEAxMJbG9jYWxob3N0MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDEhqjNMe8M8CA0yGX1pUlJZ4r5WKvQfJ6k8DPBR/VGcvbWPBeiHRjBhH5I5kszg04on8M+FFg0RkW1cfmQRc8kf1XLudHdBUxJx3nfLXH2OscsxQkgcNfdvo5/GCewDIRHMxI+2TO9tgLP6SEJBdprO/55q8t4k/VW4Yi9u9/2VQIDAQABo0swSTBHBgNVHQEEQDA+gBAS5AktBh0dTwCNYSHcFmRjoRgwFjEUMBIGA1UEAxMLUm9vdCBBZ2VuY3mCEAY3bACqAGSKEc+41KpcNfQwCQYFKw4DAh0FAANBAEWsx9wociNjb8uidCy6WRHfS8qhIYjSbgGCYL8/bZ0Xoc8ETPOgZsnD0zSgLuWj7tlY0cHNtY4Nvu8tRo/U2ts=</X509Certificate>
					</X509Data>
				</KeyInfo>
			</saml:SubjectConfirmation>
		</saml:Subject>

Have previously submitted this an email request directly to rampart-dev but thought it best to formally raise the issue here as it's blocking any further use of rampart for this particular project.

Thanks,
Jason

",smartshark_2_2,520,wss4j,"""[0.08872248232364655, 0.9112775921821594]"""
3042,231301,SignatureProcessor does not provide correct signature coverage results with STR Dereference Transform,"SignatureProcessor does not report correct info when STR Dereference Transform is used.  The implementation does not follow the dereference pointer to the security token and reports that the signed content is the SecurityTokenReference itself and not the referenced token.  The URI in the signature part is dereferenced with no regard to the transform used in the signature part.

This issue makes it difficult to validate signature coverage over something like an embedded SAML assertion when that assertion is also used as the key material for the signature and is referenced and signed through a SecurityTokenReference.",smartshark_2_2,492,wss4j,"""[0.1595970243215561, 0.8404029607772827]"""
3043,231302,Inbound Processing code fails with an Encrypted Signature,"
The streaming inbound processing case fails on a message like the below (generated using the streaming code, with an Asymmetric Binding + sp:EncryptSignature):

BinarySecurityToken
EncryptedKey (ref BST above)
EncryptedData (encrypted Sig, ref EncryptedKey above)
BinarySecurityToken (referenced via the encrypted Signature)
(Encrypted/signed SOAP Body)

Error is: 

java.lang.IllegalStateException: javax.xml.stream.XMLStreamException: org.apache.wss4j.common.ext.WSSecurityException: Referenced Token ""G3618ed5a-e569-4fd6-af3b-6255353dd2b7"" not found
        at org.apache.xml.security.stax.impl.XMLSecurityStreamReader.getEventType(XMLSecurityStreamReader.java:386)

Can the Signature processing code not find a signing BST if it is after the Signature?
",smartshark_2_2,119,wss4j,"""[0.07984890788793564, 0.9201511144638062]"""
3044,231303,UsernameToken handles long strings badly,"While using WSS4J with CXF webservices I've found an issue with large strings in usernametoken fields. When username or password is very long (3500 chars and more) their content is trimmed - and don't even ask how I've found it :)

The problem is in class org.apache.ws.security.message.token.UsernameToken. There is a method ""private String nodeString(Element e)"" that takes an Element of token and returns its text representation.

When token property is very long XML parser emits more than one text node, but method gets text only from the first node - this is totally bad.

Possible solution is to replace this method with something like

private String nodeString(Element e) {
        if (e != null) {
                return e.getTextContent()
       } 
       return null;
}",smartshark_2_2,14,wss4j,"""[0.1060367077589035, 0.8939632773399353]"""
3045,231304,Problem when body is signed and then an XPath is encrypted,"Hi everybody,
there is a problem when when a message body is signed and then an XPath expression pointing to a body element is encrypted.
The problem is that the verification of the signature cannot pass. This is caused by the fact that there is a difference between the signed body and the body used for signature verification. The body used for signature verification is modified because after XPath element decryption an ID is added to the element. This ID is used to verify the decryption, but changes the original body. 

I am doing the tests with :

Rampart from the trunk with WSS4J 1.5.7.

Exception thrown is:

[WARN] Verification failed for URI ""#Id-11235685""
[WARN] Expected Digest: o0jyc1pJHEawRaLNry+cnYeCc80=
[WARN] Actual Digest: VMEF6KgvE6t3PNLlYR49LGEW+xM=
[ERROR] The signature or decryption was invalid
org.apache.axis2.AxisFault: The signature or decryption was invalid
	at org.apache.rampart.handler.RampartReceiver.setFaultCodeAndThrowAxisFault(RampartReceiver.java:172)
	at org.apache.rampart.handler.RampartReceiver.invoke(RampartReceiver.java:95)
	at org.apache.axis2.engine.Phase.invoke(Phase.java:317)
	at org.apache.axis2.engine.AxisEngine.invoke(AxisEngine.java:264)
	at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:163)
	at org.apache.axis2.transport.http.HTTPTransportUtils.processHTTPPostRequest(HTTPTransportUtils.java:275)
	at org.apache.axis2.transport.http.AxisServlet.doPost(AxisServlet.java:133)
	at com.mycompany.deployment.server.SAGAdminServlet.doPost(SAGAdminServlet.java:30)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:647)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:269)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:172)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:174)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:875)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:665)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:528)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:81)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:689)
	at java.lang.Thread.run(Thread.java:595)
Caused by: org.apache.ws.security.WSSecurityException: The signature or decryption was invalid
	at org.apache.ws.security.processor.SignatureProcessor.verifyXMLSignature(SignatureProcessor.java:527)
	at org.apache.ws.security.processor.SignatureProcessor.handleToken(SignatureProcessor.java:97)
	at org.apache.ws.security.WSSecurityEngine.processSecurityHeader(WSSecurityEngine.java:326)
	at org.apache.ws.security.WSSecurityEngine.processSecurityHeader(WSSecurityEngine.java:243)
	at org.apache.rampart.RampartEngine.process(RampartEngine.java:151)
	at org.apache.rampart.handler.RampartReceiver.invoke(RampartReceiver.java:92)
	... 22 more

I will try to apply a patch tomorrow.

Any comments and ideas are appreciated.

Regards,
Dobri

",smartshark_2_2,467,wss4j,"""[0.09379557520151138, 0.9062044024467468]"""
3046,231305,Cannot deploy WSS4J 1.6.8 to an OSGi container,"
An unintended consequence of WSS-410 is that you can no longer deploy WSS4J to an OSGi container. Santuario will not be loaded correctly as a provider in WSSConfig.",smartshark_2_2,37,wss4j,"""[0.2592977285385132, 0.7407022714614868]"""
3047,231306,UUIDGenerator generates duplicate identifiers when used in a multi-threaded environment,"The unique identifier generator used in wss4j generates duplicate identifiers in a multi-threaded environment.  The problem is because the getUUID() method is not synchronized, but internally modifies a number of variables (in particular the incrementingValue).  If multiple threads call this simultaneously then the same identifier can be returned.

This causes a problem in Axis where this is used for encrypted key token identifiers, so if multiple threads are processing messages simultaneously it is possible for two different keys to have the same identifier. These keys then get placed in the same token store which obviously causes a problem.

This is the same problem as previously reported in WSCOMMONS-201 with the UUIDGenerator in AXIOM (this class seems to have been originally copied from that one, but before the fix was applied).  The fix is to simply make the UUIDGenerator.getUUID() method synchronized.",smartshark_2_2,484,wss4j,"""[0.09310118108987808, 0.9068987965583801]"""
3048,231307,NullPointerException in WSSecEncrypt when encrypted header element has attributes,"If any header to be encrypted has an attribute that doesn't have an explicit namespace (which would include any unqualified attributes, which for me is almost all of them), WSSecEncrypt throws an NPE:

{code:title=Exception|borderStyle=solid}
org.apache.wss4j.common.ext.WSSecurityException: null
	at org.apache.wss4j.dom.message.WSSecEncrypt.createEncryptedHeaderElement(WSSecEncrypt.java:711)
	at org.apache.wss4j.dom.message.WSSecEncrypt.encryptElement(WSSecEncrypt.java:667)
	at org.apache.wss4j.dom.message.WSSecEncrypt.doEncryption(WSSecEncrypt.java:417)
	at org.apache.wss4j.dom.message.WSSecEncrypt.encryptForRef(WSSecEncrypt.java:255)
	at org.apache.wss4j.dom.message.WSSecEncrypt.encrypt(WSSecEncrypt.java:221)
	at org.apache.wss4j.dom.message.WSSecEncrypt.build(WSSecEncrypt.java:199)
	at org.apache.wss4j.dom.message.EncryptionPartsTest.testSOAPEncryptedHeaderWithAttributes(EncryptionPartsTest.java:321)
{code}

This is because Node.getNamespaceURI() returns null, and the code checks with:

{code:title=WSSecEncrypt.java Excerpt|borderStyle=solid}
            if (attr.getNamespaceURI().equals(WSConstants.URI_SOAP11_ENV)
                || attr.getNamespaceURI().equals(WSConstants.URI_SOAP12_ENV)) {                         
{code}

Solution is to switch the equals condition:

{code:title=WSSecEncrypt.java Fix|borderStyle=solid}
            if (WSConstants.URI_SOAP11_ENV.equals(attr.getNamespaceURI())
                || WSConstants.URI_SOAP12_ENV.equals(attr.getNamespaceURI())) {
{code}

I'm adding four patches:
- a test for code vs. version 2.0.6
- code fix vs. version 2.0.6
- a test for code vs. version 2.1.4
- a code fix vs. version 2.1.4",smartshark_2_2,261,wss4j,"""[0.06585104763507843, 0.9341489672660828]"""
3049,231308,Inbound streaming does not handle Symmetric Holder-Of-Key correctly,"
The streaming code has a problem when processing a request which contains a Holder-of-key SAML Assertion with a Subject which has an EncryptedKey in the KeyInfo, and a Signature in the security header which uses HMAC + points to the SAML Assertion.

The following code in SecurityTokenFactoryImpl:

if (keyInfoType != null) {
            final SecurityTokenReferenceType securityTokenReferenceType
                    = XMLSecurityUtils.getQNameType(keyInfoType.getContent(), WSSConstants.TAG_wsse_SecurityTokenReference);

... bypasses the EncryptedKey, and instead only returns a SecurityToken of the (encrypting) certificate. Instead it should detect that the immediate child of KeyInfo is an EncryptedKey + process this accordingly.",smartshark_2_2,166,wss4j,"""[0.07553355395793915, 0.9244663715362549]"""
3050,231309,Improper date check in SamlAssertionWrapper.checkIssueInstant,"On line 574, the code is supposed to be calculating the SAML Assertions expiration.  The code is calculating the lower bound on the time window, but is not properly storing the calculated DateTime.  So rather than checking the Issue, and is effectively checking to see if the issue date is after the current time, which is never the case.

The code reads:
   currentTime.minusSeconds(ttl);
The code should read:
   currentTime = currentTime.minusSeconds(ttl);
",smartshark_2_2,297,wss4j,"""[0.08899541944265366, 0.9110045433044434]"""
3051,231310,Some work on KeyIdentifiers,"
Enclosed is a patch of some work on KeyIdentifiers:

 - I added a test for encryption/decryption using ThumbprintSHA1
 - I added a test for encryption/decryption using EncryptedKeySHA1
 - I fixed a bug in WSSecEncryptedKey for the latter use-case.",smartshark_2_2,358,wss4j,"""[0.3247043192386627, 0.6752956509590149]"""
3052,231311,LDAP Certificate Store Support for WSS4J,The main focus of this isuue is to integrate LDAP certificate store support to current WSS4J implementation. ,smartshark_2_2,425,wss4j,"""[0.9980192184448242, 0.001980768283829093]"""
3053,231312,Allow encryption using a symmetric key and EncryptedKeySHA1,"
This task is to allow WSSecEncrypt to take in a SecretKey object, and use it to encrypt data, without generating an ephemeral key or encrypting the SecretKey into an EncryptedKey element. The SecretKey is added using EncryptedKeySHA1.",smartshark_2_2,444,wss4j,"""[0.9984922409057617, 0.0015078018186613917]"""
3054,231313,SignatureProcessor is not reusing results from BinarySecurityTokenProcessor or DerivedKeyTokenProcessor,"
SignatureProcessor is not reusing results from BinarySecurityTokenProcessor or DerivedKeyTokenProcessor, and is processing the elements again. This fix was already made on trunk, so backporting this to 1_5_x-fixes is all that's required.",smartshark_2_2,475,wss4j,"""[0.15966053307056427, 0.8403394818305969]"""
3055,231314,Add ability to specify a reference to an absolute URI in the derived key functionality,"
Currently, WSSecDKSign and WSSecDKEncrypt only allow references using a relative URI. This is problematic for the case of refering to a SecurityContextToken via the wsc:Identifier, which must be an absolute reference.",smartshark_2_2,479,wss4j,"""[0.9975312352180481, 0.0024688178673386574]"""
3056,231315,Maven unittest failed,"Many tests failed with the following messages (full error log is attached):


[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building WSS4J 1.6.6-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] >>> maven-pmd-plugin:2.5:check (validate) @ wss4j >>>
[INFO] 
[INFO] --- maven-pmd-plugin:2.5:pmd (pmd) @ wss4j ---
[INFO] 
[INFO] <<< maven-pmd-plugin:2.5:check (validate) @ wss4j <<<
[INFO] 
[INFO] --- maven-pmd-plugin:2.5:check (validate) @ wss4j ---
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.0:process (default) @ wss4j ---
[INFO] Setting property: classpath.resource.loader.class => 'org.codehaus.plexus.velocity.ContextClassLoaderResourceLoader'.
[INFO] Setting property: velocimacro.messages.on => 'false'.
[INFO] Setting property: resource.loader => 'classpath'.
[INFO] Setting property: resource.manager.logwhenfound => 'false'.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ wss4j ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] Copying 4 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ wss4j ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ wss4j ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] Copying 68 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ wss4j ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.6:test (default-test) @ wss4j ---
[INFO] Surefire report directory: d:\javapack\wss4j\target\surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.ws.security.message.ReplayTest
Tests run: 4, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 1.28 sec <<< FAILURE!
testReplayedTimestampBelowSignature(org.apache.ws.security.message.ReplayTest)  Time elapsed: 0.41 sec  <<< ERROR!
org.apache.ws.security.WSSecurityException: class org.apache.ws.security.components.crypto.Merlin cannot create instance
	at org.apache.ws.security.components.crypto.CryptoFactory.loadClass(CryptoFactory.java:224)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:117)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:169)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:161)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:60)
	at org.apache.ws.security.message.ReplayTest.<init>(ReplayTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:209)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:258)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:255)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:103)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.ws.security.components.crypto.CryptoFactory.loadClass(CryptoFactory.java:219)
	... 32 more
Caused by: org.apache.ws.security.components.crypto.CredentialException: Failed to load credentials.
	at org.apache.ws.security.components.crypto.Merlin.load(Merlin.java:371)
	at org.apache.ws.security.components.crypto.Merlin.loadProperties(Merlin.java:190)
	at org.apache.ws.security.components.crypto.Merlin.<init>(Merlin.java:140)
	... 37 more
Caused by: java.io.IOException: exception unwrapping private key - java.security.InvalidKeyException: Illegal key size
	at org.bouncycastle.jce.provider.JDKPKCS12KeyStore.unwrapKey(Unknown Source)
	at org.bouncycastle.jce.provider.JDKPKCS12KeyStore.engineLoad(Unknown Source)
	at java.security.KeyStore.load(KeyStore.java:1185)
	at org.apache.ws.security.components.crypto.Merlin.load(Merlin.java:365)
	... 39 more

testReplayedTimestampNoExpires(org.apache.ws.security.message.ReplayTest)  Time elapsed: 0 sec  <<< ERROR!
org.apache.ws.security.WSSecurityException: class org.apache.ws.security.components.crypto.Merlin cannot create instance
	at org.apache.ws.security.components.crypto.CryptoFactory.loadClass(CryptoFactory.java:224)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:117)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:169)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:161)
	at org.apache.ws.security.components.crypto.CryptoFactory.getInstance(CryptoFactory.java:60)
	at org.apache.ws.security.message.ReplayTest.<init>(ReplayTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:209)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:258)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:255)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:103)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.ws.security.components.crypto.CryptoFactory.loadClass(CryptoFactory.java:219)
	... 32 more
Caused by: org.apache.ws.security.components.crypto.CredentialException: Failed to load credentials.
	at org.apache.ws.security.components.crypto.Merlin.load(Merlin.java:371)
	at org.apache.ws.security.components.crypto.Merlin.loadProperties(Merlin.java:190)
	at org.apache.ws.security.components.crypto.Merlin.<init>(Merlin.java:140)
	... 37 more
Caused by: java.io.IOException: exception unwrapping private key - java.security.InvalidKeyException: Illegal key size
	at org.bouncycastle.jce.provider.JDKPKCS12KeyStore.unwrapKey(Unknown Source)
	at org.bouncycastle.jce.provider.JDKPKCS12KeyStore.engineLoad(Unknown Source)
	at java.security.KeyStore.load(KeyStore.java:1185)
	at org.apache.ws.security.components.crypto.Merlin.load(Merlin.java:365)
	... 39 more
",smartshark_2_2,10,wss4j,"""[0.9809516668319702, 0.019048333168029785]"""
3057,231316,Port Kerberos & SPNEGO work to streaming code,"
This task is to port Kerberos & SPNEGO functionality to the streaming code.",smartshark_2_2,93,wss4j,"""[0.9983357787132263, 0.0016642020782455802]"""
3058,231317,Interoperability b/w Java consumer of .NET Web Service with WS-Security on WSE 2.0 ,"There is an issue with using WSS4J for Java Consumer calling .NET 2.0 Web Service with WS-Security.
The default SecretKey size that is generated in WSS4J is 16 whereas .NET used 32

I changed the UsernameToken class to generate the secretkey using 32 as the length and then it worked.
This shoule be officially released in a version.
",smartshark_2_2,545,wss4j,"""[0.6416196823120117, 0.35838034749031067]"""
3059,231318,carriage return/line feed in the security header,"Hi All,

i have the problem that my soap message my not contain carriage return/line feed characters. 
So my question/wish is how to insert a  security header without such characters.


Thanks for any help.",smartshark_2_2,405,wss4j,"""[0.7946267127990723, 0.20537331700325012]"""
3060,231319,"Typo in the constant name for ""RSAOAEP""",There was a typo in the constant name for RSAOAEP.,smartshark_2_2,259,wss4j,"""[0.9953418970108032, 0.004658142104744911]"""
3061,231320,handleUsernameToken gives too much information. Can be used to deternine if a username exists or not,"I am using WSS4J with CXF to authenticate a soap connection. 

If digest password mode is used and the user name is known the password is set and WSS handles the calculation of the digest. Example:

	        if (pc.getIdentifer().equals(username))
	        {
	            /* 
	             * set the password on the callback. This will be compared to the
	             * password which was sent from the client.
	             */
	            pc.setPassword(password);
	        }

Now what should be done when the user name does not match any known user? If you do not set the passsword an exception will be thrown (see below) containing  something like : ""Callback supplied no password for: adminX"". 

The behavior of the authentication handler is different for a known user than for an unknown user. This makes it easy for an attacker to determine valid user names.

I saw a related message from 2005 (see http://www.mail-archive.com/fx-dev@ws.apache.org/msg00044.html) but it appears it has not been fixed.

  
Stack trace:

INFO: Interceptor has thrown exception, unwinding now
org.apache.cxf.binding.soap.SoapFault: General security error (WSSecurityEngine: Callback supplied no password for: adminm)
	at org.apache.cxf.ws.security.wss4j.WSS4JInInterceptor.createSoapFault(WSS4JInInterceptor.java:398)
	at org.apache.cxf.ws.security.wss4j.WSS4JInInterceptor.handleMessage(WSS4JInInterceptor.java:247)
	at org.apache.cxf.ws.security.wss4j.WSS4JInInterceptor.handleMessage(WSS4JInInterceptor.java:65)
	at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:220)
	at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:78)
	at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.serviceRequest(JettyHTTPDestination.java:278)
	at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:252)
	at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:726)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:206)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:842)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:648)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:395)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:450)
Caused by: org.apache.ws.security.WSSecurityException: General security error (WSSecurityEngine: Callback supplied no password for: adminm)
	at org.apache.ws.security.processor.UsernameTokenProcessor.handleUsernameToken(UsernameTokenProcessor.java:124)
	at org.apache.ws.security.processor.UsernameTokenProcessor.handleToken(UsernameTokenProcessor.java:53)
	at org.apache.ws.security.WSSecurityEngine.processSecurityHeader(WSSecurityEngine.java:311)
	at org.apache.ws.security.WSSecurityEngine.processSecurityHeader(WSSecurityEngine.java:228)
	at org.apache.cxf.ws.security.wss4j.WSS4JInInterceptor.handleMessage(WSS4JInInterceptor.java:153)
	... 17 more",smartshark_2_2,365,wss4j,"""[0.7601272463798523, 0.2398727685213089]"""
3062,231321,Improve the configurability of the SAML signature creation in AssertionWrapper,"The AssertionWrapper class is used in WSS4J to create a SAML Assertion
via a CallbackHandler. It has a method that can be used to sign the SAML
Assertion:

     public void signAssertion(
         String issuerKeyName,
         String issuerKeyPassword,
         Crypto issuerCrypto,
         boolean sendKeyValue
     )

The signature and canonicaliazation algorithms currently used are hard coded as showed by the following code snippet:

signature.setCanonicalizationAlgorithm(
             SignatureConstants.ALGO_ID_C14N_EXCL_OMIT_COMMENTS
         );

String sigAlgo = SignatureConstants.ALGO_ID_SIGNATURE_RSA_SHA1;
String pubKeyAlgo = issuerCerts[0].getPublicKey().getAlgorithm();

  if (pubKeyAlgo.equalsIgnoreCase(""DSA"")) {
         sigAlgo = SignatureConstants.ALGO_ID_SIGNATURE_DSA;
}

There should be a way for the user to be able to configure these algorithms

",smartshark_2_2,581,wss4j,"""[0.9984583854675293, 0.0015416251262649894]"""
3063,231322,WSConfig static initializer attempts to modify JCE Providers fail in JVM with restrictive security policies ,I am attempting to use WSS4j 2.0.2 to perform WS-Security x509 signatures on XML DOM objects in a restrictive Cloud based JVM environment. When I attempt to sign a document the org.apache.wss4j.dom.WSConfig class gets initialized and it has a static initializer to load several JCE libraries. Because the JVM SecurityManager has restrictive polices defined any attempts to read system properties or modify JCE providers are denied and the class fails to load. The org.apache.wss4j.common.crypto.WSProviderConfig class has a setAddJceProviders and I think the WSConfig class should honor that setting. ,smartshark_2_2,219,wss4j,"""[0.14911988377571106, 0.8508800864219666]"""
3064,231323,add more details to the error message when a signature verification failes,"In org.apache.ws.security.processor.SignatureProcessor.verifyXMLSignature() a WSSecurityException with the message ""The signature verification failed"" is thrown when the signature verification fails.

The error message should be enhanced with more specific info about the error, for example that one of the referenced digests didn't match.
See also http://marc.info/?l=xml-security-dev&m=112557776332544&w=2

",smartshark_2_2,428,wss4j,"""[0.9978058934211731, 0.0021940553560853004]"""
3065,231324,Implement equals/hashCode for the WSS4J policy model,There are some issues caused in the CXF policy layer by the fact that the WSS4J policy model does not implement equals/hashCode.,smartshark_2_2,299,wss4j,"""[0.9895141124725342, 0.010485825128853321]"""
3066,231325,Reconcile AssertionWrapper & SAMLAssertionWrapper,"
This task is to reconcile AssertionWrapper & SAMLAssertionWrapper (the two AssertionWrapper implementations in the DOM & streaming code) in a new common SAML module. It also involves moving the Crypto & SAMLUtil stuff out of SAMLAssertionWrapper.",smartshark_2_2,156,wss4j,"""[0.998478353023529, 0.0015216723550111055]"""
3067,231326,X509NameTokenizer.java contains Bouncy Castle JCE copyright code,"The Eclipse Foundation IP review rejected wss4j 1.5.latest for approval in its projects because of this file (found under
src\org\apache\ws\security\components\crypto) contains a comment:

/*
 * This source is a plain copy from bouncycastle software.
 * Thus:
 * Copyright (c) 2000 The Legion Of The Bouncy Castle
(http://www.bouncycastle.org)
 */

Apparently there are some legal issues with BC - they are being sued somewhere in Europe for inclusion of a patented algorithm and Eclipse Legal wants to stay away from anything BC. They noted the ripoff code comment and alarms started ringing. However that stops us of including WSS4J in an Eclipse project I am comitter of and makes things complicated for our users.

Besides all that, the X509Tokenizer included in wss4j is very simple and rudimentary and doesn't conform to RFC2253. In fact in X509 certs with more complex DNs it would give incorrect results. 

So in light of all this, and with the fact that Apache XML-Security 1.4.x already has a nice RFC2253 parser, can we replace the file in question with the version assigned to this email? It uses the XML-Security DN parser and just creates a wrapper with same WSS4J interface already implemented and consumed now. I copied 2 utility functions (trim() and countQuotes() from there locally and based the constructor on the RFC2253Parser normalize() method (same logic).
Instead of lazily evaluating the DN, I construct an ArrayList with to hold the tokenized OIDs).",smartshark_2_2,282,wss4j,"""[0.9721447825431824, 0.027855215594172478]"""
3068,231327,wss4j signed soap message call .net  wse2.0 web service does not work,"wss4j 1.5.4 java client sign soap message with x.509 cert, then consume .net wse2.0 web service, server throw a exception as ""Descryption or signature was invalid"". 
 In soap message , header ,timestamp and body are signed. I compare the signed message by wss4j and .net , only timestamp and ws-address order are diffeent. 
Others  are same. 

Here is my wss4j java code

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.FileWriter;

import javax.xml.namespace.QName;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
 
import org.apache.axis.client.Call;

import org.apache.axis.message.SOAPBodyElement;
import org.apache.axis.message.SOAPEnvelope;
import org.apache.axis.utils.XMLUtils;
import org.apache.ws.security.WSConstants;
import org.apache.ws.security.WSEncryptionPart;
import org.apache.ws.security.components.crypto.*;
import org.apache.ws.security.message.WSSecHeader;
import org.apache.ws.security.message.WSSecSignature;
import org.apache.ws.security.util.WSSecurityUtil;


import org.w3c.dom.Document;

//import org.perfsonar.client.base.requests.authService.AuthNEERequestGenerator;
import java.util.*;
 import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.SimpleTimeZone;
import javax.xml.soap.Name;
import javax.xml.soap.SOAPHeader;
import javax.xml.soap.SOAPPart;
import org.apache.axis.message.SOAPHeaderElement;
import org.apache.ws.security.message.WSSecTimestamp;
import org.apache.ws.security.SOAPConstants;
import org.apache.ws.security.util.Base64;


//import org.apache.commons.ssl.;

/**
 *  Class which provides the basic web service (Doc/Lit) client capability
 * 
 *
 *  @author Guo dong liang
 */
 
public class SOAPX509Client_home {
    //@SuppressWarnings(value={""deprecation""})
    String wsa=""http://schemas.xmlsoap.org/ws/2004/03/addressing/"";
    String wsu=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd"";
    String wsse=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd"";
    String alias=""f5897568954049c99a1d057df74e54e0_72b30743-1afb-418c-9272-1c13014cf103"";
   
    String propertyFile = ""D:/java/JavaApplication1/build/classes/crypto.property"";
    String actorName =""http://10.215.15.200:18001/MEDINET_BIZTALK/EMRX_WEBSERVICE/SERVICEINTERCHANGEENGINE2004S/RequestAdapter.asmx?wsdl"";// ""http://extranet.msb-uat.moh.gov.sg:16001/MEDINET_BIZTALK/EMRX_WEBSERVICE/SERVICEINTERCHANGEENGINE2004S/RequestAdapter.asmx?wsdl"";
    String actionName=""http://EMRX.Orchestration.ServiceInterchangeEngine/RequestAdapter/InitiateService"";
  
    String inputFile = ""D:/java/JavaApplication1/build/classes/Request.xml"";
    String outputFile = ""D:/java/JavaApplication1/build/classes/Response.xml"";
    String inputSOAP = ""D:/java/JavaApplication1/build/classes/inputSOAP.xml"";
    String beforeSignFile = ""D:/java/JavaApplication1/build/classes/beforeSign"";
            
                  
    public void makeRequest(String[] args) {
        try {
            
            SOAPBodyElement requestMessage = getSOAPBodyElement(inputFile);            
            SOAPEnvelope envelope = new SOAPEnvelope();
            envelope.addNamespaceDeclaration(""wsa"", wsa );
            envelope.addNamespaceDeclaration(""wsu"", wsu );
            envelope.addNamespaceDeclaration(""wsse"", wsse );
             envelope.addBodyElement(requestMessage);
      /**insert ws-addressing header element*****/
            insertHeaderElement(envelope);
     /*******adding to be  signed parts***/
         SOAPConstants soapConstants = WSSecurityUtil.getSOAPConstants(envelope.getAsDOM());
             // Set up to use InitiateService to sign the signature token
         Vector<WSEncryptionPart> parts = new Vector<WSEncryptionPart>();
           //sign body ,to ,action,timestamp, 4 parts
         String localPart =soapConstants.getBodyQName().getLocalPart();
         String envelopeURI =soapConstants.getEnvelopeURI();         
          WSEncryptionPart eAction = new WSEncryptionPart(""Action"", wsa, ""Content"");
          parts.add(eAction);
           WSEncryptionPart eMessageID = new WSEncryptionPart(""MessageID"", wsa, ""Content"");
          parts.add(eMessageID);
          
           WSEncryptionPart eReplyTo = new WSEncryptionPart(""ReplyTo"", wsa, ""Content"");
          parts.add(eReplyTo);
          
          WSEncryptionPart eTo = new WSEncryptionPart(""To"", wsa, ""Content"");
          parts.add(eTo);
          WSEncryptionPart body = new WSEncryptionPart(localPart, envelopeURI, ""Content"");
          parts.add(body);          

          /***************insert sec header***************/      
            
            java.util.Properties  property=new java.util.Properties();//(propertyFile);
            property.load(new FileInputStream(propertyFile));      
            Crypto crypto1=new Merlin(property) ;
         
       try{
          
            /***************sign soap message****************/

            WSSecSignature sec509 = new WSSecSignature();
            sec509.setUserInfo(alias, ""Password"");            
            sec509.setKeyIdentifierType(WSConstants.BST_DIRECT_REFERENCE);                
            Document doc1 = envelope.getAsDocument();
            
            //create security header
           WSSecHeader secHeader = new WSSecHeader();
           secHeader.insertSecurityHeader(doc1);
       
           /*
            Adds a new Timestamp to a soap envelope. 
           A complete Timestamp is constructed and added to the wsse:Security header. 
            * */
            WSSecTimestamp timestamp = new WSSecTimestamp();
            timestamp.setTimeToLive(600);
            timestamp.build(doc1, secHeader);
            sec509.setParts(parts);   
            parts.add(new WSEncryptionPart(timestamp.getId()));
            
             saveSOAPMessage(envelope.getAsDocument(), beforeSignFile);
             /*
              Builds a signed soap envelope. This is a convenience method and for backward compatibility. 
              The method creates a Signature and puts it into the Security header. 
              It does so by calling the single functions in order to perform a one shot signature
              */
             
             Document signedDoc = sec509.build(doc1, crypto1, secHeader);
             org.apache.axis.Message signedMsg = (org.apache.axis.Message) SOAPUtil.toAxisMessage(signedDoc);
             envelope = signedMsg.getSOAPEnvelope();   
             System.out.println(sec509.getId()+"":""+""""+"":""+sec509.getSignatureValue().length);
    
            // Saving SOAP message
            saveSOAPMessage(envelope.getAsDocument(), inputSOAP);
 
   /*****************Call soap web service*************************/       
       
   
            } catch (ClassCastException e) {
                e.printStackTrace();
                System.out.println(""SOAPX509Client.makeRequest: We didn't get a Vector of SOAPBodyElements!"");
            } 
         //   System.out.println(""time taken :""+((endTime-startTime)/1000d)+"" secs"");
        }catch(Exception e)
        {
            e.printStackTrace();
        }
 
    }
 
    private void saveSOAPMessage(Document doc, String file) {
        try {
            File response = new File(file+"".soap.xml"");
            FileWriter outWriter = new FileWriter(response);
           // XMLUtils.ElementToWriter(doc.getDocumentElement(),outWriter);
              XMLUtils.DocumentToWriter(doc, outWriter);
         //   XMLUtils.PrettyElementToWriter(doc.getDocumentElement(),outWriter);
            outWriter.close();
        } catch (Exception e) {
            System.out.println(""SOAPX509Client.saveSOAPMessage: General exception while writing SOAP message"");
            e.printStackTrace();
        }
    }
 private void insertHeaderElement(SOAPEnvelope envelope)
 {
     /*************************insert To, Action header element*************************************/
     try
     {
                SOAPHeader                aHeader        = envelope.getHeader();
                Name headerActionElement =envelope.createName(""Action"", ""wsa"",wsa);
		javax.xml.soap.SOAPHeaderElement action = aHeader.addHeaderElement(headerActionElement);
                action.addTextNode(""http://EMRX.Orchestration.ServiceInterchangeEngine/RequestAdapter/InitiateService"");
         
                Name headerMessageIDElement =envelope.createName(""MessageID"", ""wsa"",wsa);
		javax.xml.soap.SOAPHeaderElement messageID = aHeader.addHeaderElement(headerMessageIDElement);
                messageID.addTextNode(""uuid:ff494257-2921-47d4-9c02-f3c2722c8775"");
         
                Name headerReplyToElement =envelope.createName(""ReplyTo"", ""wsa"",wsa);
		javax.xml.soap.SOAPHeaderElement replyTo = aHeader.addHeaderElement(headerReplyToElement);                           
                javax.xml.soap.SOAPElement addressElement=replyTo.addChildElement(envelope.createName(""Address"", ""wsa"",wsa));
                addressElement.addTextNode(""http://schemas.xmlsoap.org/ws/2004/03/addressing/role/anonymous"");
                        
                Name headerToElement =envelope.createName(""To"", ""wsa"",wsa);
		javax.xml.soap.SOAPHeaderElement to = aHeader.addHeaderElement(headerToElement);		     
                to.addTextNode(""http://extranet.msb-uat.moh.gov.sg:16001/MEDINET_BIZTALK/EMRX_WEBSERVICE/SERVICEINTERCHANGEENGINE2004S/RequestAdapter.asmx"");
		      
		
              //  saveSOAPMessage(envelope.getAsDocument(), outputFile2);
     }catch(Exception e)
     {
     System.out.println(e.getMessage());
     }
        

         
        /*******************************************/
 }

     private String getTimestamp() 
    {
        //<aws:Timestamp>2008-02-10T23:59:59Z</aws:Timestamp>
        Calendar         aGregorian = Calendar.getInstance();
        SimpleTimeZone   aUTC       = new SimpleTimeZone(0, ""UTC"");
        SimpleDateFormat aISO8601   = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss'Z'"");
        
        aISO8601.setTimeZone(aUTC);
        return aISO8601.format(aGregorian.getTime());
    }

private SOAPBodyElement getSOAPBodyElement(String inputFile)
{
// read the request into a org.w3c.DOM.Document
    SOAPBodyElement requestMessage=null;
    try
    {
            Document request = null;
            DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
            factory.setNamespaceAware(true);
   
            DocumentBuilder builder = factory.newDocumentBuilder();
            request = builder.parse(new File(inputFile));
 
            // build a SOAPBodyElement from the document
            requestMessage = new SOAPBodyElement(request.getDocumentElement());
    }catch(Exception e)
    {}
        return requestMessage;

}
    public static void main(String[] args) {
        SOAPX509Client_home doclitClient = new SOAPX509Client_home();
        doclitClient.makeRequest(args);
 
    }
}


Wss4j signed soap message 

<?xml version=""1.0"" encoding=""UTF-8""?>
<soapenv:Envelope xmlns:soapenv=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:wsa=""http://schemas.xmlsoap.org/ws/2004/03/addressing/"" xmlns:wsse=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd"" xmlns:wsu=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><soapenv:Header><wsse:Security soapenv:mustUnderstand=""1""><wsse:BinarySecurityToken EncodingType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary"" ValueType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3"" wsu:Id=""CertId-12737140"">MIIEiDCCA3CgAwIBAgIEQH2PFzANBgkqhkiG9w0BAQUFADBXMQswCQYDVQQGEwJTRzEtMCsGA1UEChMkTmV0cnVzdCBUZXN0IENlcnRpZmljYXRlIEF1dGhvcml0eSAxMRkwFwYDVQQLExBOZXRydXN0IFRlc3QgQ0ExMB4XDTA4MDUyMTA3MzA0NloXDTEwMDUyMDE2MDAwMFowgbMxCzAJBgNVBAYTAlNHMS0wKwYDVQQKEyROZXRydXN0IFRlc3QgQ2VydGlmaWNhdGUgQXV0aG9yaXR5IDExJTAjBgNVBAsTHE5ldHJ1c3QgVGVzdCBDQTEgKENvcnBvcmF0ZSkxKzApBgNVBAsTIlBhcmt3YXkgSG9zcGl0YWwgU2luZ2Fwb3JlIFB0ZSBMdGQxITAfBgNVBAMTGFBhcmt3YXkgTGFiIFNlcnZpY2VzIDAwMTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAsHFBZeScvyEaAXkX+XodsTmvN0Wmq4pHl1dTLcwluMyO/1c4rGzRfO58mMEKMPf1c0PHfrlbKVyrSL/kNkyhihp9iNDt+v9WzXRt4C2d2TQDEzPvdXAXx6cQNCeHTqQ5wLFynMaQsAnJvpiRjF5yIA+jGurSaNQREM1g6LYN+K8CAwEAAaOCAYEwggF9MAsGA1UdDwQEAwIFoDArBgNVHRAEJDAigA8yMDA4MDUyMTA3MzA0NlqBDzIwMTAwNTIwMTYwMDAwWjAVBgNVHSAEDjAMMAoGCCqFPgCHag1mMBEGCWCGSAGG+EIBAQQEAwIGwDCBsAYDVR0fBIGoMIGlMG+gbaBrpGkwZzELMAkGA1UEBhMCU0cxLTArBgNVBAoTJE5ldHJ1c3QgVGVzdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkgMTEZMBcGA1UECxMQTmV0cnVzdCBUZXN0IENBMTEOMAwGA1UEAxMFQ1JMMTAwMqAwoC6GLGh0dHA6Ly90ZXN0Y29ubmVjdG9yLm5ldHJ1c3QubmV0L25ldHJ1c3QuY3JsMB8GA1UdIwQYMBaAFPeDjjNHRaMnlVr9oEBvALV1456BMB0GA1UdDgQWBBSN0OEUCH3vYuIWnQd+N7ABu6KomjAJBgNVHRMEAjAAMBkGCSqGSIb2fQdBAAQMMAobBFY3LjEDAgMoMA0GCSqGSIb3DQEBBQUAA4IBAQCQ79zRqGdDBeSuJx9z+GgnVw2JNSO0O0BNrmxQqcd3qHagFuk9QnNw9kpAqwxixPCvdwzLAiBjbdlN52cabyTdHKbk+YD0/t6y3/Gu2G5cLvSgiXsIdX1op+0zNsvT07XSLINoAuqWmBfSX0k5m0ajlQ2aQ63MaHLXxsKMt+eVzZcrpvH6Njr/lJmx9u+rUqgzOUu6OoHRG26TQ7P4WwEubJF1ctebM0zze4dPDKolnwbtUBVSvZSQI9bHXsBmxRngHM+S9DDLjNXt4lpwaWdHoRXp1ZRljUmkU9aHyW0GhM95wtsp2VMEe46uBDyyTHvlMrEh1A5rFrUt1F8p4Bv4</wsse:BinarySecurityToken><ds:Signature Id=""Signature-6301159"" xmlns:ds=""http://www.w3.org/2000/09/xmldsig#"">
<ds:SignedInfo>
<ds:CanonicalizationMethod Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
<ds:SignatureMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#rsa-sha1""/>
<ds:Reference URI=""#id-21471211"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>kBoY7azVy7lGu+mODDcGlpIzeIQ=</ds:DigestValue>
</ds:Reference>
<ds:Reference URI=""#id-21762307"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>ngT+UYRsOHX92EIS1/TXq8dgrIM=</ds:DigestValue>
</ds:Reference>
<ds:Reference URI=""#id-15799300"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>r4ZL95V4Apg84D+SrRBWUgOjftc=</ds:DigestValue>
</ds:Reference>
<ds:Reference URI=""#id-22618484"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>CzrAq/f0oSP8PBMkQ6cNe5/KkQw=</ds:DigestValue>
</ds:Reference>
<ds:Reference URI=""#id-10703525"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>JT1wkPEimT8L9tFWX/gQ17PxqoU=</ds:DigestValue>
</ds:Reference>
<ds:Reference URI=""#Timestamp-4729123"">
<ds:Transforms>
<ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#""/>
</ds:Transforms>
<ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""/>
<ds:DigestValue>0LcScoyqJxsxRoyfeodMJ1Aod/E=</ds:DigestValue>
</ds:Reference>
</ds:SignedInfo>
<ds:SignatureValue>
qDPg64mXTLFkctAKXRPdxyS1kVcdl+aYe3UeiJaNrcv1ifxh/Pa6R77tpfzoqo6uNuR88Uol8A/Z
fRJdD6HGAI4tQQBBYDQNR/KwJOvDJwEC8qgBqvvpNs2f2ocr8DishMOaUlNicrT4X8xpUuqr+S28
+WscFyafCkvHgNlvi4o=
</ds:SignatureValue>
<ds:KeyInfo Id=""KeyId-22743805"">
<wsse:SecurityTokenReference wsu:Id=""STRId-13665843""><wsse:Reference URI=""#CertId-12737140"" ValueType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3""/></wsse:SecurityTokenReference>
</ds:KeyInfo>
</ds:Signature><wsu:Timestamp wsu:Id=""Timestamp-4729123""><wsu:Created>2008-07-08T08:00:31.512Z</wsu:Created><wsu:Expires>2008-07-08T08:10:31.512Z</wsu:Expires></wsu:Timestamp></wsse:Security><wsa:Action soapenv:actor=""http://schemas.xmlsoap.org/soap/actor/next"" soapenv:mustUnderstand=""0"" wsu:Id=""id-21471211"">http://EMRX.Orchestration.ServiceInterchangeEngine/RequestAdapter/InitiateService</wsa:Action><wsa:MessageID soapenv:actor=""http://schemas.xmlsoap.org/soap/actor/next"" soapenv:mustUnderstand=""0"" wsu:Id=""id-21762307"">uuid:ff494257-2921-47d4-9c02-f3c2722c8775</wsa:MessageID><wsa:ReplyTo soapenv:actor=""http://schemas.xmlsoap.org/soap/actor/next"" soapenv:mustUnderstand=""0"" wsu:Id=""id-15799300""><wsa:Address>http://schemas.xmlsoap.org/ws/2004/03/addressing/role/anonymous</wsa:Address></wsa:ReplyTo><wsa:To soapenv:actor=""http://schemas.xmlsoap.org/soap/actor/next"" soapenv:mustUnderstand=""0"" wsu:Id=""id-22618484"">http://extranet.msb-uat.moh.gov.sg:16001/MEDINET_BIZTALK/EMRX_WEBSERVICE/SERVICEINTERCHANGEENGINE2004S/RequestAdapter.asmx</wsa:To></soapenv:Header><soapenv:Body wsu:Id=""id-10703525""><InitiateService xmlns=""http://EMRX.Orchestration.ServiceInterchangeEngine/"" xmlns:ns1=""http://EMRX.Orchestration.ServiceInterchangeEngine/"">
  <EMRXRequestResponseMessage xmlns=""http://EMRX.Orchestration.ServiceInterchangeEngine.Schemas.EMRXRequestResponseMessage"" xmlns:ns2=""http://EMRX.Orchestration.ServiceInterchangeEngine.Schemas.EMRXRequestResponseMessage"">
    <ServiceAuditID>61c545c8-98b2-435d-ac5a-6d5b2a2a3754</ServiceAuditID>
    <ServiceRequesterID>66</ServiceRequesterID>
    <ServiceProviderID>13</ServiceProviderID>
    <ServiceID>9D72C4AF-2ADD-4F1D-8E41-3BACFB7DB001</ServiceID>
    <ServiceVersion>1.0</ServiceVersion>
    <MessageContent>&lt;NewDataSet&gt;
  &lt;RequestInputs&gt;
    &lt;Patient_HRN_No&gt;602006000113Z&lt;/Patient_HRN_No&gt;
    &lt;Submission_Date&gt;3/26/2008 5:29:00 AM&lt;/Submission_Date&gt;
  &lt;/RequestInputs&gt;
&lt;/NewDataSet&gt;</MessageContent>
    <MessageTimeStamp>6/5/2008 1:54:46 PM</MessageTimeStamp>
  </EMRXRequestResponseMessage>
</InitiateService></soapenv:Body></soapenv:Envelope>

.Net signed message

<?xml version=""1.0"" encoding=""utf-8""?><soap:Envelope xmlns:soap=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:wsa=""http://schemas.xmlsoap.org/ws/2004/03/addressing"" xmlns:wsse=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd"" xmlns:wsu=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd""><soap:Header><wsa:Action wsu:Id=""Id-6d8ef8d5-12c7-4f91-b1cf-92e4aa2f187f"">http://EMRX.Orchestration.ServiceInterchangeEngine/RequestAdapter/InitiateService</wsa:Action><wsa:MessageID wsu:Id=""Id-d406cdbc-3ab5-4d8d-885f-8c2e7285c68e"">uuid:ff494257-2921-47d4-9c02-f3c2722c8775</wsa:MessageID><wsa:ReplyTo wsu:Id=""Id-d7f472e9-02d8-4849-ae66-5c0e066e986f""><wsa:Address>http://schemas.xmlsoap.org/ws/2004/03/addressing/role/anonymous</wsa:Address></wsa:ReplyTo><wsa:To wsu:Id=""Id-0f3adfc8-2a87-45d5-84cf-57dc629068ae"">http://extranet.msb-uat.moh.gov.sg:16001/MEDINET_BIZTALK/EMRX_WEBSERVICE/SERVICEINTERCHANGEENGINE2004S/RequestAdapter.asmx</wsa:To><wsse:Security soap:mustUnderstand=""1"">
  <wsu:Timestamp wsu:Id=""Timestamp-c086b981-3dcf-4c7d-a517-95b6ed3de4d8"">
  <wsu:Created>2008-06-05T05:54:48Z</wsu:Created><wsu:Expires>2008-06-05T05:55:48Z</wsu:Expires></wsu:Timestamp><wsse:BinarySecurityToken ValueType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3"" EncodingType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary"" xmlns:wsu=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd"" wsu:Id=""SecurityToken-bfc37ca9-67d9-4e6d-ac6a-8d22fe98212f"">MIIEiDCCA3CgAwIBAgIEQH2PFzANBgkqhkiG9w0BAQUFADBXMQswCQYDVQQGEwJTRzEtMCsGA1UEChMkTmV0cnVzdCBUZXN0IENlcnRpZmljYXRlIEF1dGhvcml0eSAxMRkwFwYDVQQLExBOZXRydXN0IFRlc3QgQ0ExMB4XDTA4MDUyMTA3MzA0NloXDTEwMDUyMDE2MDAwMFowgbMxCzAJBgNVBAYTAlNHMS0wKwYDVQQKEyROZXRydXN0IFRlc3QgQ2VydGlmaWNhdGUgQXV0aG9yaXR5IDExJTAjBgNVBAsTHE5ldHJ1c3QgVGVzdCBDQTEgKENvcnBvcmF0ZSkxKzApBgNVBAsTIlBhcmt3YXkgSG9zcGl0YWwgU2luZ2Fwb3JlIFB0ZSBMdGQxITAfBgNVBAMTGFBhcmt3YXkgTGFiIFNlcnZpY2VzIDAwMTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAsHFBZeScvyEaAXkX+XodsTmvN0Wmq4pHl1dTLcwluMyO/1c4rGzRfO58mMEKMPf1c0PHfrlbKVyrSL/kNkyhihp9iNDt+v9WzXRt4C2d2TQDEzPvdXAXx6cQNCeHTqQ5wLFynMaQsAnJvpiRjF5yIA+jGurSaNQREM1g6LYN+K8CAwEAAaOCAYEwggF9MAsGA1UdDwQEAwIFoDArBgNVHRAEJDAigA8yMDA4MDUyMTA3MzA0NlqBDzIwMTAwNTIwMTYwMDAwWjAVBgNVHSAEDjAMMAoGCCqFPgCHag1mMBEGCWCGSAGG+EIBAQQEAwIGwDCBsAYDVR0fBIGoMIGlMG+gbaBrpGkwZzELMAkGA1UEBhMCU0cxLTArBgNVBAoTJE5ldHJ1c3QgVGVzdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkgMTEZMBcGA1UECxMQTmV0cnVzdCBUZXN0IENBMTEOMAwGA1UEAxMFQ1JMMTAwMqAwoC6GLGh0dHA6Ly90ZXN0Y29ubmVjdG9yLm5ldHJ1c3QubmV0L25ldHJ1c3QuY3JsMB8GA1UdIwQYMBaAFPeDjjNHRaMnlVr9oEBvALV1456BMB0GA1UdDgQWBBSN0OEUCH3vYuIWnQd+N7ABu6KomjAJBgNVHRMEAjAAMBkGCSqGSIb2fQdBAAQMMAobBFY3LjEDAgMoMA0GCSqGSIb3DQEBBQUAA4IBAQCQ79zRqGdDBeSuJx9z+GgnVw2JNSO0O0BNrmxQqcd3qHagFuk9QnNw9kpAqwxixPCvdwzLAiBjbdlN52cabyTdHKbk+YD0/t6y3/Gu2G5cLvSgiXsIdX1op+0zNsvT07XSLINoAuqWmBfSX0k5m0ajlQ2aQ63MaHLXxsKMt+eVzZcrpvH6Njr/lJmx9u+rUqgzOUu6OoHRG26TQ7P4WwEubJF1ctebM0zze4dPDKolnwbtUBVSvZSQI9bHXsBmxRngHM+S9DDLjNXt4lpwaWdHoRXp1ZRljUmkU9aHyW0GhM95wtsp2VMEe46uBDyyTHvlMrEh1A5rFrUt1F8p4Bv4</wsse:BinarySecurityToken><Signature xmlns=""http://www.w3.org/2000/09/xmldsig#""><SignedInfo><ds:CanonicalizationMethod Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" xmlns:ds=""http://www.w3.org/2000/09/xmldsig#"" /><SignatureMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#rsa-sha1"" /><Reference URI=""#Id-6d8ef8d5-12c7-4f91-b1cf-92e4aa2f187f""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>6ytOjPFhvR4GLhgZ5QhlE/YVEwY=</DigestValue></Reference><Reference URI=""#Id-d406cdbc-3ab5-4d8d-885f-8c2e7285c68e""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>XSCf8X2PCqxiB1iWEIdLYY23XIc=</DigestValue></Reference><Reference URI=""#Id-d7f472e9-02d8-4849-ae66-5c0e066e986f""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>2r9qqOlhnh0ANzMBa725zeCt7HE=</DigestValue></Reference><Reference URI=""#Id-0f3adfc8-2a87-45d5-84cf-57dc629068ae""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>BX7yOtWm8u42Vxe/NDso/zCzSBo=</DigestValue></Reference><Reference URI=""#Timestamp-c086b981-3dcf-4c7d-a517-95b6ed3de4d8""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>GFAvCiexgSpjWycr0IqggxwaRIk=</DigestValue></Reference><Reference URI=""#Id-788d8809-d4f8-47b6-a4da-0de13456d463""><Transforms><Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"" /></Transforms><DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1"" /><DigestValue>Gau6dFd8HFLo9GZSYgiLQDPLJR4=</DigestValue></Reference></SignedInfo><SignatureValue>D9XbA0jM9gWfmial89aFwmy0UuM5/KYSzi27rRhAqEsmQUoB5L8mK+4RmLscWzcsMZoWsMkElLZgDMA6hQgaohf3qOt7NnpPsCZ2q/3iJoRjof2cdindB7K6WVXQ55B3TVfjZwMz5+BksXJBzzxDiw/9Fg+MOpqEhi5xQrwKy/o=</SignatureValue><KeyInfo><wsse:SecurityTokenReference><wsse:Reference URI=""#SecurityToken-bfc37ca9-67d9-4e6d-ac6a-8d22fe98212f"" ValueType=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3"" /></wsse:SecurityTokenReference></KeyInfo></Signature></wsse:Security></soap:Header><soap:Body wsu:Id=""Id-788d8809-d4f8-47b6-a4da-0de13456d463""><InitiateService xmlns=""http://EMRX.Orchestration.ServiceInterchangeEngine/""><EMRXRequestResponseMessage xmlns=""http://EMRX.Orchestration.ServiceInterchangeEngine.Schemas.EMRXRequestResponseMessage""><ServiceAuditID xmlns="""">c4cb50ba-b906-4804-bae6-91b83001fbc6</ServiceAuditID><ServiceRequesterID xmlns="""">66</ServiceRequesterID><ServiceProviderID xmlns="""">13</ServiceProviderID><ServiceID xmlns="""">9D72C4AF-2ADD-4F1D-8E41-3BACFB7DB001</ServiceID><ServiceVersion xmlns="""">1.0</ServiceVersion><MessageContent xmlns="""">&lt;NewDataSet&gt;
  &lt;RequestInputs&gt;
    &lt;Patient_HRN_No&gt;602006000113Z&lt;/Patient_HRN_No&gt;
    &lt;Submission_Date&gt;3/26/2008 5:29:00 AM&lt;/Submission_Date&gt;
  &lt;/RequestInputs&gt;
&lt;/NewDataSet&gt;</MessageContent><MessageTimeStamp xmlns="""">6/5/2008 1:54:46 PM</MessageTimeStamp></EMRXRequestResponseMessage></InitiateService></soap:Body></soap:Envelope>",smartshark_2_2,426,wss4j,"""[0.3510003387928009, 0.6489996910095215]"""
3069,231328,Improve outbound DOM element location,"
This task is to improve the way DOM Elements are located for outbound signature, encryption and STRTransforms. The current code does some highly inefficient stuff in relation to searching the DOM tree, sometimes multiple times. ",smartshark_2_2,518,wss4j,"""[0.9981918931007385, 0.0018080529989674687]"""
3070,231329,Record how a certificate was referenced for signature or encryption,"
Make it possible to see how a certificate was referenced for signature or encryption by storing an enum in the WSSecurityEngineResult.",smartshark_2_2,16,wss4j,"""[0.9985417127609253, 0.0014582963194698095]"""
3071,231330,AbstractCrypto's constructor does not allow null properties in WSS4J 1.5.2 and 1.5.3 for custom Crypto with no input stream source,"The ealier implemantation (before 1.5.2) looked like:

    public AbstractCrypto(Properties properties, ClassLoader loader) throws CredentialException, IOException {
        /*
        * if no properties .. just return an instance, the rest will be
        * done later or this instance is just used to handle certificate
        * conversions in this implementatio
        */
        if (properties == null) {
            return;
        }
        this.properties = properties;
        String location = this.properties.getProperty(""org.apache.ws.security.crypto.merlin.file"");

This allowed a custom Crypto class extending from AbstractCrypto to pass a null properties and do the rest in its own constructor.

The above implementation was changed in 1.5.2 so that it does not accept a null properties, as in

    public AbstractCrypto(Properties properties, ClassLoader loader) throws CredentialException, IOException {
        this.properties = properties;
        String location = this.properties.getProperty(""org.apache.ws.security.crypto.merlin.file"");

The rest of the code in this constructor assumes an input stream based keystore source and therefore it is not useful if your environment has only access to the keystore object but not its source stream.

Is there any strong reason to remove the original ""if (properties == null) { return; } "" block?
If not, could you reinsert that block?

Thank you.
Aki Yoshida


",smartshark_2_2,373,wss4j,"""[0.5902045369148254, 0.4097954034805298]"""
3072,231331,Move certificate validation our of WSHandler and into SignatureProcessor,"
Move certificate validation our of WSHandler and into SignatureProcessor. Trust should be verified on the certificates before signature verification takes place. This also removes the onus of verifying trust from the handlers.",smartshark_2_2,456,wss4j,"""[0.9984179735183716, 0.0015820362605154514]"""
3073,231332,No way to create a UsernameToken with absent <Password> element,We should be able to create UsernameTokens without <Password> in them if needed. Password is an optional element,smartshark_2_2,48,wss4j,"""[0.9898752570152283, 0.010124722495675087]"""
3074,231333,Add the ability to specify a MGF-SHA algorithm in the policy AlgorithmSuiteType,"
Add the ability to specify a MGF-SHA algorithm in the policy AlgorithmSuiteType.",smartshark_2_2,245,wss4j,"""[0.9984179735183716, 0.0015820662956684828]"""
3075,231334,Specifying alternate cacerts keystore via properties?,"I'm wondering if it would be possible for the Crypto classes to be able to use an alternate cacerts file? As I use Glassfish for my application, it would be nice for me to be able to specify Glassfish's cacerts keystore as the one to use instead of the default Java one, for both certificate generation and certificate validation. Currently, AbstractCrypto has it essentially hard-coded as $JAVA_HOME/lib/security/cacerts, which isn't ideal.

In an ideal world, this would also apply to WSHandler.verifyTrust (and so on).

Is this a feasible idea? I'm not an expert in these things (at all. Not even close), so I'm not even sure if I should be using the Glassfish keystore for things other than the SSL key/cert. The answer is, I think, that I have to, regardless of whether or not it's a bad idea, as we don't necessarily have full control over the machines we use, and modifying the system-wide keystore could land me in a lot of trouble.

Thanks all,

- Andrew",smartshark_2_2,455,wss4j,"""[0.9984330534934998, 0.0015669356798753142]"""
3076,231335,SignatureProcessor is not reusing results from WSDocInfo for the Reference case.,"
The SignatureProcessor is not reusing results from WSDocInfo for the Reference case, but is manually finding the Reference element. This fix should speed things up a bit...",smartshark_2_2,476,wss4j,"""[0.9729588627815247, 0.027041148394346237]"""
3077,231336,Reconcile SAMLCallback between the two implementations,"
The SAMLCallback in SWSSF contains signing information, whereas it does not in WSS4J. SAMLParms and SAMLIssuer* will be removed in the DOM code as part of this work.",smartshark_2_2,87,wss4j,"""[0.8669460415840149, 0.1330539435148239]"""
3078,231337,Replace all Vector references with Lists.,"
Replace all Vector references with Lists. Some of this work has been done already.",smartshark_2_2,454,wss4j,"""[0.9981493949890137, 0.0018506530905142426]"""
3079,231338,assure compatibility with the change in ehcache CacheManager's create method from version 2.5.1 to 2.5.2 and up,"ehcache CacheManager's create method previously instantiated a new instance but from version 2.5.2, it simply returns a singleton, where a new method is introduced to create a new instance. Thus, the correct method needs to be invoked while instantiating a cache manager.

a patch file attached.",smartshark_2_2,139,wss4j,"""[0.972571849822998, 0.02742820978164673]"""
3080,231339,Support Kerberos token policy validation.,"
Support Kerberos token policy validation.",smartshark_2_2,94,wss4j,"""[0.998018741607666, 0.001981257926672697]"""
3081,231340,Refactor Derived Key / SecureConveration functionality to be used by both implementations,"
Refactor Derived Key / SecureConveration functionality to be used by both implementations.",smartshark_2_2,116,wss4j,"""[0.9981841444969177, 0.001815799274481833]"""
3082,231341,WS-Trust 1.3 namespace not supported when looking for a BinarySecret in a SAML KeyInfo,"
The WS-Trust 1.3 namespace not supported when looking for a BinarySecret in a SAML KeyInfo. See this for an example:

http://cxf.547215.n5.nabble.com/Re-General-security-error-Provided-SAML-token-does-not-contain-a-suitable-key-td4990489.html",smartshark_2_2,650,wss4j,"""[0.3033372163772583, 0.6966627836227417]"""
3083,231342,UsernameTokenSignedAction support for PasswordDigest option,"At now UsernameTokenSignedAction has not support for digested password, this is beacause of the password type is hardcoded inside the class code.
This kind of action is used for .NET web services interoperability, so it doesn't suffice to make the passwordoption configurable.
Microsft WSE ( I tested it with version 2.0) use  the original password (the one returned from the passwordcallback) to create the secretkey while wss4j use the digegested password, so you need to make the original password avaiable to the method getsecretkey of the Usernametoken class.",smartshark_2_2,355,wss4j,"""[0.9811443090438843, 0.01885577104985714]"""
3084,231343,Fix for minor checkstyle issues,"
The attached patch is a start on fixing the checkstyle issues with WSS4J. This patch fixes the following problems:

 - Unused imports are removed
 - Variables that aren't read are removed",smartshark_2_2,369,wss4j,"""[0.9981135129928589, 0.0018865555757656693]"""
3085,231344,Support creating SAML 2.0 Tokens with the AuthnStatement SessionNotOnOrAfter attribute. ,"
Currently there is no way to specify a SessionNotOnOrAfter attribute for a SAML 2.0 AuthenticationStatement.",smartshark_2_2,190,wss4j,"""[0.9982917904853821, 0.0017081575933843851]"""
3086,231345,Option for checking EncryptedData elements are covered by signature,This feature request is about adding a mechanism for enforcing the engine on checking each referenced EncryptedData element is covered by a signature.,smartshark_2_2,8,wss4j,"""[0.9983545541763306, 0.0016454682918265462]"""
3087,231346,Add WSSE and WSU xmlns definitions to signature's SecurityTokenReference,"Hello,

when <ds:Signature> is created with WSS4J, it contains <wsse:SecurityTokenReference> within it which uses *wsse* and *wsu* namespaces. Those namespaces are defined ""above"" <ds:Signature> tag in the XML document so <ds:Signature> does not validate as standalone fragment. For example:

{code:xml}
<ds:Signature Id=""SIG-3E9A9AB1F5821FE8E81429475914581153"" xmlns:ds=""http://www.w3.org/2000/09/xmldsig#"">
  <ds:SignedInfo>
    <ds:CanonicalizationMethod Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
      <ec:InclusiveNamespaces PrefixList=""wsa soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
    </ds:CanonicalizationMethod>
    <ds:SignatureMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#rsa-sha1""></ds:SignatureMethod>
    <ds:Reference URI=""#id-3E9A9AB1F5821FE8E81429475914580148"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>n1FO7gH3mlf7xwN9NV7BtdhqqNM=</ds:DigestValue>
    </ds:Reference>
    <ds:Reference URI=""#TS-3E9A9AB1F5821FE8E81429475914579144"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""wsse wsa soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>8IPio9C93C+IYpVOtFUX+Ig6eFQ=</ds:DigestValue>
    </ds:Reference>
    <ds:Reference URI=""#id-3E9A9AB1F5821FE8E81429475914581149"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>T5t9Lg+/6tnL3XMUqi/XBa2RPgs=</ds:DigestValue>
    </ds:Reference>
    <ds:Reference URI=""#id-3E9A9AB1F5821FE8E81429475914581150"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>dNjOA0ZosOLeB7R1YnBWvW5RoWI=</ds:DigestValue>
    </ds:Reference>
    <ds:Reference URI=""#id-3E9A9AB1F5821FE8E81429475914581151"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>LqsYd2ZbZG39gMytaAfebfw0Jpc=</ds:DigestValue>
    </ds:Reference>
    <ds:Reference URI=""#id-3E9A9AB1F5821FE8E81429475914581152"">
      <ds:Transforms>
        <ds:Transform Algorithm=""http://www.w3.org/2001/10/xml-exc-c14n#"">
          <ec:InclusiveNamespaces PrefixList=""soapenv urn"" xmlns:ec=""http://www.w3.org/2001/10/xml-exc-c14n#""></ec:InclusiveNamespaces>
        </ds:Transform>
      </ds:Transforms>
      <ds:DigestMethod Algorithm=""http://www.w3.org/2000/09/xmldsig#sha1""></ds:DigestMethod>
      <ds:DigestValue>KBXU/UkCBosBKxaP+pPv7qFfLmw=</ds:DigestValue>
    </ds:Reference>
  </ds:SignedInfo>
  <ds:SignatureValue>CKwqqOizXZUS21GUbOK0U87u2XL+OBLj9Sfy4GaRmovCGuj8Wfm855oxbzHNaBw2rl9cFzEIUp5Pz6PKglE/KFc9E9TtKqp8aRPcRjcUvsbBZk9ntfKeJtYDF30Vsfcr6NFahCg+I2N61Mv5B622LLc7UnM8xlrUVgcBLHJwAcbX6GcQCm9hwRhO2f8n/HgHzdWW7KFw9sUQdGRyzm+k7Vhz/A6FxyqpECwIt9FWjTCaAQMo8/jS899y05UkFEFzMZy8Y6z1aODOR1W4QBp5D3+kMrG2bZHgi6UsBlCOgCH5EjolhD5grkM7wfvDbsWBw+41eswdY+at8tBhYvUFog==</ds:SignatureValue>
  <ds:KeyInfo Id=""KI-3E9A9AB1F5821FE8E81429475914580146"">
    <wsse:SecurityTokenReference wsu:Id=""STR-3E9A9AB1F5821FE8E81429475914580147"">
      <ds:X509Data>
        <ds:X509IssuerSerial>
          <ds:X509IssuerName>CN=CERT,OU=Development,O=Org,L=City,ST=State,C=US</ds:X509IssuerName>
          <ds:X509SerialNumber>13887123756357751743</ds:X509SerialNumber>
        </ds:X509IssuerSerial>
      </ds:X509Data>
    </wsse:SecurityTokenReference>
  </ds:KeyInfo>
</ds:Signature>
{code}

This is generally fine. However, when <ds:Signature> is encrypted, some other platforms (for example, some versions of .NET) have trouble validating decrypted <ds:Signature> since they cannot resolve *wsse* and *wsu* namespaces (as they are not in the decrypted fragment). I suppose, they should put decrypted <ds:Signature> back to the context of the rest of XML but this does not happen.

I think it would be a good idea to add definitions of wsse and wsu namespaces to the <wsse:SecurityTokenReference> in order to improve compatibility with WSS implementations from other vendors. Or at least make this behaviour configurable.

The following patch always adds *wsse* and *wsu* definitions:

{code}
diff --git a/ws-security-dom/src/main/java/org/apache/wss4j/dom/message/WSSecSignature.java b/ws-security-dom/src/main/java/org/apache/wss4j/dom/message/WSSecSignature.java
index 0258f0c..35bd3ba 100644
--- a/ws-security-dom/src/main/java/org/apache/wss4j/dom/message/WSSecSignature.java
+++ b/ws-security-dom/src/main/java/org/apache/wss4j/dom/message/WSSecSignature.java
@@ -181,6 +181,8 @@ public class WSSecSignature extends WSSecSignatureBase {
         if (!useCustomSecRef) {
             secRef = new SecurityTokenReference(doc);
             strUri = getWsConfig().getIdAllocator().createSecureId(""STR-"", secRef);
+            secRef.addWSSENamespace();
+            secRef.addWSUNamespace();
             secRef.setID(strUri);
             
             //
{code}

Then:

{code:xml}
....
<wsse:SecurityTokenReference wsu:Id=""STR-906b1964-8e27-40a5-a2ed-7f4ac9dabd69"" xmlns:wsse=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd"" xmlns:wsu=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd"">
      <ds:X509Data>
        <ds:X509IssuerSerial>
<ds:X509IssuerName>CN=CERT,OU=Development,O=Org,L=City,ST=State,C=US</ds:X509IssuerName>          <ds:X509SerialNumber>13887123756357751743</ds:X509SerialNumber>
        </ds:X509IssuerSerial>
      </ds:X509Data>

        </ds:X509IssuerSerial>
      </ds:X509Data>
    </wsse:SecurityTokenReference>
...
{code}

As far as I can tell, the same problem is present in earlier versions (1.6) as well.",smartshark_2_2,229,wss4j,"""[0.9014054536819458, 0.09859451651573181]"""
3088,231347,Impossible to sign BST since 1.6.x,"The org.apache.ws.security.message.WSSecSignature::build method has been changed to first compute the signature and afterwards prepend the BST to the security header. If the requirement is to sign the BST, the computeSignature doesn't find the BST.

	Vector<WSEncryptionPart> parts = new Vector<WSEncryptionPart>();
	parts.add(new WSEncryptionPart(WSConstants.BINARY_TOKEN_LN, WSConstants.WSSE_NS, ""Element""));
	signature.setParts(parts);

In 1.5.x there was also a wildcard ""Token"" to sign the security token, but this is not supported anymore in 1.6.x.

	Vector<WSEncryptionPart> parts = new Vector<WSEncryptionPart>();
	parts.add(new WSEncryptionPart(""Token"", null, ""Element""));
	signature.setParts(parts);

",smartshark_2_2,207,wss4j,"""[0.16406846046447754, 0.8359315395355225]"""
3089,231348,Refactor Exception functionality to be used by both implementations,"
Refactor Exception functionality to be used by both implementations.",smartshark_2_2,151,wss4j,"""[0.9981253743171692, 0.0018746155546978116]"""
